## Project Popcorn Synthetic Data Generation

This is repo synthetically generate PyTorch programs.

The PyTorch programs must meet the following specifications
* File name must match the `nn.Module` name, aka our entry point. For example, `SynthModel_ConvTranspose2d_LPPool3d_Bilinear_Sum.py` will have `class SynthModel_ConvTranspose2d_LPPool3d_Bilinear_Sum(nn.Module)` as the PyTorch architecture class name.
* must have `get_inputs` and `get_init_inputs` with shapes defined inside the functions


With these synthetic PyTorch programs, we then feed into the Parity Bench infrastructure to get `<pytorch, triton>` pairs, where the triton code is generated by Torch Inductor. This forms the basis for our dataset.

### Requirements

```
uv pip install -r requirements.txt
```
Put your API key in `.env`.

Make sure you dedicate a path of where your synthetic programs will be living at.

## Workflow
High Level workflow:
1. We have a list of PyTorch operators which we defined in `operators.py`
2. We sample combinations of operators from this list
3. We ask a Language Model (API) to create a PyTorch program with the desired operators (any order / combination)
4. We verify if the program is correct and can be used, aka it could be ran in PyTorch Eager and with `torch.compile`
5. We save that as a valid Synthetic program.

**Important**: we filter out programs that are similar to [KernelBench](https://github.com/ScalingIntelligence/KernelBench) level 2 as we don't want to contaminate our test set.

## Usage
Try genereate one!
```
python3 generate_synth_torch.py .single_debug
```

Start generate a ton of them!
```
python3 generate_synth_torch.py .parallel  num_total_samples=5000
```
make sure you check the generation file path and API configs


### Increasing yield / diversity / vary configurations
**Yield** is defined by the percentage of problems LM generated was able to pass our criteria and be used as a valid PyTorch synthetic programs.
We will show live yield as we are generating, so you can update your config.

This will be determined by your language model quality and your sampling strategy.

You can change the range of number of operators to sample from

You can also tune hyper parameter `p_value`. A lower p_value will generate more operators per program, leading to more unique but potentially error-prone programs. A higher p_value generates fewer operators, resulting in simpler programs with higher success rates. 
It's recommended to do a sweep or combination of `p_value` to ensure program diversity.

Here is a example config
```
python3 generate_synth_torch.py .parallel model_name=local num_total_samples=2000 num_worker=500 num_core_ops_range=[1,5] num_compound_ops_range=[0,4] num_supporting_ops_range=[2,10] p_value=0.25"

```

### Analysis and Dedup
To come
* fail operators (low yield)
* diveristy
* dedup



## DEPRECATED [Sahan]
### Running the code

```bash
# clean out the inductor cache
rm -rf /tmp/torchinductor_<username>/
rm generated/*

export OPENAI_API_KEY=<openai api key>

python generate_code_random_torch.py --num_files <some number we used 3000>
python clean.py --input_dir generated
python create_dataset.py --gen_dir_path generated --uuid_file filtered_uuids.json
# you should now have a dataset in the format of dataset.json
```

### Todo:
- [ ] Put clean.py into generate_code.py
