[
    {
        "uuid": "f5981454-ca3b-4b58-9446-0c5be11af4fa",
        "generated_code": "\n# This is a random torch model generated by the following modules: ['ReLU', 'Linear', 'Conv2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RandomModel(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Input channels 3, output channels 16\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # Input channels 16, output channels 32\n        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Assuming input is downsampled to 8x8 after convolutions\n        self.fc2 = nn.Linear(128, 10)  # Final output size of 10\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)  # First ReLU after first convolution\n        x = F.max_pool2d(x, kernel_size=2, stride=2)  # Downsample\n\n        x = self.conv2(x)\n        x = F.relu(x)  # Second ReLU after second convolution\n        x = F.max_pool2d(x, kernel_size=2, stride=2)  # Downsample\n\n        x = x.view(x.size(0), -1)  # Flatten for the linear layer\n        x = self.fc1(x)\n        x = F.relu(x)  # Third ReLU after first linear layer\n        x = self.fc2(x)\n        return x  # No softmax here for raw logits\n\n\ndef get_random_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32)  # Random input tensor of shape (1, 3, 32, 32)\n    return [x]\n\n",
        "triton_code": "# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import (\n    grid,\n    split_scan_grid,\n    grid_combo_kernels,\n    start_graph,\n    end_graph,\n    cooperative_reduction_grid,\n)\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\n\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/torchinductor_sahanp/n3/cn3szxx6ut4mvtifmbzhdzffbpwthmfhqify457bjkd6murezytz.py\n# Topologically Sorted Source Nodes: [x, x_1], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   x => convolution\n#   x_1 => relu\n# Graph fragment:\n#   %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%primals_3, %primals_1, %primals_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 1024\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/qo/cqocbfll3ohawfthnb2m67trzdellkg64rfz5p4it3avdcwl2qxo.py\n# Topologically Sorted Source Nodes: [x_2], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x_2 => getitem, getitem_1\n# Graph fragment:\n#   %getitem : [num_users=2] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 0), kwargs = {})\n#   %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_1(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 16)\n    x1 = xindex // 16\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (32 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr0 + (33 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp6 = triton_helpers.maximum(tmp5, tmp4)\n    tmp7 = tmp1 > tmp0\n    tmp8 = tl.full([1], 1, tl.int8)\n    tmp9 = tl.full([1], 0, tl.int8)\n    tmp10 = tl.where(tmp7, tmp8, tmp9)\n    tmp11 = tmp3 > tmp2\n    tmp12 = tl.full([1], 2, tl.int8)\n    tmp13 = tl.where(tmp11, tmp12, tmp10)\n    tmp14 = tmp5 > tmp4\n    tmp15 = tl.full([1], 3, tl.int8)\n    tmp16 = tl.where(tmp14, tmp15, tmp13)\n    tl.store(out_ptr0 + (x2), tmp6, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/gn/cgngs6id6vx4yxtb2m6qy7har3facca3en4olf3cimlcjp2h42uy.py\n# Topologically Sorted Source Nodes: [x_3, x_4], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   x_3 => convolution_1\n#   x_4 => relu_1\n# Graph fragment:\n#   %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem, %primals_4, %primals_5, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_1,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_2(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 8192\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 256\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/lr/clrp7havhmizbhstashq7p664h3fj3vtyxs5fdrr4jyaylyecgwp.py\n# Topologically Sorted Source Nodes: [x_5], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x_5 => _low_memory_max_pool2d_with_offsets_1, getitem_3\n# Graph fragment:\n#   %_low_memory_max_pool2d_with_offsets_1 : [num_users=2] = call_function[target=torch.ops.prims._low_memory_max_pool2d_with_offsets.default](args = (%relu_1, [2, 2], [2, 2], [0, 0], [1, 1], False), kwargs = {})\n#   %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets_1, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_3(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = (xindex % 8)\n    x1 = xindex // 8\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 32*x1), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 32*x1), xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (16 + 2*x0 + 32*x1), xmask, eviction_policy='evict_last')\n    tmp12 = tl.load(in_ptr0 + (17 + 2*x0 + 32*x1), xmask, eviction_policy='evict_last')\n    tmp2 = tmp1 > tmp0\n    tmp3 = tl.full([1], 1, tl.int8)\n    tmp4 = tl.full([1], 0, tl.int8)\n    tmp5 = tl.where(tmp2, tmp3, tmp4)\n    tmp6 = triton_helpers.maximum(tmp1, tmp0)\n    tmp8 = tmp7 > tmp6\n    tmp9 = tl.full([1], 2, tl.int8)\n    tmp10 = tl.where(tmp8, tmp9, tmp5)\n    tmp11 = triton_helpers.maximum(tmp7, tmp6)\n    tmp13 = tmp12 > tmp11\n    tmp14 = tl.full([1], 3, tl.int8)\n    tmp15 = tl.where(tmp13, tmp14, tmp10)\n    tmp16 = triton_helpers.maximum(tmp12, tmp11)\n    tl.store(out_ptr0 + (x2), tmp15, xmask)\n    tl.store(out_ptr1 + (x2), tmp16, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/zl/czlxijcx7yrdnigzjgtdojyzcuvt6ms4mhm7rfiho6n3r6lhlx36.py\n# Topologically Sorted Source Nodes: [x_7, x_8], Original ATen: [aten.addmm, aten.relu]\n# Source node to ATen node mapping:\n#   x_7 => add_tensor\n#   x_8 => relu_2\n# Graph fragment:\n#   %add_tensor : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default, %primals_7), kwargs = {})\n#   %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_addmm_relu_4(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x0), tmp4, xmask)\n\n\n\n\n\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9 = args\n    args.clear()\n    assert_size_stride(primals_1, (16, 3, 3, 3), (27, 9, 3, 1))\n    assert_size_stride(primals_2, (16, ), (1, ))\n    assert_size_stride(primals_3, (1, 3, 32, 32), (3072, 1024, 32, 1))\n    assert_size_stride(primals_4, (32, 16, 3, 3), (144, 9, 3, 1))\n    assert_size_stride(primals_5, (32, ), (1, ))\n    assert_size_stride(primals_6, (128, 2048), (2048, 1))\n    assert_size_stride(primals_7, (128, ), (1, ))\n    assert_size_stride(primals_8, (10, 128), (128, 1))\n    assert_size_stride(primals_9, (10, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.convolution]\n        buf0 = extern_kernels.convolution(primals_3, primals_1, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (1, 16, 32, 32), (16384, 1024, 32, 1))\n        buf1 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [x, x_1], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_0[grid(16384)](buf1, primals_2, 16384, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_2\n        buf2 = empty_strided_cuda((1, 16, 16, 16), (4096, 256, 16, 1), torch.float32)\n        buf3 = empty_strided_cuda((1, 16, 16, 16), (4096, 256, 16, 1), torch.int8)\n        # Topologically Sorted Source Nodes: [x_2], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_1[grid(4096)](buf1, buf2, buf3, 4096, XBLOCK=256, num_warps=4, num_stages=1)\n        # Topologically Sorted Source Nodes: [x_3], Original ATen: [aten.convolution]\n        buf4 = extern_kernels.convolution(buf2, primals_4, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf4, (1, 32, 16, 16), (8192, 256, 16, 1))\n        buf5 = buf4; del buf4  # reuse\n        # Topologically Sorted Source Nodes: [x_3, x_4], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_2[grid(8192)](buf5, primals_5, 8192, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_5\n        buf6 = empty_strided_cuda((1, 32, 8, 8), (2048, 64, 8, 1), torch.int8)\n        buf7 = empty_strided_cuda((1, 32, 8, 8), (2048, 64, 8, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_5], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_3[grid(2048)](buf5, buf6, buf7, 2048, XBLOCK=128, num_warps=4, num_stages=1)\n        buf8 = empty_strided_cuda((1, 128), (128, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_7], Original ATen: [aten.addmm]\n        extern_kernels.mm(reinterpret_tensor(buf7, (1, 2048), (0, 1), 0), reinterpret_tensor(primals_6, (2048, 128), (1, 2048), 0), out=buf8)\n        buf9 = buf8; del buf8  # reuse\n        # Topologically Sorted Source Nodes: [x_7, x_8], Original ATen: [aten.addmm, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_addmm_relu_4[grid(128)](buf9, primals_7, 128, XBLOCK=128, num_warps=4, num_stages=1)\n        del primals_7\n        buf10 = empty_strided_cuda((1, 10), (10, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_9], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_9, buf9, reinterpret_tensor(primals_8, (128, 10), (1, 128), 0), alpha=1, beta=1, out=buf10)\n        del primals_9\n    return (buf10, primals_1, primals_3, primals_4, buf1, buf2, buf3, buf5, buf6, reinterpret_tensor(buf7, (1, 2048), (2048, 1), 0), buf9, primals_8, primals_6, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((16, 3, 3, 3), (27, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((1, 3, 32, 32), (3072, 1024, 32, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((32, 16, 3, 3), (144, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_6 = rand_strided((128, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)\n    primals_7 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_8 = rand_strided((10, 128), (128, 1), device='cuda:0', dtype=torch.float32)\n    primals_9 = rand_strided((10, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \"__main__\":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n"
    },
    {
        "uuid": "f5981454-ca3b-4b58-9446-0c5be11af4fa copy",
        "generated_code": "\n# This is a random torch model generated by the following modules: ['ReLU', 'Linear', 'Conv2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RandomModel(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Input channels 3, output channels 16\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # Input channels 16, output channels 32\n        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Assuming input is downsampled to 8x8 after convolutions\n        self.fc2 = nn.Linear(128, 10)  # Final output size of 10\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)  # First ReLU after first convolution\n        x = F.max_pool2d(x, kernel_size=2, stride=2)  # Downsample\n\n        x = self.conv2(x)\n        x = F.relu(x)  # Second ReLU after second convolution\n        x = F.max_pool2d(x, kernel_size=2, stride=2)  # Downsample\n\n        x = x.view(x.size(0), -1)  # Flatten for the linear layer\n        x = self.fc1(x)\n        x = F.relu(x)  # Third ReLU after first linear layer\n        x = self.fc2(x)\n        return x  # No softmax here for raw logits\n\n\ndef get_random_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32)  # Random input tensor of shape (1, 3, 32, 32)\n    return [x]\n\n",
        "triton_code": "# AOT ID: ['0_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import (\n    grid,\n    split_scan_grid,\n    grid_combo_kernels,\n    start_graph,\n    end_graph,\n    cooperative_reduction_grid,\n)\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\n\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/torchinductor_sahanp/n3/cn3szxx6ut4mvtifmbzhdzffbpwthmfhqify457bjkd6murezytz.py\n# Topologically Sorted Source Nodes: [x, x_1], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   x => convolution\n#   x_1 => relu\n# Graph fragment:\n#   %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%primals_3, %primals_1, %primals_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 1024\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/qo/cqocbfll3ohawfthnb2m67trzdellkg64rfz5p4it3avdcwl2qxo.py\n# Topologically Sorted Source Nodes: [x_2], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x_2 => getitem, getitem_1\n# Graph fragment:\n#   %getitem : [num_users=2] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 0), kwargs = {})\n#   %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_1(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 16)\n    x1 = xindex // 16\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (32 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr0 + (33 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp6 = triton_helpers.maximum(tmp5, tmp4)\n    tmp7 = tmp1 > tmp0\n    tmp8 = tl.full([1], 1, tl.int8)\n    tmp9 = tl.full([1], 0, tl.int8)\n    tmp10 = tl.where(tmp7, tmp8, tmp9)\n    tmp11 = tmp3 > tmp2\n    tmp12 = tl.full([1], 2, tl.int8)\n    tmp13 = tl.where(tmp11, tmp12, tmp10)\n    tmp14 = tmp5 > tmp4\n    tmp15 = tl.full([1], 3, tl.int8)\n    tmp16 = tl.where(tmp14, tmp15, tmp13)\n    tl.store(out_ptr0 + (x2), tmp6, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/gn/cgngs6id6vx4yxtb2m6qy7har3facca3en4olf3cimlcjp2h42uy.py\n# Topologically Sorted Source Nodes: [x_3, x_4], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   x_3 => convolution_1\n#   x_4 => relu_1\n# Graph fragment:\n#   %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem, %primals_4, %primals_5, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_1,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_2(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 8192\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 256\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/lr/clrp7havhmizbhstashq7p664h3fj3vtyxs5fdrr4jyaylyecgwp.py\n# Topologically Sorted Source Nodes: [x_5], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x_5 => _low_memory_max_pool2d_with_offsets_1, getitem_3\n# Graph fragment:\n#   %_low_memory_max_pool2d_with_offsets_1 : [num_users=2] = call_function[target=torch.ops.prims._low_memory_max_pool2d_with_offsets.default](args = (%relu_1, [2, 2], [2, 2], [0, 0], [1, 1], False), kwargs = {})\n#   %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets_1, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_3(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = (xindex % 8)\n    x1 = xindex // 8\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 32*x1), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 32*x1), xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (16 + 2*x0 + 32*x1), xmask, eviction_policy='evict_last')\n    tmp12 = tl.load(in_ptr0 + (17 + 2*x0 + 32*x1), xmask, eviction_policy='evict_last')\n    tmp2 = tmp1 > tmp0\n    tmp3 = tl.full([1], 1, tl.int8)\n    tmp4 = tl.full([1], 0, tl.int8)\n    tmp5 = tl.where(tmp2, tmp3, tmp4)\n    tmp6 = triton_helpers.maximum(tmp1, tmp0)\n    tmp8 = tmp7 > tmp6\n    tmp9 = tl.full([1], 2, tl.int8)\n    tmp10 = tl.where(tmp8, tmp9, tmp5)\n    tmp11 = triton_helpers.maximum(tmp7, tmp6)\n    tmp13 = tmp12 > tmp11\n    tmp14 = tl.full([1], 3, tl.int8)\n    tmp15 = tl.where(tmp13, tmp14, tmp10)\n    tmp16 = triton_helpers.maximum(tmp12, tmp11)\n    tl.store(out_ptr0 + (x2), tmp15, xmask)\n    tl.store(out_ptr1 + (x2), tmp16, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/zl/czlxijcx7yrdnigzjgtdojyzcuvt6ms4mhm7rfiho6n3r6lhlx36.py\n# Topologically Sorted Source Nodes: [x_7, x_8], Original ATen: [aten.addmm, aten.relu]\n# Source node to ATen node mapping:\n#   x_7 => add_tensor\n#   x_8 => relu_2\n# Graph fragment:\n#   %add_tensor : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default, %primals_7), kwargs = {})\n#   %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_addmm_relu_4(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x0), tmp4, xmask)\n\n\n\n\n\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9 = args\n    args.clear()\n    assert_size_stride(primals_1, (16, 3, 3, 3), (27, 9, 3, 1))\n    assert_size_stride(primals_2, (16, ), (1, ))\n    assert_size_stride(primals_3, (1, 3, 32, 32), (3072, 1024, 32, 1))\n    assert_size_stride(primals_4, (32, 16, 3, 3), (144, 9, 3, 1))\n    assert_size_stride(primals_5, (32, ), (1, ))\n    assert_size_stride(primals_6, (128, 2048), (2048, 1))\n    assert_size_stride(primals_7, (128, ), (1, ))\n    assert_size_stride(primals_8, (10, 128), (128, 1))\n    assert_size_stride(primals_9, (10, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.convolution]\n        buf0 = extern_kernels.convolution(primals_3, primals_1, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (1, 16, 32, 32), (16384, 1024, 32, 1))\n        buf1 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [x, x_1], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_0[grid(16384)](buf1, primals_2, 16384, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_2\n        buf2 = empty_strided_cuda((1, 16, 16, 16), (4096, 256, 16, 1), torch.float32)\n        buf3 = empty_strided_cuda((1, 16, 16, 16), (4096, 256, 16, 1), torch.int8)\n        # Topologically Sorted Source Nodes: [x_2], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_1[grid(4096)](buf1, buf2, buf3, 4096, XBLOCK=256, num_warps=4, num_stages=1)\n        # Topologically Sorted Source Nodes: [x_3], Original ATen: [aten.convolution]\n        buf4 = extern_kernels.convolution(buf2, primals_4, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf4, (1, 32, 16, 16), (8192, 256, 16, 1))\n        buf5 = buf4; del buf4  # reuse\n        # Topologically Sorted Source Nodes: [x_3, x_4], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_2[grid(8192)](buf5, primals_5, 8192, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_5\n        buf6 = empty_strided_cuda((1, 32, 8, 8), (2048, 64, 8, 1), torch.int8)\n        buf7 = empty_strided_cuda((1, 32, 8, 8), (2048, 64, 8, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_5], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_3[grid(2048)](buf5, buf6, buf7, 2048, XBLOCK=128, num_warps=4, num_stages=1)\n        buf8 = empty_strided_cuda((1, 128), (128, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_7], Original ATen: [aten.addmm]\n        extern_kernels.mm(reinterpret_tensor(buf7, (1, 2048), (0, 1), 0), reinterpret_tensor(primals_6, (2048, 128), (1, 2048), 0), out=buf8)\n        buf9 = buf8; del buf8  # reuse\n        # Topologically Sorted Source Nodes: [x_7, x_8], Original ATen: [aten.addmm, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_addmm_relu_4[grid(128)](buf9, primals_7, 128, XBLOCK=128, num_warps=4, num_stages=1)\n        del primals_7\n        buf10 = empty_strided_cuda((1, 10), (10, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_9], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_9, buf9, reinterpret_tensor(primals_8, (128, 10), (1, 128), 0), alpha=1, beta=1, out=buf10)\n        del primals_9\n    return (buf10, primals_1, primals_3, primals_4, buf1, buf2, buf3, buf5, buf6, reinterpret_tensor(buf7, (1, 2048), (2048, 1), 0), buf9, primals_8, primals_6, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((16, 3, 3, 3), (27, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((1, 3, 32, 32), (3072, 1024, 32, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((32, 16, 3, 3), (144, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_6 = rand_strided((128, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)\n    primals_7 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_8 = rand_strided((10, 128), (128, 1), device='cuda:0', dtype=torch.float32)\n    primals_9 = rand_strided((10, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \"__main__\":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n"
    },
    {
        "uuid": "7b06b9d3-65b1-4e9f-bcad-82c20a45bc6d",
        "generated_code": "\n# This is a random torch model generated by the following modules: ['MaxPool2d', 'LogSoftmax', 'Conv2d', 'ReLU', 'Linear']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RandomModel(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # Input channel: 3, Output channel: 16\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) # Input channel: 16, Output channel: 32\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)        # Max pooling layer\n        self.fc1 = nn.Linear(32 * 16 * 16, 128)                  # Fully connected layer\n        self.fc2 = nn.Linear(128, 10)                             # Second fully connected layer\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # Conv -> ReLU -> MaxPool\n        x = self.pool(F.relu(self.conv2(x)))  # Conv -> ReLU -> MaxPool\n        x = x.view(x.size(0), -1)              # Flatten the output\n        x = F.relu(self.fc1(x))                # Fully connected -> ReLU\n        x = self.fc2(x)                        # Second fully connected layer\n        return F.log_softmax(x, dim=1)        # Log softmax output\n\n\ndef get_random_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64)  # Example input: batch size 1, 3 channels, 64x64 image\n    return [x]\n\n",
        "triton_code": "# AOT ID: ['2_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import (\n    grid,\n    split_scan_grid,\n    grid_combo_kernels,\n    start_graph,\n    end_graph,\n    cooperative_reduction_grid,\n)\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\n\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/torchinductor_sahanp/2u/c2uguk3jobbg7xolounytcobclav7cncb466fnewumxfwcbvm5fi.py\n# Topologically Sorted Source Nodes: [conv2d, relu], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   conv2d => convolution\n#   relu => relu\n# Graph fragment:\n#   %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%primals_3, %primals_1, %primals_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 65536\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 4096\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/uf/cuf2qxxwpmpspuigf3nrz22qprk7z454x3t2hpsgqng42if6roub.py\n# Topologically Sorted Source Nodes: [x], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x => getitem, getitem_1\n# Graph fragment:\n#   %getitem : [num_users=2] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 0), kwargs = {})\n#   %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_1(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 32)\n    x1 = xindex // 32\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (64 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr0 + (65 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp6 = triton_helpers.maximum(tmp5, tmp4)\n    tmp7 = tmp1 > tmp0\n    tmp8 = tl.full([1], 1, tl.int8)\n    tmp9 = tl.full([1], 0, tl.int8)\n    tmp10 = tl.where(tmp7, tmp8, tmp9)\n    tmp11 = tmp3 > tmp2\n    tmp12 = tl.full([1], 2, tl.int8)\n    tmp13 = tl.where(tmp11, tmp12, tmp10)\n    tmp14 = tmp5 > tmp4\n    tmp15 = tl.full([1], 3, tl.int8)\n    tmp16 = tl.where(tmp14, tmp15, tmp13)\n    tl.store(out_ptr0 + (x2), tmp6, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/o2/co2ghbebbgfcl3zz6yyidwlkg35yyzdccsbyuuume5xh6z25nklg.py\n# Topologically Sorted Source Nodes: [conv2d_1, relu_1], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   conv2d_1 => convolution_1\n#   relu_1 => relu_1\n# Graph fragment:\n#   %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem, %primals_4, %primals_5, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_1,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_2(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 32768\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 1024\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/nr/cnrr3bcdexeaxpmycjefbjmq5udbfywfen2qerkrkzh7rm2cud7e.py\n# Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x_1 => _low_memory_max_pool2d_with_offsets_1, getitem_3\n# Graph fragment:\n#   %_low_memory_max_pool2d_with_offsets_1 : [num_users=2] = call_function[target=torch.ops.prims._low_memory_max_pool2d_with_offsets.default](args = (%relu_1, [2, 2], [2, 2], [0, 0], [1, 1], False), kwargs = {})\n#   %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets_1, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_3(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 8192\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 16)\n    x1 = xindex // 16\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (32 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp12 = tl.load(in_ptr0 + (33 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp2 = tmp1 > tmp0\n    tmp3 = tl.full([1], 1, tl.int8)\n    tmp4 = tl.full([1], 0, tl.int8)\n    tmp5 = tl.where(tmp2, tmp3, tmp4)\n    tmp6 = triton_helpers.maximum(tmp1, tmp0)\n    tmp8 = tmp7 > tmp6\n    tmp9 = tl.full([1], 2, tl.int8)\n    tmp10 = tl.where(tmp8, tmp9, tmp5)\n    tmp11 = triton_helpers.maximum(tmp7, tmp6)\n    tmp13 = tmp12 > tmp11\n    tmp14 = tl.full([1], 3, tl.int8)\n    tmp15 = tl.where(tmp13, tmp14, tmp10)\n    tmp16 = triton_helpers.maximum(tmp12, tmp11)\n    tl.store(out_ptr0 + (x2), tmp15, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/zl/czlxijcx7yrdnigzjgtdojyzcuvt6ms4mhm7rfiho6n3r6lhlx36.py\n# Topologically Sorted Source Nodes: [linear, x_3], Original ATen: [aten.addmm, aten.relu]\n# Source node to ATen node mapping:\n#   linear => add_tensor\n#   x_3 => relu_2\n# Graph fragment:\n#   %add_tensor : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default, %primals_7), kwargs = {})\n#   %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_addmm_relu_4(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x0), tmp4, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/vk/cvkxam7itbvlwap5jowfxyic5roiadgbgwilulnktdwcbjv7erx4.py\n# Topologically Sorted Source Nodes: [log_softmax], Original ATen: [aten._log_softmax]\n# Source node to ATen node mapping:\n#   log_softmax => amax, exp, log, sub, sub_1, sum_1\n# Graph fragment:\n#   %amax : [num_users=1] = call_function[target=torch.ops.aten.amax.default](args = (%addmm_1, [1], True), kwargs = {})\n#   %sub : [num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%addmm_1, %amax), kwargs = {})\n#   %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%sub,), kwargs = {})\n#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%exp, [1], True), kwargs = {})\n#   %log : [num_users=1] = call_function[target=torch.ops.aten.log.default](args = (%sum_1,), kwargs = {})\n#   %sub_1 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub, %log), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_per_fused__log_softmax_5(in_out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):\n    xnumel = 1\n    r0_numel = 10\n    R0_BLOCK: tl.constexpr = 16\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_index = tl.arange(0, R0_BLOCK)[None, :]\n    r0_offset = 0\n    r0_mask = r0_index < r0_numel\n    roffset = r0_offset\n    rindex = r0_index\n    r0_0 = r0_index\n    tmp0 = tl.load(in_out_ptr0 + (r0_0), r0_mask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])\n    tmp3 = tl.where(r0_mask, tmp1, float(\"-inf\"))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])\n    tmp9 = tl.where(r0_mask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tmp11 = tl_math.log(tmp10)\n    tmp12 = tmp5 - tmp11\n    tl.store(in_out_ptr0 + (tl.broadcast_to(r0_0, [XBLOCK, R0_BLOCK])), tmp12, r0_mask)\n\n\n\n\n\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9 = args\n    args.clear()\n    assert_size_stride(primals_1, (16, 3, 3, 3), (27, 9, 3, 1))\n    assert_size_stride(primals_2, (16, ), (1, ))\n    assert_size_stride(primals_3, (1, 3, 64, 64), (12288, 4096, 64, 1))\n    assert_size_stride(primals_4, (32, 16, 3, 3), (144, 9, 3, 1))\n    assert_size_stride(primals_5, (32, ), (1, ))\n    assert_size_stride(primals_6, (128, 8192), (8192, 1))\n    assert_size_stride(primals_7, (128, ), (1, ))\n    assert_size_stride(primals_8, (10, 128), (128, 1))\n    assert_size_stride(primals_9, (10, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        # Topologically Sorted Source Nodes: [conv2d], Original ATen: [aten.convolution]\n        buf0 = extern_kernels.convolution(primals_3, primals_1, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (1, 16, 64, 64), (65536, 4096, 64, 1))\n        buf1 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [conv2d, relu], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_0[grid(65536)](buf1, primals_2, 65536, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_2\n        buf2 = empty_strided_cuda((1, 16, 32, 32), (16384, 1024, 32, 1), torch.float32)\n        buf3 = empty_strided_cuda((1, 16, 32, 32), (16384, 1024, 32, 1), torch.int8)\n        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_1[grid(16384)](buf1, buf2, buf3, 16384, XBLOCK=128, num_warps=4, num_stages=1)\n        # Topologically Sorted Source Nodes: [conv2d_1], Original ATen: [aten.convolution]\n        buf4 = extern_kernels.convolution(buf2, primals_4, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf4, (1, 32, 32, 32), (32768, 1024, 32, 1))\n        buf5 = buf4; del buf4  # reuse\n        # Topologically Sorted Source Nodes: [conv2d_1, relu_1], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_2[grid(32768)](buf5, primals_5, 32768, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_5\n        buf6 = empty_strided_cuda((1, 32, 16, 16), (8192, 256, 16, 1), torch.int8)\n        buf7 = empty_strided_cuda((1, 32, 16, 16), (8192, 256, 16, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_3[grid(8192)](buf5, buf6, buf7, 8192, XBLOCK=256, num_warps=4, num_stages=1)\n        buf8 = empty_strided_cuda((1, 128), (128, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.addmm]\n        extern_kernels.mm(reinterpret_tensor(buf7, (1, 8192), (0, 1), 0), reinterpret_tensor(primals_6, (8192, 128), (1, 8192), 0), out=buf8)\n        buf9 = buf8; del buf8  # reuse\n        # Topologically Sorted Source Nodes: [linear, x_3], Original ATen: [aten.addmm, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_addmm_relu_4[grid(128)](buf9, primals_7, 128, XBLOCK=128, num_warps=4, num_stages=1)\n        del primals_7\n        buf10 = empty_strided_cuda((1, 10), (10, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_9, buf9, reinterpret_tensor(primals_8, (128, 10), (1, 128), 0), alpha=1, beta=1, out=buf10)\n        del primals_9\n        buf13 = buf10; del buf10  # reuse\n        # Topologically Sorted Source Nodes: [log_softmax], Original ATen: [aten._log_softmax]\n        stream0 = get_raw_stream(0)\n        triton_per_fused__log_softmax_5[grid(1)](buf13, 1, 10, XBLOCK=1, num_warps=2, num_stages=1)\n    return (buf13, primals_1, primals_3, primals_4, buf1, buf2, buf3, buf5, buf6, reinterpret_tensor(buf7, (1, 8192), (8192, 1), 0), buf9, buf13, primals_8, primals_6, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((16, 3, 3, 3), (27, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((1, 3, 64, 64), (12288, 4096, 64, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((32, 16, 3, 3), (144, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_6 = rand_strided((128, 8192), (8192, 1), device='cuda:0', dtype=torch.float32)\n    primals_7 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_8 = rand_strided((10, 128), (128, 1), device='cuda:0', dtype=torch.float32)\n    primals_9 = rand_strided((10, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \"__main__\":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n"
    },
    {
        "uuid": "8bd32c4c-1901-4dee-bfe5-ab8fe6c397f7",
        "generated_code": "\n# This is a random torch model generated by the following modules: ['MaxPool2d', 'Linear', 'ReLU', 'Conv2d', 'LogSoftmax']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RandomModel(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # Input channels: 3, Output channels: 16\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # Input channels: 16, Output channels: 32\n        self.pool = nn.MaxPool2d(2, 2)  # 2x2 pooling\n        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # Linear layer following flattening\n        self.fc2 = nn.Linear(128, 10)  # Final output layer (for 10 classes)\n\n    def forward(self, x):\n        # Applying first convolution and activation\n        x = F.relu(self.conv1(x))  \n        x = self.pool(x)  # MaxPooling after first convolution\n        \n        # Applying second convolution and activation\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)  # MaxPooling after second convolution\n        \n        # Flattening the tensor for fully connected layer\n        x = x.view(-1, 32 * 16 * 16)  # Reshape for linear layer\n        \n        # First fully connected layer with ReLU\n        x = F.relu(self.fc1(x))\n        \n        # Final output layer\n        x = self.fc2(x)\n        \n        return F.log_softmax(x, dim=1)  # Applying LogSoftmax on the output\n\n\ndef get_random_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64)  # 1 sample, 3 channels, 64x64 image\n    return [x]\n\n",
        "triton_code": "# AOT ID: ['3_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import (\n    grid,\n    split_scan_grid,\n    grid_combo_kernels,\n    start_graph,\n    end_graph,\n    cooperative_reduction_grid,\n)\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\n\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/torchinductor_sahanp/2u/c2uguk3jobbg7xolounytcobclav7cncb466fnewumxfwcbvm5fi.py\n# Topologically Sorted Source Nodes: [conv2d, x], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   conv2d => convolution\n#   x => relu\n# Graph fragment:\n#   %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%primals_3, %primals_1, %primals_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 65536\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 4096\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/uf/cuf2qxxwpmpspuigf3nrz22qprk7z454x3t2hpsgqng42if6roub.py\n# Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x_1 => getitem, getitem_1\n# Graph fragment:\n#   %getitem : [num_users=2] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 0), kwargs = {})\n#   %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_1(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 32)\n    x1 = xindex // 32\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (64 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr0 + (65 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp6 = triton_helpers.maximum(tmp5, tmp4)\n    tmp7 = tmp1 > tmp0\n    tmp8 = tl.full([1], 1, tl.int8)\n    tmp9 = tl.full([1], 0, tl.int8)\n    tmp10 = tl.where(tmp7, tmp8, tmp9)\n    tmp11 = tmp3 > tmp2\n    tmp12 = tl.full([1], 2, tl.int8)\n    tmp13 = tl.where(tmp11, tmp12, tmp10)\n    tmp14 = tmp5 > tmp4\n    tmp15 = tl.full([1], 3, tl.int8)\n    tmp16 = tl.where(tmp14, tmp15, tmp13)\n    tl.store(out_ptr0 + (x2), tmp6, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/o2/co2ghbebbgfcl3zz6yyidwlkg35yyzdccsbyuuume5xh6z25nklg.py\n# Topologically Sorted Source Nodes: [conv2d_1, x_2], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   conv2d_1 => convolution_1\n#   x_2 => relu_1\n# Graph fragment:\n#   %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem, %primals_4, %primals_5, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_1,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_2(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 32768\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 1024\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/nr/cnrr3bcdexeaxpmycjefbjmq5udbfywfen2qerkrkzh7rm2cud7e.py\n# Topologically Sorted Source Nodes: [x_3], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x_3 => _low_memory_max_pool2d_with_offsets_1, getitem_3\n# Graph fragment:\n#   %_low_memory_max_pool2d_with_offsets_1 : [num_users=2] = call_function[target=torch.ops.prims._low_memory_max_pool2d_with_offsets.default](args = (%relu_1, [2, 2], [2, 2], [0, 0], [1, 1], False), kwargs = {})\n#   %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets_1, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_3(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 8192\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 16)\n    x1 = xindex // 16\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (32 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp12 = tl.load(in_ptr0 + (33 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp2 = tmp1 > tmp0\n    tmp3 = tl.full([1], 1, tl.int8)\n    tmp4 = tl.full([1], 0, tl.int8)\n    tmp5 = tl.where(tmp2, tmp3, tmp4)\n    tmp6 = triton_helpers.maximum(tmp1, tmp0)\n    tmp8 = tmp7 > tmp6\n    tmp9 = tl.full([1], 2, tl.int8)\n    tmp10 = tl.where(tmp8, tmp9, tmp5)\n    tmp11 = triton_helpers.maximum(tmp7, tmp6)\n    tmp13 = tmp12 > tmp11\n    tmp14 = tl.full([1], 3, tl.int8)\n    tmp15 = tl.where(tmp13, tmp14, tmp10)\n    tmp16 = triton_helpers.maximum(tmp12, tmp11)\n    tl.store(out_ptr0 + (x2), tmp15, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/zl/czlxijcx7yrdnigzjgtdojyzcuvt6ms4mhm7rfiho6n3r6lhlx36.py\n# Topologically Sorted Source Nodes: [linear, x_5], Original ATen: [aten.addmm, aten.relu]\n# Source node to ATen node mapping:\n#   linear => add_tensor\n#   x_5 => relu_2\n# Graph fragment:\n#   %add_tensor : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default, %primals_7), kwargs = {})\n#   %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_addmm_relu_4(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x0), tmp4, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/vk/cvkxam7itbvlwap5jowfxyic5roiadgbgwilulnktdwcbjv7erx4.py\n# Topologically Sorted Source Nodes: [log_softmax], Original ATen: [aten._log_softmax]\n# Source node to ATen node mapping:\n#   log_softmax => amax, exp, log, sub, sub_1, sum_1\n# Graph fragment:\n#   %amax : [num_users=1] = call_function[target=torch.ops.aten.amax.default](args = (%addmm_1, [1], True), kwargs = {})\n#   %sub : [num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%addmm_1, %amax), kwargs = {})\n#   %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%sub,), kwargs = {})\n#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%exp, [1], True), kwargs = {})\n#   %log : [num_users=1] = call_function[target=torch.ops.aten.log.default](args = (%sum_1,), kwargs = {})\n#   %sub_1 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub, %log), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_per_fused__log_softmax_5(in_out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):\n    xnumel = 1\n    r0_numel = 10\n    R0_BLOCK: tl.constexpr = 16\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_index = tl.arange(0, R0_BLOCK)[None, :]\n    r0_offset = 0\n    r0_mask = r0_index < r0_numel\n    roffset = r0_offset\n    rindex = r0_index\n    r0_0 = r0_index\n    tmp0 = tl.load(in_out_ptr0 + (r0_0), r0_mask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])\n    tmp3 = tl.where(r0_mask, tmp1, float(\"-inf\"))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])\n    tmp9 = tl.where(r0_mask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tmp11 = tl_math.log(tmp10)\n    tmp12 = tmp5 - tmp11\n    tl.store(in_out_ptr0 + (tl.broadcast_to(r0_0, [XBLOCK, R0_BLOCK])), tmp12, r0_mask)\n\n\n\n\n\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9 = args\n    args.clear()\n    assert_size_stride(primals_1, (16, 3, 3, 3), (27, 9, 3, 1))\n    assert_size_stride(primals_2, (16, ), (1, ))\n    assert_size_stride(primals_3, (1, 3, 64, 64), (12288, 4096, 64, 1))\n    assert_size_stride(primals_4, (32, 16, 3, 3), (144, 9, 3, 1))\n    assert_size_stride(primals_5, (32, ), (1, ))\n    assert_size_stride(primals_6, (128, 8192), (8192, 1))\n    assert_size_stride(primals_7, (128, ), (1, ))\n    assert_size_stride(primals_8, (10, 128), (128, 1))\n    assert_size_stride(primals_9, (10, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        # Topologically Sorted Source Nodes: [conv2d], Original ATen: [aten.convolution]\n        buf0 = extern_kernels.convolution(primals_3, primals_1, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (1, 16, 64, 64), (65536, 4096, 64, 1))\n        buf1 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [conv2d, x], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_0[grid(65536)](buf1, primals_2, 65536, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_2\n        buf2 = empty_strided_cuda((1, 16, 32, 32), (16384, 1024, 32, 1), torch.float32)\n        buf3 = empty_strided_cuda((1, 16, 32, 32), (16384, 1024, 32, 1), torch.int8)\n        # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_1[grid(16384)](buf1, buf2, buf3, 16384, XBLOCK=128, num_warps=4, num_stages=1)\n        # Topologically Sorted Source Nodes: [conv2d_1], Original ATen: [aten.convolution]\n        buf4 = extern_kernels.convolution(buf2, primals_4, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf4, (1, 32, 32, 32), (32768, 1024, 32, 1))\n        buf5 = buf4; del buf4  # reuse\n        # Topologically Sorted Source Nodes: [conv2d_1, x_2], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_2[grid(32768)](buf5, primals_5, 32768, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_5\n        buf6 = empty_strided_cuda((1, 32, 16, 16), (8192, 256, 16, 1), torch.int8)\n        buf7 = empty_strided_cuda((1, 32, 16, 16), (8192, 256, 16, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_3], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_3[grid(8192)](buf5, buf6, buf7, 8192, XBLOCK=256, num_warps=4, num_stages=1)\n        buf8 = empty_strided_cuda((1, 128), (128, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.addmm]\n        extern_kernels.mm(reinterpret_tensor(buf7, (1, 8192), (0, 1), 0), reinterpret_tensor(primals_6, (8192, 128), (1, 8192), 0), out=buf8)\n        buf9 = buf8; del buf8  # reuse\n        # Topologically Sorted Source Nodes: [linear, x_5], Original ATen: [aten.addmm, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_addmm_relu_4[grid(128)](buf9, primals_7, 128, XBLOCK=128, num_warps=4, num_stages=1)\n        del primals_7\n        buf10 = empty_strided_cuda((1, 10), (10, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_6], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_9, buf9, reinterpret_tensor(primals_8, (128, 10), (1, 128), 0), alpha=1, beta=1, out=buf10)\n        del primals_9\n        buf13 = buf10; del buf10  # reuse\n        # Topologically Sorted Source Nodes: [log_softmax], Original ATen: [aten._log_softmax]\n        stream0 = get_raw_stream(0)\n        triton_per_fused__log_softmax_5[grid(1)](buf13, 1, 10, XBLOCK=1, num_warps=2, num_stages=1)\n    return (buf13, primals_1, primals_3, primals_4, buf1, buf2, buf3, buf5, buf6, reinterpret_tensor(buf7, (1, 8192), (8192, 1), 0), buf9, buf13, primals_8, primals_6, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((16, 3, 3, 3), (27, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((1, 3, 64, 64), (12288, 4096, 64, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((32, 16, 3, 3), (144, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_6 = rand_strided((128, 8192), (8192, 1), device='cuda:0', dtype=torch.float32)\n    primals_7 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_8 = rand_strided((10, 128), (128, 1), device='cuda:0', dtype=torch.float32)\n    primals_9 = rand_strided((10, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \"__main__\":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n"
    },
    {
        "uuid": "908ac3ab-a39b-4730-8651-c4103b5bf9b1",
        "generated_code": "\n# This is a random torch model generated by the following modules: ['Linear', 'LogSoftmax', 'Conv2d', 'ReLU', 'MaxPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RandomModel(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n        self.fc1 = nn.Linear(32 * 6 * 6, 128)  # Assuming input is 32x32\n        self.fc2 = nn.Linear(128, 10)  # 10 classes for output\n\n    def forward(self, x):\n        # Check input shape for flexibility\n        x = self.relu(self.pool(self.conv1(x)))  # Conv + ReLU + MaxPool\n        x = self.relu(self.pool(self.conv2(x)))  # Conv + ReLU + MaxPool\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.relu(self.fc1(x))  # Fully connected + ReLU\n        x = self.fc2(x)  # Fully connected\n        return F.log_softmax(x, dim=1)  # LogSoftmax output\n\n\ndef get_random_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 32, 32)  # Example input shape\n    return [x]\n",
        "triton_code": "# AOT ID: ['4_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import (\n    grid,\n    split_scan_grid,\n    grid_combo_kernels,\n    start_graph,\n    end_graph,\n    cooperative_reduction_grid,\n)\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\n\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/torchinductor_sahanp/cj/ccj24pybi23xbr7llz2ubktlta7vzctwur7ambccevq3lehfobif.py\n# Topologically Sorted Source Nodes: [conv2d], Original ATen: [aten.convolution]\n# Source node to ATen node mapping:\n#   conv2d => convolution\n# Graph fragment:\n#   %convolution : [num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%primals_3, %primals_1, %primals_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 14400\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x1 = xindex // 900\n    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (x1), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tl.store(in_out_ptr0 + (x2), tmp2, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/7v/c7vpspzswhahlbjn7zrj3626iyrpppwkr7kkdztohujytaysnrgv.py\n# Topologically Sorted Source Nodes: [max_pool2d, x], Original ATen: [aten.max_pool2d_with_indices, aten.relu]\n# Source node to ATen node mapping:\n#   max_pool2d => _low_memory_max_pool2d_with_offsets, getitem_1\n#   x => relu\n# Graph fragment:\n#   %_low_memory_max_pool2d_with_offsets : [num_users=2] = call_function[target=torch.ops.prims._low_memory_max_pool2d_with_offsets.default](args = (%convolution, [2, 2], [2, 2], [0, 0], [1, 1], False), kwargs = {})\n#   %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 1), kwargs = {})\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%getitem,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_relu_1(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 3600\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = (xindex % 15)\n    x1 = xindex // 15\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 60*x1), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 60*x1), xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (30 + 2*x0 + 60*x1), xmask, eviction_policy='evict_last')\n    tmp12 = tl.load(in_ptr0 + (31 + 2*x0 + 60*x1), xmask, eviction_policy='evict_last')\n    tmp2 = tmp1 > tmp0\n    tmp3 = tl.full([1], 1, tl.int8)\n    tmp4 = tl.full([1], 0, tl.int8)\n    tmp5 = tl.where(tmp2, tmp3, tmp4)\n    tmp6 = triton_helpers.maximum(tmp1, tmp0)\n    tmp8 = tmp7 > tmp6\n    tmp9 = tl.full([1], 2, tl.int8)\n    tmp10 = tl.where(tmp8, tmp9, tmp5)\n    tmp11 = triton_helpers.maximum(tmp7, tmp6)\n    tmp13 = tmp12 > tmp11\n    tmp14 = tl.full([1], 3, tl.int8)\n    tmp15 = tl.where(tmp13, tmp14, tmp10)\n    tmp16 = triton_helpers.maximum(tmp12, tmp11)\n    tmp17 = tl.full([1], 0, tl.int32)\n    tmp18 = triton_helpers.maximum(tmp17, tmp16)\n    tl.store(out_ptr0 + (x2), tmp15, xmask)\n    tl.store(out_ptr1 + (x2), tmp18, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/bw/cbwmfosb5rzq3wun7qlep6xb6ahedwoaze4ab5glpzmbg24wy3bl.py\n# Topologically Sorted Source Nodes: [conv2d_1], Original ATen: [aten.convolution]\n# Source node to ATen node mapping:\n#   conv2d_1 => convolution_1\n# Graph fragment:\n#   %convolution_1 : [num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %primals_4, %primals_5, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_2(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 5408\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x1 = xindex // 169\n    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (x1), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tl.store(in_out_ptr0 + (x2), tmp2, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/sa/csaaz6maondjf5dzz5kwc76lcviqxjnooysa7kp3tqhw5ptymdqn.py\n# Topologically Sorted Source Nodes: [max_pool2d_1, x_1], Original ATen: [aten.max_pool2d_with_indices, aten.relu, aten.threshold_backward]\n# Source node to ATen node mapping:\n#   max_pool2d_1 => _low_memory_max_pool2d_with_offsets_1, getitem_3\n#   x_1 => relu_1\n# Graph fragment:\n#   %_low_memory_max_pool2d_with_offsets_1 : [num_users=2] = call_function[target=torch.ops.prims._low_memory_max_pool2d_with_offsets.default](args = (%convolution_1, [2, 2], [2, 2], [0, 0], [1, 1], False), kwargs = {})\n#   %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets_1, 1), kwargs = {})\n#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_2,), kwargs = {})\n#   %le_1 : [num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_1, 0), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_relu_threshold_backward_3(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 1152\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = (xindex % 6)\n    x1 = ((xindex // 6) % 6)\n    x2 = xindex // 36\n    x3 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 26*x1 + 169*x2), xmask, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 26*x1 + 169*x2), xmask, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (13 + 2*x0 + 26*x1 + 169*x2), xmask, eviction_policy='evict_last')\n    tmp12 = tl.load(in_ptr0 + (14 + 2*x0 + 26*x1 + 169*x2), xmask, eviction_policy='evict_last')\n    tmp2 = tmp1 > tmp0\n    tmp3 = tl.full([1], 1, tl.int8)\n    tmp4 = tl.full([1], 0, tl.int8)\n    tmp5 = tl.where(tmp2, tmp3, tmp4)\n    tmp6 = triton_helpers.maximum(tmp1, tmp0)\n    tmp8 = tmp7 > tmp6\n    tmp9 = tl.full([1], 2, tl.int8)\n    tmp10 = tl.where(tmp8, tmp9, tmp5)\n    tmp11 = triton_helpers.maximum(tmp7, tmp6)\n    tmp13 = tmp12 > tmp11\n    tmp14 = tl.full([1], 3, tl.int8)\n    tmp15 = tl.where(tmp13, tmp14, tmp10)\n    tmp16 = triton_helpers.maximum(tmp12, tmp11)\n    tmp17 = tl.full([1], 0, tl.int32)\n    tmp18 = triton_helpers.maximum(tmp17, tmp16)\n    tmp19 = 0.0\n    tmp20 = tmp18 <= tmp19\n    tl.store(out_ptr0 + (x3), tmp15, xmask)\n    tl.store(out_ptr1 + (x3), tmp18, xmask)\n    tl.store(out_ptr2 + (x3), tmp20, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/zl/czlxijcx7yrdnigzjgtdojyzcuvt6ms4mhm7rfiho6n3r6lhlx36.py\n# Topologically Sorted Source Nodes: [linear, x_3], Original ATen: [aten.addmm, aten.relu]\n# Source node to ATen node mapping:\n#   linear => add_tensor\n#   x_3 => relu_2\n# Graph fragment:\n#   %add_tensor : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default, %primals_7), kwargs = {})\n#   %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_addmm_relu_4(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x0), tmp4, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/vk/cvkxam7itbvlwap5jowfxyic5roiadgbgwilulnktdwcbjv7erx4.py\n# Topologically Sorted Source Nodes: [log_softmax], Original ATen: [aten._log_softmax]\n# Source node to ATen node mapping:\n#   log_softmax => amax, exp, log, sub, sub_1, sum_1\n# Graph fragment:\n#   %amax : [num_users=1] = call_function[target=torch.ops.aten.amax.default](args = (%addmm_1, [1], True), kwargs = {})\n#   %sub : [num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%addmm_1, %amax), kwargs = {})\n#   %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%sub,), kwargs = {})\n#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%exp, [1], True), kwargs = {})\n#   %log : [num_users=1] = call_function[target=torch.ops.aten.log.default](args = (%sum_1,), kwargs = {})\n#   %sub_1 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub, %log), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_per_fused__log_softmax_5(in_out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):\n    xnumel = 1\n    r0_numel = 10\n    R0_BLOCK: tl.constexpr = 16\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_index = tl.arange(0, R0_BLOCK)[None, :]\n    r0_offset = 0\n    r0_mask = r0_index < r0_numel\n    roffset = r0_offset\n    rindex = r0_index\n    r0_0 = r0_index\n    tmp0 = tl.load(in_out_ptr0 + (r0_0), r0_mask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])\n    tmp3 = tl.where(r0_mask, tmp1, float(\"-inf\"))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])\n    tmp9 = tl.where(r0_mask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tmp11 = tl_math.log(tmp10)\n    tmp12 = tmp5 - tmp11\n    tl.store(in_out_ptr0 + (tl.broadcast_to(r0_0, [XBLOCK, R0_BLOCK])), tmp12, r0_mask)\n\n\n\n\n\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9 = args\n    args.clear()\n    assert_size_stride(primals_1, (16, 1, 3, 3), (9, 9, 3, 1))\n    assert_size_stride(primals_2, (16, ), (1, ))\n    assert_size_stride(primals_3, (1, 1, 32, 32), (1024, 1024, 32, 1))\n    assert_size_stride(primals_4, (32, 16, 3, 3), (144, 9, 3, 1))\n    assert_size_stride(primals_5, (32, ), (1, ))\n    assert_size_stride(primals_6, (128, 1152), (1152, 1))\n    assert_size_stride(primals_7, (128, ), (1, ))\n    assert_size_stride(primals_8, (10, 128), (128, 1))\n    assert_size_stride(primals_9, (10, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        # Topologically Sorted Source Nodes: [conv2d], Original ATen: [aten.convolution]\n        buf0 = extern_kernels.convolution(primals_3, primals_1, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (1, 16, 30, 30), (14400, 900, 30, 1))\n        buf1 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [conv2d], Original ATen: [aten.convolution]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_0[grid(14400)](buf1, primals_2, 14400, XBLOCK=128, num_warps=4, num_stages=1)\n        del primals_2\n        buf2 = empty_strided_cuda((1, 16, 15, 15), (3712, 225, 15, 1), torch.int8)\n        buf3 = empty_strided_cuda((1, 16, 15, 15), (3600, 225, 15, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [max_pool2d, x], Original ATen: [aten.max_pool2d_with_indices, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_relu_1[grid(3600)](buf1, buf2, buf3, 3600, XBLOCK=256, num_warps=4, num_stages=1)\n        # Topologically Sorted Source Nodes: [conv2d_1], Original ATen: [aten.convolution]\n        buf4 = extern_kernels.convolution(buf3, primals_4, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf4, (1, 32, 13, 13), (5408, 169, 13, 1))\n        buf5 = buf4; del buf4  # reuse\n        # Topologically Sorted Source Nodes: [conv2d_1], Original ATen: [aten.convolution]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_2[grid(5408)](buf5, primals_5, 5408, XBLOCK=128, num_warps=4, num_stages=1)\n        del primals_5\n        buf6 = empty_strided_cuda((1, 32, 6, 6), (1152, 36, 6, 1), torch.int8)\n        buf7 = empty_strided_cuda((1, 32, 6, 6), (1152, 36, 6, 1), torch.float32)\n        buf14 = empty_strided_cuda((1, 32, 6, 6), (1152, 36, 6, 1), torch.bool)\n        # Topologically Sorted Source Nodes: [max_pool2d_1, x_1], Original ATen: [aten.max_pool2d_with_indices, aten.relu, aten.threshold_backward]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_relu_threshold_backward_3[grid(1152)](buf5, buf6, buf7, buf14, 1152, XBLOCK=256, num_warps=4, num_stages=1)\n        buf8 = empty_strided_cuda((1, 128), (128, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.addmm]\n        extern_kernels.mm(reinterpret_tensor(buf7, (1, 1152), (0, 1), 0), reinterpret_tensor(primals_6, (1152, 128), (1, 1152), 0), out=buf8)\n        buf9 = buf8; del buf8  # reuse\n        # Topologically Sorted Source Nodes: [linear, x_3], Original ATen: [aten.addmm, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_addmm_relu_4[grid(128)](buf9, primals_7, 128, XBLOCK=128, num_warps=4, num_stages=1)\n        del primals_7\n        buf10 = empty_strided_cuda((1, 10), (10, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_9, buf9, reinterpret_tensor(primals_8, (128, 10), (1, 128), 0), alpha=1, beta=1, out=buf10)\n        del primals_9\n        buf13 = buf10; del buf10  # reuse\n        # Topologically Sorted Source Nodes: [log_softmax], Original ATen: [aten._log_softmax]\n        stream0 = get_raw_stream(0)\n        triton_per_fused__log_softmax_5[grid(1)](buf13, 1, 10, XBLOCK=1, num_warps=2, num_stages=1)\n    return (buf13, primals_1, primals_3, primals_4, buf1, buf2, buf3, buf5, buf6, reinterpret_tensor(buf7, (1, 1152), (1152, 1), 0), buf9, buf13, primals_8, primals_6, buf14, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((16, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((1, 1, 32, 32), (1024, 1024, 32, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((32, 16, 3, 3), (144, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_6 = rand_strided((128, 1152), (1152, 1), device='cuda:0', dtype=torch.float32)\n    primals_7 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_8 = rand_strided((10, 128), (128, 1), device='cuda:0', dtype=torch.float32)\n    primals_9 = rand_strided((10, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \"__main__\":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n"
    },
    {
        "uuid": "9a57c5a7-a6d7-4876-b78c-2959108841b0",
        "generated_code": "\n# This is a random torch model generated by the following modules: ['ReLU', 'Conv2d', 'Linear', 'MaxPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RandomModel(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # From 3 channels to 16 channels\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # From 16 to 32 channels\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 2x2 max pooling\n        self.fc1 = nn.Linear(32 * 16 * 16, 120)  # Assume input is 64x64, pooled feature map size is reduced\n        self.fc2 = nn.Linear(120, 84)  # From 120 to 84 features\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # Apply Conv2d, ReLU, and MaxPool2d\n        x = self.pool(F.relu(self.conv2(x)))  # Another Conv2d, ReLU, and MaxPool2d\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x = F.relu(self.fc1(x))  # Fully connected layer with ReLU\n        x = self.fc2(x)  # Final fully connected layer\n        return x  # No softmax, for raw class scores\n\n\ndef get_random_inputs():\n    # Randomly generate input tensors assuming input has 3 color channels\n    x = torch.randn(1, 3, 64, 64)  # Example input with shape: (batch_size, channels, height, width)\n    return [x]\n\n",
        "triton_code": "# AOT ID: ['5_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import (\n    grid,\n    split_scan_grid,\n    grid_combo_kernels,\n    start_graph,\n    end_graph,\n    cooperative_reduction_grid,\n)\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\n\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/torchinductor_sahanp/2u/c2uguk3jobbg7xolounytcobclav7cncb466fnewumxfwcbvm5fi.py\n# Topologically Sorted Source Nodes: [conv2d, relu], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   conv2d => convolution\n#   relu => relu\n# Graph fragment:\n#   %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%primals_3, %primals_1, %primals_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 65536\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 4096\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/uf/cuf2qxxwpmpspuigf3nrz22qprk7z454x3t2hpsgqng42if6roub.py\n# Topologically Sorted Source Nodes: [x], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x => getitem, getitem_1\n# Graph fragment:\n#   %getitem : [num_users=2] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 0), kwargs = {})\n#   %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_1(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 32)\n    x1 = xindex // 32\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (64 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr0 + (65 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp6 = triton_helpers.maximum(tmp5, tmp4)\n    tmp7 = tmp1 > tmp0\n    tmp8 = tl.full([1], 1, tl.int8)\n    tmp9 = tl.full([1], 0, tl.int8)\n    tmp10 = tl.where(tmp7, tmp8, tmp9)\n    tmp11 = tmp3 > tmp2\n    tmp12 = tl.full([1], 2, tl.int8)\n    tmp13 = tl.where(tmp11, tmp12, tmp10)\n    tmp14 = tmp5 > tmp4\n    tmp15 = tl.full([1], 3, tl.int8)\n    tmp16 = tl.where(tmp14, tmp15, tmp13)\n    tl.store(out_ptr0 + (x2), tmp6, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/o2/co2ghbebbgfcl3zz6yyidwlkg35yyzdccsbyuuume5xh6z25nklg.py\n# Topologically Sorted Source Nodes: [conv2d_1, relu_1], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   conv2d_1 => convolution_1\n#   relu_1 => relu_1\n# Graph fragment:\n#   %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem, %primals_4, %primals_5, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_1,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_2(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 32768\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 1024\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/nr/cnrr3bcdexeaxpmycjefbjmq5udbfywfen2qerkrkzh7rm2cud7e.py\n# Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x_1 => _low_memory_max_pool2d_with_offsets_1, getitem_3\n# Graph fragment:\n#   %_low_memory_max_pool2d_with_offsets_1 : [num_users=2] = call_function[target=torch.ops.prims._low_memory_max_pool2d_with_offsets.default](args = (%relu_1, [2, 2], [2, 2], [0, 0], [1, 1], False), kwargs = {})\n#   %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets_1, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_3(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 8192\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 16)\n    x1 = xindex // 16\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (32 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp12 = tl.load(in_ptr0 + (33 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp2 = tmp1 > tmp0\n    tmp3 = tl.full([1], 1, tl.int8)\n    tmp4 = tl.full([1], 0, tl.int8)\n    tmp5 = tl.where(tmp2, tmp3, tmp4)\n    tmp6 = triton_helpers.maximum(tmp1, tmp0)\n    tmp8 = tmp7 > tmp6\n    tmp9 = tl.full([1], 2, tl.int8)\n    tmp10 = tl.where(tmp8, tmp9, tmp5)\n    tmp11 = triton_helpers.maximum(tmp7, tmp6)\n    tmp13 = tmp12 > tmp11\n    tmp14 = tl.full([1], 3, tl.int8)\n    tmp15 = tl.where(tmp13, tmp14, tmp10)\n    tmp16 = triton_helpers.maximum(tmp12, tmp11)\n    tl.store(out_ptr0 + (x2), tmp15, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/uj/cuju4zpkieds2c6tt5c2vp5lptgf7xbplckl4swdgrjvd2ykteii.py\n# Topologically Sorted Source Nodes: [linear, x_3], Original ATen: [aten.addmm, aten.relu]\n# Source node to ATen node mapping:\n#   linear => add_tensor\n#   x_3 => relu_2\n# Graph fragment:\n#   %add_tensor : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default, %primals_7), kwargs = {})\n#   %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_addmm_relu_4(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 120\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x0), tmp4, xmask)\n\n\n\n\n\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9 = args\n    args.clear()\n    assert_size_stride(primals_1, (16, 3, 3, 3), (27, 9, 3, 1))\n    assert_size_stride(primals_2, (16, ), (1, ))\n    assert_size_stride(primals_3, (1, 3, 64, 64), (12288, 4096, 64, 1))\n    assert_size_stride(primals_4, (32, 16, 3, 3), (144, 9, 3, 1))\n    assert_size_stride(primals_5, (32, ), (1, ))\n    assert_size_stride(primals_6, (120, 8192), (8192, 1))\n    assert_size_stride(primals_7, (120, ), (1, ))\n    assert_size_stride(primals_8, (84, 120), (120, 1))\n    assert_size_stride(primals_9, (84, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        # Topologically Sorted Source Nodes: [conv2d], Original ATen: [aten.convolution]\n        buf0 = extern_kernels.convolution(primals_3, primals_1, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (1, 16, 64, 64), (65536, 4096, 64, 1))\n        buf1 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [conv2d, relu], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_0[grid(65536)](buf1, primals_2, 65536, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_2\n        buf2 = empty_strided_cuda((1, 16, 32, 32), (16384, 1024, 32, 1), torch.float32)\n        buf3 = empty_strided_cuda((1, 16, 32, 32), (16384, 1024, 32, 1), torch.int8)\n        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_1[grid(16384)](buf1, buf2, buf3, 16384, XBLOCK=128, num_warps=4, num_stages=1)\n        # Topologically Sorted Source Nodes: [conv2d_1], Original ATen: [aten.convolution]\n        buf4 = extern_kernels.convolution(buf2, primals_4, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf4, (1, 32, 32, 32), (32768, 1024, 32, 1))\n        buf5 = buf4; del buf4  # reuse\n        # Topologically Sorted Source Nodes: [conv2d_1, relu_1], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_2[grid(32768)](buf5, primals_5, 32768, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_5\n        buf6 = empty_strided_cuda((1, 32, 16, 16), (8192, 256, 16, 1), torch.int8)\n        buf7 = empty_strided_cuda((1, 32, 16, 16), (8192, 256, 16, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_3[grid(8192)](buf5, buf6, buf7, 8192, XBLOCK=256, num_warps=4, num_stages=1)\n        buf8 = empty_strided_cuda((1, 120), (120, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.addmm]\n        extern_kernels.mm(reinterpret_tensor(buf7, (1, 8192), (0, 1), 0), reinterpret_tensor(primals_6, (8192, 120), (1, 8192), 0), out=buf8)\n        buf9 = buf8; del buf8  # reuse\n        # Topologically Sorted Source Nodes: [linear, x_3], Original ATen: [aten.addmm, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_addmm_relu_4[grid(120)](buf9, primals_7, 120, XBLOCK=128, num_warps=4, num_stages=1)\n        del primals_7\n        buf10 = empty_strided_cuda((1, 84), (84, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_9, buf9, reinterpret_tensor(primals_8, (120, 84), (1, 120), 0), alpha=1, beta=1, out=buf10)\n        del primals_9\n    return (buf10, primals_1, primals_3, primals_4, buf1, buf2, buf3, buf5, buf6, reinterpret_tensor(buf7, (1, 8192), (8192, 1), 0), buf9, primals_8, primals_6, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((16, 3, 3, 3), (27, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((1, 3, 64, 64), (12288, 4096, 64, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((32, 16, 3, 3), (144, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_6 = rand_strided((120, 8192), (8192, 1), device='cuda:0', dtype=torch.float32)\n    primals_7 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_8 = rand_strided((84, 120), (120, 1), device='cuda:0', dtype=torch.float32)\n    primals_9 = rand_strided((84, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \"__main__\":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n"
    },
    {
        "uuid": "f1c4adb0-aad4-474a-b09b-67e93e3eaa41",
        "generated_code": "\n# This is a random torch model generated by the following modules: ['MaxPool2d', 'ReLU', 'LogSoftmax', 'Conv2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RandomModel(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)  # Applies MaxPool2d\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)  # Applies MaxPool2d\n        x = x.view(x.size(0), -1)  # Reshape for fully connected layer\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef get_random_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64)  # Example input for an image with 3 channels (RGB)\n    return [x]\n",
        "triton_code": "# AOT ID: ['2_forward']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nfrom torch._inductor.codegen.multi_kernel import MultiKernelCall\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import (\n    grid,\n    split_scan_grid,\n    grid_combo_kernels,\n    start_graph,\n    end_graph,\n    cooperative_reduction_grid,\n)\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\n\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/torchinductor_sahanp/2u/c2uguk3jobbg7xolounytcobclav7cncb466fnewumxfwcbvm5fi.py\n# Topologically Sorted Source Nodes: [conv2d, relu], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   conv2d => convolution\n#   relu => relu\n# Graph fragment:\n#   %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%primals_3, %primals_1, %primals_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 65536\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 4096\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/uf/cuf2qxxwpmpspuigf3nrz22qprk7z454x3t2hpsgqng42if6roub.py\n# Topologically Sorted Source Nodes: [x], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x => getitem, getitem_1\n# Graph fragment:\n#   %getitem : [num_users=2] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 0), kwargs = {})\n#   %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_1(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 16384\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 32)\n    x1 = xindex // 32\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr0 + (64 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp5 = tl.load(in_ptr0 + (65 + 2*x0 + 128*x1), None, eviction_policy='evict_last')\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tmp6 = triton_helpers.maximum(tmp5, tmp4)\n    tmp7 = tmp1 > tmp0\n    tmp8 = tl.full([1], 1, tl.int8)\n    tmp9 = tl.full([1], 0, tl.int8)\n    tmp10 = tl.where(tmp7, tmp8, tmp9)\n    tmp11 = tmp3 > tmp2\n    tmp12 = tl.full([1], 2, tl.int8)\n    tmp13 = tl.where(tmp11, tmp12, tmp10)\n    tmp14 = tmp5 > tmp4\n    tmp15 = tl.full([1], 3, tl.int8)\n    tmp16 = tl.where(tmp14, tmp15, tmp13)\n    tl.store(out_ptr0 + (x2), tmp6, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/o2/co2ghbebbgfcl3zz6yyidwlkg35yyzdccsbyuuume5xh6z25nklg.py\n# Topologically Sorted Source Nodes: [conv2d_1, relu_1], Original ATen: [aten.convolution, aten.relu]\n# Source node to ATen node mapping:\n#   conv2d_1 => convolution_1\n#   relu_1 => relu_1\n# Graph fragment:\n#   %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem, %primals_4, %primals_5, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})\n#   %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_1,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_convolution_relu_2(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 32768\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x2 = xindex\n    x1 = xindex // 1024\n    tmp0 = tl.load(in_out_ptr0 + (x2), None)\n    tmp1 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x2), tmp4, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/nr/cnrr3bcdexeaxpmycjefbjmq5udbfywfen2qerkrkzh7rm2cud7e.py\n# Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.max_pool2d_with_indices]\n# Source node to ATen node mapping:\n#   x_1 => _low_memory_max_pool2d_with_offsets_1, getitem_3\n# Graph fragment:\n#   %_low_memory_max_pool2d_with_offsets_1 : [num_users=2] = call_function[target=torch.ops.prims._low_memory_max_pool2d_with_offsets.default](args = (%relu_1, [2, 2], [2, 2], [0, 0], [1, 1], False), kwargs = {})\n#   %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_low_memory_max_pool2d_with_offsets_1, 1), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_max_pool2d_with_indices_3(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 8192\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x0 = (xindex % 16)\n    x1 = xindex // 16\n    x2 = xindex\n    tmp0 = tl.load(in_ptr0 + (2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp1 = tl.load(in_ptr0 + (1 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp7 = tl.load(in_ptr0 + (32 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp12 = tl.load(in_ptr0 + (33 + 2*x0 + 64*x1), None, eviction_policy='evict_last')\n    tmp2 = tmp1 > tmp0\n    tmp3 = tl.full([1], 1, tl.int8)\n    tmp4 = tl.full([1], 0, tl.int8)\n    tmp5 = tl.where(tmp2, tmp3, tmp4)\n    tmp6 = triton_helpers.maximum(tmp1, tmp0)\n    tmp8 = tmp7 > tmp6\n    tmp9 = tl.full([1], 2, tl.int8)\n    tmp10 = tl.where(tmp8, tmp9, tmp5)\n    tmp11 = triton_helpers.maximum(tmp7, tmp6)\n    tmp13 = tmp12 > tmp11\n    tmp14 = tl.full([1], 3, tl.int8)\n    tmp15 = tl.where(tmp13, tmp14, tmp10)\n    tmp16 = triton_helpers.maximum(tmp12, tmp11)\n    tl.store(out_ptr0 + (x2), tmp15, None)\n    tl.store(out_ptr1 + (x2), tmp16, None)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/zl/czlxijcx7yrdnigzjgtdojyzcuvt6ms4mhm7rfiho6n3r6lhlx36.py\n# Topologically Sorted Source Nodes: [linear, x_3], Original ATen: [aten.addmm, aten.relu]\n# Source node to ATen node mapping:\n#   linear => add_tensor\n#   x_3 => relu_2\n# Graph fragment:\n#   %add_tensor : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default, %primals_7), kwargs = {})\n#   %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused_addmm_relu_4(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask)\n    tmp2 = tmp0 + tmp1\n    tmp3 = tl.full([1], 0, tl.int32)\n    tmp4 = triton_helpers.maximum(tmp3, tmp2)\n    tl.store(in_out_ptr0 + (x0), tmp4, xmask)\n\n\n\n\n# kernel path: /tmp/torchinductor_sahanp/vk/cvkxam7itbvlwap5jowfxyic5roiadgbgwilulnktdwcbjv7erx4.py\n# Topologically Sorted Source Nodes: [log_softmax], Original ATen: [aten._log_softmax]\n# Source node to ATen node mapping:\n#   log_softmax => amax, exp, log, sub, sub_1, sum_1\n# Graph fragment:\n#   %amax : [num_users=1] = call_function[target=torch.ops.aten.amax.default](args = (%addmm_1, [1], True), kwargs = {})\n#   %sub : [num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%addmm_1, %amax), kwargs = {})\n#   %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%sub,), kwargs = {})\n#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%exp, [1], True), kwargs = {})\n#   %log : [num_users=1] = call_function[target=torch.ops.aten.log.default](args = (%sum_1,), kwargs = {})\n#   %sub_1 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub, %log), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_per_fused__log_softmax_5(in_out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):\n    xnumel = 1\n    r0_numel = 10\n    R0_BLOCK: tl.constexpr = 16\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_index = tl.arange(0, R0_BLOCK)[None, :]\n    r0_offset = 0\n    r0_mask = r0_index < r0_numel\n    roffset = r0_offset\n    rindex = r0_index\n    r0_0 = r0_index\n    tmp0 = tl.load(in_out_ptr0 + (r0_0), r0_mask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])\n    tmp3 = tl.where(r0_mask, tmp1, float(\"-inf\"))\n    tmp4 = triton_helpers.max2(tmp3, 1)[:, None]\n    tmp5 = tmp0 - tmp4\n    tmp6 = tl_math.exp(tmp5)\n    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])\n    tmp9 = tl.where(r0_mask, tmp7, 0)\n    tmp10 = tl.sum(tmp9, 1)[:, None]\n    tmp11 = tl_math.log(tmp10)\n    tmp12 = tmp5 - tmp11\n    tl.store(in_out_ptr0 + (tl.broadcast_to(r0_0, [XBLOCK, R0_BLOCK])), tmp12, r0_mask)\n\n\n\n\n\n\n\ndef call(args):\n    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9 = args\n    args.clear()\n    assert_size_stride(primals_1, (16, 3, 3, 3), (27, 9, 3, 1))\n    assert_size_stride(primals_2, (16, ), (1, ))\n    assert_size_stride(primals_3, (1, 3, 64, 64), (12288, 4096, 64, 1))\n    assert_size_stride(primals_4, (32, 16, 3, 3), (144, 9, 3, 1))\n    assert_size_stride(primals_5, (32, ), (1, ))\n    assert_size_stride(primals_6, (128, 8192), (8192, 1))\n    assert_size_stride(primals_7, (128, ), (1, ))\n    assert_size_stride(primals_8, (10, 128), (128, 1))\n    assert_size_stride(primals_9, (10, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        # Topologically Sorted Source Nodes: [conv2d], Original ATen: [aten.convolution]\n        buf0 = extern_kernels.convolution(primals_3, primals_1, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf0, (1, 16, 64, 64), (65536, 4096, 64, 1))\n        buf1 = buf0; del buf0  # reuse\n        # Topologically Sorted Source Nodes: [conv2d, relu], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_0[grid(65536)](buf1, primals_2, 65536, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_2\n        buf2 = empty_strided_cuda((1, 16, 32, 32), (16384, 1024, 32, 1), torch.float32)\n        buf3 = empty_strided_cuda((1, 16, 32, 32), (16384, 1024, 32, 1), torch.int8)\n        # Topologically Sorted Source Nodes: [x], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_1[grid(16384)](buf1, buf2, buf3, 16384, XBLOCK=128, num_warps=4, num_stages=1)\n        # Topologically Sorted Source Nodes: [conv2d_1], Original ATen: [aten.convolution]\n        buf4 = extern_kernels.convolution(buf2, primals_4, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)\n        assert_size_stride(buf4, (1, 32, 32, 32), (32768, 1024, 32, 1))\n        buf5 = buf4; del buf4  # reuse\n        # Topologically Sorted Source Nodes: [conv2d_1, relu_1], Original ATen: [aten.convolution, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_convolution_relu_2[grid(32768)](buf5, primals_5, 32768, XBLOCK=256, num_warps=4, num_stages=1)\n        del primals_5\n        buf6 = empty_strided_cuda((1, 32, 16, 16), (8192, 256, 16, 1), torch.int8)\n        buf7 = empty_strided_cuda((1, 32, 16, 16), (8192, 256, 16, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.max_pool2d_with_indices]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_max_pool2d_with_indices_3[grid(8192)](buf5, buf6, buf7, 8192, XBLOCK=256, num_warps=4, num_stages=1)\n        buf8 = empty_strided_cuda((1, 128), (128, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.addmm]\n        extern_kernels.mm(reinterpret_tensor(buf7, (1, 8192), (0, 1), 0), reinterpret_tensor(primals_6, (8192, 128), (1, 8192), 0), out=buf8)\n        buf9 = buf8; del buf8  # reuse\n        # Topologically Sorted Source Nodes: [linear, x_3], Original ATen: [aten.addmm, aten.relu]\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_addmm_relu_4[grid(128)](buf9, primals_7, 128, XBLOCK=128, num_warps=4, num_stages=1)\n        del primals_7\n        buf10 = empty_strided_cuda((1, 10), (10, 1), torch.float32)\n        # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.addmm]\n        extern_kernels.addmm(primals_9, buf9, reinterpret_tensor(primals_8, (128, 10), (1, 128), 0), alpha=1, beta=1, out=buf10)\n        del primals_9\n        buf13 = buf10; del buf10  # reuse\n        # Topologically Sorted Source Nodes: [log_softmax], Original ATen: [aten._log_softmax]\n        stream0 = get_raw_stream(0)\n        triton_per_fused__log_softmax_5[grid(1)](buf13, 1, 10, XBLOCK=1, num_warps=2, num_stages=1)\n    return (buf13, primals_1, primals_3, primals_4, buf1, buf2, buf3, buf5, buf6, reinterpret_tensor(buf7, (1, 8192), (8192, 1), 0), buf9, buf13, primals_8, primals_6, )\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((16, 3, 3, 3), (27, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_2 = rand_strided((16, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_3 = rand_strided((1, 3, 64, 64), (12288, 4096, 64, 1), device='cuda:0', dtype=torch.float32)\n    primals_4 = rand_strided((32, 16, 3, 3), (144, 9, 3, 1), device='cuda:0', dtype=torch.float32)\n    primals_5 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_6 = rand_strided((128, 8192), (8192, 1), device='cuda:0', dtype=torch.float32)\n    primals_7 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)\n    primals_8 = rand_strided((10, 128), (128, 1), device='cuda:0', dtype=torch.float32)\n    primals_9 = rand_strided((10, ), (1, ), device='cuda:0', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == \"__main__\":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main('None', benchmark_compiled_module)\n"
    }
]