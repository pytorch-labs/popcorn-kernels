[
    {
        "uuid": "000727f5-10f0-4186-ad1d-98f377c51f22",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LogSigmoid', 'Tanhshrink', 'PixelShuffle', 'BatchNorm3d', 'ModuleList', 'LSTMCell', 'PReLU', 'HuberLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.batch_norm = nn.BatchNorm3d(16)\n        self.pixel_shuffle = nn.PixelShuffle(2)\n        self.lstm_cell1 = nn.LSTMCell(64, 128)\n        self.lstm_cell2 = nn.LSTMCell(128, 64)\n        self.prelu = nn.PReLU()\n        self.module_list = nn.ModuleList([\n            nn.LogSigmoid(),\n            nn.Tanhshrink(),\n            nn.LogSigmoid(),\n            nn.Tanhshrink(),\n            nn.LogSigmoid()\n        ])\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Assume input is of shape (batch_size, channels, depth, height, width)\n        x = self.batch_norm(x)\n        \n        # Reshape for PixelShuffle\n        x = x.view(-1, 64, 32, 32)\n        x = self.pixel_shuffle(x)\n        \n        # Reshape for LSTMCell\n        x = x.view(-1, 64)\n        hx1 = torch.zeros(x.size(0), 128).to(x.device)\n        cx1 = torch.zeros(x.size(0), 128).to(x.device)\n        hx1, cx1 = self.lstm_cell1(x, (hx1, cx1))\n        \n        hx2 = torch.zeros(x.size(0), 64).to(x.device)\n        cx2 = torch.zeros(x.size(0), 64).to(x.device)\n        hx2, cx2 = self.lstm_cell2(hx1, (hx2, cx2))\n        \n        x = self.prelu(hx2)\n        \n        # Apply ModuleList layers\n        for layer in self.module_list:\n            x = layer(x)\n        \n        # Compute Huber loss (assuming target is zeros for demonstration)\n        target = torch.zeros_like(x)\n        loss = self.huber_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 16, 8, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimporttorch \nfromtorch ._inductor .select_algorithmimportextern_kernels \nimporttriton \nimporttriton .languageastl \nfromtorch ._inductor .runtime .triton_heuristicsimport (\ngrid ,\n)\nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__native_batch_norm_legit_functional_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \nr0_numel =8192 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \ntmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp0 =tl .load (in_ptr0 +(r0_1 +8192 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\ntmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\ntmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n)\ntmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\ntmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\ntmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\ntmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\ntmp2 =tmp5 [:,None ]\ntmp3 =tmp6 [:,None ]\ntmp4 =tmp7 [:,None ]\ntl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\ntl .store (out_ptr1 +(x0 ),tmp3 ,xmask )\ntl .store (out_ptr2 +(x0 ),tmp4 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_per_fused__native_batch_norm_legit_functional_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,out_ptr4 ,out_ptr6 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \nr0_numel =4 \nR0_BLOCK :tl .constexpr =4 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\nr0_offset =0 \nr0_mask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \nx0 =xindex \ntmp0 =tl .load (in_ptr0 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\ntmp1 =tl .load (in_ptr1 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\ntmp2 =tl .load (in_ptr2 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\ntmp25 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp30 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\ntmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\ntmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\ntmp7 =tl .where (xmask ,tmp3 ,0 )\ntmp8 =tl .where (xmask ,tmp4 ,0 )\ntmp9 =tl .where (xmask ,tmp5 ,0 )\ntmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\ntmp13 =tmp10 [:,None ]\ntmp14 =tmp11 [:,None ]\ntmp15 =tmp12 [:,None ]\ntmp16 =32768.0 \ntmp17 =tmp14 /tmp16 \ntmp18 =1e-05 \ntmp19 =tmp17 +tmp18 \ntmp20 =libdevice .rsqrt (tmp19 )\ntmp21 =1.000030518509476 \ntmp22 =tmp17 *tmp21 \ntmp23 =0.1 \ntmp24 =tmp22 *tmp23 \ntmp26 =0.9 \ntmp27 =tmp25 *tmp26 \ntmp28 =tmp24 +tmp27 \ntmp29 =tmp13 *tmp23 \ntmp31 =tmp30 *tmp26 \ntmp32 =tmp29 +tmp31 \ntl .store (out_ptr2 +(x0 ),tmp20 ,xmask )\ntl .store (out_ptr4 +(x0 ),tmp28 ,xmask )\ntl .store (out_ptr6 +(x0 ),tmp32 ,xmask )\ntl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\ntl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_pixel_shuffle_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    ynumel =262144 \nxnumel =2 \nyoffset =(tl .program_id (1 )+tl .program_id (2 )*tl .num_programs (1 ))*YBLOCK \nyindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\nymask =yindex <ynumel \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nx5 =xindex \ny0 =(yindex %32 )\ny1 =((yindex //32 )%2 )\ny2 =((yindex //64 )%32 )\ny6 =yindex //2048 \ny3 =((yindex //2048 )%16 )\ny4 =yindex //32768 \ny7 =yindex \ntmp0 =tl .load (in_ptr0 +(y0 +32 *y2 +1024 *x5 +2048 *y1 +4096 *y6 +4096 *((y0 +32 *y2 +1024 *x5 +2048 *y1 )//4096 )),xmask &ymask ,eviction_policy ='evict_last')\ntmp1 =tl .load (in_ptr1 +(2 *y4 +((y0 +32 *y2 +1024 *x5 +2048 *y1 +4096 *y3 )//32768 )),xmask &ymask ,eviction_policy ='evict_last')\ntmp3 =tl .load (in_ptr2 +(2 *y4 +((y0 +32 *y2 +1024 *x5 +2048 *y1 +4096 *y3 )//32768 )),xmask &ymask ,eviction_policy ='evict_last')\ntmp10 =tl .load (in_ptr3 +(2 *y4 +((y0 +32 *y2 +1024 *x5 +2048 *y1 +4096 *y3 )//32768 )),xmask &ymask ,eviction_policy ='evict_last')\ntmp12 =tl .load (in_ptr4 +(2 *y4 +((y0 +32 *y2 +1024 *x5 +2048 *y1 +4096 *y3 )//32768 )),xmask &ymask ,eviction_policy ='evict_last')\ntmp2 =tmp0 -tmp1 \ntmp4 =32768.0 \ntmp5 =tmp3 /tmp4 \ntmp6 =1e-05 \ntmp7 =tmp5 +tmp6 \ntmp8 =libdevice .rsqrt (tmp7 )\ntmp9 =tmp2 *tmp8 \ntmp11 =tmp9 *tmp10 \ntmp13 =tmp11 +tmp12 \ntl .store (out_ptr0 +(x5 +2 *y7 ),tmp13 ,xmask &ymask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused__to_copy_3 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1048576 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =tl .full ([XBLOCK ],True ,tl .int1 )\nx0 =xindex \ntmp0 =0.0 \ntl .store (out_ptr0 +(x0 ),tmp0 ,None )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused__to_copy_4 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =524288 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =tl .full ([XBLOCK ],True ,tl .int1 )\nx0 =xindex \ntmp0 =0.0 \ntl .store (out_ptr0 +(x0 ),tmp0 ,None )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__prelu_kernel_huber_loss_huber_loss_backward_log_sigmoid_forward_sub_tanh_5 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \nr0_numel =8192 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \ntmp3 =tl .load (in_ptr1 +(0 ))\ntmp4 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n_tmp39 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp0 =tl .load (in_ptr0 +(r0_1 +8192 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp1 =0.0 \ntmp2 =tmp0 >tmp1 \ntmp5 =tmp4 *tmp0 \ntmp6 =tl .where (tmp2 ,tmp0 ,tmp5 )\ntmp7 =triton_helpers .minimum (tmp1 ,tmp6 )\ntmp8 =tl_math .abs (tmp6 )\ntmp9 =-tmp8 \ntmp10 =tl_math .exp (tmp9 )\ntmp11 =libdevice .log1p (tmp10 )\ntmp12 =tmp7 -tmp11 \ntmp13 =libdevice .tanh (tmp12 )\ntmp14 =tmp12 -tmp13 \ntmp15 =triton_helpers .minimum (tmp1 ,tmp14 )\ntmp16 =tl_math .abs (tmp14 )\ntmp17 =-tmp16 \ntmp18 =tl_math .exp (tmp17 )\ntmp19 =libdevice .log1p (tmp18 )\ntmp20 =tmp15 -tmp19 \ntmp21 =libdevice .tanh (tmp20 )\ntmp22 =tmp20 -tmp21 \ntmp23 =triton_helpers .minimum (tmp1 ,tmp22 )\ntmp24 =tl_math .abs (tmp22 )\ntmp25 =-tmp24 \ntmp26 =tl_math .exp (tmp25 )\ntmp27 =libdevice .log1p (tmp26 )\ntmp28 =tmp23 -tmp27 \ntmp29 =tl_math .abs (tmp28 )\ntmp30 =1.0 \ntmp31 =tmp29 <tmp30 \ntmp32 =0.5 \ntmp33 =tmp29 *tmp32 \ntmp34 =tmp33 *tmp29 \ntmp35 =tmp29 -tmp32 \ntmp36 =tmp35 *tmp30 \ntmp37 =tl .where (tmp31 ,tmp34 ,tmp36 )\ntmp38 =tl .broadcast_to (tmp37 ,[XBLOCK ,R0_BLOCK ])\ntmp40 =_tmp39 +tmp38 \n_tmp39 =tl .where (r0_mask &xmask ,tmp40 ,_tmp39 )\ntmp39 =tl .sum (_tmp39 ,1 )[:,None ]\ntl .store (out_ptr0 +(x0 ),tmp39 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_per_fused__prelu_kernel_huber_loss_huber_loss_backward_log_sigmoid_forward_sub_tanh_6 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =1 \nr0_numel =64 \nR0_BLOCK :tl .constexpr =64 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\nr0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\nr0_offset =0 \nr0_mask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\nroffset =r0_offset \nrindex =r0_index \nr0_0 =r0_index \ntmp0 =tl .load (in_ptr0 +(r0_0 ),None )\ntmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\ntmp3 =tl .sum (tmp1 ,1 )[:,None ]\ntmp4 =524288.0 \ntmp5 =tmp3 /tmp4 \ntl .debug_barrier ()\ntl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp5 ,None )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_add_7 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =tl .full ([XBLOCK ],True ,tl .int1 )\ntmp0 =tl .load (in_ptr0 +(0 ))\ntmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\ntmp2 =tl .full ([1 ],1 ,tl .int64 )\ntmp3 =tmp1 +tmp2 \ntl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndefcall (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 =args \nargs .clear ()\nassert_size_stride (primals_1 ,(1 ,16 ,8 ,64 ,64 ),(524288 ,32768 ,4096 ,64 ,1 ))\nassert_size_stride (primals_2 ,(),())\nassert_size_stride (primals_3 ,(16 ,),(1 ,))\nassert_size_stride (primals_4 ,(16 ,),(1 ,))\nassert_size_stride (primals_5 ,(16 ,),(1 ,))\nassert_size_stride (primals_6 ,(16 ,),(1 ,))\nassert_size_stride (primals_7 ,(512 ,64 ),(64 ,1 ))\nassert_size_stride (primals_8 ,(512 ,128 ),(128 ,1 ))\nassert_size_stride (primals_9 ,(512 ,),(1 ,))\nassert_size_stride (primals_10 ,(512 ,),(1 ,))\nassert_size_stride (primals_11 ,(256 ,128 ),(128 ,1 ))\nassert_size_stride (primals_12 ,(256 ,64 ),(64 ,1 ))\nassert_size_stride (primals_13 ,(256 ,),(1 ,))\nassert_size_stride (primals_14 ,(256 ,),(1 ,))\nassert_size_stride (primals_15 ,(1 ,),(1 ,))\nwithtorch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\nbuf0 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ,4 ),(64 ,4 ,64 ,64 ,64 ,1 ),torch .float32 )\nbuf1 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ,4 ),(64 ,4 ,64 ,64 ,64 ,1 ),torch .float32 )\nbuf2 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ,4 ),(64 ,4 ,64 ,64 ,64 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_red_fused__native_batch_norm_legit_functional_0 [grid (64 )](primals_1 ,buf0 ,buf1 ,buf2 ,64 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\nbuf3 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ),(16 ,1 ,16 ,16 ,16 ),torch .float32 )\nbuf4 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ),(16 ,1 ,16 ,16 ,16 ),torch .float32 )\nbuf6 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ),(16 ,1 ,16 ,16 ,16 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_per_fused__native_batch_norm_legit_functional_1 [grid (16 )](buf0 ,buf1 ,buf2 ,primals_4 ,primals_3 ,buf3 ,buf4 ,buf6 ,primals_4 ,primals_3 ,16 ,4 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\ndelbuf0 \ndelbuf1 \ndelprimals_3 \ndelprimals_4 \nbuf7 =empty_strided_cuda ((8 ,16 ,32 ,2 ,32 ,2 ),(65536 ,4096 ,128 ,64 ,2 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_pixel_shuffle_2 [grid (262144 ,2 )](primals_1 ,buf3 ,buf4 ,primals_5 ,primals_6 ,buf7 ,262144 ,2 ,XBLOCK =2 ,YBLOCK =1024 ,num_warps =8 ,num_stages =1 )\ndelbuf4 \ndelprimals_5 \ndelprimals_6 \nbuf8 =empty_strided_cuda ((8192 ,128 ),(128 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused__to_copy_3 [grid (1048576 )](buf8 ,1048576 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\nbuf9 =empty_strided_cuda ((8192 ,512 ),(512 ,1 ),torch .float32 )\n\nextern_kernels .mm (reinterpret_tensor (buf7 ,(8192 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf9 )\nbuf10 =empty_strided_cuda ((8192 ,512 ),(512 ,1 ),torch .float32 )\n\nextern_kernels .mm (buf8 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf10 )\ndelprimals_8 \n\nbuf11 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf9 ,buf10 ,buf8 ,primals_9 ,primals_10 )\ndelbuf10 \ndelbuf9 \ndelprimals_10 \ndelprimals_9 \nbuf12 =buf11 [0 ]\nbuf13 =buf11 [1 ]\nbuf14 =buf11 [2 ]\ndelbuf11 \nbuf15 =empty_strided_cuda ((8192 ,64 ),(64 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused__to_copy_4 [grid (524288 )](buf15 ,524288 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\nbuf16 =empty_strided_cuda ((8192 ,256 ),(256 ,1 ),torch .float32 )\n\nextern_kernels .mm (buf12 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf16 )\nbuf17 =empty_strided_cuda ((8192 ,256 ),(256 ,1 ),torch .float32 )\n\nextern_kernels .mm (buf15 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf17 )\ndelprimals_12 \n\nbuf18 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf16 ,buf17 ,buf15 ,primals_13 ,primals_14 )\ndelbuf16 \ndelbuf17 \ndelprimals_13 \ndelprimals_14 \nbuf19 =buf18 [0 ]\nbuf20 =buf18 [1 ]\nbuf21 =buf18 [2 ]\ndelbuf18 \nbuf22 =reinterpret_tensor (buf2 ,(64 ,),(1 ,),0 );delbuf2 \n\nstream0 =get_raw_stream (0 )\ntriton_red_fused__prelu_kernel_huber_loss_huber_loss_backward_log_sigmoid_forward_sub_tanh_5 [grid (64 )](buf19 ,primals_15 ,buf22 ,64 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\nbuf23 =empty_strided_cuda ((),(),torch .float32 )\nbuf32 =buf23 ;delbuf23 \n\nstream0 =get_raw_stream (0 )\ntriton_per_fused__prelu_kernel_huber_loss_huber_loss_backward_log_sigmoid_forward_sub_tanh_6 [grid (1 )](buf32 ,buf22 ,1 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\ndelbuf22 \n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_add_7 [grid (1 )](primals_2 ,primals_2 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\ndelprimals_2 \nreturn (buf32 ,primals_1 ,primals_15 ,reinterpret_tensor (buf6 ,(16 ,),(1 ,),0 ),reinterpret_tensor (buf7 ,(8192 ,64 ),(64 ,1 ),0 ),buf8 ,buf12 ,buf13 ,buf14 ,buf15 ,buf19 ,buf20 ,buf21 ,primals_11 ,primals_7 ,reinterpret_tensor (buf3 ,(1 ,16 ,1 ,1 ,1 ),(16 ,1 ,1 ,1 ,1 ),0 ),)\n\ndefbenchmark_compiled_module (times =10 ,repeat =10 ):\n    fromtorch ._dynamo .testingimportrand_strided \nfromtorch ._inductor .utilsimportprint_performance \nprimals_1 =rand_strided ((1 ,16 ,8 ,64 ,64 ),(524288 ,32768 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_2 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\nprimals_3 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_4 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_5 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_6 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_7 =rand_strided ((512 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_8 =rand_strided ((512 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_9 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_10 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_11 =rand_strided ((256 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_12 =rand_strided ((256 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_13 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_14 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_15 =rand_strided ((1 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nfn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ])\nreturnprint_performance (fn ,times =times ,repeat =repeat )\n\nif__name__ ==\"__main__\":\n    fromtorch ._inductor .wrapper_benchmarkimportcompiled_module_main \ncompiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "002f98d6-2f12-4a9e-adb4-ca980b4b77fd",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool2d', 'ConvTranspose3d', 'HuberLoss', 'BatchNorm1d', 'Dropout3d', 'GRU', 'PixelShuffle', 'Sequential', 'Linear', 'GroupNorm']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.convtranspose3d = nn.ConvTranspose3d(1, 10, kernel_size=3, stride=2)\n        self.batchnorm1d = nn.BatchNorm1d(100)\n        self.dropout3d = nn.Dropout3d(p=0.5)\n        self.gru = nn.GRU(input_size=100, hidden_size=50, num_layers=2, batch_first=True)\n        self.pixelshuffle = nn.PixelShuffle(2)\n        self.sequential = nn.Sequential(\n            nn.Linear(50, 100),\n            nn.ReLU(),\n            nn.Linear(100, 50)\n        )\n        self.linear = nn.Linear(50, 10)\n        self.groupnorm = nn.GroupNorm(num_groups=2, num_channels=10)\n        self.huberloss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Assuming input is 4D (batch, channels, height, width)\n        x = self.maxpool2d(x)\n        \n        # Reshape for ConvTranspose3d\n        x = x.unsqueeze(1)  # Add a dummy depth dimension\n        x = self.convtranspose3d(x)\n        \n        # Reshape for Dropout3d\n        x = self.dropout3d(x)\n        \n        # Reshape for GroupNorm\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions\n        x = self.groupnorm(x)\n        \n        # Reshape for GRU\n        x = x.view(x.size(0), x.size(1), -1).transpose(1, 2)  # (batch, seq_len, features)\n        x, _ = self.gru(x)\n        \n        # Reshape for PixelShuffle\n        x = x.transpose(1, 2).unsqueeze(-1)  # Add a dummy dimension for PixelShuffle\n        x = self.pixelshuffle(x)\n        \n        # Reshape for BatchNorm1d\n        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n        x = self.batchnorm1d(x)\n        \n        # Sequential layers\n        x = self.sequential(x)\n        \n        # Final Linear layer\n        x = self.linear(x)\n        \n        # HuberLoss is typically used for loss computation, not in forward pass\n        # So we return the output and let the user compute the loss if needed\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Assuming 3 channels for input\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimporttorch \nfromtorch ._inductor .select_algorithmimportextern_kernels \nimporttriton \nimporttriton .languageastl \nfromtorch ._inductor .runtime .triton_heuristicsimport (\ngrid ,\n)\nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3072 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx0 =(xindex %32 )\nx1 =xindex //32 \nx2 =xindex \ntmp0 =tl .load (in_ptr0 +(2 *x0 +128 *x1 ),xmask ,eviction_policy ='evict_last')\ntmp1 =tl .load (in_ptr0 +(1 +2 *x0 +128 *x1 ),xmask ,eviction_policy ='evict_last')\ntmp3 =tl .load (in_ptr0 +(64 +2 *x0 +128 *x1 ),xmask ,eviction_policy ='evict_last')\ntmp5 =tl .load (in_ptr0 +(65 +2 *x0 +128 *x1 ),xmask ,eviction_policy ='evict_last')\ntmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\ntmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\ntmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\ntl .store (out_ptr0 +(x2 ),tmp6 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_convolution_1 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =295750 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx2 =xindex \nx1 =xindex //29575 \ntmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\ntmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\ntmp2 =tmp0 +tmp1 \ntl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_bernoulli_2 (in_ptr0 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx0 =xindex \ntmp0 =tl .load (in_ptr0 +load_seed_offset )\ntmp1 =x0 \ntmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\ntmp3 =0.5 \ntmp4 =tmp2 <tmp3 \ntl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused_native_group_norm_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =38 \nr0_numel =7783 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =(xindex %19 )\nx1 =xindex //19 \ntmp20_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp20_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp20_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\nx3 =xindex \nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_2 =r0_index \ntmp0 =r0_2 +7783 *x0 \ntmp1 =tl .full ([1 ,1 ],147875 ,tl .int32 )\ntmp2 =tmp0 <tmp1 \ntmp3 =tl .load (in_ptr0 +(147875 *x1 +(((r0_2 +7783 *x0 )%147875 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp4 =tl .load (in_ptr1 +(5 *x1 +((((r0_2 +7783 *x0 )//29575 )%5 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 ).to (tl .int1 )\ntmp5 =tmp4 .to (tl .float32 )\ntmp6 =2.0 \ntmp7 =tmp5 *tmp6 \ntmp8 =tmp3 *tmp7 \ntmp9 =tl .full (tmp8 .shape ,0 ,tmp8 .dtype )\ntmp10 =tl .where (tmp2 ,tmp8 ,tmp9 )\ntmp11 =0.0 \ntmp12 =tl .full (tmp11 .shape ,0 ,tmp11 .dtype )\ntmp13 =tl .where (tmp2 ,tmp11 ,tmp12 )\ntmp14 =1.0 \ntmp15 =tl .full (tmp14 .shape ,0 ,tmp14 .dtype )\ntmp16 =tl .where (tmp2 ,tmp14 ,tmp15 )\ntmp17 =tl .broadcast_to (tmp10 ,[XBLOCK ,R0_BLOCK ])\ntmp18 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\ntmp19 =tl .broadcast_to (tmp16 ,[XBLOCK ,R0_BLOCK ])\ntmp20_mean_next ,tmp20_m2_next ,tmp20_weight_next =triton_helpers .welford_combine (\ntmp20_mean ,tmp20_m2 ,tmp20_weight ,\ntmp17 ,tmp18 ,tmp19 \n)\ntmp20_mean =tl .where (r0_mask &xmask ,tmp20_mean_next ,tmp20_mean )\ntmp20_m2 =tl .where (r0_mask &xmask ,tmp20_m2_next ,tmp20_m2 )\ntmp20_weight =tl .where (r0_mask &xmask ,tmp20_weight_next ,tmp20_weight )\ntmp23 ,tmp24 ,tmp25 =triton_helpers .welford (tmp20_mean ,tmp20_m2 ,tmp20_weight ,1 )\ntmp20 =tmp23 [:,None ]\ntmp21 =tmp24 [:,None ]\ntmp22 =tmp25 [:,None ]\ntl .store (out_ptr0 +(x3 ),tmp20 ,xmask )\ntl .store (out_ptr1 +(x3 ),tmp21 ,xmask )\ntl .store (out_ptr2 +(x3 ),tmp22 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_per_fused_native_group_norm_4 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =2 \nr0_numel =19 \nR0_BLOCK :tl .constexpr =32 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\nr0_offset =0 \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \nx0 =xindex \ntmp0 =tl .load (in_ptr0 +(r0_1 +19 *x0 ),r0_mask &xmask ,other =0.0 )\ntmp1 =tl .load (in_ptr1 +(r0_1 +19 *x0 ),r0_mask &xmask ,other =0.0 )\ntmp2 =tl .load (in_ptr2 +(r0_1 +19 *x0 ),r0_mask &xmask ,other =0.0 )\ntmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\ntmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\ntmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\ntmp7 =tl .where (r0_mask &xmask ,tmp3 ,0 )\ntmp8 =tl .where (r0_mask &xmask ,tmp4 ,0 )\ntmp9 =tl .where (r0_mask &xmask ,tmp5 ,0 )\ntmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\ntmp13 =tmp10 [:,None ]\ntmp14 =tmp11 [:,None ]\ntmp15 =tmp12 [:,None ]\ntmp16 =147875.0 \ntmp17 =tmp14 /tmp16 \ntmp18 =1e-05 \ntmp19 =tmp17 +tmp18 \ntmp20 =libdevice .rsqrt (tmp19 )\ntl .store (out_ptr2 +(x0 ),tmp20 ,xmask )\ntl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\ntl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_native_group_norm_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =295750 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx2 =xindex \nx1 =xindex //29575 \ntmp0 =tl .load (in_ptr0 +(x2 ),xmask )\ntmp1 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last').to (tl .int1 )\ntmp6 =tl .load (in_ptr2 +(x1 //5 ),xmask ,eviction_policy ='evict_last')\ntmp8 =tl .load (in_ptr3 +(x1 //5 ),xmask ,eviction_policy ='evict_last')\ntmp15 =tl .load (in_ptr4 +(x1 ),xmask ,eviction_policy ='evict_last')\ntmp17 =tl .load (in_ptr5 +(x1 ),xmask ,eviction_policy ='evict_last')\ntmp2 =tmp1 .to (tl .float32 )\ntmp3 =2.0 \ntmp4 =tmp2 *tmp3 \ntmp5 =tmp0 *tmp4 \ntmp7 =tmp5 -tmp6 \ntmp9 =147875.0 \ntmp10 =tmp8 /tmp9 \ntmp11 =1e-05 \ntmp12 =tmp10 +tmp11 \ntmp13 =libdevice .rsqrt (tmp12 )\ntmp14 =tmp7 *tmp13 \ntmp16 =tmp14 *tmp15 \ntmp18 =tmp16 +tmp17 \ntl .store (out_ptr0 +(x2 ),tmp18 ,xmask )\n\ndefcall (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \nargs .clear ()\nassert_size_stride (primals_1 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\nassert_size_stride (primals_2 ,(1 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ))\nassert_size_stride (primals_3 ,(10 ,),(1 ,))\nassert_size_stride (primals_4 ,(10 ,),(1 ,))\nassert_size_stride (primals_5 ,(10 ,),(1 ,))\nwithtorch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\nbuf0 =empty_strided_cuda ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_max_pool2d_with_indices_0 [grid (3072 )](primals_1 ,buf0 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\ndelprimals_1 \n\nbuf1 =extern_kernels .convolution (reinterpret_tensor (buf0 ,(1 ,1 ,3 ,32 ,32 ),(0 ,0 ,1024 ,32 ,1 ),0 ),primals_2 ,stride =(2 ,2 ,2 ),padding =(0 ,0 ,0 ),dilation =(1 ,1 ,1 ),transposed =True ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\nassert_size_stride (buf1 ,(1 ,10 ,7 ,65 ,65 ),(295750 ,29575 ,4225 ,65 ,1 ))\nbuf2 =buf1 ;delbuf1 \n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_convolution_1 [grid (295750 )](buf2 ,primals_3 ,295750 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\ndelprimals_3 \nbuf3 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\naten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf3 )\nbuf5 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,1 ,1 ,1 ),torch .bool )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_bernoulli_2 [grid (10 )](buf3 ,buf5 ,0 ,10 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\ndelbuf3 \nbuf6 =empty_strided_cuda ((1 ,2 ,1 ,1 ,19 ),(38 ,19 ,38 ,38 ,1 ),torch .float32 )\nbuf7 =empty_strided_cuda ((1 ,2 ,1 ,1 ,19 ),(38 ,19 ,38 ,38 ,1 ),torch .float32 )\nbuf8 =empty_strided_cuda ((1 ,2 ,1 ,1 ,19 ),(38 ,19 ,38 ,38 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_red_fused_native_group_norm_3 [grid (38 )](buf2 ,buf5 ,buf6 ,buf7 ,buf8 ,38 ,7783 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\nbuf9 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,2 ,2 ),torch .float32 )\nbuf10 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,2 ,2 ),torch .float32 )\nbuf12 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,2 ,2 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_per_fused_native_group_norm_4 [grid (2 )](buf6 ,buf7 ,buf8 ,buf9 ,buf10 ,buf12 ,2 ,19 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\ndelbuf6 \ndelbuf7 \ndelbuf8 \nbuf13 =empty_strided_cuda ((1 ,10 ,29575 ),(295750 ,29575 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_native_group_norm_5 [grid (295750 )](buf2 ,buf5 ,buf9 ,buf10 ,primals_4 ,primals_5 ,buf13 ,295750 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\ndelbuf10 \ndelprimals_5 \nreturn (reinterpret_tensor (buf13 ,(1 ,29575 ,10 ),(295750 ,1 ,29575 ),0 ),primals_2 ,primals_4 ,reinterpret_tensor (buf0 ,(1 ,1 ,3 ,32 ,32 ),(3072 ,3072 ,1024 ,32 ,1 ),0 ),buf2 ,buf5 ,reinterpret_tensor (buf9 ,(1 ,2 ),(2 ,1 ),0 ),reinterpret_tensor (buf12 ,(1 ,2 ),(2 ,1 ),0 ),)\n\ndefbenchmark_compiled_module (times =10 ,repeat =10 ):\n    fromtorch ._dynamo .testingimportrand_strided \nfromtorch ._inductor .utilsimportprint_performance \nprimals_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_2 =rand_strided ((1 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_3 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_4 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nfn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\nreturnprint_performance (fn ,times =times ,repeat =repeat )\n\nif__name__ ==\"__main__\":\n    fromtorch ._inductor .wrapper_benchmarkimportcompiled_module_main \ncompiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "007d3203-ddaa-4344-ad24-5438938e78fd",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Mish', 'ConstantPad2d', 'LazyConv1d', 'LogSigmoid', 'LazyBatchNorm1d', 'Hardswish']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad2d(1, 0.5)\n        self.conv1 = nn.LazyConv1d(out_channels=32, kernel_size=3)\n        self.bn1 = nn.LazyBatchNorm1d()\n        self.conv2 = nn.LazyConv1d(out_channels=64, kernel_size=3)\n        self.bn2 = nn.LazyBatchNorm1d()\n        self.conv3 = nn.LazyConv1d(out_channels=128, kernel_size=3)\n        self.bn3 = nn.LazyBatchNorm1d()\n        self.conv4 = nn.LazyConv1d(out_channels=256, kernel_size=3)\n        self.bn4 = nn.LazyBatchNorm1d()\n        self.conv5 = nn.LazyConv1d(out_channels=512, kernel_size=3)\n        self.bn5 = nn.LazyBatchNorm1d()\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, sequence_length)\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.mish(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.hardswish(x)\n        \n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = F.mish(x)\n        \n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = F.hardswish(x)\n        \n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = F.logsigmoid(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimporttorch \nfromtorch ._inductor .select_algorithmimportextern_kernels \nimporttriton \nimporttriton .languageastl \nfromtorch ._inductor .runtime .triton_heuristicsimport (\ngrid ,\n)\nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =13068 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx1 =((xindex //66 )%66 )\nx0 =(xindex %66 )\nx2 =xindex //4356 \nx4 =xindex \ntmp0 =(-1 )+x1 \ntmp1 =tl .full ([1 ],0 ,tl .int64 )\ntmp2 =tmp0 >=tmp1 \ntmp3 =tl .full ([1 ],64 ,tl .int64 )\ntmp4 =tmp0 <tmp3 \ntmp5 =(-1 )+x0 \ntmp6 =tmp5 >=tmp1 \ntmp7 =tmp5 <tmp3 \ntmp8 =tmp2 &tmp4 \ntmp9 =tmp8 &tmp6 \ntmp10 =tmp9 &tmp7 \ntmp11 =tl .load (in_ptr0 +((-65 )+x0 +64 *x1 +4096 *x2 ),tmp10 &xmask ,other =0.5 )\ntl .store (out_ptr0 +(x4 ),tmp11 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__native_batch_norm_legit_functional_convolution_mish_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr4 ,out_ptr6 ,out_ptr8 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =32 \nr0_numel =4354 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \ntmp1 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp0 =tl .load (in_out_ptr0 +(r0_1 +4354 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp2 =tmp0 +tmp1 \ntmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\ntmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\ntmp3 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n)\ntmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\ntmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\ntmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\ntl .store (in_out_ptr0 +(r0_1 +4354 *x0 ),tmp2 ,r0_mask &xmask )\ntmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\ntmp4 =tmp7 [:,None ]\ntmp5 =tmp8 [:,None ]\ntmp6 =tmp9 [:,None ]\ntl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\ntmp19 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp24 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp10 =4354.0 \ntmp11 =tmp5 /tmp10 \ntmp12 =1e-05 \ntmp13 =tmp11 +tmp12 \ntmp14 =libdevice .rsqrt (tmp13 )\ntmp15 =1.000229726625316 \ntmp16 =tmp11 *tmp15 \ntmp17 =0.1 \ntmp18 =tmp16 *tmp17 \ntmp20 =0.9 \ntmp21 =tmp19 *tmp20 \ntmp22 =tmp18 +tmp21 \ntmp23 =tmp4 *tmp17 \ntmp25 =tmp24 *tmp20 \ntmp26 =tmp23 +tmp25 \ntl .store (out_ptr2 +(x0 ),tmp14 ,xmask )\ntl .store (out_ptr4 +(x0 ),tmp22 ,xmask )\ntl .store (out_ptr6 +(x0 ),tmp26 ,xmask )\ntmp30 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp32 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp27 =tl .load (in_out_ptr0 +(r0_1 +4354 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp28 =tmp27 -tmp4 \ntmp29 =tmp28 *tmp14 \ntmp31 =tmp29 *tmp30 \ntmp33 =tmp31 +tmp32 \ntmp34 =20.0 \ntmp35 =tmp33 >tmp34 \ntmp36 =tl_math .exp (tmp33 )\ntmp37 =libdevice .log1p (tmp36 )\ntmp38 =tl .where (tmp35 ,tmp33 ,tmp37 )\ntmp39 =libdevice .tanh (tmp38 )\ntmp40 =tmp33 *tmp39 \ntl .store (out_ptr8 +(r0_1 +4354 *x0 ),tmp40 ,r0_mask &xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__native_batch_norm_legit_functional_convolution_hardswish_2 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr4 ,out_ptr6 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \nr0_numel =4352 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \ntmp1 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp0 =tl .load (in_out_ptr0 +(r0_1 +4352 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp2 =tmp0 +tmp1 \ntmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\ntmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\ntmp3 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n)\ntmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\ntmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\ntmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\ntl .store (in_out_ptr0 +(r0_1 +4352 *x0 ),tmp2 ,r0_mask &xmask )\ntmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\ntmp4 =tmp7 [:,None ]\ntmp5 =tmp8 [:,None ]\ntmp6 =tmp9 [:,None ]\ntl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\ntmp19 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp24 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp10 =4352.0 \ntmp11 =tmp5 /tmp10 \ntmp12 =1e-05 \ntmp13 =tmp11 +tmp12 \ntmp14 =libdevice .rsqrt (tmp13 )\ntmp15 =1.0002298322224776 \ntmp16 =tmp11 *tmp15 \ntmp17 =0.1 \ntmp18 =tmp16 *tmp17 \ntmp20 =0.9 \ntmp21 =tmp19 *tmp20 \ntmp22 =tmp18 +tmp21 \ntmp23 =tmp4 *tmp17 \ntmp25 =tmp24 *tmp20 \ntmp26 =tmp23 +tmp25 \ntl .store (out_ptr2 +(x0 ),tmp14 ,xmask )\ntl .store (out_ptr4 +(x0 ),tmp22 ,xmask )\ntl .store (out_ptr6 +(x0 ),tmp26 ,xmask )\ntmp30 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp32 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp27 =tl .load (in_out_ptr0 +(r0_1 +4352 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp28 =tmp27 -tmp4 \ntmp29 =tmp28 *tmp14 \ntmp31 =tmp29 *tmp30 \ntmp33 =tmp31 +tmp32 \ntmp34 =3.0 \ntmp35 =tmp33 +tmp34 \ntmp36 =0.0 \ntmp37 =triton_helpers .maximum (tmp35 ,tmp36 )\ntmp38 =6.0 \ntmp39 =triton_helpers .minimum (tmp37 ,tmp38 )\ntmp40 =tmp33 *tmp39 \ntmp41 =0.16666666666666666 \ntmp42 =tmp40 *tmp41 \ntl .store (in_out_ptr1 +(r0_1 +4352 *x0 ),tmp42 ,r0_mask &xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__native_batch_norm_legit_functional_convolution_mish_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr4 ,out_ptr6 ,out_ptr8 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =128 \nr0_numel =4350 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \ntmp1 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp0 =tl .load (in_out_ptr0 +(r0_1 +4350 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp2 =tmp0 +tmp1 \ntmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\ntmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\ntmp3 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n)\ntmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\ntmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\ntmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\ntl .store (in_out_ptr0 +(r0_1 +4350 *x0 ),tmp2 ,r0_mask &xmask )\ntmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\ntmp4 =tmp7 [:,None ]\ntmp5 =tmp8 [:,None ]\ntmp6 =tmp9 [:,None ]\ntl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\ntmp19 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp24 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp10 =4350.0 \ntmp11 =tmp5 /tmp10 \ntmp12 =1e-05 \ntmp13 =tmp11 +tmp12 \ntmp14 =libdevice .rsqrt (tmp13 )\ntmp15 =1.0002299379167625 \ntmp16 =tmp11 *tmp15 \ntmp17 =0.1 \ntmp18 =tmp16 *tmp17 \ntmp20 =0.9 \ntmp21 =tmp19 *tmp20 \ntmp22 =tmp18 +tmp21 \ntmp23 =tmp4 *tmp17 \ntmp25 =tmp24 *tmp20 \ntmp26 =tmp23 +tmp25 \ntl .store (out_ptr2 +(x0 ),tmp14 ,xmask )\ntl .store (out_ptr4 +(x0 ),tmp22 ,xmask )\ntl .store (out_ptr6 +(x0 ),tmp26 ,xmask )\ntmp30 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp32 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp27 =tl .load (in_out_ptr0 +(r0_1 +4350 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp28 =tmp27 -tmp4 \ntmp29 =tmp28 *tmp14 \ntmp31 =tmp29 *tmp30 \ntmp33 =tmp31 +tmp32 \ntmp34 =20.0 \ntmp35 =tmp33 >tmp34 \ntmp36 =tl_math .exp (tmp33 )\ntmp37 =libdevice .log1p (tmp36 )\ntmp38 =tl .where (tmp35 ,tmp33 ,tmp37 )\ntmp39 =libdevice .tanh (tmp38 )\ntmp40 =tmp33 *tmp39 \ntl .store (out_ptr8 +(r0_1 +4350 *x0 ),tmp40 ,r0_mask &xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__native_batch_norm_legit_functional_convolution_hardswish_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr4 ,out_ptr6 ,out_ptr8 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =256 \nr0_numel =4348 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \ntmp1 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp0 =tl .load (in_out_ptr0 +(r0_1 +4348 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp2 =tmp0 +tmp1 \ntmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\ntmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\ntmp3 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n)\ntmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\ntmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\ntmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\ntl .store (in_out_ptr0 +(r0_1 +4348 *x0 ),tmp2 ,r0_mask &xmask )\ntmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\ntmp4 =tmp7 [:,None ]\ntmp5 =tmp8 [:,None ]\ntmp6 =tmp9 [:,None ]\ntl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\ntmp19 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp24 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp10 =4348.0 \ntmp11 =tmp5 /tmp10 \ntmp12 =1e-05 \ntmp13 =tmp11 +tmp12 \ntmp14 =libdevice .rsqrt (tmp13 )\ntmp15 =1.0002300437083045 \ntmp16 =tmp11 *tmp15 \ntmp17 =0.1 \ntmp18 =tmp16 *tmp17 \ntmp20 =0.9 \ntmp21 =tmp19 *tmp20 \ntmp22 =tmp18 +tmp21 \ntmp23 =tmp4 *tmp17 \ntmp25 =tmp24 *tmp20 \ntmp26 =tmp23 +tmp25 \ntl .store (out_ptr2 +(x0 ),tmp14 ,xmask )\ntl .store (out_ptr4 +(x0 ),tmp22 ,xmask )\ntl .store (out_ptr6 +(x0 ),tmp26 ,xmask )\ntmp30 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp32 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp27 =tl .load (in_out_ptr0 +(r0_1 +4348 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp28 =tmp27 -tmp4 \ntmp29 =tmp28 *tmp14 \ntmp31 =tmp29 *tmp30 \ntmp33 =tmp31 +tmp32 \ntmp34 =3.0 \ntmp35 =tmp33 +tmp34 \ntmp36 =0.0 \ntmp37 =triton_helpers .maximum (tmp35 ,tmp36 )\ntmp38 =6.0 \ntmp39 =triton_helpers .minimum (tmp37 ,tmp38 )\ntmp40 =tmp33 *tmp39 \ntmp41 =0.16666666666666666 \ntmp42 =tmp40 *tmp41 \ntl .store (out_ptr8 +(r0_1 +4348 *x0 ),tmp42 ,r0_mask &xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__native_batch_norm_legit_functional_convolution_log_sigmoid_forward_5 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr4 ,out_ptr6 ,out_ptr8 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =512 \nr0_numel =4346 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \ntmp1 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp0 =tl .load (in_out_ptr0 +(r0_1 +4346 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp2 =tmp0 +tmp1 \ntmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\ntmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\ntmp3 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n)\ntmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\ntmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\ntmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\ntl .store (in_out_ptr0 +(r0_1 +4346 *x0 ),tmp2 ,r0_mask &xmask )\ntmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\ntmp4 =tmp7 [:,None ]\ntmp5 =tmp8 [:,None ]\ntmp6 =tmp9 [:,None ]\ntl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\ntmp19 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp24 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp10 =4346.0 \ntmp11 =tmp5 /tmp10 \ntmp12 =1e-05 \ntmp13 =tmp11 +tmp12 \ntmp14 =libdevice .rsqrt (tmp13 )\ntmp15 =1.000230149597238 \ntmp16 =tmp11 *tmp15 \ntmp17 =0.1 \ntmp18 =tmp16 *tmp17 \ntmp20 =0.9 \ntmp21 =tmp19 *tmp20 \ntmp22 =tmp18 +tmp21 \ntmp23 =tmp4 *tmp17 \ntmp25 =tmp24 *tmp20 \ntmp26 =tmp23 +tmp25 \ntl .store (out_ptr2 +(x0 ),tmp14 ,xmask )\ntl .store (out_ptr4 +(x0 ),tmp22 ,xmask )\ntl .store (out_ptr6 +(x0 ),tmp26 ,xmask )\ntmp30 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp32 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp27 =tl .load (in_out_ptr0 +(r0_1 +4346 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp28 =tmp27 -tmp4 \ntmp29 =tmp28 *tmp14 \ntmp31 =tmp29 *tmp30 \ntmp33 =tmp31 +tmp32 \ntmp34 =0.0 \ntmp35 =triton_helpers .minimum (tmp34 ,tmp33 )\ntmp36 =tl_math .abs (tmp33 )\ntmp37 =-tmp36 \ntmp38 =tl_math .exp (tmp37 )\ntmp39 =libdevice .log1p (tmp38 )\ntmp40 =tmp35 -tmp39 \ntl .store (out_ptr8 +(r0_1 +4346 *x0 ),tmp40 ,r0_mask &xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_add_6 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =tl .full ([XBLOCK ],True ,tl .int1 )\ntmp0 =tl .load (in_ptr0 +(0 ))\ntmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\ntmp2 =tl .full ([1 ],1 ,tl .int64 )\ntmp3 =tmp1 +tmp2 \ntl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndefcall (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 ,primals_27 ,primals_28 ,primals_29 ,primals_30 ,primals_31 ,primals_32 ,primals_33 ,primals_34 ,primals_35 ,primals_36 =args \nargs .clear ()\nassert_size_stride (primals_1 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\nassert_size_stride (primals_2 ,(32 ,3 ,3 ),(9 ,3 ,1 ))\nassert_size_stride (primals_3 ,(32 ,),(1 ,))\nassert_size_stride (primals_4 ,(),())\nassert_size_stride (primals_5 ,(32 ,),(1 ,))\nassert_size_stride (primals_6 ,(32 ,),(1 ,))\nassert_size_stride (primals_7 ,(32 ,),(1 ,))\nassert_size_stride (primals_8 ,(32 ,),(1 ,))\nassert_size_stride (primals_9 ,(64 ,32 ,3 ),(96 ,3 ,1 ))\nassert_size_stride (primals_10 ,(64 ,),(1 ,))\nassert_size_stride (primals_11 ,(),())\nassert_size_stride (primals_12 ,(64 ,),(1 ,))\nassert_size_stride (primals_13 ,(64 ,),(1 ,))\nassert_size_stride (primals_14 ,(64 ,),(1 ,))\nassert_size_stride (primals_15 ,(64 ,),(1 ,))\nassert_size_stride (primals_16 ,(128 ,64 ,3 ),(192 ,3 ,1 ))\nassert_size_stride (primals_17 ,(128 ,),(1 ,))\nassert_size_stride (primals_18 ,(),())\nassert_size_stride (primals_19 ,(128 ,),(1 ,))\nassert_size_stride (primals_20 ,(128 ,),(1 ,))\nassert_size_stride (primals_21 ,(128 ,),(1 ,))\nassert_size_stride (primals_22 ,(128 ,),(1 ,))\nassert_size_stride (primals_23 ,(256 ,128 ,3 ),(384 ,3 ,1 ))\nassert_size_stride (primals_24 ,(256 ,),(1 ,))\nassert_size_stride (primals_25 ,(),())\nassert_size_stride (primals_26 ,(256 ,),(1 ,))\nassert_size_stride (primals_27 ,(256 ,),(1 ,))\nassert_size_stride (primals_28 ,(256 ,),(1 ,))\nassert_size_stride (primals_29 ,(256 ,),(1 ,))\nassert_size_stride (primals_30 ,(512 ,256 ,3 ),(768 ,3 ,1 ))\nassert_size_stride (primals_31 ,(512 ,),(1 ,))\nassert_size_stride (primals_32 ,(),())\nassert_size_stride (primals_33 ,(512 ,),(1 ,))\nassert_size_stride (primals_34 ,(512 ,),(1 ,))\nassert_size_stride (primals_35 ,(512 ,),(1 ,))\nassert_size_stride (primals_36 ,(512 ,),(1 ,))\nwithtorch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\nbuf0 =empty_strided_cuda ((1 ,3 ,66 ,66 ),(13068 ,4356 ,66 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_constant_pad_nd_0 [grid (13068 )](primals_1 ,buf0 ,13068 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\ndelprimals_1 \n\nbuf1 =extern_kernels .convolution (reinterpret_tensor (buf0 ,(1 ,3 ,4356 ),(0 ,4356 ,1 ),0 ),primals_2 ,stride =(1 ,),padding =(0 ,),dilation =(1 ,),transposed =False ,output_padding =(0 ,),groups =1 ,bias =None )\nassert_size_stride (buf1 ,(1 ,32 ,4354 ),(139328 ,4354 ,1 ))\nbuf2 =buf1 ;delbuf1 \nbuf3 =empty_strided_cuda ((1 ,32 ,1 ),(32 ,1 ,1 ),torch .float32 )\nbuf6 =empty_strided_cuda ((1 ,32 ,1 ),(32 ,1 ,1 ),torch .float32 )\nbuf8 =empty_strided_cuda ((1 ,32 ,4354 ),(139328 ,4354 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_red_fused__native_batch_norm_legit_functional_convolution_mish_1 [grid (32 )](buf2 ,primals_3 ,primals_6 ,primals_5 ,primals_7 ,primals_8 ,buf3 ,buf6 ,primals_6 ,primals_5 ,buf8 ,32 ,4354 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\ndelprimals_3 \ndelprimals_5 \ndelprimals_6 \n\nbuf9 =extern_kernels .convolution (buf8 ,primals_9 ,stride =(1 ,),padding =(0 ,),dilation =(1 ,),transposed =False ,output_padding =(0 ,),groups =1 ,bias =None )\nassert_size_stride (buf9 ,(1 ,64 ,4352 ),(278528 ,4352 ,1 ))\nbuf10 =buf9 ;delbuf9 \nbuf11 =empty_strided_cuda ((1 ,64 ,1 ),(64 ,1 ,1 ),torch .float32 )\nbuf14 =empty_strided_cuda ((1 ,64 ,1 ),(64 ,1 ,1 ),torch .float32 )\nbuf15 =empty_strided_cuda ((1 ,64 ,4352 ),(278528 ,4352 ,1 ),torch .float32 )\nbuf16 =buf15 ;delbuf15 \n\nstream0 =get_raw_stream (0 )\ntriton_red_fused__native_batch_norm_legit_functional_convolution_hardswish_2 [grid (64 )](buf10 ,buf16 ,primals_10 ,primals_13 ,primals_12 ,primals_14 ,primals_15 ,buf11 ,buf14 ,primals_13 ,primals_12 ,64 ,4352 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\ndelprimals_10 \ndelprimals_12 \ndelprimals_13 \n\nbuf17 =extern_kernels .convolution (buf16 ,primals_16 ,stride =(1 ,),padding =(0 ,),dilation =(1 ,),transposed =False ,output_padding =(0 ,),groups =1 ,bias =None )\nassert_size_stride (buf17 ,(1 ,128 ,4350 ),(556800 ,4350 ,1 ))\nbuf18 =buf17 ;delbuf17 \nbuf19 =empty_strided_cuda ((1 ,128 ,1 ),(128 ,1 ,1 ),torch .float32 )\nbuf22 =empty_strided_cuda ((1 ,128 ,1 ),(128 ,1 ,1 ),torch .float32 )\nbuf24 =empty_strided_cuda ((1 ,128 ,4350 ),(556800 ,4350 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_red_fused__native_batch_norm_legit_functional_convolution_mish_3 [grid (128 )](buf18 ,primals_17 ,primals_20 ,primals_19 ,primals_21 ,primals_22 ,buf19 ,buf22 ,primals_20 ,primals_19 ,buf24 ,128 ,4350 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\ndelprimals_17 \ndelprimals_19 \ndelprimals_20 \n\nbuf25 =extern_kernels .convolution (buf24 ,primals_23 ,stride =(1 ,),padding =(0 ,),dilation =(1 ,),transposed =False ,output_padding =(0 ,),groups =1 ,bias =None )\nassert_size_stride (buf25 ,(1 ,256 ,4348 ),(1113088 ,4348 ,1 ))\nbuf26 =buf25 ;delbuf25 \nbuf27 =empty_strided_cuda ((1 ,256 ,1 ),(256 ,1 ,1 ),torch .float32 )\nbuf30 =empty_strided_cuda ((1 ,256 ,1 ),(256 ,1 ,1 ),torch .float32 )\nbuf32 =empty_strided_cuda ((1 ,256 ,4348 ),(1113088 ,4348 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_red_fused__native_batch_norm_legit_functional_convolution_hardswish_4 [grid (256 )](buf26 ,primals_24 ,primals_27 ,primals_26 ,primals_28 ,primals_29 ,buf27 ,buf30 ,primals_27 ,primals_26 ,buf32 ,256 ,4348 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\ndelprimals_24 \ndelprimals_26 \ndelprimals_27 \n\nbuf33 =extern_kernels .convolution (buf32 ,primals_30 ,stride =(1 ,),padding =(0 ,),dilation =(1 ,),transposed =False ,output_padding =(0 ,),groups =1 ,bias =None )\nassert_size_stride (buf33 ,(1 ,512 ,4346 ),(2225152 ,4346 ,1 ))\nbuf34 =buf33 ;delbuf33 \nbuf35 =empty_strided_cuda ((1 ,512 ,1 ),(512 ,1 ,1 ),torch .float32 )\nbuf38 =empty_strided_cuda ((1 ,512 ,1 ),(512 ,1 ,1 ),torch .float32 )\nbuf40 =empty_strided_cuda ((1 ,512 ,4346 ),(2225152 ,4346 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_red_fused__native_batch_norm_legit_functional_convolution_log_sigmoid_forward_5 [grid (512 )](buf34 ,primals_31 ,primals_34 ,primals_33 ,primals_35 ,primals_36 ,buf35 ,buf38 ,primals_34 ,primals_33 ,buf40 ,512 ,4346 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\ndelprimals_31 \ndelprimals_33 \ndelprimals_34 \n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_add_6 [grid (1 )](primals_4 ,primals_4 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\ndelprimals_4 \n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_add_6 [grid (1 )](primals_11 ,primals_11 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\ndelprimals_11 \n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_add_6 [grid (1 )](primals_18 ,primals_18 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\ndelprimals_18 \n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_add_6 [grid (1 )](primals_25 ,primals_25 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\ndelprimals_25 \n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_add_6 [grid (1 )](primals_32 ,primals_32 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\ndelprimals_32 \nreturn (buf40 ,primals_2 ,primals_7 ,primals_8 ,primals_9 ,primals_14 ,primals_15 ,primals_16 ,primals_21 ,primals_22 ,primals_23 ,primals_28 ,primals_29 ,primals_30 ,primals_35 ,primals_36 ,reinterpret_tensor (buf0 ,(1 ,3 ,4356 ),(13068 ,4356 ,1 ),0 ),buf2 ,buf3 ,buf6 ,buf8 ,buf10 ,buf11 ,buf14 ,buf16 ,buf18 ,buf19 ,buf22 ,buf24 ,buf26 ,buf27 ,buf30 ,buf32 ,buf34 ,buf35 ,buf38 ,)\n\ndefbenchmark_compiled_module (times =10 ,repeat =10 ):\n    fromtorch ._dynamo .testingimportrand_strided \nfromtorch ._inductor .utilsimportprint_performance \nprimals_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_2 =rand_strided ((32 ,3 ,3 ),(9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_3 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_4 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\nprimals_5 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_6 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_7 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_8 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_9 =rand_strided ((64 ,32 ,3 ),(96 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_10 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_11 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\nprimals_12 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_13 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_14 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_15 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_16 =rand_strided ((128 ,64 ,3 ),(192 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_17 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_18 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\nprimals_19 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_20 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_21 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_22 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_23 =rand_strided ((256 ,128 ,3 ),(384 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_24 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_25 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\nprimals_26 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_27 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_28 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_29 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_30 =rand_strided ((512 ,256 ,3 ),(768 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_31 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_32 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\nprimals_33 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_34 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_35 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_36 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nfn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 ,primals_27 ,primals_28 ,primals_29 ,primals_30 ,primals_31 ,primals_32 ,primals_33 ,primals_34 ,primals_35 ,primals_36 ])\nreturnprint_performance (fn ,times =times ,repeat =repeat )\n\nif__name__ ==\"__main__\":\n    fromtorch ._inductor .wrapper_benchmarkimportcompiled_module_main \ncompiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "00bf8f60-0367-4c64-b7c6-111709465d6c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['NLLLoss', 'LazyBatchNorm2d', 'ReplicationPad1d', 'FractionalMaxPool2d', 'Softsign', 'Softmax2d', 'Hardswish', 'CrossMapLRN2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.bn1 = nn.LazyBatchNorm2d()\n        self.pad1 = nn.ReplicationPad1d(2)\n        self.pool1 = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.softsign1 = nn.Softsign()\n        self.softmax2d1 = nn.Softmax2d()\n        self.hardswish1 = nn.Hardswish()\n        self.lrn1 = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.bn2 = nn.LazyBatchNorm2d()\n        self.pad2 = nn.ReplicationPad1d(2)\n        self.pool2 = nn.FractionalMaxPool2d(kernel_size=2, output_size=(7, 7))\n        self.softsign2 = nn.Softsign()\n        self.softmax2d2 = nn.Softmax2d()\n        self.hardswish2 = nn.Hardswish()\n        self.lrn2 = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.loss = nn.NLLLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        x = self.bn1(x)\n        x = x.squeeze(2)  # Remove height dimension to apply 1d padding\n        x = self.pad1(x)\n        x = x.unsqueeze(2)  # Add height dimension back\n        x = self.pool1(x)\n        x = self.softsign1(x)\n        x = self.softmax2d1(x)\n        x = self.hardswish1(x)\n        x = self.lrn1(x)\n        x = self.bn2(x)\n        x = x.squeeze(2)  # Remove height dimension to apply 1d padding\n        x = self.pad2(x)\n        x = x.unsqueeze(2)  # Add height dimension back\n        x = self.pool2(x)\n        x = self.softsign2(x)\n        x = self.softmax2d2(x)\n        x = self.hardswish2(x)\n        x = self.lrn2(x)\n        # Assuming we have a target tensor for NLLLoss\n        target = torch.randint(0, x.size(1), (x.size(0),), device=x.device)\n        x = x.view(x.size(0), -1)  # Flatten for NLLLoss\n        x = F.log_softmax(x, dim=1)  # Apply log_softmax for NLLLoss\n        loss = self.loss(x, target)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_squeeze_0 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +1024 *x0 ),None )\n    tmp21 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp23 =tl .load (in_ptr2 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr3 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp34 =tl .load (in_ptr4 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp1 =tl .broadcast_to (tmp0 ,[R0_BLOCK ])\n    tmp3 =tl .broadcast_to (tmp1 ,[R0_BLOCK ])\n    tmp5 =triton_helpers .promote_to_tensor (tl .sum (tmp3 ,0 ))\n    tmp6 =tl .full ([1 ],1024 ,tl .int32 )\n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp5 /tmp7 \n    tmp9 =tmp1 -tmp8 \n    tmp10 =tmp9 *tmp9 \n    tmp11 =tl .broadcast_to (tmp10 ,[R0_BLOCK ])\n    tmp13 =triton_helpers .promote_to_tensor (tl .sum (tmp11 ,0 ))\n    tmp14 =tmp0 -tmp8 \n    tmp15 =1024.0 \n    tmp16 =tmp13 /tmp15 \n    tmp17 =1e-05 \n    tmp18 =tmp16 +tmp17 \n    tmp19 =libdevice .rsqrt (tmp18 )\n    tmp20 =tmp14 *tmp19 \n    tmp22 =tmp20 *tmp21 \n    tmp24 =tmp22 +tmp23 \n    tmp25 =1.0009775171065494 \n    tmp26 =tmp16 *tmp25 \n    tmp27 =0.1 \n    tmp28 =tmp26 *tmp27 \n    tmp30 =0.9 \n    tmp31 =tmp29 *tmp30 \n    tmp32 =tmp28 +tmp31 \n    tmp33 =tmp8 *tmp27 \n    tmp35 =tmp34 *tmp30 \n    tmp36 =tmp33 +tmp35 \n    tl .store (out_ptr2 +(r0_1 +1024 *x0 ),tmp24 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp19 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp32 ,None )\n    tl .store (out_ptr7 +(x0 ),tmp36 ,None )\n    tl .store (out_ptr0 +(x0 ),tmp8 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_1 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .full ([1 ],1 ,tl .int64 )\n    tmp3 =tmp1 +tmp2 \n    tl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_2 ,(),())\n    assert_size_stride (primals_3 ,(3 ,),(1 ,))\n    assert_size_stride (primals_4 ,(3 ,),(1 ,))\n    assert_size_stride (primals_5 ,(3 ,),(1 ,))\n    assert_size_stride (primals_6 ,(3 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),torch .float32 )\n        buf3 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_squeeze_0 [grid (3 )](primals_1 ,primals_5 ,primals_6 ,primals_4 ,primals_3 ,buf0 ,buf4 ,buf3 ,primals_4 ,primals_3 ,3 ,1024 ,num_warps =8 ,num_stages =1 )\n        del primals_3 \n        del primals_4 \n        del primals_5 \n        del primals_6 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_1 [grid (1 )](primals_2 ,primals_2 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_2 \n    return (buf4 ,primals_1 ,reinterpret_tensor (buf3 ,(3 ,),(1 ,),0 ),reinterpret_tensor (buf0 ,(1 ,3 ,1 ,1 ),(3 ,1 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_3 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "0143e31e-5b1e-46b0-b51c-f768d6bed883",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardsigmoid', 'ReLU', 'FeatureAlphaDropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.relu = nn.ReLU()\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n\n    def forward(self, x):\n        # Apply Hardsigmoid\n        x = self.hardsigmoid(x)\n        \n        # Apply ReLU\n        x = self.relu(x)\n        \n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Apply Hardsigmoid again\n        x = self.hardsigmoid(x)\n        \n        # Apply ReLU again\n        x = self.relu(x)\n        \n        # Apply FeatureAlphaDropout again\n        x = self.feature_alpha_dropout(x)\n        \n        # Reshape the output to match the input shape\n        x = x.view(x.size(0), -1)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimporttorch \nimporttriton \nimporttriton .languageastl \nfromtorch ._inductor .runtime .triton_heuristicsimport (\ngrid ,\n)\nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx0 =xindex \ntmp0 =tl .load (in_ptr0 +load_seed_offset )\ntmp1 =x0 \ntmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\ntl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_bernoulli_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx0 =xindex \ntmp0 =tl .load (in_ptr0 +load_seed_offset )\ntmp1 =x0 \ntmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\ntl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused__to_copy_add_bernoulli_hardsigmoid_mul_relu_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =12288 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =tl .full ([XBLOCK ],True ,tl .int1 )\nx2 =xindex \nx1 =xindex //4096 \ntmp0 =tl .load (in_ptr0 +(x2 ),None )\ntmp11 =tl .load (in_ptr1 +(x1 ),None ,eviction_policy ='evict_last')\ntmp30 =tl .load (in_ptr2 +(x1 ),None ,eviction_policy ='evict_last')\ntmp1 =3.0 \ntmp2 =tmp0 +tmp1 \ntmp3 =0.0 \ntmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\ntmp5 =6.0 \ntmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\ntmp7 =0.16666666666666666 \ntmp8 =tmp6 *tmp7 \ntmp9 =tl .full ([1 ],0 ,tl .int32 )\ntmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\ntmp12 =0.5 \ntmp13 =tmp11 <tmp12 \ntmp14 =tmp13 .to (tl .float32 )\ntmp15 =0.8864048946659319 \ntmp16 =tmp14 *tmp15 \ntmp17 =tmp10 *tmp16 \ntmp18 =-1.0 \ntmp19 =tmp14 +tmp18 \ntmp20 =1.558387861036063 \ntmp21 =tmp19 *tmp20 \ntmp22 =0.7791939305180315 \ntmp23 =tmp21 +tmp22 \ntmp24 =tmp17 +tmp23 \ntmp25 =tmp24 +tmp1 \ntmp26 =triton_helpers .maximum (tmp25 ,tmp3 )\ntmp27 =triton_helpers .minimum (tmp26 ,tmp5 )\ntmp28 =tmp27 *tmp7 \ntmp29 =triton_helpers .maximum (tmp9 ,tmp28 )\ntmp31 =tmp30 <tmp12 \ntmp32 =tmp31 .to (tl .float32 )\ntmp33 =tmp32 *tmp15 \ntmp34 =tmp29 *tmp33 \ntmp35 =tmp32 +tmp18 \ntmp36 =tmp35 *tmp20 \ntmp37 =tmp36 +tmp22 \ntmp38 =tmp34 +tmp37 \ntl .store (in_out_ptr0 +(x2 ),tmp38 ,None )\n\ndefcall (args ):\n    arg0_1 ,=args \nargs .clear ()\nassert_size_stride (arg0_1 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\nwithtorch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\nbuf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\naten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\nbuf1 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_bernoulli_0 [grid (3 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\nbuf2 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_bernoulli_1 [grid (3 )](buf0 ,buf2 ,1 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\ndelbuf0 \nbuf3 =empty_strided_cuda ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),torch .float32 )\nbuf4 =buf3 ;delbuf3 \n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused__to_copy_add_bernoulli_hardsigmoid_mul_relu_2 [grid (12288 )](buf4 ,arg0_1 ,buf1 ,buf2 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\ndelarg0_1 \ndelbuf1 \ndelbuf2 \nreturn (reinterpret_tensor (buf4 ,(1 ,12288 ),(12288 ,1 ),0 ),)\n\ndefbenchmark_compiled_module (times =10 ,repeat =10 ):\n    fromtorch ._dynamo .testingimportrand_strided \nfromtorch ._inductor .utilsimportprint_performance \narg0_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\nfn =lambda :call ([arg0_1 ])\nreturnprint_performance (fn ,times =times ,repeat =repeat )\n\nif__name__ ==\"__main__\":\n    fromtorch ._inductor .wrapper_benchmarkimportcompiled_module_main \ncompiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "01b0d66b-8d6e-4f49-9803-07c73c0dfee5",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CrossEntropyLoss', 'LeakyReLU', 'Softshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.leaky_relu1 = nn.LeakyReLU(negative_slope=0.01)\n        self.leaky_relu2 = nn.LeakyReLU(negative_slope=0.01)\n        self.softshrink1 = nn.Softshrink(lambd=0.5)\n        self.softshrink2 = nn.Softshrink(lambd=0.5)\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        # Apply LeakyReLU and Softshrink in sequence\n        x = self.leaky_relu1(x)\n        x = self.softshrink1(x)\n        x = self.leaky_relu2(x)\n        x = self.softshrink2(x)\n        \n        # Flatten the input for CrossEntropyLoss\n        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n        \n        # Dummy target for CrossEntropyLoss (assuming classification task)\n        # In a real scenario, the target would be provided externally\n        target = torch.randint(0, 10, (x.size(0),), device=x.device)\n        \n        # Apply CrossEntropyLoss\n        loss = self.cross_entropy_loss(x, target)\n        \n        # Return the loss (typically, you would return the output for inference)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimporttorch \nimporttriton \nimporttriton .languageastl \nfromtorch ._inductor .runtime .triton_heuristicsimport (\ngrid ,\n)\nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportmathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__log_softmax_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \n_tmp41 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )\ntmp1 =ks0 *ks1 *ks2 \ntmp2 =tmp0 <tmp1 \ntmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 ))%(ks0 *ks1 *ks2 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp4 =0.0 \ntmp5 =tmp3 >tmp4 \ntmp6 =0.01 \ntmp7 =tmp3 *tmp6 \ntmp8 =tl .where (tmp5 ,tmp3 ,tmp7 )\ntmp9 =tl_math .abs (tmp8 )\ntmp10 =0.5 \ntmp11 =tmp9 >tmp10 \ntmp12 =tl .full ([1 ,1 ],0 ,tl .int32 )\ntmp13 =tmp12 <tmp8 \ntmp14 =tmp13 .to (tl .int8 )\ntmp15 =tmp8 <tmp12 \ntmp16 =tmp15 .to (tl .int8 )\ntmp17 =tmp14 -tmp16 \ntmp18 =tmp17 .to (tmp8 .dtype )\ntmp19 =tmp18 *tmp10 \ntmp20 =tmp8 -tmp19 \ntmp21 =tmp8 *tmp4 \ntmp22 =tl .where (tmp11 ,tmp20 ,tmp21 )\ntmp23 =tmp22 >tmp4 \ntmp24 =tmp22 *tmp6 \ntmp25 =tl .where (tmp23 ,tmp22 ,tmp24 )\ntmp26 =tl_math .abs (tmp25 )\ntmp27 =tmp26 >tmp10 \ntmp28 =tmp12 <tmp25 \ntmp29 =tmp28 .to (tl .int8 )\ntmp30 =tmp25 <tmp12 \ntmp31 =tmp30 .to (tl .int8 )\ntmp32 =tmp29 -tmp31 \ntmp33 =tmp32 .to (tmp25 .dtype )\ntmp34 =tmp33 *tmp10 \ntmp35 =tmp25 -tmp34 \ntmp36 =tmp25 *tmp4 \ntmp37 =tl .where (tmp27 ,tmp35 ,tmp36 )\ntmp38 =tl .full (tmp37 .shape ,float (\"-inf\"),tmp37 .dtype )\ntmp39 =tl .where (tmp2 ,tmp37 ,tmp38 )\ntmp40 =tl .broadcast_to (tmp39 ,[XBLOCK ,R0_BLOCK ])\ntmp42 =triton_helpers .maximum (_tmp41 ,tmp40 )\n_tmp41 =tl .where (r0_mask &xmask ,tmp42 ,_tmp41 )\ntmp41 =triton_helpers .max2 (_tmp41 ,1 )[:,None ]\ntl .store (out_ptr0 +(x0 ),tmp41 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportmathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_per_fused__log_softmax_1 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =1 \nr0_numel =2 \nR0_BLOCK :tl .constexpr =2 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\nr0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\nr0_offset =0 \nr0_mask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\nroffset =r0_offset \nrindex =r0_index \nr0_0 =r0_index \ntmp0 =tl .load (in_ptr0 +(r0_0 ),None )\ntmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\ntmp3 =triton_helpers .max2 (tmp1 ,1 )[:,None ]\ntl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp3 ,None )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportmathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__log_softmax_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \n_tmp44 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =r0_index \ntmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )\ntmp1 =ks0 *ks1 *ks2 \ntmp2 =tmp0 <tmp1 \ntmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 ))%(ks0 *ks1 *ks2 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp4 =0.0 \ntmp5 =tmp3 >tmp4 \ntmp6 =0.01 \ntmp7 =tmp3 *tmp6 \ntmp8 =tl .where (tmp5 ,tmp3 ,tmp7 )\ntmp9 =tl_math .abs (tmp8 )\ntmp10 =0.5 \ntmp11 =tmp9 >tmp10 \ntmp12 =tl .full ([1 ,1 ],0 ,tl .int32 )\ntmp13 =tmp12 <tmp8 \ntmp14 =tmp13 .to (tl .int8 )\ntmp15 =tmp8 <tmp12 \ntmp16 =tmp15 .to (tl .int8 )\ntmp17 =tmp14 -tmp16 \ntmp18 =tmp17 .to (tmp8 .dtype )\ntmp19 =tmp18 *tmp10 \ntmp20 =tmp8 -tmp19 \ntmp21 =tmp8 *tmp4 \ntmp22 =tl .where (tmp11 ,tmp20 ,tmp21 )\ntmp23 =tmp22 >tmp4 \ntmp24 =tmp22 *tmp6 \ntmp25 =tl .where (tmp23 ,tmp22 ,tmp24 )\ntmp26 =tl_math .abs (tmp25 )\ntmp27 =tmp26 >tmp10 \ntmp28 =tmp12 <tmp25 \ntmp29 =tmp28 .to (tl .int8 )\ntmp30 =tmp25 <tmp12 \ntmp31 =tmp30 .to (tl .int8 )\ntmp32 =tmp29 -tmp31 \ntmp33 =tmp32 .to (tmp25 .dtype )\ntmp34 =tmp33 *tmp10 \ntmp35 =tmp25 -tmp34 \ntmp36 =tmp25 *tmp4 \ntmp37 =tl .where (tmp27 ,tmp35 ,tmp36 )\ntmp38 =tl .load (in_ptr1 +(tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int32 )),tmp2 ,eviction_policy ='evict_last',other =0.0 )\ntmp39 =tmp37 -tmp38 \ntmp40 =tl_math .exp (tmp39 )\ntmp41 =tl .full (tmp40 .shape ,0 ,tmp40 .dtype )\ntmp42 =tl .where (tmp2 ,tmp40 ,tmp41 )\ntmp43 =tl .broadcast_to (tmp42 ,[XBLOCK ,R0_BLOCK ])\ntmp45 =_tmp44 +tmp43 \n_tmp44 =tl .where (r0_mask &xmask ,tmp45 ,_tmp44 )\ntmp44 =tl .sum (_tmp44 ,1 )[:,None ]\ntl .store (out_ptr0 +(x0 ),tmp44 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportmathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_per_fused__log_softmax_nll_loss_forward_randint_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,load_seed_offset ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =1 \nr0_numel =2 \nR0_BLOCK :tl .constexpr =2 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\nr0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\nr0_offset =0 \nr0_mask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\nroffset =r0_offset \nrindex =r0_index \nr0_0 =r0_index \ntmp0 =tl .load (in_ptr0 +(r0_0 ),None )\ntmp51 =tl .load (in_out_ptr0 +(0 ))\ntmp52 =tl .broadcast_to (tmp51 ,[XBLOCK ,1 ])\ntmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\ntmp3 =tl .sum (tmp1 ,1 )[:,None ]\ntmp4 =tl .load (in_ptr1 +load_seed_offset )\ntmp5 =tl .full ([1 ,1 ],0 ,tl .int32 )\ntmp6 =tl .full ([1 ,1 ],0 ,tl .int64 )\ntmp7 =tl .full ([1 ,1 ],10 ,tl .int64 )\ntmp8 =triton_helpers .randint64 (tmp4 ,(tmp5 ).to (tl .uint32 ),tmp6 ,tmp7 )\ntmp9 =tl .full ([1 ,1 ],-100 ,tl .int64 )\ntmp10 =tmp8 !=tmp9 \ntmp11 =tl .where (tmp10 ,tmp8 ,tmp6 )\ntmp12 =ks1 *ks2 *ks3 \ntmp13 =tmp11 +tmp12 \ntmp14 =tmp11 <0 \ntmp15 =tl .where (tmp14 ,tmp13 ,tmp11 )\ntl .device_assert ((0 <=tmp15 )&(tmp15 <ks1 *ks2 *ks3 ),\"index out of bounds: 0 <= tmp15 < ks1*ks2*ks3\")\ntmp17 =tl .load (in_ptr2 +((tmp15 %(ks1 *ks2 *ks3 ))),None ,eviction_policy ='evict_last')\ntmp18 =0.0 \ntmp19 =tmp17 >tmp18 \ntmp20 =0.01 \ntmp21 =tmp17 *tmp20 \ntmp22 =tl .where (tmp19 ,tmp17 ,tmp21 )\ntmp23 =tl_math .abs (tmp22 )\ntmp24 =0.5 \ntmp25 =tmp23 >tmp24 \ntmp26 =tmp5 <tmp22 \ntmp27 =tmp26 .to (tl .int8 )\ntmp28 =tmp22 <tmp5 \ntmp29 =tmp28 .to (tl .int8 )\ntmp30 =tmp27 -tmp29 \ntmp31 =tmp30 .to (tmp22 .dtype )\ntmp32 =tmp31 *tmp24 \ntmp33 =tmp22 -tmp32 \ntmp34 =tmp22 *tmp18 \ntmp35 =tl .where (tmp25 ,tmp33 ,tmp34 )\ntmp36 =tmp35 >tmp18 \ntmp37 =tmp35 *tmp20 \ntmp38 =tl .where (tmp36 ,tmp35 ,tmp37 )\ntmp39 =tl_math .abs (tmp38 )\ntmp40 =tmp39 >tmp24 \ntmp41 =tmp5 <tmp38 \ntmp42 =tmp41 .to (tl .int8 )\ntmp43 =tmp38 <tmp5 \ntmp44 =tmp43 .to (tl .int8 )\ntmp45 =tmp42 -tmp44 \ntmp46 =tmp45 .to (tmp38 .dtype )\ntmp47 =tmp46 *tmp24 \ntmp48 =tmp38 -tmp47 \ntmp49 =tmp38 *tmp18 \ntmp50 =tl .where (tmp40 ,tmp48 ,tmp49 )\ntmp53 =tmp50 -tmp52 \ntmp54 =tl_math .log (tmp3 )\ntmp55 =tmp53 -tmp54 \ntmp56 =-tmp55 \ntmp57 =tl .where (tmp10 ,tmp56 ,tmp18 )\ntmp58 =tmp10 .to (tl .int32 )\ntmp59 =tmp58 .to (tl .float32 )\ntmp60 =tmp57 /tmp59 \ntl .debug_barrier ()\ntl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp60 ,None )\n\ndefcall (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \nargs .clear ()\ns0 =arg0_1 \ns1 =arg1_1 \ns2 =arg2_1 \nassert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\nwithtorch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\nbuf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\naten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\nbuf1 =empty_strided_cuda ((1 ,1 ,2 ),(2 ,2 ,1 ),torch .float32 )\n\ntriton_red_fused__log_softmax_0_r0_numel =(1 +s0 *s1 *s2 )//2 \nstream0 =get_raw_stream (0 )\ntriton_red_fused__log_softmax_0 [grid (2 )](arg3_1 ,buf1 ,3 ,64 ,64 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\nbuf2 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_per_fused__log_softmax_1 [grid (1 )](buf1 ,buf2 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\nbuf3 =buf1 ;delbuf1 \n\ntriton_red_fused__log_softmax_2_r0_numel =(1 +s0 *s1 *s2 )//2 \nstream0 =get_raw_stream (0 )\ntriton_red_fused__log_softmax_2 [grid (2 )](arg3_1 ,buf2 ,buf3 ,3 ,64 ,64 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\nbuf5 =reinterpret_tensor (buf2 ,(),(),0 );delbuf2 \n\nstream0 =get_raw_stream (0 )\ntriton_per_fused__log_softmax_nll_loss_forward_randint_3 [grid (1 )](buf5 ,buf3 ,buf0 ,arg3_1 ,0 ,3 ,64 ,64 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\ndelarg3_1 \ndelbuf0 \ndelbuf3 \nreturn (buf5 ,)\n\ndefbenchmark_compiled_module (times =10 ,repeat =10 ):\n    fromtorch ._dynamo .testingimportrand_strided \nfromtorch ._inductor .utilsimportprint_performance \narg0_1 =3 \narg1_1 =64 \narg2_1 =64 \narg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\nfn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\nreturnprint_performance (fn ,times =times ,repeat =repeat )\n\nif__name__ ==\"__main__\":\n    fromtorch ._inductor .wrapper_benchmarkimportcompiled_module_main \ncompiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "0232a1d1-c7a1-47ac-8ce5-35871a3dc543",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveMaxPool2d', 'Mish', 'ParameterDict', 'MaxUnpool3d', 'LazyInstanceNorm2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool2d = nn.AdaptiveMaxPool2d((16, 16))\n        self.mish = nn.Mish()\n        self.parameter_dict = nn.ParameterDict({\n            'param1': nn.Parameter(torch.randn(16, 16)),\n            'param2': nn.Parameter(torch.randn(16, 16))\n        })\n        self.max_unpool3d = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.lazy_instance_norm2d = nn.LazyInstanceNorm2d()\n        \n        # Additional layers to accommodate the input and output shapes\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # Initial convolution and activation\n        x = self.conv1(x)\n        x = self.mish(x)\n        \n        # Adaptive max pooling\n        x = self.adaptive_max_pool2d(x)\n        \n        # Lazy instance normalization\n        x = self.lazy_instance_norm2d(x)\n        \n        # Another convolution and activation\n        x = self.conv2(x)\n        x = self.mish(x)\n        \n        # Reshape for fully connected layers\n        x = x.view(x.size(0), -1)\n        \n        # Fully connected layers\n        x = self.fc1(x)\n        x = self.mish(x)\n        x = self.fc2(x)\n        \n        # Apply parameter dict parameters (just for demonstration)\n        x = x * self.parameter_dict['param1'].mean() + self.parameter_dict['param2'].mean()\n        \n        # Max unpooling (requires indices from a previous max pooling operation)\n        # Since we don't have a previous max pooling operation, we'll skip this step\n        # x = self.max_unpool3d(x, indices)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimporttorch \nfromtorch ._inductor .select_algorithmimportextern_kernels \nimporttriton \nimporttriton .languageastl \nfromtorch ._inductor .runtime .triton_heuristicsimport (\ngrid ,\n)\nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_convolution_mish_0 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =65536 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =tl .full ([XBLOCK ],True ,tl .int1 )\nx2 =xindex \nx1 =xindex //4096 \ntmp0 =tl .load (in_out_ptr0 +(x2 ),None )\ntmp1 =tl .load (in_ptr0 +(x1 ),None ,eviction_policy ='evict_last')\ntmp2 =tmp0 +tmp1 \ntmp3 =20.0 \ntmp4 =tmp2 >tmp3 \ntmp5 =tl_math .exp (tmp2 )\ntmp6 =libdevice .log1p (tmp5 )\ntmp7 =tl .where (tmp4 ,tmp2 ,tmp6 )\ntmp8 =libdevice .tanh (tmp7 )\ntmp9 =tmp2 *tmp8 \ntl .store (in_out_ptr0 +(x2 ),tmp2 ,None )\ntl .store (out_ptr0 +(x2 ),tmp9 ,None )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_red_fused__native_batch_norm_legit_functional_adaptive_max_pool2d_mean_native_batch_norm_backward_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,out_ptr6 ,out_ptr8 ,out_ptr10 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =16 \nr0_numel =256 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\nxmask =xindex <xnumel \nr0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\nrbase =r0_base \nx0 =xindex \ntmp78_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp78_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\ntmp78_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_1 =(r0_index %16 )\nr0_2 =r0_index //16 \nr0_3 =r0_index \ntmp0 =tl .load (in_ptr0 +(4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp1 =tl .load (in_ptr0 +(1 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp3 =tl .load (in_ptr0 +(2 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp5 =tl .load (in_ptr0 +(3 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp7 =tl .load (in_ptr0 +(64 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp9 =tl .load (in_ptr0 +(65 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp11 =tl .load (in_ptr0 +(66 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp13 =tl .load (in_ptr0 +(67 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp15 =tl .load (in_ptr0 +(128 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp17 =tl .load (in_ptr0 +(129 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp19 =tl .load (in_ptr0 +(130 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp21 =tl .load (in_ptr0 +(131 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp23 =tl .load (in_ptr0 +(192 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp25 =tl .load (in_ptr0 +(193 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp27 =tl .load (in_ptr0 +(194 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp29 =tl .load (in_ptr0 +(195 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\ntmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\ntmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\ntmp8 =triton_helpers .maximum (tmp7 ,tmp6 )\ntmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\ntmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\ntmp14 =triton_helpers .maximum (tmp13 ,tmp12 )\ntmp16 =triton_helpers .maximum (tmp15 ,tmp14 )\ntmp18 =triton_helpers .maximum (tmp17 ,tmp16 )\ntmp20 =triton_helpers .maximum (tmp19 ,tmp18 )\ntmp22 =triton_helpers .maximum (tmp21 ,tmp20 )\ntmp24 =triton_helpers .maximum (tmp23 ,tmp22 )\ntmp26 =triton_helpers .maximum (tmp25 ,tmp24 )\ntmp28 =triton_helpers .maximum (tmp27 ,tmp26 )\ntmp30 =triton_helpers .maximum (tmp29 ,tmp28 )\ntmp31 =tmp1 >tmp0 \ntmp32 =tl .full ([1 ,1 ],1 ,tl .int8 )\ntmp33 =tl .full ([1 ,1 ],0 ,tl .int8 )\ntmp34 =tl .where (tmp31 ,tmp32 ,tmp33 )\ntmp35 =tmp3 >tmp2 \ntmp36 =tl .full ([1 ,1 ],2 ,tl .int8 )\ntmp37 =tl .where (tmp35 ,tmp36 ,tmp34 )\ntmp38 =tmp5 >tmp4 \ntmp39 =tl .full ([1 ,1 ],3 ,tl .int8 )\ntmp40 =tl .where (tmp38 ,tmp39 ,tmp37 )\ntmp41 =tmp7 >tmp6 \ntmp42 =tl .full ([1 ,1 ],4 ,tl .int8 )\ntmp43 =tl .where (tmp41 ,tmp42 ,tmp40 )\ntmp44 =tmp9 >tmp8 \ntmp45 =tl .full ([1 ,1 ],5 ,tl .int8 )\ntmp46 =tl .where (tmp44 ,tmp45 ,tmp43 )\ntmp47 =tmp11 >tmp10 \ntmp48 =tl .full ([1 ,1 ],6 ,tl .int8 )\ntmp49 =tl .where (tmp47 ,tmp48 ,tmp46 )\ntmp50 =tmp13 >tmp12 \ntmp51 =tl .full ([1 ,1 ],7 ,tl .int8 )\ntmp52 =tl .where (tmp50 ,tmp51 ,tmp49 )\ntmp53 =tmp15 >tmp14 \ntmp54 =tl .full ([1 ,1 ],8 ,tl .int8 )\ntmp55 =tl .where (tmp53 ,tmp54 ,tmp52 )\ntmp56 =tmp17 >tmp16 \ntmp57 =tl .full ([1 ,1 ],9 ,tl .int8 )\ntmp58 =tl .where (tmp56 ,tmp57 ,tmp55 )\ntmp59 =tmp19 >tmp18 \ntmp60 =tl .full ([1 ,1 ],10 ,tl .int8 )\ntmp61 =tl .where (tmp59 ,tmp60 ,tmp58 )\ntmp62 =tmp21 >tmp20 \ntmp63 =tl .full ([1 ,1 ],11 ,tl .int8 )\ntmp64 =tl .where (tmp62 ,tmp63 ,tmp61 )\ntmp65 =tmp23 >tmp22 \ntmp66 =tl .full ([1 ,1 ],12 ,tl .int8 )\ntmp67 =tl .where (tmp65 ,tmp66 ,tmp64 )\ntmp68 =tmp25 >tmp24 \ntmp69 =tl .full ([1 ,1 ],13 ,tl .int8 )\ntmp70 =tl .where (tmp68 ,tmp69 ,tmp67 )\ntmp71 =tmp27 >tmp26 \ntmp72 =tl .full ([1 ,1 ],14 ,tl .int8 )\ntmp73 =tl .where (tmp71 ,tmp72 ,tmp70 )\ntmp74 =tmp29 >tmp28 \ntmp75 =tl .full ([1 ,1 ],15 ,tl .int8 )\ntmp76 =tl .where (tmp74 ,tmp75 ,tmp73 )\ntmp77 =tl .broadcast_to (tmp30 ,[XBLOCK ,R0_BLOCK ])\ntmp78_mean_next ,tmp78_m2_next ,tmp78_weight_next =triton_helpers .welford_reduce (\ntmp77 ,tmp78_mean ,tmp78_m2 ,tmp78_weight ,roffset ==0 \n)\ntmp78_mean =tl .where (r0_mask &xmask ,tmp78_mean_next ,tmp78_mean )\ntmp78_m2 =tl .where (r0_mask &xmask ,tmp78_m2_next ,tmp78_m2 )\ntmp78_weight =tl .where (r0_mask &xmask ,tmp78_weight_next ,tmp78_weight )\ntl .store (out_ptr0 +(r0_3 +256 *x0 ),tmp30 ,r0_mask &xmask )\ntl .store (out_ptr1 +(r0_3 +256 *x0 ),tmp76 ,r0_mask &xmask )\ntmp81 ,tmp82 ,tmp83 =triton_helpers .welford (tmp78_mean ,tmp78_m2 ,tmp78_weight ,1 )\ntmp78 =tmp81 [:,None ]\ntmp79 =tmp82 [:,None ]\ntmp80 =tmp83 [:,None ]\ntmp92 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp94 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\nforr0_offsetinrange (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \nr0_mask =r0_index <r0_numel \nroffset =r0_offset \nrindex =r0_index \nr0_3 =r0_index \ntmp84 =tl .load (out_ptr0 +(r0_3 +256 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\ntmp85 =tmp84 -tmp78 \ntmp86 =256.0 \ntmp87 =tmp79 /tmp86 \ntmp88 =1e-05 \ntmp89 =tmp87 +tmp88 \ntmp90 =libdevice .rsqrt (tmp89 )\ntmp91 =tmp85 *tmp90 \ntmp93 =tmp91 *tmp92 \ntmp95 =tmp93 +tmp94 \ntl .store (out_ptr4 +(r0_3 +256 *x0 ),tmp95 ,r0_mask &xmask )\ntl .store (out_ptr5 +(r0_3 +256 *x0 ),tmp85 ,r0_mask &xmask )\ntmp105 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp112 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\ntmp96 =256.0 \ntmp97 =tmp79 /tmp96 \ntmp98 =1e-05 \ntmp99 =tmp97 +tmp98 \ntmp100 =libdevice .rsqrt (tmp99 )\ntmp101 =1.003921568627451 \ntmp102 =tmp97 *tmp101 \ntmp103 =0.1 \ntmp104 =tmp102 *tmp103 \ntmp106 =0.9 \ntmp107 =tmp105 *tmp106 \ntmp108 =tmp104 +tmp107 \ntmp109 =1.0 \ntmp110 =tmp108 /tmp109 \ntmp111 =tmp78 *tmp103 \ntmp113 =tmp112 *tmp106 \ntmp114 =tmp111 +tmp113 \ntmp115 =tmp114 /tmp109 \ntl .store (out_ptr6 +(x0 ),tmp100 ,xmask )\ntl .store (out_ptr8 +(x0 ),tmp110 ,xmask )\ntl .store (out_ptr10 +(x0 ),tmp115 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_convolution_mish_2 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =8192 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =tl .full ([XBLOCK ],True ,tl .int1 )\nx2 =xindex \nx1 =xindex //256 \ntmp0 =tl .load (in_out_ptr0 +(x2 ),None )\ntmp1 =tl .load (in_ptr0 +(x1 ),None ,eviction_policy ='evict_last')\ntmp2 =tmp0 +tmp1 \ntmp3 =20.0 \ntmp4 =tmp2 >tmp3 \ntmp5 =tl_math .exp (tmp2 )\ntmp6 =libdevice .log1p (tmp5 )\ntmp7 =tl .where (tmp4 ,tmp2 ,tmp6 )\ntmp8 =libdevice .tanh (tmp7 )\ntmp9 =tmp2 *tmp8 \ntl .store (in_out_ptr0 +(x2 ),tmp2 ,None )\ntl .store (out_ptr0 +(x2 ),tmp9 ,None )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_mish_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx0 =xindex \ntmp0 =tl .load (in_ptr0 +(x0 ),xmask )\ntmp1 =20.0 \ntmp2 =tmp0 >tmp1 \ntmp3 =tl_math .exp (tmp0 )\ntmp4 =libdevice .log1p (tmp3 )\ntmp5 =tl .where (tmp2 ,tmp0 ,tmp4 )\ntmp6 =libdevice .tanh (tmp5 )\ntmp7 =tmp0 *tmp6 \ntl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_per_fused_mean_4 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ):\n    xnumel =1 \nXBLOCK :tl .constexpr =1 \nr0_numel =256 \nR0_BLOCK :tl .constexpr =256 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =tl .full ([1 ],xoffset ,tl .int32 )\nxmask =tl .full ([R0_BLOCK ],True ,tl .int1 )\nr0_index =tl .arange (0 ,R0_BLOCK )[:]\nr0_offset =0 \nr0_mask =tl .full ([R0_BLOCK ],True ,tl .int1 )\nroffset =r0_offset \nrindex =r0_index \nr0_0 =r0_index \ntmp0 =tl .load (in_ptr0 +(r0_0 ),None )\ntmp1 =tl .broadcast_to (tmp0 ,[R0_BLOCK ])\ntmp3 =triton_helpers .promote_to_tensor (tl .sum (tmp1 ,0 ))\ntmp4 =256.0 \ntmp5 =tmp3 /tmp4 \ntl .debug_barrier ()\ntl .store (in_out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp5 ,None )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_per_fused_mean_5 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ):\n    xnumel =1 \nXBLOCK :tl .constexpr =1 \nr0_numel =256 \nR0_BLOCK :tl .constexpr =256 \nrnumel =r0_numel \nRBLOCK :tl .constexpr =R0_BLOCK \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =tl .full ([1 ],xoffset ,tl .int32 )\nxmask =tl .full ([R0_BLOCK ],True ,tl .int1 )\nr0_index =tl .arange (0 ,R0_BLOCK )[:]\nr0_offset =0 \nr0_mask =tl .full ([R0_BLOCK ],True ,tl .int1 )\nroffset =r0_offset \nrindex =r0_index \nr0_0 =r0_index \ntmp0 =tl .load (in_ptr0 +(r0_0 ),None )\ntmp1 =tl .broadcast_to (tmp0 ,[R0_BLOCK ])\ntmp3 =triton_helpers .promote_to_tensor (tl .sum (tmp1 ,0 ))\ntl .store (out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp3 ,None )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice ,mathastl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_add_mean_mul_6 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \nxoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx0 =xindex \ntmp0 =tl .load (in_ptr0 +(x0 ),xmask )\ntmp1 =tl .load (in_ptr1 +(0 ))\ntmp2 =tl .broadcast_to (tmp1 ,[XBLOCK ])\ntmp4 =tl .load (in_ptr2 +(0 ))\ntmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ])\ntmp3 =tmp0 *tmp2 \ntmp6 =256.0 \ntmp7 =tmp5 /tmp6 \ntmp8 =tmp3 +tmp7 \ntl .store (out_ptr0 +(x0 ),tmp8 ,xmask )\n\ndefcall (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 =args \nargs .clear ()\nassert_size_stride (primals_1 ,(16 ,3 ,3 ,3 ),(27 ,9 ,3 ,1 ))\nassert_size_stride (primals_2 ,(16 ,),(1 ,))\nassert_size_stride (primals_3 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\nassert_size_stride (primals_4 ,(16 ,),(1 ,))\nassert_size_stride (primals_5 ,(16 ,),(1 ,))\nassert_size_stride (primals_6 ,(16 ,),(1 ,))\nassert_size_stride (primals_7 ,(16 ,),(1 ,))\nassert_size_stride (primals_8 ,(32 ,16 ,3 ,3 ),(144 ,9 ,3 ,1 ))\nassert_size_stride (primals_9 ,(32 ,),(1 ,))\nassert_size_stride (primals_10 ,(128 ,8192 ),(8192 ,1 ))\nassert_size_stride (primals_11 ,(128 ,),(1 ,))\nassert_size_stride (primals_12 ,(10 ,128 ),(128 ,1 ))\nassert_size_stride (primals_13 ,(10 ,),(1 ,))\nassert_size_stride (primals_14 ,(16 ,16 ),(16 ,1 ))\nassert_size_stride (primals_15 ,(16 ,16 ),(16 ,1 ))\nwithtorch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\nbuf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,1 ),padding =(1 ,1 ),dilation =(1 ,1 ),transposed =False ,output_padding =(0 ,0 ),groups =1 ,bias =None )\nassert_size_stride (buf0 ,(1 ,16 ,64 ,64 ),(65536 ,4096 ,64 ,1 ))\nbuf1 =buf0 ;delbuf0 \nbuf2 =empty_strided_cuda ((1 ,16 ,64 ,64 ),(65536 ,4096 ,64 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_convolution_mish_0 [grid (65536 )](buf1 ,primals_2 ,buf2 ,65536 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\ndelprimals_2 \nbuf3 =empty_strided_cuda ((1 ,16 ,16 ,16 ),(4096 ,256 ,16 ,1 ),torch .float32 )\nbuf4 =empty_strided_cuda ((1 ,16 ,16 ,16 ),(4096 ,256 ,16 ,1 ),torch .int8 )\nbuf9 =empty_strided_cuda ((1 ,16 ,16 ,16 ),(4096 ,256 ,16 ,1 ),torch .float32 )\nbuf20 =empty_strided_cuda ((1 ,16 ,16 ,16 ),(4096 ,256 ,16 ,1 ),torch .float32 )\nbuf8 =empty_strided_cuda ((1 ,16 ,1 ,1 ),(16 ,1 ,16 ,16 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_red_fused__native_batch_norm_legit_functional_adaptive_max_pool2d_mean_native_batch_norm_backward_1 [grid (16 )](buf2 ,primals_6 ,primals_7 ,primals_5 ,primals_4 ,buf3 ,buf4 ,buf9 ,buf20 ,buf8 ,primals_5 ,primals_4 ,16 ,256 ,XBLOCK =2 ,R0_BLOCK =256 ,num_warps =4 ,num_stages =1 )\ndelbuf3 \ndelprimals_4 \ndelprimals_5 \ndelprimals_7 \n\nbuf10 =extern_kernels .convolution (buf9 ,primals_8 ,stride =(1 ,1 ),padding =(1 ,1 ),dilation =(1 ,1 ),transposed =False ,output_padding =(0 ,0 ),groups =1 ,bias =None )\nassert_size_stride (buf10 ,(1 ,32 ,16 ,16 ),(8192 ,256 ,16 ,1 ))\nbuf11 =buf10 ;delbuf10 \nbuf12 =empty_strided_cuda ((1 ,32 ,16 ,16 ),(8192 ,256 ,16 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_convolution_mish_2 [grid (8192 )](buf11 ,primals_9 ,buf12 ,8192 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\ndelprimals_9 \nbuf13 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\nextern_kernels .addmm (primals_11 ,reinterpret_tensor (buf12 ,(1 ,8192 ),(0 ,1 ),0 ),reinterpret_tensor (primals_10 ,(8192 ,128 ),(1 ,8192 ),0 ),alpha =1 ,beta =1 ,out =buf13 )\ndelprimals_11 \nbuf14 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_mish_3 [grid (128 )](buf13 ,buf14 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\nbuf15 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n\nextern_kernels .addmm (primals_13 ,buf14 ,reinterpret_tensor (primals_12 ,(128 ,10 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf15 )\ndelprimals_13 \nbuf16 =empty_strided_cuda ((),(),torch .float32 )\nbuf17 =buf16 ;delbuf16 \n\nstream0 =get_raw_stream (0 )\ntriton_per_fused_mean_4 [grid (1 )](buf17 ,primals_14 ,1 ,256 ,num_warps =2 ,num_stages =1 )\ndelprimals_14 \nbuf18 =empty_strided_cuda ((),(),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_per_fused_mean_5 [grid (1 )](primals_15 ,buf18 ,1 ,256 ,num_warps =2 ,num_stages =1 )\ndelprimals_15 \nbuf19 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_add_mean_mul_6 [grid (10 )](buf15 ,buf17 ,buf18 ,buf19 ,10 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\ndelbuf18 \nreturn (buf19 ,primals_1 ,primals_3 ,primals_6 ,primals_8 ,buf1 ,buf2 ,buf4 ,reinterpret_tensor (buf8 ,(16 ,),(1 ,),0 ),buf9 ,buf11 ,reinterpret_tensor (buf12 ,(1 ,8192 ),(8192 ,1 ),0 ),buf13 ,buf14 ,buf15 ,buf17 ,primals_12 ,primals_10 ,buf20 ,)\n\ndefbenchmark_compiled_module (times =10 ,repeat =10 ):\n    fromtorch ._dynamo .testingimportrand_strided \nfromtorch ._inductor .utilsimportprint_performance \nprimals_1 =rand_strided ((16 ,3 ,3 ,3 ),(27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_2 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_3 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_4 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_5 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_6 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_7 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_8 =rand_strided ((32 ,16 ,3 ,3 ),(144 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_9 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_10 =rand_strided ((128 ,8192 ),(8192 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_11 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_12 =rand_strided ((10 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_13 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\nprimals_14 =rand_strided ((16 ,16 ),(16 ,1 ),device ='cuda:0',dtype =torch .float32 )\nprimals_15 =rand_strided ((16 ,16 ),(16 ,1 ),device ='cuda:0',dtype =torch .float32 )\nfn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ])\nreturnprint_performance (fn ,times =times ,repeat =repeat )\n\nif__name__ ==\"__main__\":\n    fromtorch ._inductor .wrapper_benchmarkimportcompiled_module_main \ncompiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "03fdc34a-22af-4bb2-8c12-dad84143b675",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool2d', 'LocalResponseNorm', 'AdaptiveMaxPool2d', 'CircularPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.local_response_norm = nn.LocalResponseNorm(size=5)\n        self.adaptive_maxpool2d = nn.AdaptiveMaxPool2d(output_size=(5, 5))\n        self.circular_pad3d = nn.CircularPad3d(padding=(1, 1, 1, 1, 1, 1))\n\n    def forward(self, x):\n        # Assuming input is 4D (batch, channels, height, width)\n        x = self.maxpool2d(x)\n        x = self.local_response_norm(x)\n        x = self.adaptive_maxpool2d(x)\n        \n        # Reshape to 5D (batch, channels, depth, height, width) for CircularPad3d\n        x = x.unsqueeze(2)  # Add depth dimension\n        x = self.circular_pad3d(x)\n        \n        # Reshape back to 4D (batch, channels, height, width)\n        x = x.squeeze(2)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input: batch_size=1, channels=3, height=64, width=64\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimporttorch \nimporttriton \nimporttriton .languageastl \nfromtorch ._inductor .runtime .triton_heuristicsimport (\ngrid ,\n)\nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx2 =xindex //ks0 \nx0 =(xindex %ks2 )\nx1 =((xindex //ks2 )%ks3 )\nx4 =xindex \ntmp0 =(-2 )+x2 \ntmp1 =tl .full ([1 ],0 ,tl .int64 )\ntmp2 =tmp0 >=tmp1 \ntmp3 =ks1 \ntmp4 =tmp0 <tmp3 \ntmp5 =tmp2 &tmp4 \ntmp6 =tl .load (in_ptr0 +(2 *x0 +((-2 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp7 =tl .load (in_ptr0 +(1 +2 *x0 +((-2 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp8 =triton_helpers .maximum (tmp7 ,tmp6 )\ntmp9 =tl .load (in_ptr0 +(ks5 +2 *x0 +((-2 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\ntmp11 =tl .load (in_ptr0 +(1 +ks5 +2 *x0 +((-2 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\ntmp13 =tmp12 *tmp12 \ntmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\ntmp15 =tl .where (tmp5 ,tmp13 ,tmp14 )\ntl .store (out_ptr0 +(x4 ),tmp15 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_add_div_max_pool2d_with_indices_mul_pow_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx0 =(xindex %ks0 )\nx1 =((xindex //ks0 )%ks1 )\nx2 =xindex //ks2 \nx3 =xindex \ntmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\ntmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\ntmp3 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\ntmp5 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\ntmp7 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\ntmp8 =tl .load (in_ptr1 +(ks2 +x3 ),xmask ,eviction_policy ='evict_last')\ntmp10 =tl .load (in_ptr1 +(x3 +2 *ks0 *ks1 ),xmask ,eviction_policy ='evict_last')\ntmp12 =tl .load (in_ptr1 +(x3 +3 *ks0 *ks1 ),xmask ,eviction_policy ='evict_last')\ntmp14 =tl .load (in_ptr1 +(x3 +4 *ks0 *ks1 ),xmask ,eviction_policy ='evict_last')\ntmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\ntmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\ntmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\ntmp9 =tmp8 +tmp7 \ntmp11 =tmp10 +tmp9 \ntmp13 =tmp12 +tmp11 \ntmp15 =tmp14 +tmp13 \ntmp16 =0.2 \ntmp17 =tmp15 *tmp16 \ntmp18 =0.0001 \ntmp19 =tmp17 *tmp18 \ntmp20 =1.0 \ntmp21 =tmp19 +tmp20 \ntmp22 =0.75 \ntmp23 =libdevice .pow (tmp21 ,tmp22 )\ntmp24 =tmp6 /tmp23 \ntl .store (out_ptr0 +(x3 ),tmp24 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_copy_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx0 =(xindex %7 )\nx1 =((xindex //7 )%7 )\nx2 =((xindex //49 )%3 )\nx3 =xindex //147 \nx5 =xindex \ntmp0 =x0 \ntmp1 =tl .full ([1 ],1 ,tl .int64 )\ntmp2 =tmp0 <tmp1 \ntmp3 =5 +x0 \ntmp4 =tl .full ([1 ],1 ,tl .int64 )\ntmp5 =tmp3 >=tmp4 \ntmp6 =tl .full ([1 ],6 ,tl .int64 )\ntmp7 =tmp3 <tmp6 \ntmp8 =tmp5 &tmp7 \ntmp9 =tmp8 &tmp2 \ntmp10 =x1 \ntmp11 =tl .full ([1 ],1 ,tl .int64 )\ntmp12 =tmp10 >=tmp11 \ntmp13 =tl .full ([1 ],6 ,tl .int64 )\ntmp14 =tmp10 <tmp13 \ntmp15 =tmp12 &tmp14 \ntmp16 =tmp15 &tmp9 \ntmp17 =x2 \ntmp18 =tl .full ([1 ],1 ,tl .int64 )\ntmp19 =tmp17 >=tmp18 \ntmp20 =tl .full ([1 ],2 ,tl .int64 )\ntmp21 =tmp17 <tmp20 \ntmp22 =tmp19 &tmp21 \ntmp23 =tmp22 &tmp16 \ntmp24 =tl .load (in_ptr0 +((-1 )+x0 +5 *x1 +25 *x3 ),tmp23 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp25 =tl .load (in_ptr1 +(5 +x5 ),tmp16 &xmask ,other =0.0 )\ntmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\ntmp27 =tl .full (tmp26 .shape ,0.0 ,tmp26 .dtype )\ntmp28 =tl .where (tmp16 ,tmp26 ,tmp27 )\ntmp29 =tl .load (in_ptr1 +(5 +x5 ),tmp9 &xmask ,other =0.0 )\ntmp30 =tl .where (tmp15 ,tmp28 ,tmp29 )\ntmp31 =tl .full (tmp30 .shape ,0.0 ,tmp30 .dtype )\ntmp32 =tl .where (tmp9 ,tmp30 ,tmp31 )\ntmp33 =float (\"nan\")\ntmp34 =tl .where (tmp8 ,tmp32 ,tmp33 )\ntmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\ntmp36 =tl .where (tmp2 ,tmp34 ,tmp35 )\ntmp37 =tmp0 >=tmp1 \ntmp38 =tl .full ([1 ],6 ,tl .int64 )\ntmp39 =tmp0 <tmp38 \ntmp40 =tmp37 &tmp39 \ntmp41 =x1 \ntmp42 =tl .full ([1 ],1 ,tl .int64 )\ntmp43 =tmp41 >=tmp42 \ntmp44 =tl .full ([1 ],6 ,tl .int64 )\ntmp45 =tmp41 <tmp44 \ntmp46 =tmp43 &tmp45 \ntmp47 =tmp46 &tmp40 \ntmp48 =x2 \ntmp49 =tl .full ([1 ],1 ,tl .int64 )\ntmp50 =tmp48 >=tmp49 \ntmp51 =tl .full ([1 ],2 ,tl .int64 )\ntmp52 =tmp48 <tmp51 \ntmp53 =tmp50 &tmp52 \ntmp54 =tmp53 &tmp47 \ntmp55 =tl .load (in_ptr0 +((-6 )+x0 +5 *x1 +25 *x3 ),tmp54 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp56 =tl .load (in_ptr1 +(x5 ),tmp47 &xmask ,other =0.0 )\ntmp57 =tl .where (tmp53 ,tmp55 ,tmp56 )\ntmp58 =tl .full (tmp57 .shape ,0.0 ,tmp57 .dtype )\ntmp59 =tl .where (tmp47 ,tmp57 ,tmp58 )\ntmp60 =tl .load (in_ptr1 +(x5 ),tmp40 &xmask ,other =0.0 )\ntmp61 =tl .where (tmp46 ,tmp59 ,tmp60 )\ntmp62 =tl .full (tmp61 .shape ,0.0 ,tmp61 .dtype )\ntmp63 =tl .where (tmp40 ,tmp61 ,tmp62 )\ntmp64 =float (\"nan\")\ntmp65 =tl .where (tmp40 ,tmp63 ,tmp64 )\ntmp66 =tl .where (tmp2 ,tmp36 ,tmp65 )\ntl .store (out_ptr0 +(x5 ),tmp66 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx1 =((xindex //7 )%7 )\nx0 =(xindex %7 )\nx3 =xindex //7 \nx4 =xindex \ntmp40 =tl .load (in_ptr0 +(x4 ),xmask )\ntmp0 =x1 \ntmp1 =tl .full ([1 ],6 ,tl .int64 )\ntmp2 =tmp0 >=tmp1 \ntmp3 =(-5 )+x1 \ntmp4 =tl .full ([1 ],1 ,tl .int64 )\ntmp5 =tmp3 <tmp4 \ntmp6 =tmp5 &tmp2 \ntmp7 =x0 \ntmp8 =tl .full ([1 ],6 ,tl .int64 )\ntmp9 =tmp7 >=tmp8 \ntmp10 =tmp9 &tmp6 \ntmp11 =tl .load (in_ptr0 +(1 +7 *x3 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp12 =tl .load (in_ptr0 +(x4 ),tmp6 &xmask ,other =0.0 )\ntmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\ntmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\ntmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\ntmp16 =x0 \ntmp17 =tl .full ([1 ],6 ,tl .int64 )\ntmp18 =tmp16 >=tmp17 \ntmp19 =tmp18 &tmp2 \ntmp20 =tl .load (in_ptr0 +((-34 )+7 *x3 ),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp21 =tl .load (in_ptr0 +((-35 )+x4 ),tmp2 &xmask ,other =0.0 )\ntmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\ntmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\ntmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\ntmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\ntmp26 =tl .full ([1 ],1 ,tl .int64 )\ntmp27 =tmp0 <tmp26 \ntmp28 =x0 \ntmp29 =tl .full ([1 ],6 ,tl .int64 )\ntmp30 =tmp28 >=tmp29 \ntmp31 =tmp30 &tmp27 \ntmp32 =tl .load (in_ptr0 +(36 +7 *x3 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp33 =tl .load (in_ptr0 +(35 +x4 ),tmp27 &xmask ,other =0.0 )\ntmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\ntmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\ntmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\ntmp37 =x0 \ntmp38 =tmp37 >=tmp1 \ntmp39 =tl .load (in_ptr0 +(1 +7 *x3 ),tmp38 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp41 =tl .where (tmp38 ,tmp39 ,tmp40 )\ntmp42 =tl .where (tmp27 ,tmp36 ,tmp41 )\ntmp43 =tl .where (tmp2 ,tmp25 ,tmp42 )\ntl .store (out_ptr0 +(x4 ),tmp43 ,xmask )\n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \nfromtorch ._inductor .runtime .triton_helpersimportlibdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused_4 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx1 =((xindex //49 )%3 )\nx0 =(xindex %49 )\nx2 =xindex //147 \nx3 =xindex \ntmp15 =tl .load (in_ptr0 +(x3 ),xmask )\ntmp0 =x1 \ntmp1 =tl .full ([1 ],2 ,tl .int64 )\ntmp2 =tmp0 >=tmp1 \ntmp3 =(-1 )+x1 \ntmp4 =tl .full ([1 ],1 ,tl .int64 )\ntmp5 =tmp3 <tmp4 \ntmp6 =tmp5 &tmp2 \ntmp7 =tl .load (in_ptr0 +(49 +x0 +147 *x2 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp8 =tl .load (in_ptr0 +((-49 )+x3 ),tmp2 &xmask ,other =0.0 )\ntmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\ntmp10 =tl .full (tmp9 .shape ,0.0 ,tmp9 .dtype )\ntmp11 =tl .where (tmp2 ,tmp9 ,tmp10 )\ntmp12 =tl .full ([1 ],1 ,tl .int64 )\ntmp13 =tmp0 <tmp12 \ntmp14 =tl .load (in_ptr0 +(49 +x0 +147 *x2 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\ntmp16 =tl .where (tmp13 ,tmp14 ,tmp15 )\ntmp17 =tl .where (tmp2 ,tmp11 ,tmp16 )\ntl .store (out_ptr0 +(x3 ),tmp17 ,xmask )\n\ndefcall (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \nargs .clear ()\ns0 =arg0_1 \ns1 =arg1_1 \ns2 =arg2_1 \nassert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\nwithtorch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\nps0 =(s1 //2 )*(s2 //2 )\nps1 =s2 //2 \nps2 =s1 //2 \nbuf0 =empty_strided_cuda ((1 ,1 ,4 +s0 ,s1 //2 ,s2 //2 ),(4 *(s1 //2 )*(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),4 *(s1 //2 )*(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\ntriton_poi_fused_constant_pad_nd_0_xnumel =4 *(s1 //2 )*(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg3_1 ,buf0 ,1024 ,3 ,32 ,32 ,64 ,64 ,7168 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\nbuf1 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\ntriton_poi_fused_add_div_max_pool2d_with_indices_mul_pow_1_xnumel =s0 *(s1 //2 )*(s2 //2 )\nstream0 =get_raw_stream (0 )\ntriton_poi_fused_add_div_max_pool2d_with_indices_mul_pow_1 [grid (triton_poi_fused_add_div_max_pool2d_with_indices_mul_pow_1_xnumel )](arg3_1 ,buf0 ,buf1 ,32 ,32 ,1024 ,64 ,64 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\ndelarg3_1 \ndelbuf0 \n\nbuf2 =torch .ops .aten .adaptive_max_pool2d .default (buf1 ,[5 ,5 ])\ndelbuf1 \nbuf3 =buf2 [0 ]\ndelbuf2 \nbuf5 =empty_strided_cuda ((1 ,s0 ,3 ,7 ,7 ),(147 *s0 ,147 ,49 ,7 ,1 ),torch .float32 )\nbuf6 =empty_strided_cuda ((1 ,s0 ,3 ,7 ,7 ),(147 *s0 ,147 ,49 ,7 ,1 ),torch .float32 )\n\ntriton_poi_fused_copy_2_xnumel =147 *s0 \nstream0 =get_raw_stream (0 )\ntriton_poi_fused_copy_2 [grid (triton_poi_fused_copy_2_xnumel )](buf3 ,buf5 ,buf6 ,441 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\ndelbuf3 \nbuf7 =buf5 ;delbuf5 \n\ntriton_poi_fused_3_xnumel =147 *s0 \nstream0 =get_raw_stream (0 )\ntriton_poi_fused_3 [grid (triton_poi_fused_3_xnumel )](buf6 ,buf7 ,441 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\nbuf8 =buf6 ;delbuf6 \n\ntriton_poi_fused_4_xnumel =147 *s0 \nstream0 =get_raw_stream (0 )\ntriton_poi_fused_4 [grid (triton_poi_fused_4_xnumel )](buf7 ,buf8 ,441 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\ndelbuf7 \nreturn (buf8 ,)\n\ndefbenchmark_compiled_module (times =10 ,repeat =10 ):\n    fromtorch ._dynamo .testingimportrand_strided \nfromtorch ._inductor .utilsimportprint_performance \narg0_1 =3 \narg1_1 =64 \narg2_1 =64 \narg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\nfn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\nreturnprint_performance (fn ,times =times ,repeat =repeat )\n\nif__name__ ==\"__main__\":\n    fromtorch ._inductor .wrapper_benchmarkimportcompiled_module_main \ncompiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "04f72e11-221a-4a10-90e6-10cce4ec010e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CrossEntropyLoss', 'AdaptiveAvgPool1d', 'GRU', 'Flatten', 'NLLLoss2d', 'Sigmoid', 'SyncBatchNorm']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=64)\n        self.gru1 = nn.GRU(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.gru2 = nn.GRU(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n        self.flatten = nn.Flatten()\n        self.sync_batch_norm = nn.SyncBatchNorm(64)\n        self.sigmoid = nn.Sigmoid()\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n        self.nll_loss2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, sequence_length)\n        x = self.adaptive_avg_pool1d(x)  # Shape: (batch_size, channels, 64)\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, 64, channels)\n        \n        # GRU layers\n        x, _ = self.gru1(x)  # Shape: (batch_size, 64, 128)\n        x, _ = self.gru2(x)  # Shape: (batch_size, 64, 64)\n        \n        # SyncBatchNorm\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, 64, 64)\n        x = self.sync_batch_norm(x)\n        \n        # Flatten\n        x = self.flatten(x)  # Shape: (batch_size, 64 * 64)\n        \n        # Sigmoid\n        x = self.sigmoid(x)  # Shape: (batch_size, 64 * 64)\n        \n        # Reshape for NLLLoss2d\n        x = x.view(-1, 64, 8, 8)  # Shape: (batch_size, 64, 8, 8)\n        \n        # NLLLoss2d requires log probabilities, so we apply log_softmax\n        x = F.log_softmax(x, dim=1)  # Shape: (batch_size, 64, 8, 8)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 128).cuda()  # Example input shape: (batch_size, channels, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimporttorch \nimporttriton \nimporttriton .languageastl \nfromtorch ._inductor .runtime .triton_heuristicsimport (\ngrid ,\n)\nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \nfromtorch ._Cimport_cuda_getCurrentRawStreamasget_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimporttriton \nimporttriton .languageastl \n\nfromtorch ._inductor .runtimeimporttriton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndeftriton_poi_fused__adaptive_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \nxindex =xoffset +tl .arange (0 ,XBLOCK )[:]\nxmask =xindex <xnumel \nx0 =xindex \ntmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\ntmp1 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\ntmp2 =tmp1 +tmp0 \ntmp3 =0.5 \ntmp4 =tmp2 *tmp3 \ntl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\ndefcall (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \nargs .clear ()\ns0 =arg0_1 \ns1 =arg1_1 \nassert_size_stride (arg2_1 ,(1 ,s0 ,128 ),(128 *s0 ,128 ,1 ))\nwithtorch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\nbuf0 =empty_strided_cuda ((1 ,s0 ,1 ,64 ),(64 *s0 ,64 ,64 ,1 ),torch .float32 )\n\ntriton_poi_fused__adaptive_avg_pool2d_0_xnumel =64 *s0 \nstream0 =get_raw_stream (0 )\ntriton_poi_fused__adaptive_avg_pool2d_0 [grid (triton_poi_fused__adaptive_avg_pool2d_0_xnumel )](arg2_1 ,buf0 ,192 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\ndelarg2_1 \nreturn (reinterpret_tensor (buf0 ,(1 ,64 ,s0 ),(64 *s0 ,1 ,64 ),0 ),)\n\ndefbenchmark_compiled_module (times =10 ,repeat =10 ):\n    fromtorch ._dynamo .testingimportrand_strided \nfromtorch ._inductor .utilsimportprint_performance \narg0_1 =3 \narg1_1 =128 \narg2_1 =rand_strided ((1 ,3 ,128 ),(384 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\nfn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\nreturnprint_performance (fn ,times =times ,repeat =repeat )\n\nif__name__ ==\"__main__\":\n    fromtorch ._inductor .wrapper_benchmarkimportcompiled_module_main \ncompiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "07e34d22-b593-4780-9217-3e2da6347b55",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReflectionPad2d', 'ReLU', 'AvgPool2d', 'BCELoss', 'GELU', 'LPPool3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reflection_pad = nn.ReflectionPad2d(2)\n        self.relu = nn.ReLU()\n        self.avg_pool = nn.AvgPool2d(kernel_size=2)\n        self.gelu = nn.GELU()\n        self.lp_pool = nn.LPPool3d(norm_type=2, kernel_size=3, stride=2)\n        self.bce_loss = nn.BCELoss()\n\n    def forward(self, x):\n        # Apply ReflectionPad2d\n        x = self.reflection_pad(x)\n        \n        # Apply ReLU\n        x = self.relu(x)\n        \n        # Apply AvgPool2d\n        x = self.avg_pool(x)\n        \n        # Apply GELU\n        x = self.gelu(x)\n        \n        # Reshape for LPPool3d\n        x = x.unsqueeze(1)  # Add a channel dimension for 3D pooling\n        x = self.lp_pool(x)\n        \n        # Reshape back to 2D\n        x = x.squeeze(1)\n        \n        # Apply ReLU again\n        x = self.relu(x)\n        \n        # Apply AvgPool2d again\n        x = self.avg_pool(x)\n        \n        # Apply GELU again\n        x = self.gelu(x)\n        \n        # Compute BCE Loss (assuming binary classification)\n        # For BCE Loss, we need a target tensor. Here, we generate a dummy target.\n        target = torch.zeros_like(x)\n        loss = self.bce_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Assuming 3 channels for input\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_reflection_pad2d_relu_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp6 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp1 ,tmp3 )\n    tmp5 =tmp4 +tmp2 \n    tmp7 =triton_helpers .maximum (tmp1 ,tmp6 )\n    tmp8 =tmp7 +tmp5 \n    tmp10 =triton_helpers .maximum (tmp1 ,tmp9 )\n    tmp11 =tmp10 +tmp8 \n    tmp12 =0.25 \n    tmp13 =tmp11 *tmp12 \n    tl .store (out_ptr0 +(x3 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_pow_1 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =0.5 \n    tmp2 =tmp0 *tmp1 \n    tmp3 =0.7071067811865476 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .erf (tmp4 )\n    tmp6 =1.0 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =tmp2 *tmp7 \n    tmp9 =tmp8 *tmp8 \n    tl .store (in_out_ptr0 +(x0 ),tmp9 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_relu_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x2 +2 *x0 +2 *x1 +x2 *(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 ))+x2 *(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+x2 *(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 ))*(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(1 +x2 +2 *x0 +2 *x1 +x2 *(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 ))+x2 *(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+x2 *(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 ))*(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr0 +(1 +x2 +2 *x0 +2 *x1 +x2 *(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 ))+x2 *(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+x2 *(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 ))*(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))),xmask ,eviction_policy ='evict_last')\n    tmp43 =tl .load (in_ptr0 +(2 +x2 +2 *x0 +2 *x1 +x2 *(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 ))+x2 *(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+x2 *(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 ))*(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))+(triton_helpers .div_floor_integer ((-1 )+(ks4 //2 ),2 ))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tmp1 <tmp0 \n    tmp3 =tmp2 .to (tl .int8 )\n    tmp4 =tmp0 <tmp1 \n    tmp5 =tmp4 .to (tl .int8 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =tmp6 .to (tmp0 .dtype )\n    tmp8 =tl_math .abs (tmp0 )\n    tmp9 =triton_helpers .maximum (tmp1 ,tmp8 )\n    tmp10 =tmp7 *tmp9 \n    tmp11 =27.0 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =libdevice .sqrt (tmp12 )\n    tmp14 =triton_helpers .maximum (tmp1 ,tmp13 )\n    tmp16 =tmp1 <tmp15 \n    tmp17 =tmp16 .to (tl .int8 )\n    tmp18 =tmp15 <tmp1 \n    tmp19 =tmp18 .to (tl .int8 )\n    tmp20 =tmp17 -tmp19 \n    tmp21 =tmp20 .to (tmp15 .dtype )\n    tmp22 =tl_math .abs (tmp15 )\n    tmp23 =triton_helpers .maximum (tmp1 ,tmp22 )\n    tmp24 =tmp21 *tmp23 \n    tmp25 =tmp24 *tmp11 \n    tmp26 =libdevice .sqrt (tmp25 )\n    tmp27 =triton_helpers .maximum (tmp1 ,tmp26 )\n    tmp28 =tmp27 +tmp14 \n    tmp30 =tmp1 <tmp29 \n    tmp31 =tmp30 .to (tl .int8 )\n    tmp32 =tmp29 <tmp1 \n    tmp33 =tmp32 .to (tl .int8 )\n    tmp34 =tmp31 -tmp33 \n    tmp35 =tmp34 .to (tmp29 .dtype )\n    tmp36 =tl_math .abs (tmp29 )\n    tmp37 =triton_helpers .maximum (tmp1 ,tmp36 )\n    tmp38 =tmp35 *tmp37 \n    tmp39 =tmp38 *tmp11 \n    tmp40 =libdevice .sqrt (tmp39 )\n    tmp41 =triton_helpers .maximum (tmp1 ,tmp40 )\n    tmp42 =tmp41 +tmp28 \n    tmp44 =tmp1 <tmp43 \n    tmp45 =tmp44 .to (tl .int8 )\n    tmp46 =tmp43 <tmp1 \n    tmp47 =tmp46 .to (tl .int8 )\n    tmp48 =tmp45 -tmp47 \n    tmp49 =tmp48 .to (tmp43 .dtype )\n    tmp50 =tl_math .abs (tmp43 )\n    tmp51 =triton_helpers .maximum (tmp1 ,tmp50 )\n    tmp52 =tmp49 *tmp51 \n    tmp53 =tmp52 *tmp11 \n    tmp54 =libdevice .sqrt (tmp53 )\n    tmp55 =triton_helpers .maximum (tmp1 ,tmp54 )\n    tmp56 =tmp55 +tmp42 \n    tmp57 =0.25 \n    tmp58 =tmp56 *tmp57 \n    tl .store (out_ptr0 +(x3 ),tmp58 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_binary_cross_entropy_gelu_zeros_like_3 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp21 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =(r0_index %ks0 )\n        r0_1 =r0_index //ks0 \n        tmp0 =tl .load (in_ptr0 +(r0_0 +ks1 *r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =0.5 \n        tmp2 =tmp0 *tmp1 \n        tmp3 =0.7071067811865476 \n        tmp4 =tmp0 *tmp3 \n        tmp5 =libdevice .erf (tmp4 )\n        tmp6 =1.0 \n        tmp7 =tmp5 +tmp6 \n        tmp8 =tmp2 *tmp7 \n        tmp9 =-tmp8 \n        tmp10 =libdevice .log1p (tmp9 )\n        tmp11 =-100.0 \n        tmp12 =triton_helpers .maximum (tmp10 ,tmp11 )\n        tmp13 =-1.0 \n        tmp14 =tmp13 *tmp12 \n        tmp15 =tl_math .log (tmp8 )\n        tmp16 =triton_helpers .maximum (tmp15 ,tmp11 )\n        tmp17 =0.0 \n        tmp18 =tmp17 *tmp16 \n        tmp19 =tmp14 -tmp18 \n        tmp20 =tl .broadcast_to (tmp19 ,[XBLOCK ,R0_BLOCK ])\n        tmp22 =_tmp21 +tmp20 \n        _tmp21 =tl .where (r0_mask ,tmp22 ,_tmp21 )\n    tmp21 =tl .sum (_tmp21 ,1 )[:,None ]\n    tmp23 =1 +(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks2 //2 ),2 )),2 ))*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 )),2 ))+(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks2 //2 ),2 )),2 ))+(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks3 //2 ),2 )),2 ))\n    tmp24 =tmp23 .to (tl .float32 )\n    tmp25 =tmp21 /tmp24 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp25 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +(s2 //2 )\n        2 +(s1 //2 )\n        4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +(s1 //2 ),2 +(s2 //2 )),(4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 ),2 +(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_reflection_pad2d_relu_0_xnumel =4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_reflection_pad2d_relu_0 [grid (triton_poi_fused_avg_pool2d_reflection_pad2d_relu_0_xnumel )](arg3_1 ,buf0 ,34 ,34 ,1156 ,64 ,64 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =reinterpret_tensor (buf0 ,(1 ,1 ,s0 ,2 +(s1 //2 ),2 +(s2 //2 )),(4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 ),2 +(s2 //2 ),1 ),0 );del buf0 \n\n        triton_poi_fused_pow_1_xnumel =4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_pow_1 [grid (triton_poi_fused_pow_1_xnumel )](buf1 ,3468 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        buf2 =torch .ops .aten .avg_pool3d .default (buf1 ,[3 ,3 ,3 ],[2 ,2 ,2 ],[0 ,0 ,0 ],False ,True ,None )\n        del buf1 \n        buf3 =buf2 \n        del buf2 \n        (1 +(((-1 )+(s2 //2 ))//2 ))//2 \n        (1 +(((-1 )+(s1 //2 ))//2 ))//2 \n        ((1 +(((-1 )+(s1 //2 ))//2 ))//2 )*((1 +(((-1 )+(s2 //2 ))//2 ))//2 )\n        buf4 =empty_strided_cuda ((1 ,1 +(((-3 )+s0 )//2 ),(1 +(((-1 )+(s1 //2 ))//2 ))//2 ,(1 +(((-1 )+(s2 //2 ))//2 ))//2 ),(((1 +(((-1 )+(s1 //2 ))//2 ))//2 )*((1 +(((-1 )+(s2 //2 ))//2 ))//2 )+((1 +(((-1 )+(s1 //2 ))//2 ))//2 )*((1 +(((-1 )+(s2 //2 ))//2 ))//2 )*(((-3 )+s0 )//2 ),((1 +(((-1 )+(s1 //2 ))//2 ))//2 )*((1 +(((-1 )+(s2 //2 ))//2 ))//2 ),(1 +(((-1 )+(s2 //2 ))//2 ))//2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_relu_2_xnumel =((1 +(((-1 )+(s1 //2 ))//2 ))//2 )*((1 +(((-1 )+(s2 //2 ))//2 ))//2 )+((1 +(((-1 )+(s1 //2 ))//2 ))//2 )*((1 +(((-1 )+(s2 //2 ))//2 ))//2 )*(((-3 )+s0 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_relu_2 [grid (triton_poi_fused_avg_pool2d_relu_2_xnumel )](buf3 ,buf4 ,8 ,8 ,64 ,64 ,64 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del buf3 \n        1 +(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )\n        buf5 =empty_strided_cuda ((),(),torch .float32 )\n        buf6 =buf5 ;del buf5 \n\n        1 +(((-1 )+(((-1 )+(s1 //2 ))//2 ))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )+(((-1 )+(((-1 )+(s1 //2 ))//2 ))//2 )+(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )\n        get_raw_stream (0 )\n        triton_red_fused_binary_cross_entropy_gelu_zeros_like_3 [grid (1 )](buf6 ,buf4 ,8 ,8 ,64 ,64 ,1 ,64 ,XBLOCK =1 ,R0_BLOCK =64 ,num_warps =2 ,num_stages =1 )\n        del buf4 \n    return (buf6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "08b8ad7e-15dc-427d-82d1-b0bc18676925",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LPPool1d', 'HuberLoss', 'UpsamplingBilinear2d', 'Bilinear', 'ZeroPad3d', 'LogSigmoid', 'ConstantPad1d', 'InstanceNorm2d', 'ReflectionPad2d', 'RNNCell']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lp_pool = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.bilinear = nn.Bilinear(10, 10, 20)\n        self.zero_pad = nn.ZeroPad3d(1)\n        self.log_sigmoid = nn.LogSigmoid()\n        self.constant_pad = nn.ConstantPad1d(2, 0.5)\n        self.instance_norm = nn.InstanceNorm2d(10)\n        self.reflection_pad = nn.ReflectionPad2d(2)\n        self.rnn_cell = nn.RNNCell(20, 10)\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        x = self.reflection_pad(x)  # Apply ReflectionPad2d\n        x = self.instance_norm(x)  # Apply InstanceNorm2d\n        x = self.upsample(x)  # Apply UpsamplingBilinear2d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape for LPPool1d\n        x = self.lp_pool(x)  # Apply LPPool1d\n        x = x.view(x.size(0), x.size(1), 1, 1)  # Reshape back to 4D\n        x = self.zero_pad(x)  # Apply ZeroPad3d\n        x = x.view(x.size(0), -1)  # Flatten for RNNCell\n        x = self.rnn_cell(x, torch.zeros(x.size(0), 10).to(x.device))  # Apply RNNCell\n        x = self.bilinear(x, x)  # Apply Bilinear\n        x = self.constant_pad(x)  # Apply ConstantPad1d\n        x = self.log_sigmoid(x)  # Apply LogSigmoid\n        # Assuming target is a tensor of the same shape as x\n        target = torch.randn_like(x)\n        loss = self.huber_loss(x, target)  # Apply HuberLoss\n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_reflection_pad2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+x0 )))))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x3 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_reflection_pad2d_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_reflection_pad2d_0 [grid (triton_poi_fused_reflection_pad2d_0_xnumel )](arg3_1 ,buf0 ,36 ,36 ,1296 ,32 ,32 ,3888 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "09423269-bcef-4dca-9673-b2bb18519e36",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool2d', 'Identity', 'CircularPad2d', 'InstanceNorm2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.identity1 = nn.Identity()\n        self.circular_pad1 = nn.CircularPad2d(padding=1)\n        self.instance_norm1 = nn.InstanceNorm2d(num_features=3)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.identity2 = nn.Identity()\n        self.circular_pad2 = nn.CircularPad2d(padding=1)\n        self.instance_norm2 = nn.InstanceNorm2d(num_features=3)\n\n    def forward(self, x):\n        x = self.maxpool1(x)\n        x = self.identity1(x)\n        x = self.circular_pad1(x)\n        x = self.instance_norm1(x)\n        x = self.maxpool2(x)\n        x = self.identity2(x)\n        x = self.circular_pad2(x)\n        x = self.instance_norm2(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Assuming 3 channels for InstanceNorm2d\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks2 )\n    x2 =xindex //ks4 \n    x3 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =1 +(ks1 //2 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =x1 \n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 >=tmp7 \n    tmp9 =tl .broadcast_to (1 +(ks3 //2 ),[XBLOCK ])\n    tmp10 =tmp6 <tmp9 \n    tmp11 =tmp8 &tmp10 \n    tmp12 =tmp11 &tmp5 \n    tmp13 =tl .load (in_ptr0 +((-2 )+((-2 )*ks1 )+2 *x0 +2 *ks1 *x1 +ks1 *ks3 *x2 ),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp14 =tl .load (in_ptr0 +((-1 )+((-2 )*ks1 )+2 *x0 +2 *ks1 *x1 +ks1 *ks3 *x2 ),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =triton_helpers .maximum (tmp14 ,tmp13 )\n    tmp16 =tl .load (in_ptr0 +((-2 )+((-1 )*ks1 )+2 *x0 +2 *ks1 *x1 +ks1 *ks3 *x2 ),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp17 =triton_helpers .maximum (tmp16 ,tmp15 )\n    tmp18 =tl .load (in_ptr0 +((-1 )+((-1 )*ks1 )+2 *x0 +2 *ks1 *x1 +ks1 *ks3 *x2 ),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =triton_helpers .maximum (tmp18 ,tmp17 )\n    tmp20 =tl .full (tmp19 .shape ,0.0 ,tmp19 .dtype )\n    tmp21 =tl .where (tmp12 ,tmp19 ,tmp20 )\n    tmp22 =tl .load (in_ptr1 +(x3 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp23 =tl .where (tmp11 ,tmp21 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp5 ,tmp23 ,tmp24 )\n    tmp26 =float (\"nan\")\n    tmp27 =tl .where (tmp5 ,tmp25 ,tmp26 )\n    tl .store (out_ptr0 +(x3 ),tmp27 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x4 =xindex //ks0 \n    x3 =xindex \n    tmp39 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =x0 \n    tmp4 =tl .broadcast_to (1 +(ks2 //2 ),[XBLOCK ])\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 +((-1 )*(ks2 //2 ))\n    tmp8 =tl .full ([1 ],1 ,tl .int64 )\n    tmp9 =tmp7 <tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(2 *x4 +2 *(ks3 //2 )+x4 *(ks2 //2 )+(ks2 //2 )*(ks3 //2 )+(ks2 //2 )),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x3 +((-1 )*(ks2 //2 ))+2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =tl .full ([1 ],1 ,tl .int64 )\n    tmp17 =tmp3 <tmp16 \n    tmp18 =tmp17 &tmp2 \n    tmp19 =tl .load (in_ptr0 +(2 *x4 +2 *(ks3 //2 )+x4 *(ks2 //2 )+(ks2 //2 )*(ks3 //2 )+(ks2 //2 )),tmp18 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =tl .load (in_ptr0 +(x3 +2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .where (tmp17 ,tmp19 ,tmp20 )\n    tmp22 =tl .where (tmp5 ,tmp15 ,tmp21 )\n    tmp23 =tl .full (tmp22 .shape ,0.0 ,tmp22 .dtype )\n    tmp24 =tl .where (tmp2 ,tmp22 ,tmp23 )\n    tmp25 =x0 \n    tmp26 =1 +(ks2 //2 )\n    tmp27 =tmp25 >=tmp26 \n    tmp28 =x0 +((-1 )*(ks2 //2 ))\n    tmp29 =tl .full ([1 ],1 ,tl .int64 )\n    tmp30 =tmp28 <tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(2 *x4 +x4 *(ks2 //2 )+(ks2 //2 )),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(x3 +((-1 )*(ks2 //2 ))),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =tmp25 <tmp1 \n    tmp38 =tl .load (in_ptr0 +(2 *x4 +x4 *(ks2 //2 )+(ks2 //2 )),tmp37 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp40 =tl .where (tmp37 ,tmp38 ,tmp39 )\n    tmp41 =tl .where (tmp27 ,tmp36 ,tmp40 )\n    tmp42 =tl .where (tmp2 ,tmp24 ,tmp41 )\n    tl .store (out_ptr0 +(x3 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_view_2 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp7_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp7_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp7_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index //ks0 \n        r0_1 =(r0_index %ks0 )\n        r0_3 =r0_index \n        tmp4 =tl .load (in_ptr0 +(r0_3 +4 *x0 +2 *x0 *(ks1 //2 )+2 *x0 *(ks2 //2 )+x0 *(ks1 //2 )*(ks2 //2 )),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp0 =r0_2 \n        tmp1 =1 +(ks1 //2 )\n        tmp2 =tmp0 >=tmp1 \n        tmp3 =tl .load (in_ptr0 +(2 +r0_1 +4 *x0 +2 *x0 *(ks1 //2 )+2 *x0 *(ks2 //2 )+x0 *(ks1 //2 )*(ks2 //2 )+(ks2 //2 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n        tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n        tmp7_mean_next ,tmp7_m2_next ,tmp7_weight_next =triton_helpers .welford_reduce (\n        tmp6 ,tmp7_mean ,tmp7_m2 ,tmp7_weight ,roffset ==0 \n        )\n        tmp7_mean =tl .where (r0_mask &xmask ,tmp7_mean_next ,tmp7_mean )\n        tmp7_m2 =tl .where (r0_mask &xmask ,tmp7_m2_next ,tmp7_m2 )\n        tmp7_weight =tl .where (r0_mask &xmask ,tmp7_weight_next ,tmp7_weight )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7_mean ,tmp7_m2 ,tmp7_weight ,1 )\n    tmp7 =tmp10 [:,None ]\n    tmp8 =tmp11 [:,None ]\n    tmp9 =tmp12 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_view_3 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks3 \n    x3 =xindex \n    tmp4 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp6 =tl .load (in_ptr1 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr2 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =1 +(ks2 //2 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .load (in_ptr0 +(2 +x0 +4 *x2 +2 *x2 *(ks2 //2 )+2 *x2 *(ks4 //2 )+x2 *(ks2 //2 )*(ks4 //2 )+(ks4 //2 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp7 =tmp5 -tmp6 \n    tmp9 =((tl .full ([],0.0 ,tl .float64 ))*((tl .full ([],0.0 ,tl .float64 ))>=(4 +2 *(ks2 //2 )+2 *(ks4 //2 )+(ks2 //2 )*(ks4 //2 )))+(4 +2 *(ks2 //2 )+2 *(ks4 //2 )+(ks2 //2 )*(ks4 //2 ))*((4 +2 *(ks2 //2 )+2 *(ks4 //2 )+(ks2 //2 )*(ks4 //2 ))>(tl .full ([],0.0 ,tl .float64 ))))\n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp8 /tmp10 \n    tmp12 =1e-05 \n    tmp13 =tmp11 +tmp12 \n    tmp14 =libdevice .rsqrt (tmp13 )\n    tmp15 =tmp7 *tmp14 \n    tl .store (out_ptr0 +(x3 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_4 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks2 )\n    x2 =xindex //ks4 \n    x3 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =2 +(ks1 //4 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =x1 \n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 >=tmp7 \n    tmp9 =tl .broadcast_to (2 +(ks3 //4 ),[XBLOCK ])\n    tmp10 =tmp6 <tmp9 \n    tmp11 =tmp8 &tmp10 \n    tmp12 =tmp11 &tmp5 \n    tmp13 =tl .load (in_ptr0 +((-6 )+((-2 )*(ks1 //2 ))+2 *x0 +4 *x1 +4 *x2 +2 *x1 *(ks1 //2 )+2 *x2 *(ks1 //2 )+2 *x2 *(ks3 //2 )+x2 *(ks1 //2 )*(ks3 //2 )),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp14 =tl .load (in_ptr0 +((-5 )+((-2 )*(ks1 //2 ))+2 *x0 +4 *x1 +4 *x2 +2 *x1 *(ks1 //2 )+2 *x2 *(ks1 //2 )+2 *x2 *(ks3 //2 )+x2 *(ks1 //2 )*(ks3 //2 )),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =triton_helpers .maximum (tmp14 ,tmp13 )\n    tmp16 =tl .load (in_ptr0 +((-4 )+((-1 )*(ks1 //2 ))+2 *x0 +4 *x1 +4 *x2 +2 *x1 *(ks1 //2 )+2 *x2 *(ks1 //2 )+2 *x2 *(ks3 //2 )+x2 *(ks1 //2 )*(ks3 //2 )),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp17 =triton_helpers .maximum (tmp16 ,tmp15 )\n    tmp18 =tl .load (in_ptr0 +((-3 )+((-1 )*(ks1 //2 ))+2 *x0 +4 *x1 +4 *x2 +2 *x1 *(ks1 //2 )+2 *x2 *(ks1 //2 )+2 *x2 *(ks3 //2 )+x2 *(ks1 //2 )*(ks3 //2 )),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =triton_helpers .maximum (tmp18 ,tmp17 )\n    tmp20 =tl .full (tmp19 .shape ,0.0 ,tmp19 .dtype )\n    tmp21 =tl .where (tmp12 ,tmp19 ,tmp20 )\n    tmp22 =tl .load (in_ptr1 +(x3 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp23 =tl .where (tmp11 ,tmp21 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp5 ,tmp23 ,tmp24 )\n    tmp26 =float (\"nan\")\n    tmp27 =tl .where (tmp5 ,tmp25 ,tmp26 )\n    tl .store (out_ptr0 +(x3 ),tmp27 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_5 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x4 =xindex //ks0 \n    x3 =xindex \n    tmp39 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =x0 \n    tmp4 =tl .broadcast_to (2 +(ks2 //4 ),[XBLOCK ])\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =(-1 )+x0 +((-1 )*(ks2 //4 ))\n    tmp8 =tl .full ([1 ],1 ,tl .int64 )\n    tmp9 =tmp7 <tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(4 +2 *(ks2 //4 )+3 *x4 +3 *(ks3 //4 )+x4 *(ks2 //4 )+(ks2 //4 )*(ks3 //4 )),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(2 +x3 +3 *(ks3 //4 )+(ks2 //4 )*(ks3 //4 )),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =tl .full ([1 ],1 ,tl .int64 )\n    tmp17 =tmp3 <tmp16 \n    tmp18 =tmp17 &tmp2 \n    tmp19 =tl .load (in_ptr0 +(4 +2 *(ks2 //4 )+3 *x4 +3 *(ks3 //4 )+x4 *(ks2 //4 )+(ks2 //4 )*(ks3 //4 )),tmp18 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =tl .load (in_ptr0 +(3 +x3 +3 *(ks3 //4 )+(ks2 //4 )*(ks3 //4 )+(ks2 //4 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .where (tmp17 ,tmp19 ,tmp20 )\n    tmp22 =tl .where (tmp5 ,tmp15 ,tmp21 )\n    tmp23 =tl .full (tmp22 .shape ,0.0 ,tmp22 .dtype )\n    tmp24 =tl .where (tmp2 ,tmp22 ,tmp23 )\n    tmp25 =x0 \n    tmp26 =2 +(ks2 //4 )\n    tmp27 =tmp25 >=tmp26 \n    tmp28 =(-1 )+x0 +((-1 )*(ks2 //4 ))\n    tmp29 =tl .full ([1 ],1 ,tl .int64 )\n    tmp30 =tmp28 <tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(1 +3 *x4 +x4 *(ks2 //4 )+(ks2 //4 )),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +((-1 )+x3 +((-1 )*(ks2 //4 ))),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =tmp25 <tmp1 \n    tmp38 =tl .load (in_ptr0 +(1 +3 *x4 +x4 *(ks2 //4 )+(ks2 //4 )),tmp37 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp40 =tl .where (tmp37 ,tmp38 ,tmp39 )\n    tmp41 =tl .where (tmp27 ,tmp36 ,tmp40 )\n    tmp42 =tl .where (tmp2 ,tmp24 ,tmp41 )\n    tl .store (out_ptr0 +(x3 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_view_6 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp7_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp7_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp7_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index //ks0 \n        r0_1 =(r0_index %ks0 )\n        r0_3 =r0_index \n        tmp4 =tl .load (in_ptr0 +(r0_3 +9 *x0 +3 *x0 *(ks1 //4 )+3 *x0 *(ks2 //4 )+x0 *(ks1 //4 )*(ks2 //4 )),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp0 =r0_2 \n        tmp1 =2 +(ks1 //4 )\n        tmp2 =tmp0 >=tmp1 \n        tmp3 =tl .load (in_ptr0 +(3 +r0_1 +9 *x0 +3 *x0 *(ks1 //4 )+3 *x0 *(ks2 //4 )+x0 *(ks1 //4 )*(ks2 //4 )+(ks2 //4 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n        tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n        tmp7_mean_next ,tmp7_m2_next ,tmp7_weight_next =triton_helpers .welford_reduce (\n        tmp6 ,tmp7_mean ,tmp7_m2 ,tmp7_weight ,roffset ==0 \n        )\n        tmp7_mean =tl .where (r0_mask &xmask ,tmp7_mean_next ,tmp7_mean )\n        tmp7_m2 =tl .where (r0_mask &xmask ,tmp7_m2_next ,tmp7_m2 )\n        tmp7_weight =tl .where (r0_mask &xmask ,tmp7_weight_next ,tmp7_weight )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7_mean ,tmp7_m2 ,tmp7_weight ,1 )\n    tmp7 =tmp10 [:,None ]\n    tmp8 =tmp11 [:,None ]\n    tmp9 =tmp12 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_view_7 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks3 \n    x3 =xindex \n    tmp4 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp6 =tl .load (in_ptr1 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr2 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =2 +(ks2 //4 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .load (in_ptr0 +(3 +x0 +9 *x2 +3 *x2 *(ks2 //4 )+3 *x2 *(ks4 //4 )+x2 *(ks2 //4 )*(ks4 //4 )+(ks4 //4 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp7 =tmp5 -tmp6 \n    tmp9 =((tl .full ([],0.0 ,tl .float64 ))*((tl .full ([],0.0 ,tl .float64 ))>=(9 +3 *(ks2 //4 )+3 *(ks4 //4 )+(ks2 //4 )*(ks4 //4 )))+(9 +3 *(ks2 //4 )+3 *(ks4 //4 )+(ks2 //4 )*(ks4 //4 ))*((9 +3 *(ks2 //4 )+3 *(ks4 //4 )+(ks2 //4 )*(ks4 //4 ))>(tl .full ([],0.0 ,tl .float64 ))))\n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp8 /tmp10 \n    tmp12 =1e-05 \n    tmp13 =tmp11 +tmp12 \n    tmp14 =libdevice .rsqrt (tmp13 )\n    tmp15 =tmp7 *tmp14 \n    tl .store (out_ptr0 +(x3 ),tmp15 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    s2 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,3 ,s1 ,s2 ),(3 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,2 +(s1 //2 ),2 +(s2 //2 )),(12 +6 *(s1 //2 )+6 *(s2 //2 )+3 *(s1 //2 )*(s2 //2 ),4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 ),2 +(s2 //2 ),1 ),torch .float32 )\n        2 +(s2 //2 )\n        2 +(s1 //2 )\n        4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        buf1 =empty_strided_cuda ((1 ,3 ,2 +(s1 //2 ),2 +(s2 //2 )),(12 +6 *(s1 //2 )+6 *(s2 //2 )+3 *(s1 //2 )*(s2 //2 ),4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 ),2 +(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_copy_0_xnumel =12 +6 *(s1 //2 )+6 *(s2 //2 )+3 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (triton_poi_fused_copy_0_xnumel )](arg2_1 ,buf0 ,buf1 ,34 ,64 ,34 ,64 ,1156 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        buf2 =buf0 ;del buf0 \n\n        triton_poi_fused_1_xnumel =12 +6 *(s1 //2 )+6 *(s2 //2 )+3 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_1 [grid (triton_poi_fused_1_xnumel )](buf1 ,buf2 ,34 ,34 ,64 ,64 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n\n        4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_view_2 [grid (3 )](buf2 ,buf3 ,buf4 ,34 ,64 ,64 ,3 ,1156 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf6 =buf1 ;del buf1 \n\n        triton_poi_fused__native_batch_norm_legit_view_3_xnumel =12 +6 *(s1 //2 )+6 *(s2 //2 )+3 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_view_3 [grid (triton_poi_fused__native_batch_norm_legit_view_3_xnumel )](buf2 ,buf3 ,buf4 ,buf6 ,34 ,34 ,64 ,1156 ,64 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        buf7 =empty_strided_cuda ((1 ,3 ,3 +(s1 //4 ),3 +(s2 //4 )),(27 +9 *(s1 //4 )+9 *(s2 //4 )+3 *(s1 //4 )*(s2 //4 ),9 +3 *(s1 //4 )+3 *(s2 //4 )+(s1 //4 )*(s2 //4 ),3 +(s2 //4 ),1 ),torch .float32 )\n        3 +(s2 //4 )\n        3 +(s1 //4 )\n        9 +3 *(s1 //4 )+3 *(s2 //4 )+(s1 //4 )*(s2 //4 )\n        buf8 =empty_strided_cuda ((1 ,3 ,3 +(s1 //4 ),3 +(s2 //4 )),(27 +9 *(s1 //4 )+9 *(s2 //4 )+3 *(s1 //4 )*(s2 //4 ),9 +3 *(s1 //4 )+3 *(s2 //4 )+(s1 //4 )*(s2 //4 ),3 +(s2 //4 ),1 ),torch .float32 )\n\n        triton_poi_fused_copy_4_xnumel =27 +9 *(s1 //4 )+9 *(s2 //4 )+3 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_copy_4 [grid (triton_poi_fused_copy_4_xnumel )](buf6 ,buf7 ,buf8 ,19 ,64 ,19 ,64 ,361 ,1083 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf6 \n        buf9 =buf7 ;del buf7 \n\n        triton_poi_fused_5_xnumel =27 +9 *(s1 //4 )+9 *(s2 //4 )+3 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_5 [grid (triton_poi_fused_5_xnumel )](buf8 ,buf9 ,19 ,19 ,64 ,64 ,1083 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf10 =buf4 ;del buf4 \n        buf11 =buf3 ;del buf3 \n\n        9 +3 *(s1 //4 )+3 *(s2 //4 )+(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_view_6 [grid (3 )](buf9 ,buf10 ,buf11 ,19 ,64 ,64 ,3 ,361 ,XBLOCK =1 ,R0_BLOCK =512 ,num_warps =4 ,num_stages =1 )\n        buf13 =buf8 ;del buf8 \n\n        triton_poi_fused__native_batch_norm_legit_view_7_xnumel =27 +9 *(s1 //4 )+9 *(s2 //4 )+3 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_view_7 [grid (triton_poi_fused__native_batch_norm_legit_view_7_xnumel )](buf9 ,buf10 ,buf11 ,buf13 ,19 ,19 ,64 ,361 ,64 ,1083 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf10 \n        del buf11 \n        del buf9 \n    return (buf13 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "09f9e755-3dde-4b08-87a7-b2e408079e19",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardsigmoid', 'AvgPool1d', 'Softshrink', 'ParameterDict', 'SmoothL1Loss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.avg_pool = nn.AvgPool1d(kernel_size=3, stride=2)\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.softshrink = nn.Softshrink(lambd=0.5)\n        self.parameter_dict = nn.ParameterDict({\n            'param1': nn.Parameter(torch.randn(10)),\n            'param2': nn.Parameter(torch.randn(10))\n        })\n        self.loss = nn.SmoothL1Loss()\n\n    def forward(self, x):\n        # Ensure the input is at least 1D\n        if x.dim() == 0:\n            x = x.unsqueeze(0)\n        \n        # Reshape input to be compatible with AvgPool1d\n        if x.dim() == 1:\n            x = x.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, L)\n        elif x.dim() == 2:\n            x = x.unsqueeze(1)  # Shape: (N, 1, L)\n        \n        # Apply AvgPool1d\n        x = self.avg_pool(x)\n        \n        # Apply Hardsigmoid\n        x = self.hardsigmoid(x)\n        \n        # Apply Softshrink\n        x = self.softshrink(x)\n        \n        # Use ParameterDict parameters\n        param1 = self.parameter_dict['param1']\n        param2 = self.parameter_dict['param2']\n        \n        # Combine parameters with the input\n        x = x.mean(dim=-1)  # Reduce to (N, 1)\n        x = x * param1 + param2\n        \n        # Compute loss (for demonstration, we use the input as target)\n        loss = self.loss(x, torch.zeros_like(x))\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(10).cuda()  # Example input of shape (10,)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_abs_gt_hardsigmoid_mean_mul_sign_sub_where_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp30 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(2 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +(1 +2 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tl .load (in_ptr0 +(2 +2 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp1 +tmp0 \n        tmp4 =tmp3 +tmp2 \n        tmp5 =0.3333333333333333 \n        tmp6 =tmp4 *tmp5 \n        tmp7 =3.0 \n        tmp8 =tmp6 +tmp7 \n        tmp9 =0.0 \n        tmp10 =triton_helpers .maximum (tmp8 ,tmp9 )\n        tmp11 =6.0 \n        tmp12 =triton_helpers .minimum (tmp10 ,tmp11 )\n        tmp13 =0.16666666666666666 \n        tmp14 =tmp12 *tmp13 \n        tmp15 =tl_math .abs (tmp14 )\n        tmp16 =0.5 \n        tmp17 =tmp15 >tmp16 \n        tmp18 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp19 =tmp18 <tmp14 \n        tmp20 =tmp19 .to (tl .int8 )\n        tmp21 =tmp14 <tmp18 \n        tmp22 =tmp21 .to (tl .int8 )\n        tmp23 =tmp20 -tmp22 \n        tmp24 =tmp23 .to (tmp14 .dtype )\n        tmp25 =tmp24 *tmp16 \n        tmp26 =tmp14 -tmp25 \n        tmp27 =tmp14 *tmp9 \n        tmp28 =tl .where (tmp17 ,tmp26 ,tmp27 )\n        tmp29 =tl .broadcast_to (tmp28 ,[XBLOCK ,R0_BLOCK ])\n        tmp31 =_tmp30 +tmp29 \n        _tmp30 =tl .where (r0_mask ,tmp31 ,_tmp30 )\n    tmp30 =tl .sum (_tmp30 ,1 )[:,None ]\n    tmp32 =triton_helpers .div_floor_integer ((-1 )+ks0 ,2 )\n    tmp33 =tmp32 .to (tl .float32 )\n    tmp34 =tmp30 /tmp33 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp34 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_mul_smooth_l1_loss_smooth_l1_loss_backward_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp2 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp4 =tl .load (in_ptr2 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp3 =tmp1 *tmp2 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tl_math .abs (tmp5 )\n    tmp7 =1.0 \n    tmp8 =tmp6 <tmp7 \n    tmp9 =tmp6 *tmp6 \n    tmp10 =0.5 \n    tmp11 =tmp9 *tmp10 \n    tmp12 =tmp11 *tmp7 \n    tmp13 =tmp6 -tmp10 \n    tmp14 =tl .where (tmp8 ,tmp12 ,tmp13 )\n    tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp17 =tl .where (r0_mask ,tmp15 ,0 )\n    tmp18 =tl .sum (tmp17 ,1 )[:,None ]\n    tmp19 =10.0 \n    tmp20 =tmp18 /tmp19 \n    tl .store (out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp5 ,r0_mask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp20 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 =args \n    args .clear ()\n    s0 =primals_1 \n    assert_size_stride (primals_2 ,(s0 ,),(1 ,))\n    assert_size_stride (primals_3 ,(10 ,),(1 ,))\n    assert_size_stride (primals_4 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf1 =buf0 ;del buf0 \n\n        ((-1 )+s0 )//2 \n        get_raw_stream (0 )\n        triton_red_fused_abs_gt_hardsigmoid_mean_mul_sign_sub_where_0 [grid (1 )](buf1 ,primals_2 ,10 ,1 ,4 ,XBLOCK =1 ,R0_BLOCK =4 ,num_warps =2 ,num_stages =1 )\n        del primals_2 \n        buf2 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n        buf3 =empty_strided_cuda ((),(),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        get_raw_stream (0 )\n        triton_per_fused_mul_smooth_l1_loss_smooth_l1_loss_backward_1 [grid (1 )](buf4 ,buf1 ,primals_3 ,primals_4 ,buf2 ,1 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_3 \n        del primals_4 \n    return (buf4 ,buf1 ,buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =10 \n    primals_2 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "0c745f98-453b-43ba-bc29-749704f82dce",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveMaxPool1d', 'ConstantPad3d', 'Identity']\nimport torch\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad3d(padding=(1, 1, 1, 1, 1, 1), value=0)\n        self.adaptive_max_pool = nn.AdaptiveMaxPool1d(output_size=10)\n        self.identity = nn.Identity()\n\n    def forward(self, x):\n        # Apply ConstantPad3d to the input\n        x = self.pad(x)\n        \n        # Reshape the tensor to fit AdaptiveMaxPool1d\n        # Assuming the input is 3D, we reshape it to 2D for AdaptiveMaxPool1d\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size * channels * depth * height, width)\n        \n        # Apply AdaptiveMaxPool1d\n        x = self.adaptive_max_pool(x)\n        \n        # Reshape back to a 3D tensor\n        x = x.view(batch_size, channels, depth, height, -1)\n        \n        # Apply Identity\n        x = self.identity(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 10, 10, 10).cuda()  # Example input with shape (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_adaptive_max_pool2d_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =4320 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %10 )\n    x1 =xindex //10 \n    x2 =xindex \n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =(6 *x0 )//5 \n    tmp4 =(21 +12 *x0 )//10 \n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp2 &tmp5 \n    tmp7 =(-1 )+(((x1 //12 )%12 ))\n    tmp8 =tl .full ([1 ],0 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tl .full ([1 ],10 ,tl .int64 )\n    tmp11 =tmp7 <tmp10 \n    tmp12 =(-1 )+((x1 %12 ))\n    tmp13 =tmp12 >=tmp8 \n    tmp14 =tmp12 <tmp10 \n    tmp15 =(-1 )+((6 *x0 )//5 )\n    tmp16 =tmp15 >=tmp8 \n    tmp17 =tmp15 <tmp10 \n    tmp18 =tmp9 &tmp11 \n    tmp19 =tmp18 &tmp13 \n    tmp20 =tmp19 &tmp14 \n    tmp21 =tmp20 &tmp16 \n    tmp22 =tmp21 &tmp17 \n    tmp23 =tmp22 &tmp6 \n    tmp24 =tl .load (in_ptr0 +((-111 )+10 *((x1 %12 ))+100 *(((x1 //12 )%12 ))+1000 *(x1 //144 )+((6 *x0 )//5 )),tmp23 &xmask ,other =0.0 )\n    tmp25 =tl .full (tmp24 .shape ,float (\"-inf\"),tmp24 .dtype )\n    tmp26 =tl .where (tmp6 ,tmp24 ,tmp25 )\n    tmp27 =1 +((6 *x0 )//5 )\n    tmp28 =tmp27 <tmp4 \n    tmp29 =tmp2 &tmp28 \n    tmp30 =(-1 )+(((x1 //12 )%12 ))\n    tmp31 =tl .full ([1 ],0 ,tl .int64 )\n    tmp32 =tmp30 >=tmp31 \n    tmp33 =tl .full ([1 ],10 ,tl .int64 )\n    tmp34 =tmp30 <tmp33 \n    tmp35 =(-1 )+((x1 %12 ))\n    tmp36 =tmp35 >=tmp31 \n    tmp37 =tmp35 <tmp33 \n    tmp38 =(6 *x0 )//5 \n    tmp39 =tmp38 >=tmp31 \n    tmp40 =tmp38 <tmp33 \n    tmp41 =tmp32 &tmp34 \n    tmp42 =tmp41 &tmp36 \n    tmp43 =tmp42 &tmp37 \n    tmp44 =tmp43 &tmp39 \n    tmp45 =tmp44 &tmp40 \n    tmp46 =tmp45 &tmp29 \n    tmp47 =tl .load (in_ptr0 +((-110 )+10 *((x1 %12 ))+100 *(((x1 //12 )%12 ))+1000 *(x1 //144 )+((6 *x0 )//5 )),tmp46 &xmask ,other =0.0 )\n    tmp48 =tl .full (tmp47 .shape ,float (\"-inf\"),tmp47 .dtype )\n    tmp49 =tl .where (tmp29 ,tmp47 ,tmp48 )\n    tmp50 =triton_helpers .maximum (tmp49 ,tmp26 )\n    tmp51 =2 +((6 *x0 )//5 )\n    tmp52 =tmp51 <tmp4 \n    tmp53 =tmp2 &tmp52 \n    tmp54 =(-1 )+(((x1 //12 )%12 ))\n    tmp55 =tl .full ([1 ],0 ,tl .int64 )\n    tmp56 =tmp54 >=tmp55 \n    tmp57 =tl .full ([1 ],10 ,tl .int64 )\n    tmp58 =tmp54 <tmp57 \n    tmp59 =(-1 )+((x1 %12 ))\n    tmp60 =tmp59 >=tmp55 \n    tmp61 =tmp59 <tmp57 \n    tmp62 =1 +((6 *x0 )//5 )\n    tmp63 =tmp62 >=tmp55 \n    tmp64 =tmp62 <tmp57 \n    tmp65 =tmp56 &tmp58 \n    tmp66 =tmp65 &tmp60 \n    tmp67 =tmp66 &tmp61 \n    tmp68 =tmp67 &tmp63 \n    tmp69 =tmp68 &tmp64 \n    tmp70 =tmp69 &tmp53 \n    tmp71 =tl .load (in_ptr0 +((-109 )+10 *((x1 %12 ))+100 *(((x1 //12 )%12 ))+1000 *(x1 //144 )+((6 *x0 )//5 )),tmp70 &xmask ,other =0.0 )\n    tmp72 =tl .full (tmp71 .shape ,float (\"-inf\"),tmp71 .dtype )\n    tmp73 =tl .where (tmp53 ,tmp71 ,tmp72 )\n    tmp74 =triton_helpers .maximum (tmp73 ,tmp50 )\n    tl .store (out_ptr0 +(x2 ),tmp74 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,3 ,10 ,10 ,10 ),(3000 ,1000 ,100 ,10 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((432 ,1 ,10 ),(10 ,10 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_adaptive_max_pool2d_0 [grid (4320 )](arg0_1 ,buf0 ,4320 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n    return (reinterpret_tensor (buf0 ,(1 ,3 ,12 ,12 ,10 ),(4320 ,1440 ,120 ,10 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,3 ,10 ,10 ,10 ),(3000 ,1000 ,100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "0f4d22a0-55a2-462b-bbf3-d7ab8c377191",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad2d', 'Softsign', 'Hardshrink', 'AdaptiveAvgPool1d', 'InstanceNorm1d', 'CrossEntropyLoss', 'LazyBatchNorm3d', 'Fold', 'BatchNorm3d', 'ELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad2d(2, 3.0)\n        self.softsign = nn.Softsign()\n        self.hardshrink = nn.Hardshrink()\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool1d(10)\n        self.instance_norm = nn.InstanceNorm1d(10)\n        self.lazy_batch_norm = nn.LazyBatchNorm3d()\n        self.fold = nn.Fold(output_size=(5, 5), kernel_size=(2, 2))\n        self.batch_norm = nn.BatchNorm3d(10)\n        self.elu = nn.ELU()\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        # Apply ConstantPad2d\n        x = self.pad(x)\n        \n        # Apply Softsign\n        x = self.softsign(x)\n        \n        # Apply Hardshrink\n        x = self.hardshrink(x)\n        \n        # Reshape for AdaptiveAvgPool1d\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.adaptive_avg_pool(x)\n        \n        # Apply InstanceNorm1d\n        x = self.instance_norm(x)\n        \n        # Reshape for LazyBatchNorm3d\n        x = x.unsqueeze(2).unsqueeze(3)\n        x = self.lazy_batch_norm(x)\n        \n        # Apply Fold\n        x = x.view(x.size(0), -1, x.size(2) * x.size(3))\n        x = self.fold(x)\n        \n        # Apply BatchNorm3d\n        x = x.unsqueeze(1)\n        x = self.batch_norm(x)\n        \n        # Apply ELU\n        x = self.elu(x)\n        \n        # Reshape for CrossEntropyLoss (assuming classification task)\n        x = x.view(x.size(0), -1)\n        target = torch.randint(0, 10, (x.size(0),), device=x.device)\n        loss = self.cross_entropy_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 28, 28).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_constant_pad_nd_div_le_scalar_tensor_where_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =3.0 )\n    tmp13 =tl_math .abs (tmp12 )\n    tmp14 =1.0 \n    tmp15 =tmp13 +tmp14 \n    tmp16 =tmp12 /tmp15 \n    tmp17 =tl_math .abs (tmp16 )\n    tmp18 =0.5 \n    tmp19 =tmp17 <=tmp18 \n    tmp20 =0.0 \n    tmp21 =tl .where (tmp19 ,tmp20 ,tmp16 )\n    tl .store (out_ptr0 +(x4 ),tmp21 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *(x0 //ks1 )+16 *x1 +ks3 *(x0 //ks1 )+4 *ks2 *x1 +4 *ks3 *x1 +ks2 *ks3 *x1 +((x0 %ks1 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_add_constant_pad_nd_div_le_scalar_tensor_where_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_constant_pad_nd_div_le_scalar_tensor_where_0 [grid (triton_poi_fused_abs_add_constant_pad_nd_div_le_scalar_tensor_where_0_xnumel )](arg3_1 ,buf0 ,32 ,32 ,28 ,28 ,1024 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,16 +4 *s1 +4 *s2 +s1 *s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__adaptive_avg_pool2d_1_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_1 [grid (triton_poi_fused__adaptive_avg_pool2d_1_xnumel )](buf0 ,buf1 ,1024 ,32 ,28 ,28 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n\n        buf2 =torch .ops .aten ._adaptive_avg_pool2d .default (buf1 ,[1 ,10 ])\n        del buf1 \n        buf3 =buf2 \n        del buf2 \n    return (reinterpret_tensor (buf3 ,(1 ,s0 ,10 ),(10 *s0 ,10 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =28 \n    arg2_1 =28 \n    arg3_1 =rand_strided ((1 ,3 ,28 ,28 ),(2352 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "105bc18e-a4d2-40e6-97f1-7fb43138ccbd",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['TripletMarginWithDistanceLoss', 'Hardshrink', 'Dropout2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout2d = nn.Dropout2d(p=0.5)\n        self.hardshrink = nn.Hardshrink(lambd=0.5)\n        self.triplet_loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n\n    def forward(self, x):\n        # Apply Dropout2d\n        x = self.dropout2d(x)\n        \n        # Apply Hardshrink\n        x = self.hardshrink(x)\n        \n        # Reshape the input to fit the triplet loss requirements\n        # Assuming the input is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, -1)  # Flatten the spatial dimensions\n        \n        # Generate anchor, positive, and negative samples for triplet loss\n        # For simplicity, we use the same input as anchor, positive, and negative\n        anchor = x\n        positive = x\n        negative = x\n        \n        # Compute triplet loss\n        loss = self.triplet_loss(anchor, positive, negative)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with batch_size=1, channels=3, height=64, width=64\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_norm_sub_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp22 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )\n        tmp1 =ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 ))%(ks0 *ks1 *ks2 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 ))//(ks1 *ks2 ))%ks0 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =0.5 \n        tmp6 =tmp4 <tmp5 \n        tmp7 =tmp6 .to (tl .float32 )\n        tmp8 =2.0 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tmp3 *tmp9 \n        tmp11 =tl_math .abs (tmp10 )\n        tmp12 =tmp11 <=tmp5 \n        tmp13 =0.0 \n        tmp14 =tl .where (tmp12 ,tmp13 ,tmp10 )\n        tmp15 =tmp14 -tmp14 \n        tmp16 =1e-06 \n        tmp17 =tmp15 +tmp16 \n        tmp18 =tmp17 *tmp17 \n        tmp19 =tl .full (tmp18 .shape ,0 ,tmp18 .dtype )\n        tmp20 =tl .where (tmp2 ,tmp18 ,tmp19 )\n        tmp21 =tl .broadcast_to (tmp20 ,[XBLOCK ,R0_BLOCK ])\n        tmp23 =_tmp22 +tmp21 \n        _tmp22 =tl .where (r0_mask &xmask ,tmp23 ,_tmp22 )\n    tmp22 =tl .sum (_tmp22 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp22 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp22 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_clamp_min_mean_norm_sub_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp4 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .sum (tmp5 ,1 )[:,None ]\n    tmp8 =libdevice .sqrt (tmp3 )\n    tmp9 =1.0 \n    tmp10 =tmp8 +tmp9 \n    tmp11 =libdevice .sqrt (tmp7 )\n    tmp12 =tmp10 -tmp11 \n    tmp13 =0.0 \n    tmp14 =triton_helpers .maximum (tmp12 ,tmp13 )\n    tmp15 =tmp14 /tmp9 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp15 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n\n        (1 +s0 *s1 *s2 )//2 \n        get_raw_stream (0 )\n        triton_red_fused_add_norm_sub_1 [grid (2 )](arg3_1 ,buf1 ,buf2 ,buf4 ,3 ,64 ,64 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        buf3 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf6 =reinterpret_tensor (buf3 ,(),(),0 );del buf3 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_clamp_min_mean_norm_sub_2 [grid (1 )](buf6 ,buf2 ,buf4 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf2 \n        del buf4 \n    return (buf6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "106037c9-4f19-478c-8d0d-c79b7336bb2c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReLU6', 'RNN', 'FractionalMaxPool2d', 'ModuleDict', 'GaussianNLLLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.relu6 = nn.ReLU6()\n        self.rnn = nn.RNN(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.fractional_max_pool = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.module_dict = nn.ModuleDict({\n            'fc1': nn.Linear(128, 64),\n            'fc2': nn.Linear(64, 32)\n        })\n        self.gaussian_nll_loss = nn.GaussianNLLLoss()\n\n    def forward(self, x):\n        # Apply ReLU6 activation\n        x = self.relu6(x)\n        \n        # Reshape for RNN input\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels * height, width)\n        \n        # Apply RNN\n        x, _ = self.rnn(x)\n        \n        # Reshape back to 4D tensor for FractionalMaxPool2d\n        x = x.view(batch_size, channels, height, width)\n        \n        # Apply FractionalMaxPool2d\n        x = self.fractional_max_pool(x)\n        \n        # Flatten for fully connected layers\n        x = x.view(batch_size, -1)\n        \n        # Apply ModuleDict layers\n        x = self.module_dict['fc1'](x)\n        x = self.module_dict['fc2'](x)\n        \n        # GaussianNLLLoss requires a target, so we return the output and a dummy target\n        # For the purpose of this model, we assume the target is the same as the output\n        target = x.detach().clone()\n        return x, target\n\n    def compute_loss(self, output, target):\n        # Compute GaussianNLLLoss\n        return self.gaussian_nll_loss(output, target, torch.ones_like(output))\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 28, 28).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_hardtanh_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =triton_helpers .maximum (tmp0 ,tmp1 )\n    tmp3 =6.0 \n    tmp4 =triton_helpers .minimum (tmp2 ,tmp3 )\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_hardtanh_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_hardtanh_0 [grid (triton_poi_fused_hardtanh_0_xnumel )](arg3_1 ,buf0 ,2352 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,s0 *s1 ,s2 ),(s0 *s1 *s2 ,s2 ,1 ),0 ),s0 ,s1 ,s2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =28 \n    arg2_1 =28 \n    arg3_1 =rand_strided ((1 ,3 ,28 ,28 ),(2352 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "10f0203e-bfa9-42ed-a0a2-ed2df824e4e1",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['UpsamplingBilinear2d', 'ReplicationPad2d', 'Sequential', 'ZeroPad3d', 'CrossMapLRN2d', 'Hardshrink', 'TripletMarginWithDistanceLoss', 'TransformerDecoderLayer', 'Transformer', 'CosineSimilarity']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.pad2d = nn.ReplicationPad2d(1)\n        self.sequential = nn.Sequential(\n            nn.ZeroPad3d(1),\n            nn.CrossMapLRN2d(size=5),\n            nn.Hardshrink()\n        )\n        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=64, nhead=8)\n        self.transformer = nn.Transformer(d_model=64, nhead=8, num_encoder_layers=2, num_decoder_layers=2)\n        self.cosine_sim = nn.CosineSimilarity(dim=1)\n        self.loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: self.cosine_sim(x, y))\n\n    def forward(self, x):\n        # Upsample the input\n        x = self.upsample(x)\n        \n        # Apply 2D padding\n        x = self.pad2d(x)\n        \n        # Pass through a sequential block\n        x = self.sequential(x)\n        \n        # Reshape for Transformer\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1).permute(2, 0, 1)  # (seq_len, batch_size, d_model)\n        \n        # Create a dummy target sequence for the Transformer\n        tgt = torch.zeros_like(x)\n        \n        # Pass through TransformerDecoderLayer\n        x = self.transformer_decoder_layer(x, tgt)\n        \n        # Pass through Transformer\n        x = self.transformer(x, tgt)\n        \n        # Reshape back to original dimensions\n        x = x.permute(1, 2, 0).view(batch_size, channels, height, width)\n        \n        # Compute cosine similarity with a dummy tensor\n        dummy = torch.ones_like(x)\n        similarity = self.cosine_sim(x, dummy)\n        \n        # Compute triplet loss with dummy anchors and negatives\n        anchor = torch.ones_like(x)\n        positive = torch.ones_like(x)\n        negative = torch.zeros_like(x)\n        loss = self.loss(anchor, positive, negative)\n        \n        return similarity, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0 (in_out_ptr1 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks4 \n    x5 =xindex \n    tmp0 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float64 )\n    tmp3 =tmp0 +tmp2 \n    tmp4 =2.0 \n    tmp5 =tmp1 .to (tl .float32 )\n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp6 .to (tl .float64 )\n    tmp8 =tmp0 +tmp7 \n    tmp9 =tmp3 /tmp8 \n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =x1 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =tmp12 *tmp10 \n    tmp14 =0.0 \n    tmp15 =triton_helpers .maximum (tmp13 ,tmp14 )\n    tmp16 =tmp15 .to (tl .int64 )\n    tmp17 =ks3 \n    tmp18 =tmp17 .to (tl .float64 )\n    tmp19 =tmp0 +tmp18 \n    tmp20 =tmp17 .to (tl .float32 )\n    tmp21 =tmp4 *tmp20 \n    tmp22 =tmp21 .to (tl .float64 )\n    tmp23 =tmp0 +tmp22 \n    tmp24 =tmp19 /tmp23 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =x0 \n    tmp27 =tmp26 .to (tl .float32 )\n    tmp28 =tmp27 *tmp25 \n    tmp29 =triton_helpers .maximum (tmp28 ,tmp14 )\n    tmp30 =tmp29 .to (tl .int64 )\n    tmp31 =tl .load (in_ptr0 +(tmp30 +ks3 *tmp16 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp32 =tl .full ([1 ],1 ,tl .int64 )\n    tmp33 =tmp16 +tmp32 \n    tmp34 =(-1 )+ks0 \n    tmp35 =triton_helpers .minimum (tmp33 ,tmp34 )\n    tmp36 =tl .load (in_ptr0 +(tmp30 +ks3 *tmp35 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp37 =tmp30 +tmp32 \n    tmp38 =(-1 )+ks3 \n    tmp39 =triton_helpers .minimum (tmp37 ,tmp38 )\n    tmp40 =tl .load (in_ptr0 +(tmp39 +ks3 *tmp35 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp41 =tmp40 -tmp36 \n    tmp42 =tl .load (in_ptr0 +(tmp39 +ks3 *tmp16 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =tmp42 -tmp31 \n    tmp44 =tmp30 .to (tl .float32 )\n    tmp45 =tmp29 -tmp44 \n    tmp46 =triton_helpers .maximum (tmp45 ,tmp14 )\n    tmp47 =1.0 \n    tmp48 =triton_helpers .minimum (tmp46 ,tmp47 )\n    tmp49 =tmp41 *tmp48 \n    tmp50 =tmp36 +tmp49 \n    tmp51 =tmp43 *tmp48 \n    tmp52 =tmp31 +tmp51 \n    tmp53 =tmp50 -tmp52 \n    tmp54 =tmp16 .to (tl .float32 )\n    tmp55 =tmp15 -tmp54 \n    tmp56 =triton_helpers .maximum (tmp55 ,tmp14 )\n    tmp57 =triton_helpers .minimum (tmp56 ,tmp47 )\n    tmp58 =tmp53 *tmp57 \n    tmp59 =tmp52 +tmp58 \n    tl .store (in_out_ptr1 +(x5 ),tmp59 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_replication_pad2d_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *ks6 *(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<((-1 )+ks4 )))+4 *ks5 *ks6 *x2 +(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))<((-1 )+ks3 )))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x3 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 *s2 \n        2 *s1 \n        4 *s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 ,2 *s1 ,2 *s2 ),(4 *s0 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n        buf5 =buf2 ;del buf2 \n\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0_xnumel =4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0 [grid (triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0_xnumel )](buf5 ,arg3_1 ,32 ,64 ,64 ,32 ,4096 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        2 +2 *s2 \n        2 +2 *s1 \n        4 +4 *s1 +4 *s2 +4 *s1 *s2 \n        buf6 =empty_strided_cuda ((1 ,s0 ,2 +2 *s1 ,2 +2 *s2 ),(4 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s1 *s2 ,4 +4 *s1 +4 *s2 +4 *s1 *s2 ,2 +2 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_replication_pad2d_1_xnumel =4 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_replication_pad2d_1 [grid (triton_poi_fused_replication_pad2d_1_xnumel )](buf5 ,buf6 ,66 ,66 ,4356 ,64 ,64 ,32 ,32 ,13068 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf5 \n    return (buf6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "1157ef3d-5afd-4dfb-8769-bcc7c336816a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad1d', 'Dropout2d', 'CircularPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ConstantPad1d(padding=2, value=0)\n        self.dropout1 = nn.Dropout2d(p=0.5)\n        self.pad2 = nn.CircularPad1d(padding=3)\n        self.pad3 = nn.ConstantPad1d(padding=1, value=0)\n        self.dropout2 = nn.Dropout2d(p=0.5)\n        self.pad4 = nn.CircularPad1d(padding=2)\n\n    def forward(self, x):\n        # Assuming input is of shape (batch_size, channels, length)\n        x = self.pad1(x)  # Apply ConstantPad1d\n        x = x.unsqueeze(2)  # Add a dimension to make it compatible with Dropout2d\n        x = self.dropout1(x)  # Apply Dropout2d\n        x = x.squeeze(2)  # Remove the added dimension\n        x = self.pad2(x)  # Apply CircularPad1d\n        x = self.pad3(x)  # Apply ConstantPad1d again\n        x = x.unsqueeze(2)  # Add a dimension to make it compatible with Dropout2d\n        x = self.dropout2(x)  # Apply Dropout2d again\n        x = x.squeeze(2)  # Remove the added dimension\n        x = self.pad4(x)  # Apply CircularPad1d again\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64).cuda()  # Example input shape (batch_size=1, channels=3, length=64)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],3 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =4 +ks1 +x0 \n    tmp4 =tl .full ([1 ],3 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .broadcast_to (7 +ks1 ,[XBLOCK ])\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =(-1 )+ks1 +x0 \n    tmp11 =tl .full ([1 ],0 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .broadcast_to (ks1 ,[XBLOCK ])\n    tmp14 =tmp10 <tmp13 \n    tmp15 =tmp12 &tmp14 \n    tmp16 =tmp15 &tmp9 \n    tmp17 =tl .load (in_ptr0 +((-1 )+ks1 +x0 +ks1 *x1 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp18 =tl .load (in_ptr1 +(x1 ),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =0.5 \n    tmp20 =tmp18 <tmp19 \n    tmp21 =tmp20 .to (tl .float32 )\n    tmp22 =2.0 \n    tmp23 =tmp21 *tmp22 \n    tmp24 =tmp17 *tmp23 \n    tmp25 =tl .full (tmp24 .shape ,0.0 ,tmp24 .dtype )\n    tmp26 =tl .where (tmp9 ,tmp24 ,tmp25 )\n    tmp27 =float (\"nan\")\n    tmp28 =tl .where (tmp8 ,tmp26 ,tmp27 )\n    tmp29 =tl .full (tmp28 .shape ,0.0 ,tmp28 .dtype )\n    tmp30 =tl .where (tmp2 ,tmp28 ,tmp29 )\n    tmp31 =tmp0 >=tmp1 \n    tmp32 =7 +ks1 \n    tmp33 =tmp0 <tmp32 \n    tmp34 =tmp31 &tmp33 \n    tmp35 =(-5 )+x0 \n    tmp36 =tl .full ([1 ],0 ,tl .int64 )\n    tmp37 =tmp35 >=tmp36 \n    tmp38 =tl .broadcast_to (ks1 ,[XBLOCK ])\n    tmp39 =tmp35 <tmp38 \n    tmp40 =tmp37 &tmp39 \n    tmp41 =tmp40 &tmp34 \n    tmp42 =tl .load (in_ptr0 +((-5 )+x0 +ks1 *x1 ),tmp41 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp43 =tl .load (in_ptr1 +(x1 ),tmp34 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp44 =0.5 \n    tmp45 =tmp43 <tmp44 \n    tmp46 =tmp45 .to (tl .float32 )\n    tmp47 =2.0 \n    tmp48 =tmp46 *tmp47 \n    tmp49 =tmp42 *tmp48 \n    tmp50 =tl .full (tmp49 .shape ,0.0 ,tmp49 .dtype )\n    tmp51 =tl .where (tmp34 ,tmp49 ,tmp50 )\n    tmp52 =float (\"nan\")\n    tmp53 =tl .where (tmp34 ,tmp51 ,tmp52 )\n    tmp54 =tl .where (tmp2 ,tmp30 ,tmp53 )\n    tl .store (out_ptr0 +(x2 ),tmp54 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],2 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =12 +ks1 +x0 \n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .broadcast_to (14 +ks1 ,[XBLOCK ])\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =9 +ks1 +x0 \n    tmp11 =tl .full ([1 ],0 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .broadcast_to (ks2 ,[XBLOCK ])\n    tmp14 =tmp10 <tmp13 \n    tmp15 =tmp12 &tmp14 \n    tmp16 =tmp15 &tmp9 \n    tmp17 =9 +ks1 +x0 \n    tmp18 =tl .broadcast_to (7 +ks1 ,[XBLOCK ])\n    tmp19 =tmp17 >=tmp18 \n    tmp20 =tmp19 &tmp16 \n    tmp21 =tl .load (in_ptr0 +(5 +x0 +10 *x1 +ks1 *x1 ),tmp20 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tl .load (in_ptr0 +(9 +ks1 +x0 +10 *x1 +ks1 *x1 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp23 =tl .where (tmp19 ,tmp21 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp16 ,tmp23 ,tmp24 )\n    tmp26 =tl .load (in_ptr1 +(x1 ),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp27 =0.5 \n    tmp28 =tmp26 <tmp27 \n    tmp29 =tmp28 .to (tl .float32 )\n    tmp30 =2.0 \n    tmp31 =tmp29 *tmp30 \n    tmp32 =tmp25 *tmp31 \n    tmp33 =tl .full (tmp32 .shape ,0.0 ,tmp32 .dtype )\n    tmp34 =tl .where (tmp9 ,tmp32 ,tmp33 )\n    tmp35 =float (\"nan\")\n    tmp36 =tl .where (tmp8 ,tmp34 ,tmp35 )\n    tmp37 =tl .full (tmp36 .shape ,0.0 ,tmp36 .dtype )\n    tmp38 =tl .where (tmp2 ,tmp36 ,tmp37 )\n    tmp39 =tmp0 >=tmp1 \n    tmp40 =14 +ks1 \n    tmp41 =tmp0 <tmp40 \n    tmp42 =tmp39 &tmp41 \n    tmp43 =(-3 )+x0 \n    tmp44 =tl .full ([1 ],0 ,tl .int64 )\n    tmp45 =tmp43 >=tmp44 \n    tmp46 =tl .broadcast_to (ks2 ,[XBLOCK ])\n    tmp47 =tmp43 <tmp46 \n    tmp48 =tmp45 &tmp47 \n    tmp49 =tmp48 &tmp42 \n    tmp50 =(-3 )+x0 \n    tmp51 =tl .broadcast_to (7 +ks1 ,[XBLOCK ])\n    tmp52 =tmp50 >=tmp51 \n    tmp53 =tmp52 &tmp49 \n    tmp54 =tl .load (in_ptr0 +((-7 )+x0 +((-1 )*ks1 )+10 *x1 +ks1 *x1 ),tmp53 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp55 =tl .load (in_ptr0 +((-3 )+x0 +10 *x1 +ks1 *x1 ),tmp49 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp56 =tl .where (tmp52 ,tmp54 ,tmp55 )\n    tmp57 =tl .full (tmp56 .shape ,0.0 ,tmp56 .dtype )\n    tmp58 =tl .where (tmp49 ,tmp56 ,tmp57 )\n    tmp59 =tl .load (in_ptr1 +(x1 ),tmp42 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp60 =0.5 \n    tmp61 =tmp59 <tmp60 \n    tmp62 =tmp61 .to (tl .float32 )\n    tmp63 =2.0 \n    tmp64 =tmp62 *tmp63 \n    tmp65 =tmp58 *tmp64 \n    tmp66 =tl .full (tmp65 .shape ,0.0 ,tmp65 .dtype )\n    tmp67 =tl .where (tmp42 ,tmp65 ,tmp66 )\n    tmp68 =float (\"nan\")\n    tmp69 =tl .where (tmp42 ,tmp67 ,tmp68 )\n    tmp70 =tl .where (tmp2 ,tmp38 ,tmp69 )\n    tl .store (out_ptr0 +(x2 ),tmp70 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_4 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x2 =xindex \n    tmp4 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x0 \n    tmp1 =14 +ks1 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .load (in_ptr0 +((-12 )+x2 +((-1 )*ks1 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tl .store (out_ptr0 +(x2 ),tmp5 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf1 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf1 ,buf2 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        10 +s1 \n        buf3 =empty_strided_cuda ((1 ,s0 ,10 +s1 ),(10 *s0 +s0 *s1 ,10 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_1_xnumel =10 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_1 [grid (triton_poi_fused_copy_1_xnumel )](arg2_1 ,buf2 ,buf3 ,74 ,64 ,222 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        buf5 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_2 [grid (s0 )](buf1 ,buf5 ,1 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf1 \n        16 +s1 \n        buf6 =empty_strided_cuda ((1 ,s0 ,16 +s1 ),(16 *s0 +s0 *s1 ,16 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_3_xnumel =16 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_3 [grid (triton_poi_fused_copy_3_xnumel )](buf3 ,buf5 ,buf6 ,80 ,64 ,74 ,240 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        del buf5 \n        buf7 =empty_strided_cuda ((1 ,s0 ,16 +s1 ),(16 *s0 +s0 *s1 ,16 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_4_xnumel =16 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_4 [grid (triton_poi_fused_4_xnumel )](buf6 ,buf7 ,80 ,64 ,240 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf6 \n    return (buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,3 ,64 ),(192 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "1239a662-0867-47d9-9003-6d6f8359a2cf",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Conv3d', 'FractionalMaxPool3d', 'LSTM', 'Tanhshrink', 'LazyLinear', 'MSELoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv3d_1 = nn.Conv3d(1, 10, kernel_size=3)\n        self.conv3d_2 = nn.Conv3d(10, 20, kernel_size=3)\n        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(10, 10, 10))\n        self.lstm = nn.LSTM(input_size=2000, hidden_size=100, num_layers=2, batch_first=True)\n        self.tanhshrink = nn.Tanhshrink()\n        self.lazy_linear = nn.LazyLinear(50)\n        self.mse_loss = nn.MSELoss()\n\n    def forward(self, x):\n        # Apply Conv3d layers\n        x = self.conv3d_1(x)\n        x = self.conv3d_2(x)\n        \n        # Apply FractionalMaxPool3d\n        x = self.fractional_max_pool3d(x)\n        \n        # Reshape for LSTM\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size, channels * depth, height * width)\n        x = x.permute(0, 2, 1)  # (batch_size, seq_len, features)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Apply Tanhshrink\n        x = self.tanhshrink(x)\n        \n        # Reshape for LazyLinear\n        x = x.reshape(batch_size, -1)\n        \n        # Apply LazyLinear\n        x = self.lazy_linear(x)\n        \n        # Apply MSELoss (assuming we have a target tensor)\n        target = torch.randn_like(x)\n        loss = self.mse_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 20, 20, 20).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =58320 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //5832 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_1 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =xindex \n    x1 =xindex //4096 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),None )\n    tmp1 =tl .load (in_ptr0 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =60 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(10 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_2 ,(10 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,1 ,20 ,20 ,20 ),(8000 ,8000 ,400 ,20 ,1 ))\n    assert_size_stride (primals_4 ,(20 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_5 ,(20 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,1 ,1 ),padding =(0 ,0 ,0 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,10 ,18 ,18 ,18 ),(58320 ,5832 ,324 ,18 ,1 ))\n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_0 [grid (58320 )](buf1 ,primals_2 ,58320 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n\n        buf2 =extern_kernels .convolution (buf1 ,primals_4 ,stride =(1 ,1 ,1 ),padding =(0 ,0 ,0 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf2 ,(1 ,20 ,16 ,16 ,16 ),(81920 ,4096 ,256 ,16 ,1 ))\n        buf3 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_1 [grid (81920 )](buf3 ,primals_5 ,81920 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del primals_5 \n        buf4 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf4 )\n        buf5 =empty_strided_cuda ((1 ,20 ,3 ),(60 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_2 [grid (60 )](buf4 ,buf5 ,0 ,60 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del buf4 \n\n        buf6 =torch .ops .aten .fractional_max_pool3d .default (buf3 ,[2 ,2 ,2 ],[10 ,10 ,10 ],buf5 )\n        del buf5 \n        buf7 =buf6 [0 ]\n        buf8 =buf6 [1 ]\n        del buf6 \n    return (reinterpret_tensor (buf7 ,(1 ,100 ,200 ),(20000 ,1 ,100 ),0 ),primals_1 ,primals_3 ,primals_4 ,buf1 ,buf3 ,buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((10 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,1 ,20 ,20 ,20 ),(8000 ,8000 ,400 ,20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((20 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "126fcf30-89d6-4bae-919d-6a98811d297a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool3d', 'Sequential', 'AvgPool1d', 'ZeroPad1d', 'LPPool3d', 'RNNCellBase']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool3d = nn.MaxPool3d(kernel_size=2, stride=2)\n        self.sequential = nn.Sequential(\n            nn.ZeroPad1d(2),\n            nn.AvgPool1d(kernel_size=3, stride=1)\n        )\n        self.lppool3d = nn.LPPool3d(norm_type=2, kernel_size=2, stride=2)\n        self.rnncell = nn.RNNCell(input_size=64, hidden_size=128)\n\n    def forward(self, x):\n        # Assuming input is 5D (batch, channels, depth, height, width)\n        x = self.maxpool3d(x)\n        \n        # Reshape to 3D (batch * depth, channels, height * width)\n        batch, channels, depth, height, width = x.shape\n        x = x.view(batch * depth, channels, height * width)\n        \n        # Apply Sequential (ZeroPad1d and AvgPool1d)\n        x = self.sequential(x)\n        \n        # Reshape back to 5D\n        x = x.view(batch, depth, channels, -1)\n        x = x.permute(0, 2, 1, 3).contiguous()\n        \n        # Apply LPPool3d\n        x = self.lppool3d(x)\n        \n        # Reshape to 2D (batch, features) for RNNCell\n        batch, channels, depth, height, width = x.shape\n        x = x.view(batch, -1)\n        \n        # Initialize hidden state for RNNCell\n        hx = torch.zeros(batch, 128).to(x.device)\n        \n        # Apply RNNCell\n        x = self.rnncell(x, hx)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 64, 64).cuda()  # Example input: (batch, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_pow_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =(-2 )+x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(ks1 //2 )*(ks2 //2 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-2 )+x0 +x1 *(ks1 //2 )*(ks2 //2 )),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =(-1 )+x0 \n    tmp8 =tmp7 >=tmp1 \n    tmp9 =tmp7 <tmp3 \n    tmp10 =tmp8 &tmp9 \n    tmp11 =tl .load (in_ptr0 +((-1 )+x0 +x1 *(ks1 //2 )*(ks2 //2 )),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tmp11 +tmp6 \n    tmp13 =x0 \n    tmp14 =tmp13 >=tmp1 \n    tmp15 =tmp13 <tmp3 \n    tmp16 =tmp14 &tmp15 \n    tmp17 =tl .load (in_ptr0 +(x0 +x1 *(ks1 //2 )*(ks2 //2 )),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp18 =tmp17 +tmp12 \n    tmp19 =0.3333333333333333 \n    tmp20 =tmp18 *tmp19 \n    tmp21 =tmp20 *tmp20 \n    tl .store (out_ptr0 +(x2 ),tmp21 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool3d_clone_pow_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x9 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +4 *x1 +4 *ks3 *x2 +2 *x1 *(ks4 //2 )*(ks5 //2 )+2 *ks3 *x2 *(ks4 //2 )*(ks5 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +4 *x1 +4 *ks3 *x2 +2 *x1 *(ks4 //2 )*(ks5 //2 )+2 *ks3 *x2 *(ks4 //2 )*(ks5 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 *ks3 +2 *x0 +4 *x1 +4 *ks3 *x2 +ks3 *(ks4 //2 )*(ks5 //2 )+2 *x1 *(ks4 //2 )*(ks5 //2 )+2 *ks3 *x2 *(ks4 //2 )*(ks5 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +2 *ks3 +2 *x0 +4 *x1 +4 *ks3 *x2 +ks3 *(ks4 //2 )*(ks5 //2 )+2 *x1 *(ks4 //2 )*(ks5 //2 )+2 *ks3 *x2 *(ks4 //2 )*(ks5 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(2 +2 *x0 +4 *x1 +(ks4 //2 )*(ks5 //2 )+4 *ks3 *x2 +2 *x1 *(ks4 //2 )*(ks5 //2 )+2 *ks3 *x2 *(ks4 //2 )*(ks5 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(3 +2 *x0 +4 *x1 +(ks4 //2 )*(ks5 //2 )+4 *ks3 *x2 +2 *x1 *(ks4 //2 )*(ks5 //2 )+2 *ks3 *x2 *(ks4 //2 )*(ks5 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(2 +2 *ks3 +2 *x0 +4 *x1 +(ks4 //2 )*(ks5 //2 )+4 *ks3 *x2 +ks3 *(ks4 //2 )*(ks5 //2 )+2 *x1 *(ks4 //2 )*(ks5 //2 )+2 *ks3 *x2 *(ks4 //2 )*(ks5 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(3 +2 *ks3 +2 *x0 +4 *x1 +(ks4 //2 )*(ks5 //2 )+4 *ks3 *x2 +ks3 *(ks4 //2 )*(ks5 //2 )+2 *x1 *(ks4 //2 )*(ks5 //2 )+2 *ks3 *x2 *(ks4 //2 )*(ks5 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp8 =tmp7 +tmp6 \n    tmp10 =tmp9 +tmp8 \n    tmp12 =tmp11 +tmp10 \n    tmp14 =tmp13 +tmp12 \n    tmp15 =0.125 \n    tmp16 =tmp14 *tmp15 \n    tl .store (out_ptr0 +(x9 ),tmp16 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_mul_pow_relu_sign_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +x2 +ks3 *x1 +x2 *(triton_helpers .div_floor_integer ((ks4 //2 )*(ks5 //2 ),2 ))+ks3 *x1 *(triton_helpers .div_floor_integer ((ks4 //2 )*(ks5 //2 ),2 ))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tmp1 <tmp0 \n    tmp3 =tmp2 .to (tl .int8 )\n    tmp4 =tmp0 <tmp1 \n    tmp5 =tmp4 .to (tl .int8 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =tmp6 .to (tmp0 .dtype )\n    tmp8 =tl_math .abs (tmp0 )\n    tmp9 =triton_helpers .maximum (tmp1 ,tmp8 )\n    tmp10 =tmp7 *tmp9 \n    tmp11 =8.0 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =libdevice .sqrt (tmp12 )\n    tl .store (out_ptr0 +(x3 ),tmp13 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .max_pool3d_with_indices .default (arg4_1 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del arg4_1 \n        buf1 =buf0 [0 ]\n        del buf0 \n        2 +(s2 //2 )*(s3 //2 )\n        buf3 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,2 +(s2 //2 )*(s3 //2 )),(2 *s0 *(s1 //2 )+s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),2 +(s2 //2 )*(s3 //2 ),2 *s0 +s0 *(s2 //2 )*(s3 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_clone_pow_0_xnumel =2 *s0 *(s1 //2 )+s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_clone_pow_0 [grid (triton_poi_fused_clone_pow_0_xnumel )](buf1 ,buf3 ,1026 ,64 ,64 ,49248 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        1 +(((s2 //2 )*(s3 //2 ))//2 )\n        s0 //2 \n        (s0 //2 )*(((s2 //2 )*(s3 //2 ))//2 )+(s0 //2 )\n        buf4 =empty_strided_cuda ((1 ,s0 //2 ,s1 //4 ,1 +(((s2 //2 )*(s3 //2 ))//2 )),((s0 //2 )*(s1 //4 )+(s0 //2 )*(s1 //4 )*(((s2 //2 )*(s3 //2 ))//2 ),1 +(((s2 //2 )*(s3 //2 ))//2 ),(s0 //2 )*(((s2 //2 )*(s3 //2 ))//2 )+(s0 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool3d_clone_pow_1_xnumel =(s0 //2 )*(s1 //4 )+(s0 //2 )*(s1 //4 )*(((s2 //2 )*(s3 //2 ))//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool3d_clone_pow_1 [grid (triton_poi_fused_avg_pool3d_clone_pow_1_xnumel )](buf3 ,buf4 ,513 ,1 ,513 ,3 ,64 ,64 ,4104 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        s1 //4 \n        (s1 //4 )*(((s2 //2 )*(s3 //2 ))//2 )+(s1 //4 )\n        buf5 =empty_strided_cuda ((1 ,s0 //2 ,s1 //4 ,1 +(((s2 //2 )*(s3 //2 ))//2 )),((s1 //4 )*(((s2 //2 )*(s3 //2 ))//2 )+(s1 //4 ),(s1 //4 )*(((s2 //2 )*(s3 //2 ))//2 )+(s1 //4 ),1 +(((s2 //2 )*(s3 //2 ))//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_abs_mul_pow_relu_sign_2_xnumel =(s0 //2 )*(s1 //4 )+(s0 //2 )*(s1 //4 )*(((s2 //2 )*(s3 //2 ))//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_mul_pow_relu_sign_2 [grid (triton_poi_fused_abs_mul_pow_relu_sign_2_xnumel )](buf4 ,buf5 ,513 ,8 ,4104 ,1 ,64 ,64 ,4104 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =64 \n    arg3_1 =64 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,64 ,64 ),(393216 ,131072 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "1322036d-faa0-4fe7-9f41-256009475f06",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softmin', 'RReLU', 'LPPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softmin = nn.Softmin(dim=1)\n        self.rrelu1 = nn.RReLU()\n        self.rrelu2 = nn.RReLU()\n        self.lppool1d = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, length)\n        x = self.rrelu1(x)  # Apply RReLU\n        x = self.lppool1d(x)  # Apply LPPool1d\n        x = self.rrelu2(x)  # Apply RReLU again\n        x = self.softmin(x)  # Apply Softmin\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 100).cuda()  # Example input shape (batch_size=1, channels=10, length=100)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_pow_rrelu_with_noise_functional_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =tmp0 <=tmp1 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =tl .where (tmp2 ,tmp4 ,tmp0 )\n    tmp6 =tmp5 *tmp5 \n    tl .store (in_out_ptr0 +(x0 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_mul_pow_relu_sign_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 +2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp5 =0.3333333333333333 \n    tmp6 =tmp4 *tmp5 \n    tmp7 =tl .full ([1 ],0 ,tl .int32 )\n    tmp8 =tmp7 <tmp6 \n    tmp9 =tmp8 .to (tl .int8 )\n    tmp10 =tmp6 <tmp7 \n    tmp11 =tmp10 .to (tl .int8 )\n    tmp12 =tmp9 -tmp11 \n    tmp13 =tmp12 .to (tmp6 .dtype )\n    tmp14 =tl_math .abs (tmp6 )\n    tmp15 =triton_helpers .maximum (tmp7 ,tmp14 )\n    tmp16 =tmp13 *tmp15 \n    tmp17 =3.0 \n    tmp18 =tmp16 *tmp17 \n    tmp19 =libdevice .sqrt (tmp18 )\n    tl .store (out_ptr0 +(x2 ),tmp19 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_neg_rrelu_with_noise_functional_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(x0 +ks0 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tl .load (in_ptr1 +(x0 +ks0 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =0.0 \n        tmp2 =tmp0 <=tmp1 \n        tmp4 =tmp0 *tmp3 \n        tmp5 =tl .where (tmp2 ,tmp4 ,tmp0 )\n        tmp6 =-tmp5 \n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =triton_helpers .maximum (_tmp8 ,tmp7 )\n        _tmp8 =tl .where (r0_mask &xmask ,tmp9 ,_tmp8 )\n    tmp8 =triton_helpers .max2 (_tmp8 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp8 ,xmask )\n    _tmp20 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp10 =tl .load (in_ptr0 +(x0 +ks0 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp13 =tl .load (in_ptr1 +(x0 +ks0 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp11 =0.0 \n        tmp12 =tmp10 <=tmp11 \n        tmp14 =tmp10 *tmp13 \n        tmp15 =tl .where (tmp12 ,tmp14 ,tmp10 )\n        tmp16 =-tmp15 \n        tmp17 =tmp16 -tmp8 \n        tmp18 =tl_math .exp (tmp17 )\n        tmp19 =tl .broadcast_to (tmp18 ,[XBLOCK ,R0_BLOCK ])\n        tmp21 =_tmp20 +tmp19 \n        _tmp20 =tl .where (r0_mask &xmask ,tmp21 ,_tmp20 )\n    tmp20 =tl .sum (_tmp20 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp20 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_neg_rrelu_with_noise_functional_3 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr1 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.0 \n    tmp2 =tmp0 <=tmp1 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =tl .where (tmp2 ,tmp4 ,tmp0 )\n    tmp6 =-tmp5 \n    tmp8 =tmp6 -tmp7 \n    tmp9 =tl_math .exp (tmp8 )\n    tmp11 =tmp9 /tmp10 \n    tl .store (out_ptr0 +(x0 +x1 +x1 *(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))),tmp11 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .uniform .default (arg2_1 ,0.125 ,0.3333333333333333 )\n        buf1 =buf0 \n        del buf0 \n        buf2 =buf1 ;del buf1 \n\n        triton_poi_fused_pow_rrelu_with_noise_functional_0_xnumel =s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_pow_rrelu_with_noise_functional_0 [grid (triton_poi_fused_pow_rrelu_with_noise_functional_0_xnumel )](buf2 ,arg2_1 ,1000 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        ((-1 )+s1 )//2 \n        buf3 =empty_strided_cuda ((1 ,s0 ,((-1 )+s1 )//2 ),(s0 *(((-1 )+s1 )//2 ),((-1 )+s1 )//2 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel =s0 *(((-1 )+s1 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_mul_pow_relu_sign_1 [grid (triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel )](buf2 ,buf3 ,49 ,100 ,490 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n\n        buf4 =torch .ops .aten .uniform .default (buf3 ,0.125 ,0.3333333333333333 )\n        buf5 =buf4 \n        del buf4 \n        buf6 =empty_strided_cuda ((1 ,1 ,((-1 )+s1 )//2 ),(((-1 )+s1 )//2 ,((-1 )+s1 )//2 ,1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,1 ,((-1 )+s1 )//2 ),(((-1 )+s1 )//2 ,((-1 )+s1 )//2 ,1 ),torch .float32 )\n\n        triton_red_fused__softmax_neg_rrelu_with_noise_functional_2_xnumel =((-1 )+s1 )//2 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_neg_rrelu_with_noise_functional_2 [grid (triton_red_fused__softmax_neg_rrelu_with_noise_functional_2_xnumel )](buf3 ,buf5 ,buf6 ,buf7 ,49 ,49 ,10 ,XBLOCK =8 ,R0_BLOCK =16 ,num_warps =2 ,num_stages =1 )\n        buf8 =empty_strided_cuda ((1 ,s0 ,((-1 )+s1 )//2 ),(s0 +s0 *(((-3 )+s1 )//2 ),1 +(((-3 )+s1 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused__softmax_neg_rrelu_with_noise_functional_3_xnumel =s0 *(((-1 )+s1 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused__softmax_neg_rrelu_with_noise_functional_3 [grid (triton_poi_fused__softmax_neg_rrelu_with_noise_functional_3_xnumel )](buf3 ,buf5 ,buf6 ,buf7 ,buf8 ,49 ,100 ,490 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        del buf5 \n        del buf6 \n        del buf7 \n    return (buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =100 \n    arg2_1 =rand_strided ((1 ,10 ,100 ),(1000 ,100 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "14b89c4b-4319-4ce8-a077-028ec8ac1c7e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GRUCell', 'BCEWithLogitsLoss', 'Upsample', 'PixelShuffle', 'ConstantPad2d', 'ELU', 'TransformerEncoder']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.gru_cell = nn.GRUCell(input_size=128, hidden_size=256)\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=2)\n        self.pad = nn.ConstantPad2d(padding=2, value=0.5)\n        self.elu = nn.ELU(alpha=1.0)\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer=nn.TransformerEncoderLayer(d_model=256, nhead=8),\n            num_layers=3\n        )\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        # Assume input x is of shape (batch_size, seq_len, features)\n        batch_size, seq_len, features = x.shape\n        \n        # Reshape for GRUCell\n        x = x.view(-1, features)  # Flatten to (batch_size * seq_len, features)\n        hx = torch.zeros(batch_size * seq_len, 256).to(x.device)  # Initialize hidden state\n        \n        # Apply GRUCell\n        x = self.gru_cell(x, hx)\n        x = x.view(batch_size, seq_len, -1)  # Reshape back to (batch_size, seq_len, hidden_size)\n        \n        # Apply TransformerEncoder\n        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, features)\n        x = self.transformer_encoder(x)\n        x = x.permute(1, 0, 2)  # Reshape back to (batch_size, seq_len, features)\n        \n        # Apply Upsample\n        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len, features)\n        x = self.upsample(x)\n        \n        # Apply PixelShuffle\n        x = self.pixel_shuffle(x)\n        \n        # Apply ConstantPad2d\n        x = self.pad(x)\n        \n        # Apply ELU\n        x = self.elu(x)\n        \n        # Compute loss (dummy target for demonstration)\n        target = torch.ones_like(x)\n        loss = self.loss(x, target)\n        \n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(2, 10, 128).cuda()  # Example input: (batch_size=2, seq_len=10, features=128)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(s0 ,s1 ,s2 ),(s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((s0 *s1 ,256 ),(256 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_0_xnumel =256 *s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_0 [grid (triton_poi_fused__to_copy_0_xnumel )](buf0 ,5120 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n    return (reinterpret_tensor (arg3_1 ,(s0 *s1 ,s2 ),(s2 ,1 ),0 ),buf0 ,s0 ,s1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =2 \n    arg1_1 =10 \n    arg2_1 =128 \n    arg3_1 =rand_strided ((2 ,10 ,128 ),(1280 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "159f0da2-ce6e-4776-88aa-6078f34dfd99",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Identity', 'CrossEntropyLoss', 'ZeroPad3d', 'CosineSimilarity', 'FeatureAlphaDropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.identity = nn.Identity()\n        self.zero_pad3d = nn.ZeroPad3d(padding=1)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        # Apply Identity\n        x = self.identity(x)\n        \n        # Apply ZeroPad3d\n        x = self.zero_pad3d(x)\n        \n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Reshape for CosineSimilarity\n        x1 = x.view(x.size(0), -1)\n        x2 = torch.ones_like(x1)  # Create a tensor of ones with the same shape as x1\n        x = self.cosine_similarity(x1, x2)\n        \n        # Reshape for CrossEntropyLoss\n        x = x.unsqueeze(0)  # Add batch dimension\n        target = torch.zeros(1, dtype=torch.long).to(x.device)  # Dummy target\n        loss = self.cross_entropy_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input with arbitrary shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_constant_pad_nd_mul_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x6 =((xindex //ks0 )%ks1 )\n    x1 =((xindex //ks3 )%ks4 )\n    x0 =(xindex %ks3 )\n    x2 =((xindex //ks7 )%ks1 )\n    x3 =xindex //ks8 \n    x10 =xindex \n    tmp19 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =(-1 )+x6 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks5 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-1 )+x0 \n    tmp10 =tmp9 >=tmp1 \n    tmp11 =ks6 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =tl .load (in_ptr0 +((-1 )+x0 +((-1 )*ks6 )+ks6 *x1 +((-1 )*ks5 *ks6 )+ks5 *ks6 *x2 +ks2 *ks5 *ks6 *x3 ),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =0.5 \n    tmp21 =tmp19 <tmp20 \n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =0.8864048946659319 \n    tmp24 =tmp22 *tmp23 \n    tmp25 =tmp18 *tmp24 \n    tmp26 =-1.0 \n    tmp27 =tmp22 +tmp26 \n    tmp28 =1.558387861036063 \n    tmp29 =tmp27 *tmp28 \n    tmp30 =0.7791939305180315 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =tmp25 +tmp31 \n    tl .store (out_ptr0 +(x10 ),tmp32 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_linalg_vector_norm_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =15 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 )\n        tmp1 =8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tmp3 *tmp3 \n        tmp5 =tl .full (tmp4 .shape ,0 ,tmp4 .dtype )\n        tmp6 =tl .where (tmp2 ,tmp4 ,tmp5 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =_tmp8 +tmp7 \n        _tmp8 =tl .where (r0_mask &xmask ,tmp9 ,_tmp8 )\n    tmp8 =tl .sum (_tmp8 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_linalg_vector_norm_3 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =15 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp4 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_linalg_vector_norm_ones_like_4 (out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =15 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp7 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 )\n        tmp1 =8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =1.0 \n        tmp4 =tl .full (tmp3 .shape ,0 ,tmp3 .dtype )\n        tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n        tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n        tmp8 =_tmp7 +tmp6 \n        _tmp7 =tl .where (r0_mask &xmask ,tmp8 ,_tmp7 )\n    tmp7 =tl .sum (_tmp7 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_clamp_min_div_linalg_vector_norm_mul_ones_like_sum_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =15 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp18 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 )\n        tmp1 =8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +(tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int32 )),tmp2 ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =libdevice .sqrt (tmp4 )\n        tmp6 =1e-08 \n        tmp7 =triton_helpers .maximum (tmp5 ,tmp6 )\n        tmp8 =tmp3 /tmp7 \n        tmp9 =tl .load (in_ptr2 +(tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int32 )),tmp2 ,eviction_policy ='evict_last',other =0.0 )\n        tmp10 =libdevice .sqrt (tmp9 )\n        tmp11 =triton_helpers .maximum (tmp10 ,tmp6 )\n        tmp12 =1.0 \n        tmp13 =tmp12 /tmp11 \n        tmp14 =tmp8 *tmp13 \n        tmp15 =tl .full (tmp14 .shape ,0 ,tmp14 .dtype )\n        tmp16 =tl .where (tmp2 ,tmp14 ,tmp15 )\n        tmp17 =tl .broadcast_to (tmp16 ,[XBLOCK ,R0_BLOCK ])\n        tmp19 =_tmp18 +tmp17 \n        _tmp18 =tl .where (r0_mask &xmask ,tmp19 ,_tmp18 )\n    tmp18 =tl .sum (_tmp18 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp18 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_clamp_min_div_linalg_vector_norm_mul_nll_loss_forward_ones_like_sum_6 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =15 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =tmp4 -tmp4 \n    tmp6 =tl_math .exp (tmp5 )\n    tmp7 =tl_math .log (tmp6 )\n    tmp8 =tmp5 -tmp7 \n    tmp9 =-tmp8 \n    tmp10 =tl .full ([1 ,1 ],True ,tl .int1 )\n    tmp11 =0.0 \n    tmp12 =tl .where (tmp10 ,tmp9 ,tmp11 )\n    tmp13 =1.0 \n    tmp14 =tmp12 /tmp13 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp14 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        2 +s1 \n        2 +s3 \n        2 +s2 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf2 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_add_bernoulli_constant_pad_nd_mul_1_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_constant_pad_nd_mul_1 [grid (triton_poi_fused__to_copy_add_bernoulli_constant_pad_nd_mul_1_xnumel )](arg4_1 ,buf1 ,buf2 ,1156 ,34 ,32 ,34 ,34 ,32 ,32 ,1156 ,39304 ,117912 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg4_1 \n        del buf1 \n        buf3 =empty_strided_cuda ((1 ,1 ,15 ),(15 ,15 ,1 ),torch .float32 )\n\n        (14 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 )//15 \n        get_raw_stream (0 )\n        triton_red_fused_linalg_vector_norm_2 [grid (15 )](buf2 ,buf3 ,3 ,32 ,32 ,32 ,15 ,7861 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_linalg_vector_norm_3 [grid (1 )](buf3 ,buf4 ,1 ,15 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf5 =buf3 ;del buf3 \n\n        (14 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 )//15 \n        get_raw_stream (0 )\n        triton_red_fused_linalg_vector_norm_ones_like_4 [grid (15 )](buf5 ,3 ,32 ,32 ,32 ,15 ,7861 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf6 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_linalg_vector_norm_3 [grid (1 )](buf5 ,buf6 ,1 ,15 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf7 =reinterpret_tensor (buf5 ,(1 ,15 ),(15 ,1 ),0 );del buf5 \n\n        (14 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 )//15 \n        get_raw_stream (0 )\n        triton_red_fused_clamp_min_div_linalg_vector_norm_mul_ones_like_sum_5 [grid (15 )](buf2 ,buf4 ,buf6 ,buf7 ,3 ,32 ,32 ,32 ,15 ,7861 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf2 \n        del buf4 \n        buf8 =reinterpret_tensor (buf6 ,(1 ,),(1 ,),0 );del buf6 \n        buf9 =reinterpret_tensor (buf8 ,(),(),0 );del buf8 \n\n        get_raw_stream (0 )\n        triton_per_fused_clamp_min_div_linalg_vector_norm_mul_nll_loss_forward_ones_like_sum_6 [grid (1 )](buf9 ,buf7 ,1 ,15 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf7 \n    return (buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "169de8c6-f2ec-48c5-bcfe-d25934f1ec39",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FeatureAlphaDropout', 'AdaptiveAvgPool2d', 'PairwiseDistance', 'CrossMapLRN2d', 'BatchNorm1d', 'LazyBatchNorm2d', 'Dropout3d', 'BCEWithLogitsLoss', 'SiLU', 'LazyBatchNorm1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.adaptive_avg_pool2d = nn.AdaptiveAvgPool2d((5, 5))\n        self.pairwise_distance = nn.PairwiseDistance(p=2)\n        self.cross_map_lrn2d = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.batch_norm1d = nn.BatchNorm1d(128)\n        self.lazy_batch_norm2d = nn.LazyBatchNorm2d()\n        self.dropout3d = nn.Dropout3d(p=0.5)\n        self.bce_with_logits_loss = nn.BCEWithLogitsLoss()\n        self.silu = nn.SiLU()\n        self.lazy_batch_norm1d = nn.LazyBatchNorm1d()\n\n    def forward(self, x):\n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Reshape for AdaptiveAvgPool2d\n        x = x.view(-1, 3, 32, 32)  # Assuming input is reshaped to (batch_size, 3, 32, 32)\n        x = self.adaptive_avg_pool2d(x)\n        \n        # Apply CrossMapLRN2d\n        x = self.cross_map_lrn2d(x)\n        \n        # Apply LazyBatchNorm2d\n        x = self.lazy_batch_norm2d(x)\n        \n        # Apply Dropout3d\n        x = x.unsqueeze(1)  # Add a channel dimension for Dropout3d\n        x = self.dropout3d(x)\n        x = x.squeeze(1)  # Remove the channel dimension\n        \n        # Apply SiLU activation\n        x = self.silu(x)\n        \n        # Flatten for BatchNorm1d\n        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n        x = self.batch_norm1d(x)\n        \n        # Apply LazyBatchNorm1d\n        x = self.lazy_batch_norm1d(x)\n        \n        # Compute PairwiseDistance (example with a dummy tensor)\n        dummy_tensor = torch.randn_like(x)\n        x = self.pairwise_distance(x, dummy_tensor)\n        \n        # Reshape for BCEWithLogitsLoss (example with a dummy target)\n        x = x.unsqueeze(1)  # Add a channel dimension\n        dummy_target = torch.randint(0, 2, (x.size(0), 1), dtype=torch.float32)\n        loss = self.bce_with_logits_loss(x, dummy_target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_mul_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =0.5 \n    tmp3 =tmp1 <tmp2 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =0.8864048946659319 \n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp0 *tmp6 \n    tmp8 =-1.0 \n    tmp9 =tmp4 +tmp8 \n    tmp10 =1.558387861036063 \n    tmp11 =tmp9 *tmp10 \n    tmp12 =0.7791939305180315 \n    tmp13 =tmp11 +tmp12 \n    tmp14 =tmp7 +tmp13 \n    tl .store (out_ptr0 +(x2 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %32 )\n    x1 =((xindex //32 )%32 )\n    x2 =((xindex //1024 )%3 )\n    x3 =xindex //3072 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(((x0 +32 *x1 +1024 *x2 +3072 *x3 )%(ks0 *ks1 *ks2 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x4 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_add_bernoulli_mul_1_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_mul_1 [grid (triton_poi_fused__to_copy_add_bernoulli_mul_1_xnumel )](arg3_1 ,buf1 ,buf2 ,4096 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        buf3 =empty_strided_cuda (((s0 *s1 *s2 )//3072 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),torch .float32 )\n\n        triton_poi_fused__adaptive_avg_pool2d_2_xnumel =3072 *((s0 *s1 *s2 )//3072 )\n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_2 [grid (triton_poi_fused__adaptive_avg_pool2d_2_xnumel )](buf2 ,buf3 ,3 ,64 ,64 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n\n        buf4 =torch .ops .aten ._adaptive_avg_pool2d .default (buf3 ,[5 ,5 ])\n        del buf3 \n        buf5 =buf4 \n        del buf4 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "17538166-3f1a-4620-bedd-d0ee0051a30f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GaussianNLLLoss', 'ReLU6', 'LogSigmoid', 'ParameterDict']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        # Using ParameterDict to store parameters\n        self.params = nn.ParameterDict({\n            'param1': nn.Parameter(torch.randn(10)),\n            'param2': nn.Parameter(torch.randn(10)),\n        })\n        # Using ReLU6 and LogSigmoid activation functions\n        self.relu6 = nn.ReLU6()\n        self.log_sigmoid = nn.LogSigmoid()\n        # Using GaussianNLLLoss as part of the forward pass\n        self.gaussian_nll_loss = nn.GaussianNLLLoss()\n\n    def forward(self, x):\n        # Apply ReLU6 activation\n        x = self.relu6(x)\n        \n        # Apply LogSigmoid activation\n        x = self.log_sigmoid(x)\n        \n        # Reshape the input to match the parameter shapes\n        x = x.view(-1, 10)\n        \n        # Use the parameters from ParameterDict\n        x = x * self.params['param1'] + self.params['param2']\n        \n        # Compute the mean and variance for GaussianNLLLoss\n        mean = x.mean(dim=1, keepdim=True)\n        var = x.var(dim=1, keepdim=True)\n        \n        # Generate a target tensor for GaussianNLLLoss\n        target = torch.randn_like(mean)\n        \n        # Compute the GaussianNLLLoss\n        loss = self.gaussian_nll_loss(mean, target, var)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_hardtanh_log_sigmoid_forward_mean_mul_sub_var_view_0 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp11 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp13 =tl .load (in_ptr2 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =0.0 \n    tmp2 =triton_helpers .maximum (tmp0 ,tmp1 )\n    tmp3 =6.0 \n    tmp4 =triton_helpers .minimum (tmp2 ,tmp3 )\n    tmp5 =triton_helpers .minimum (tmp1 ,tmp4 )\n    tmp6 =tl_math .abs (tmp4 )\n    tmp7 =-tmp6 \n    tmp8 =tl_math .exp (tmp7 )\n    tmp9 =libdevice .log1p (tmp8 )\n    tmp10 =tmp5 -tmp9 \n    tmp12 =tmp10 *tmp11 \n    tmp14 =tmp12 +tmp13 \n    tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp17 =tl .where (r0_mask ,tmp15 ,0 )\n    tmp18 =tl .sum (tmp17 ,1 )[:,None ]\n    tmp20 =tl .broadcast_to (tmp15 ,[XBLOCK ,R0_BLOCK ])\n    tmp22 =tl .where (r0_mask ,tmp20 ,0 )\n    tmp23 =tl .sum (tmp22 ,1 )[:,None ]\n    tmp24 =tl .full ([XBLOCK ,1 ],10 ,tl .int32 )\n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp23 /tmp25 \n    tmp27 =tmp15 -tmp26 \n    tmp28 =tmp27 *tmp27 \n    tmp29 =tl .broadcast_to (tmp28 ,[XBLOCK ,R0_BLOCK ])\n    tmp31 =tl .where (r0_mask ,tmp29 ,0 )\n    tmp32 =tl .sum (tmp31 ,1 )[:,None ]\n    tmp33 =10.0 \n    tmp34 =tmp18 /tmp33 \n    tmp35 =tmp14 -tmp34 \n    tmp36 =9.0 \n    tmp37 =tmp32 /tmp36 \n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp35 ,r0_mask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp34 ,None )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp37 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_randn_like_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ),(10 ,1 ))\n    assert_size_stride (primals_2 ,(10 ,),(1 ,))\n    assert_size_stride (primals_3 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n        buf8 =buf0 ;del buf0 \n        buf9 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_hardtanh_log_sigmoid_forward_mean_mul_sub_var_view_0 [grid (1 )](buf8 ,buf9 ,primals_1 ,primals_2 ,primals_3 ,buf7 ,1 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_2 \n        del primals_3 \n        buf4 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf4 )\n        buf5 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_randn_like_1 [grid (1 )](buf4 ,buf5 ,0 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf4 \n    return (buf8 ,buf5 ,buf9 ,primals_1 ,buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "177c8c42-46e4-4a73-9bff-2d69d3e8251a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LPPool3d', 'InstanceNorm1d', 'ReflectionPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lppool3d_1 = nn.LPPool3d(norm_type=2, kernel_size=2, stride=2)\n        self.lppool3d_2 = nn.LPPool3d(norm_type=2, kernel_size=2, stride=2)\n        self.instance_norm1d_1 = nn.InstanceNorm1d(num_features=64)\n        self.instance_norm1d_2 = nn.InstanceNorm1d(num_features=32)\n        self.reflection_pad1d_1 = nn.ReflectionPad1d(padding=1)\n        self.reflection_pad1d_2 = nn.ReflectionPad1d(padding=1)\n\n    def forward(self, x):\n        # Assuming input is 5D (batch, channels, depth, height, width)\n        x = self.lppool3d_1(x)\n        x = self.lppool3d_2(x)\n        \n        # Reshape to 3D (batch, channels, depth*height*width) for InstanceNorm1d\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size, channels, -1)\n        \n        x = self.instance_norm1d_1(x)\n        x = self.instance_norm1d_2(x)\n        \n        # Reshape back to 5D\n        x = x.view(batch_size, channels, depth, height, width)\n        \n        # Reshape to 3D (batch, channels, depth*height*width) for ReflectionPad1d\n        x = x.view(batch_size, channels, -1)\n        \n        x = self.reflection_pad1d_1(x)\n        x = self.reflection_pad1d_2(x)\n        \n        # Reshape back to 5D\n        x = x.view(batch_size, channels, depth, height, width)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 64, 64).cuda()  # Example input shape (batch, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool3d_pow_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =((xindex //ks2 )%ks3 )\n    x3 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(ks7 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr0 +(1 +ks7 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp14 =tl .load (in_ptr0 +(1 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr0 +(ks7 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp20 =tl .load (in_ptr0 +(1 +ks7 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tmp0 *tmp0 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tmp3 +tmp1 \n    tmp6 =tmp5 *tmp5 \n    tmp7 =tmp6 +tmp4 \n    tmp9 =tmp8 *tmp8 \n    tmp10 =tmp9 +tmp7 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tmp12 +tmp10 \n    tmp15 =tmp14 *tmp14 \n    tmp16 =tmp15 +tmp13 \n    tmp18 =tmp17 *tmp17 \n    tmp19 =tmp18 +tmp16 \n    tmp21 =tmp20 *tmp20 \n    tmp22 =tmp21 +tmp19 \n    tmp23 =0.125 \n    tmp24 =tmp22 *tmp23 \n    tl .store (out_ptr0 +(x4 ),tmp24 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_avg_pool3d_mul_pow_relu_sign_1 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =((xindex //ks2 )%ks3 )\n    x3 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr0 +(ks5 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =tl .load (in_ptr0 +(1 +ks5 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp57 =tl .load (in_ptr0 +(ks8 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp71 =tl .load (in_ptr0 +(1 +ks8 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp85 =tl .load (in_ptr0 +(ks5 +ks8 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp99 =tl .load (in_ptr0 +(1 +ks5 +ks8 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tmp1 <tmp0 \n    tmp3 =tmp2 .to (tl .int8 )\n    tmp4 =tmp0 <tmp1 \n    tmp5 =tmp4 .to (tl .int8 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =tmp6 .to (tmp0 .dtype )\n    tmp8 =tl_math .abs (tmp0 )\n    tmp9 =triton_helpers .maximum (tmp1 ,tmp8 )\n    tmp10 =tmp7 *tmp9 \n    tmp11 =8.0 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =libdevice .sqrt (tmp12 )\n    tmp14 =tmp13 *tmp13 \n    tmp16 =tmp1 <tmp15 \n    tmp17 =tmp16 .to (tl .int8 )\n    tmp18 =tmp15 <tmp1 \n    tmp19 =tmp18 .to (tl .int8 )\n    tmp20 =tmp17 -tmp19 \n    tmp21 =tmp20 .to (tmp15 .dtype )\n    tmp22 =tl_math .abs (tmp15 )\n    tmp23 =triton_helpers .maximum (tmp1 ,tmp22 )\n    tmp24 =tmp21 *tmp23 \n    tmp25 =tmp24 *tmp11 \n    tmp26 =libdevice .sqrt (tmp25 )\n    tmp27 =tmp26 *tmp26 \n    tmp28 =tmp27 +tmp14 \n    tmp30 =tmp1 <tmp29 \n    tmp31 =tmp30 .to (tl .int8 )\n    tmp32 =tmp29 <tmp1 \n    tmp33 =tmp32 .to (tl .int8 )\n    tmp34 =tmp31 -tmp33 \n    tmp35 =tmp34 .to (tmp29 .dtype )\n    tmp36 =tl_math .abs (tmp29 )\n    tmp37 =triton_helpers .maximum (tmp1 ,tmp36 )\n    tmp38 =tmp35 *tmp37 \n    tmp39 =tmp38 *tmp11 \n    tmp40 =libdevice .sqrt (tmp39 )\n    tmp41 =tmp40 *tmp40 \n    tmp42 =tmp41 +tmp28 \n    tmp44 =tmp1 <tmp43 \n    tmp45 =tmp44 .to (tl .int8 )\n    tmp46 =tmp43 <tmp1 \n    tmp47 =tmp46 .to (tl .int8 )\n    tmp48 =tmp45 -tmp47 \n    tmp49 =tmp48 .to (tmp43 .dtype )\n    tmp50 =tl_math .abs (tmp43 )\n    tmp51 =triton_helpers .maximum (tmp1 ,tmp50 )\n    tmp52 =tmp49 *tmp51 \n    tmp53 =tmp52 *tmp11 \n    tmp54 =libdevice .sqrt (tmp53 )\n    tmp55 =tmp54 *tmp54 \n    tmp56 =tmp55 +tmp42 \n    tmp58 =tmp1 <tmp57 \n    tmp59 =tmp58 .to (tl .int8 )\n    tmp60 =tmp57 <tmp1 \n    tmp61 =tmp60 .to (tl .int8 )\n    tmp62 =tmp59 -tmp61 \n    tmp63 =tmp62 .to (tmp57 .dtype )\n    tmp64 =tl_math .abs (tmp57 )\n    tmp65 =triton_helpers .maximum (tmp1 ,tmp64 )\n    tmp66 =tmp63 *tmp65 \n    tmp67 =tmp66 *tmp11 \n    tmp68 =libdevice .sqrt (tmp67 )\n    tmp69 =tmp68 *tmp68 \n    tmp70 =tmp69 +tmp56 \n    tmp72 =tmp1 <tmp71 \n    tmp73 =tmp72 .to (tl .int8 )\n    tmp74 =tmp71 <tmp1 \n    tmp75 =tmp74 .to (tl .int8 )\n    tmp76 =tmp73 -tmp75 \n    tmp77 =tmp76 .to (tmp71 .dtype )\n    tmp78 =tl_math .abs (tmp71 )\n    tmp79 =triton_helpers .maximum (tmp1 ,tmp78 )\n    tmp80 =tmp77 *tmp79 \n    tmp81 =tmp80 *tmp11 \n    tmp82 =libdevice .sqrt (tmp81 )\n    tmp83 =tmp82 *tmp82 \n    tmp84 =tmp83 +tmp70 \n    tmp86 =tmp1 <tmp85 \n    tmp87 =tmp86 .to (tl .int8 )\n    tmp88 =tmp85 <tmp1 \n    tmp89 =tmp88 .to (tl .int8 )\n    tmp90 =tmp87 -tmp89 \n    tmp91 =tmp90 .to (tmp85 .dtype )\n    tmp92 =tl_math .abs (tmp85 )\n    tmp93 =triton_helpers .maximum (tmp1 ,tmp92 )\n    tmp94 =tmp91 *tmp93 \n    tmp95 =tmp94 *tmp11 \n    tmp96 =libdevice .sqrt (tmp95 )\n    tmp97 =tmp96 *tmp96 \n    tmp98 =tmp97 +tmp84 \n    tmp100 =tmp1 <tmp99 \n    tmp101 =tmp100 .to (tl .int8 )\n    tmp102 =tmp99 <tmp1 \n    tmp103 =tmp102 .to (tl .int8 )\n    tmp104 =tmp101 -tmp103 \n    tmp105 =tmp104 .to (tmp99 .dtype )\n    tmp106 =tl_math .abs (tmp99 )\n    tmp107 =triton_helpers .maximum (tmp1 ,tmp106 )\n    tmp108 =tmp105 *tmp107 \n    tmp109 =tmp108 *tmp11 \n    tmp110 =libdevice .sqrt (tmp109 )\n    tmp111 =tmp110 *tmp110 \n    tmp112 =tmp111 +tmp98 \n    tmp113 =0.125 \n    tmp114 =tmp112 *tmp113 \n    tmp115 =tmp1 <tmp114 \n    tmp116 =tmp115 .to (tl .int8 )\n    tmp117 =tmp114 <tmp1 \n    tmp118 =tmp117 .to (tl .int8 )\n    tmp119 =tmp116 -tmp118 \n    tmp120 =tmp119 .to (tmp114 .dtype )\n    tmp121 =tl_math .abs (tmp114 )\n    tmp122 =triton_helpers .maximum (tmp1 ,tmp121 )\n    tmp123 =tmp120 *tmp122 \n    tmp124 =tmp123 *tmp11 \n    tmp125 =libdevice .sqrt (tmp124 )\n    tl .store (in_out_ptr0 +(x4 ),tmp125 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s3 //2 \n        s2 //2 \n        (s2 //2 )*(s3 //2 )\n        s1 //2 \n        (s1 //2 )*(s2 //2 )*(s3 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ,s3 //2 ),(s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),(s1 //2 )*(s2 //2 )*(s3 //2 ),(s2 //2 )*(s3 //2 ),s3 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool3d_pow_0_xnumel =s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool3d_pow_0 [grid (triton_poi_fused_avg_pool3d_pow_0_xnumel )](arg4_1 ,buf0 ,32 ,32 ,1024 ,16 ,16384 ,32 ,64 ,64 ,49152 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg4_1 \n        s3 //4 \n        s2 //4 \n        (s2 //4 )*(s3 //4 )\n        s1 //4 \n        (s1 //4 )*(s2 //4 )*(s3 //4 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,s1 //4 ,s2 //4 ,s3 //4 ),(s0 *(s1 //4 )*(s2 //4 )*(s3 //4 ),(s1 //4 )*(s2 //4 )*(s3 //4 ),(s2 //4 )*(s3 //4 ),s3 //4 ,1 ),torch .float32 )\n        buf2 =buf1 ;del buf1 \n\n        triton_poi_fused_abs_avg_pool3d_mul_pow_relu_sign_1_xnumel =s0 *(s1 //4 )*(s2 //4 )*(s3 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_avg_pool3d_mul_pow_relu_sign_1 [grid (triton_poi_fused_abs_avg_pool3d_mul_pow_relu_sign_1_xnumel )](buf2 ,buf0 ,16 ,16 ,256 ,8 ,2048 ,32 ,32 ,16 ,1024 ,6144 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n    return (reinterpret_tensor (buf2 ,(1 ,s0 ,(s1 //4 )*(s2 //4 )*(s3 //4 )),(s0 *(s1 //4 )*(s2 //4 )*(s3 //4 ),(s1 //4 )*(s2 //4 )*(s3 //4 ),1 ),0 ),s0 ,s1 //4 ,s2 //4 ,s3 //4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =64 \n    arg3_1 =64 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,64 ,64 ),(393216 ,131072 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "18feddab-38be-4d7e-b58f-edaf55793e29",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardsigmoid', 'Hardtanh', 'Flatten', 'Threshold', 'GRU', 'GroupNorm', 'CosineSimilarity']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.hardtanh = nn.Hardtanh(min_val=-1.0, max_val=1.0)\n        self.flatten = nn.Flatten()\n        self.threshold = nn.Threshold(threshold=0.5, value=0.0)\n        self.gru = nn.GRU(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n        self.groupnorm = nn.GroupNorm(num_groups=4, num_channels=64)\n        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n\n    def forward(self, x):\n        # Apply Hardsigmoid\n        x = self.hardsigmoid(x)\n        \n        # Apply Hardtanh\n        x = self.hardtanh(x)\n        \n        # Flatten the input\n        x = self.flatten(x)\n        \n        # Apply Threshold\n        x = self.threshold(x)\n        \n        # Reshape for GRU\n        x = x.view(x.size(0), -1, 128)  # Reshape to (batch_size, seq_len, input_size)\n        \n        # Apply GRU\n        x, _ = self.gru(x)\n        \n        # Apply GroupNorm\n        x = self.groupnorm(x)\n        \n        # Reshape for CosineSimilarity\n        x = x.view(x.size(0), -1)  # Flatten for CosineSimilarity\n        \n        # Create a random tensor for CosineSimilarity\n        random_tensor = torch.randn_like(x)\n        \n        # Apply CosineSimilarity\n        x = self.cosine_similarity(x, random_tensor)\n        \n        return x\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_threshold_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp13 =in_ptr1 \n    tmp1 =3.0 \n    tmp2 =tmp0 +tmp1 \n    tmp3 =0.0 \n    tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n    tmp5 =6.0 \n    tmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\n    tmp7 =0.16666666666666666 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =-1.0 \n    tmp10 =triton_helpers .maximum (tmp8 ,tmp9 )\n    tmp11 =1.0 \n    tmp12 =triton_helpers .minimum (tmp10 ,tmp11 )\n    tmp14 =tmp13 .to (tl .float32 )\n    tmp15 =tmp12 <=tmp14 \n    tmp16 =tl .where (tmp15 ,tmp3 ,tmp12 )\n    tl .store (out_ptr0 +(x0 ),tmp16 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    assert_size_stride (arg4_1 ,(),())\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 *s1 *s2 ),(s0 *s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_threshold_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_threshold_0 [grid (triton_poi_fused_threshold_0_xnumel )](arg3_1 ,arg4_1 .item (),buf0 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del arg4_1 \n    return (reinterpret_tensor (buf0 ,(1 ,(s0 *s1 *s2 )//128 ,128 ),(s0 *s1 *s2 ,128 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    arg4_1 =rand_strided ((),(),device ='cpu',dtype =torch .float64 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "19488b9f-412b-4667-97a8-eb73da064a13",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Unfold', 'LSTM', 'ReplicationPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.unfold = nn.Unfold(kernel_size=(3, 3), padding=1)\n        self.lstm = nn.LSTM(input_size=9, hidden_size=64, num_layers=2, batch_first=True)\n        self.replication_pad = nn.ReplicationPad1d(padding=2)\n        self.fc = nn.Linear(64, 10)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        \n        # Unfold the input to extract patches\n        x = self.unfold(x)  # Shape: (batch_size, channels * kernel_size[0] * kernel_size[1], num_patches)\n        x = x.view(batch_size, channels * 9, -1)  # Reshape to (batch_size, channels * 9, num_patches)\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, num_patches, channels * 9)\n        \n        # Pass through LSTM\n        x, _ = self.lstm(x)  # Shape: (batch_size, num_patches, hidden_size)\n        \n        # Apply ReplicationPad1d\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, hidden_size, num_patches)\n        x = self.replication_pad(x)  # Shape: (batch_size, hidden_size, num_patches + 2 * padding)\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, num_patches + 2 * padding, hidden_size)\n        \n        # Global average pooling\n        x = x.mean(dim=1)  # Shape: (batch_size, hidden_size)\n        \n        # Fully connected layer\n        x = self.fc(x)  # Shape: (batch_size, 10)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_im2col_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    yoffset =(tl .program_id (1 )+tl .program_id (2 )*tl .num_programs (1 ))*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x4 =xindex //ks0 \n    y1 =((yindex //3 )%3 )\n    x3 =(xindex %ks0 )\n    y0 =(yindex %3 )\n    x6 =xindex \n    y2 =yindex //9 \n    y7 =yindex \n    tl .device_assert ((x4 +y1 <2 +ks1 )|~(xmask &ymask ),\"index out of bounds: x4 + y1 < 2 + ks1\")\n    tl .device_assert ((x3 +y0 <2 +ks0 )|~(xmask &ymask ),\"index out of bounds: x3 + y0 < 2 + ks0\")\n    tmp2 =(-1 )+x4 +y1 \n    tmp3 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp4 =tmp2 >=tmp3 \n    tmp5 =ks1 \n    tmp6 =tmp2 <tmp5 \n    tmp7 =(-1 )+x3 +y0 \n    tmp8 =tmp7 >=tmp3 \n    tmp9 =ks0 \n    tmp10 =tmp7 <tmp9 \n    tmp11 =tmp4 &tmp6 \n    tmp12 =tmp11 &tmp8 \n    tmp13 =tmp12 &tmp10 \n    tmp14 =tl .load (in_ptr0 +((-1 )+x6 +y0 +((-1 )*ks0 )+ks0 *y1 +ks0 *ks1 *y2 ),tmp13 &xmask &ymask ,eviction_policy ='evict_last',other =0.0 )\n    tl .store (out_ptr0 +(x6 +ks0 *ks1 *y7 ),tmp14 ,xmask &ymask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,3 ,3 ,s1 ,s2 ),(9 *s0 *s1 *s2 ,9 *s1 *s2 ,3 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_im2col_0_ynumel =9 *s0 \n        triton_poi_fused_im2col_0_xnumel =s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_im2col_0 [grid (triton_poi_fused_im2col_0_ynumel ,triton_poi_fused_im2col_0_xnumel )](arg3_1 ,buf0 ,32 ,32 ,27 ,1024 ,XBLOCK =256 ,YBLOCK =1 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,s1 *s2 ,9 *s0 ),(9 *s0 *s1 *s2 ,1 ,s1 *s2 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "19681d82-b5e4-4334-83ab-5acc03d834a4",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Dropout1d', 'CircularPad1d', 'ModuleDict', 'FeatureAlphaDropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout1d = nn.Dropout1d(p=0.5)\n        self.circular_pad1d = nn.CircularPad1d(padding=2)\n        self.module_dict = nn.ModuleDict({\n            'feature_alpha_dropout1': nn.FeatureAlphaDropout(p=0.5),\n            'feature_alpha_dropout2': nn.FeatureAlphaDropout(p=0.5),\n            'feature_alpha_dropout3': nn.FeatureAlphaDropout(p=0.5)\n        })\n        self.dropout1d_2 = nn.Dropout1d(p=0.5)\n        self.circular_pad1d_2 = nn.CircularPad1d(padding=2)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, length)\n        x = self.dropout1d(x)\n        x = self.circular_pad1d(x)\n        \n        # Apply FeatureAlphaDropout from ModuleDict\n        x = self.module_dict['feature_alpha_dropout1'](x)\n        x = self.module_dict['feature_alpha_dropout2'](x)\n        x = self.module_dict['feature_alpha_dropout3'](x)\n        \n        x = self.dropout1d_2(x)\n        x = self.circular_pad1d_2(x)\n        \n        return x\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64).cuda()  # Example input shape (batch_size=1, channels=3, length=64)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_copy_mul_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp88 =tl .load (in_ptr2 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp102 =tl .load (in_ptr3 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp111 =tl .load (in_ptr4 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x0 \n    tmp1 =2 +ks1 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =x0 +((-1 )*ks1 )\n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .full ([1 ],2 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tl .broadcast_to (2 +ks1 ,[XBLOCK ])\n    tmp11 =tmp7 <tmp10 \n    tmp12 =tmp9 &tmp11 \n    tmp13 =tmp12 &tmp6 \n    tmp14 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *x1 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =tl .load (in_ptr1 +(x1 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp16 =0.5 \n    tmp17 =tmp15 <tmp16 \n    tmp18 =tmp17 .to (tl .float32 )\n    tmp19 =2.0 \n    tmp20 =tmp18 *tmp19 \n    tmp21 =tmp14 *tmp20 \n    tmp22 =tl .full (tmp21 .shape ,0.0 ,tmp21 .dtype )\n    tmp23 =tl .where (tmp13 ,tmp21 ,tmp22 )\n    tmp24 =float (\"nan\")\n    tmp25 =tl .where (tmp12 ,tmp23 ,tmp24 )\n    tmp26 =tl .full (tmp25 .shape ,0.0 ,tmp25 .dtype )\n    tmp27 =tl .where (tmp6 ,tmp25 ,tmp26 )\n    tmp28 =tmp3 >=tmp4 \n    tmp29 =tl .broadcast_to (2 +ks1 ,[XBLOCK ])\n    tmp30 =tmp3 <tmp29 \n    tmp31 =tmp28 &tmp30 \n    tmp32 =tmp31 &tmp2 \n    tmp33 =tl .load (in_ptr0 +((-2 )+x0 +((-1 )*ks1 )+ks1 *x1 ),tmp32 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .load (in_ptr1 +(x1 ),tmp32 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp35 =0.5 \n    tmp36 =tmp34 <tmp35 \n    tmp37 =tmp36 .to (tl .float32 )\n    tmp38 =2.0 \n    tmp39 =tmp37 *tmp38 \n    tmp40 =tmp33 *tmp39 \n    tmp41 =tl .full (tmp40 .shape ,0.0 ,tmp40 .dtype )\n    tmp42 =tl .where (tmp32 ,tmp40 ,tmp41 )\n    tmp43 =float (\"nan\")\n    tmp44 =tl .where (tmp31 ,tmp42 ,tmp43 )\n    tmp45 =tl .where (tmp5 ,tmp27 ,tmp44 )\n    tmp46 =tl .full (tmp45 .shape ,0.0 ,tmp45 .dtype )\n    tmp47 =tl .where (tmp2 ,tmp45 ,tmp46 )\n    tmp48 =tl .full ([1 ],2 ,tl .int64 )\n    tmp49 =tmp0 <tmp48 \n    tmp50 =ks1 +x0 \n    tmp51 =tl .full ([1 ],2 ,tl .int64 )\n    tmp52 =tmp50 >=tmp51 \n    tmp53 =tl .broadcast_to (2 +ks1 ,[XBLOCK ])\n    tmp54 =tmp50 <tmp53 \n    tmp55 =tmp52 &tmp54 \n    tmp56 =tmp55 &tmp49 \n    tmp57 =tl .load (in_ptr0 +((-2 )+ks1 +x0 +ks1 *x1 ),tmp56 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp58 =tl .load (in_ptr1 +(x1 ),tmp56 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp59 =0.5 \n    tmp60 =tmp58 <tmp59 \n    tmp61 =tmp60 .to (tl .float32 )\n    tmp62 =2.0 \n    tmp63 =tmp61 *tmp62 \n    tmp64 =tmp57 *tmp63 \n    tmp65 =tl .full (tmp64 .shape ,0.0 ,tmp64 .dtype )\n    tmp66 =tl .where (tmp56 ,tmp64 ,tmp65 )\n    tmp67 =float (\"nan\")\n    tmp68 =tl .where (tmp55 ,tmp66 ,tmp67 )\n    tmp69 =tl .full (tmp68 .shape ,0.0 ,tmp68 .dtype )\n    tmp70 =tl .where (tmp49 ,tmp68 ,tmp69 )\n    tmp71 =tmp0 >=tmp48 \n    tmp72 =tmp0 <tmp1 \n    tmp73 =tmp71 &tmp72 \n    tmp74 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *x1 ),tmp73 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp75 =tl .load (in_ptr1 +(x1 ),tmp73 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp76 =0.5 \n    tmp77 =tmp75 <tmp76 \n    tmp78 =tmp77 .to (tl .float32 )\n    tmp79 =2.0 \n    tmp80 =tmp78 *tmp79 \n    tmp81 =tmp74 *tmp80 \n    tmp82 =tl .full (tmp81 .shape ,0.0 ,tmp81 .dtype )\n    tmp83 =tl .where (tmp73 ,tmp81 ,tmp82 )\n    tmp84 =float (\"nan\")\n    tmp85 =tl .where (tmp73 ,tmp83 ,tmp84 )\n    tmp86 =tl .where (tmp49 ,tmp70 ,tmp85 )\n    tmp87 =tl .where (tmp2 ,tmp47 ,tmp86 )\n    tmp89 =0.5 \n    tmp90 =tmp88 <tmp89 \n    tmp91 =tmp90 .to (tl .float32 )\n    tmp92 =0.8864048946659319 \n    tmp93 =tmp91 *tmp92 \n    tmp94 =tmp87 *tmp93 \n    tmp95 =-1.0 \n    tmp96 =tmp91 +tmp95 \n    tmp97 =1.558387861036063 \n    tmp98 =tmp96 *tmp97 \n    tmp99 =0.7791939305180315 \n    tmp100 =tmp98 +tmp99 \n    tmp101 =tmp94 +tmp100 \n    tmp103 =tmp102 <tmp89 \n    tmp104 =tmp103 .to (tl .float32 )\n    tmp105 =tmp104 *tmp92 \n    tmp106 =tmp101 *tmp105 \n    tmp107 =tmp104 +tmp95 \n    tmp108 =tmp107 *tmp97 \n    tmp109 =tmp108 +tmp99 \n    tmp110 =tmp106 +tmp109 \n    tmp112 =tmp111 <tmp89 \n    tmp113 =tmp112 .to (tl .float32 )\n    tmp114 =tmp113 *tmp92 \n    tmp115 =tmp110 *tmp114 \n    tmp116 =tmp113 +tmp95 \n    tmp117 =tmp116 *tmp97 \n    tmp118 =tmp117 +tmp99 \n    tmp119 =tmp115 +tmp118 \n    tl .store (in_out_ptr0 +(x2 ),tmp119 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =x0 \n    tmp1 =6 +ks1 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(-4 )+x0 +((-1 )*ks1 )\n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .full ([1 ],2 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tl .broadcast_to (6 +ks1 ,[XBLOCK ])\n    tmp11 =tmp7 <tmp10 \n    tmp12 =tmp9 &tmp11 \n    tmp13 =tmp12 &tmp6 \n    tmp14 =tl .load (in_ptr0 +((-2 )+x0 +4 *x1 +ks1 *x1 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =tl .load (in_ptr1 +(x1 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp16 =0.5 \n    tmp17 =tmp15 <tmp16 \n    tmp18 =tmp17 .to (tl .float32 )\n    tmp19 =2.0 \n    tmp20 =tmp18 *tmp19 \n    tmp21 =tmp14 *tmp20 \n    tmp22 =tl .full (tmp21 .shape ,0.0 ,tmp21 .dtype )\n    tmp23 =tl .where (tmp13 ,tmp21 ,tmp22 )\n    tmp24 =float (\"nan\")\n    tmp25 =tl .where (tmp12 ,tmp23 ,tmp24 )\n    tmp26 =tl .full (tmp25 .shape ,0.0 ,tmp25 .dtype )\n    tmp27 =tl .where (tmp6 ,tmp25 ,tmp26 )\n    tmp28 =tmp3 >=tmp4 \n    tmp29 =tl .broadcast_to (6 +ks1 ,[XBLOCK ])\n    tmp30 =tmp3 <tmp29 \n    tmp31 =tmp28 &tmp30 \n    tmp32 =tmp31 &tmp2 \n    tmp33 =tl .load (in_ptr0 +((-6 )+x0 +((-1 )*ks1 )+4 *x1 +ks1 *x1 ),tmp32 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .load (in_ptr1 +(x1 ),tmp32 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp35 =0.5 \n    tmp36 =tmp34 <tmp35 \n    tmp37 =tmp36 .to (tl .float32 )\n    tmp38 =2.0 \n    tmp39 =tmp37 *tmp38 \n    tmp40 =tmp33 *tmp39 \n    tmp41 =tl .full (tmp40 .shape ,0.0 ,tmp40 .dtype )\n    tmp42 =tl .where (tmp32 ,tmp40 ,tmp41 )\n    tmp43 =float (\"nan\")\n    tmp44 =tl .where (tmp31 ,tmp42 ,tmp43 )\n    tmp45 =tl .where (tmp5 ,tmp27 ,tmp44 )\n    tmp46 =tl .full (tmp45 .shape ,0.0 ,tmp45 .dtype )\n    tmp47 =tl .where (tmp2 ,tmp45 ,tmp46 )\n    tmp48 =tl .full ([1 ],2 ,tl .int64 )\n    tmp49 =tmp0 <tmp48 \n    tmp50 =4 +ks1 +x0 \n    tmp51 =tl .full ([1 ],2 ,tl .int64 )\n    tmp52 =tmp50 >=tmp51 \n    tmp53 =tl .broadcast_to (6 +ks1 ,[XBLOCK ])\n    tmp54 =tmp50 <tmp53 \n    tmp55 =tmp52 &tmp54 \n    tmp56 =tmp55 &tmp49 \n    tmp57 =tl .load (in_ptr0 +(2 +ks1 +x0 +4 *x1 +ks1 *x1 ),tmp56 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp58 =tl .load (in_ptr1 +(x1 ),tmp56 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp59 =0.5 \n    tmp60 =tmp58 <tmp59 \n    tmp61 =tmp60 .to (tl .float32 )\n    tmp62 =2.0 \n    tmp63 =tmp61 *tmp62 \n    tmp64 =tmp57 *tmp63 \n    tmp65 =tl .full (tmp64 .shape ,0.0 ,tmp64 .dtype )\n    tmp66 =tl .where (tmp56 ,tmp64 ,tmp65 )\n    tmp67 =float (\"nan\")\n    tmp68 =tl .where (tmp55 ,tmp66 ,tmp67 )\n    tmp69 =tl .full (tmp68 .shape ,0.0 ,tmp68 .dtype )\n    tmp70 =tl .where (tmp49 ,tmp68 ,tmp69 )\n    tmp71 =tmp0 >=tmp48 \n    tmp72 =tmp0 <tmp1 \n    tmp73 =tmp71 &tmp72 \n    tmp74 =tl .load (in_ptr0 +((-2 )+x0 +4 *x1 +ks1 *x1 ),tmp73 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp75 =tl .load (in_ptr1 +(x1 ),tmp73 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp76 =0.5 \n    tmp77 =tmp75 <tmp76 \n    tmp78 =tmp77 .to (tl .float32 )\n    tmp79 =2.0 \n    tmp80 =tmp78 *tmp79 \n    tmp81 =tmp74 *tmp80 \n    tmp82 =tl .full (tmp81 .shape ,0.0 ,tmp81 .dtype )\n    tmp83 =tl .where (tmp73 ,tmp81 ,tmp82 )\n    tmp84 =float (\"nan\")\n    tmp85 =tl .where (tmp73 ,tmp83 ,tmp84 )\n    tmp86 =tl .where (tmp49 ,tmp70 ,tmp85 )\n    tmp87 =tl .where (tmp2 ,tmp47 ,tmp86 )\n    tl .store (out_ptr0 +(x2 ),tmp87 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((5 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[5 ],out =buf1 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,1 ),(s0 ,1 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf1 ,buf2 ,4 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((1 ,s0 ,1 ),(s0 ,1 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_1 [grid (s0 )](buf1 ,buf5 ,1 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf6 =empty_strided_cuda ((1 ,s0 ,1 ),(s0 ,1 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf1 ,buf6 ,4 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf7 =empty_strided_cuda ((1 ,s0 ,1 ),(s0 ,1 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf1 ,buf7 ,4 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf9 =empty_strided_cuda ((1 ,s0 ,1 ),(s0 ,1 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf1 ,buf9 ,4 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf1 \n        4 +s1 \n        buf4 =empty_strided_cuda ((1 ,s0 ,4 +s1 ),(4 *s0 +s0 *s1 ,4 +s1 ,1 ),torch .float32 )\n        buf8 =buf4 ;del buf4 \n\n        triton_poi_fused__to_copy_add_bernoulli_copy_mul_2_xnumel =4 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_copy_mul_2 [grid (triton_poi_fused__to_copy_add_bernoulli_copy_mul_2_xnumel )](buf8 ,arg2_1 ,buf2 ,buf5 ,buf6 ,buf7 ,68 ,64 ,204 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        del buf2 \n        del buf5 \n        del buf6 \n        del buf7 \n        8 +s1 \n        buf10 =empty_strided_cuda ((1 ,s0 ,8 +s1 ),(8 *s0 +s0 *s1 ,8 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_3_xnumel =8 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_3 [grid (triton_poi_fused_copy_3_xnumel )](buf8 ,buf9 ,buf10 ,72 ,64 ,216 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf8 \n        del buf9 \n    return (buf10 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,3 ,64 ),(192 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "1d535a27-ce3a-49e1-adf8-6140f1868ef4",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GroupNorm', 'ZeroPad3d', 'FractionalMaxPool3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.group_norm1 = nn.GroupNorm(4, 16)  # Assuming 16 channels for GroupNorm\n        self.zero_pad3d1 = nn.ZeroPad3d(1)  # Padding of 1 on all sides\n        self.fractional_max_pool3d1 = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))\n        self.group_norm2 = nn.GroupNorm(4, 16)  # Repeating GroupNorm\n        self.zero_pad3d2 = nn.ZeroPad3d(1)  # Repeating ZeroPad3d\n        self.fractional_max_pool3d2 = nn.FractionalMaxPool3d(kernel_size=2, output_size=(4, 4, 4))\n        self.group_norm3 = nn.GroupNorm(4, 16)  # Repeating GroupNorm\n        self.zero_pad3d3 = nn.ZeroPad3d(1)  # Repeating ZeroPad3d\n        self.fractional_max_pool3d3 = nn.FractionalMaxPool3d(kernel_size=2, output_size=(2, 2, 2))\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, depth, height, width)\n        x = self.group_norm1(x)\n        x = self.zero_pad3d1(x)\n        x = self.fractional_max_pool3d1(x)\n        x = self.group_norm2(x)\n        x = self.zero_pad3d2(x)\n        x = self.fractional_max_pool3d2(x)\n        x = self.group_norm3(x)\n        x = self.zero_pad3d3(x)\n        x = self.fractional_max_pool3d3(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 16, 32, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_native_group_norm_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \n    r0_numel =8192 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +8192 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp3 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_native_group_norm_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =4 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +16 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 +16 *x0 ),xmask ,other =0.0 )\n    tmp2 =tl .load (in_ptr2 +(r0_1 +16 *x0 ),xmask ,other =0.0 )\n    tmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (xmask ,tmp3 ,0 )\n    tmp8 =tl .where (xmask ,tmp4 ,0 )\n    tmp9 =tl .where (xmask ,tmp5 ,0 )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\n    tmp13 =tmp10 [:,None ]\n    tmp14 =tmp11 [:,None ]\n    tmp15 =tmp12 [:,None ]\n    tmp16 =131072.0 \n    tmp17 =tmp14 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tl .store (out_ptr2 +(x0 ),tmp20 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_native_group_norm_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =628864 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =((xindex //1156 )%34 )\n    x1 =((xindex //34 )%34 )\n    x0 =(xindex %34 )\n    x3 =xindex //39304 \n    x4 =(xindex %1156 )\n    x5 =xindex //1156 \n    tmp0 =(-1 )+x2 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],32 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =(-1 )+x0 \n    tmp9 =tmp8 >=tmp1 \n    tmp10 =tmp8 <tmp3 \n    tmp11 =tmp2 &tmp4 \n    tmp12 =tmp11 &tmp6 \n    tmp13 =tmp12 &tmp7 \n    tmp14 =tmp13 &tmp9 \n    tmp15 =tmp14 &tmp10 \n    tmp16 =tl .load (in_ptr0 +((-1057 )+x0 +32 *x1 +1024 *x2 +32768 *x3 ),tmp15 &xmask ,other =0.0 )\n    tmp17 =tl .load (in_ptr1 +(x3 //4 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp18 =tmp16 -tmp17 \n    tmp19 =tl .load (in_ptr2 +(x3 //4 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =131072.0 \n    tmp21 =tmp19 /tmp20 \n    tmp22 =1e-05 \n    tmp23 =tmp21 +tmp22 \n    tmp24 =libdevice .rsqrt (tmp23 )\n    tmp25 =tmp18 *tmp24 \n    tmp26 =tl .load (in_ptr3 +(x3 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp27 =tmp25 *tmp26 \n    tmp28 =tl .load (in_ptr4 +(x3 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp29 =tmp27 +tmp28 \n    tmp30 =tl .full (tmp29 .shape ,0.0 ,tmp29 .dtype )\n    tmp31 =tl .where (tmp15 ,tmp29 ,tmp30 )\n    tl .store (out_ptr0 +(x4 +1184 *x5 ),tmp31 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_3 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =48 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_native_group_norm_4 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =4 \n    r0_numel =2048 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +2048 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp3 ,xmask )\n    tmp8 =2048.0 \n    tmp9 =tmp3 /tmp8 \n    tmp10 =1e-05 \n    tmp11 =tmp9 +tmp10 \n    tmp12 =libdevice .rsqrt (tmp11 )\n    tl .store (out_ptr2 +(x0 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_native_group_norm_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =16000 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =((xindex //100 )%10 )\n    x1 =((xindex //10 )%10 )\n    x0 =(xindex %10 )\n    x3 =xindex //1000 \n    x8 =xindex \n    tmp0 =(-1 )+x2 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],8 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =(-1 )+x0 \n    tmp9 =tmp8 >=tmp1 \n    tmp10 =tmp8 <tmp3 \n    tmp11 =tmp2 &tmp4 \n    tmp12 =tmp11 &tmp6 \n    tmp13 =tmp12 &tmp7 \n    tmp14 =tmp13 &tmp9 \n    tmp15 =tmp14 &tmp10 \n    tmp16 =tl .load (in_ptr0 +((-73 )+x0 +8 *x1 +64 *x2 +512 *x3 ),tmp15 &xmask ,other =0.0 )\n    tmp17 =tl .load (in_ptr1 +(x3 //4 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp18 =tmp16 -tmp17 \n    tmp19 =tl .load (in_ptr2 +(x3 //4 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =2048.0 \n    tmp21 =tmp19 /tmp20 \n    tmp22 =1e-05 \n    tmp23 =tmp21 +tmp22 \n    tmp24 =libdevice .rsqrt (tmp23 )\n    tmp25 =tmp18 *tmp24 \n    tmp26 =tl .load (in_ptr3 +(x3 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp27 =tmp25 *tmp26 \n    tmp28 =tl .load (in_ptr4 +(x3 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp29 =tmp27 +tmp28 \n    tmp30 =tl .full (tmp29 .shape ,0.0 ,tmp29 .dtype )\n    tmp31 =tl .where (tmp15 ,tmp29 ,tmp30 )\n    tl .store (out_ptr0 +(x8 ),tmp31 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_6 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =48 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_native_group_norm_7 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +256 *x0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[R0_BLOCK ])\n    tmp3 =tl .broadcast_to (tmp1 ,[R0_BLOCK ])\n    tmp5 =triton_helpers .promote_to_tensor (tl .sum (tmp3 ,0 ))\n    tmp6 =tl .full ([1 ],256 ,tl .int32 )\n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp5 /tmp7 \n    tmp9 =tmp1 -tmp8 \n    tmp10 =tmp9 *tmp9 \n    tmp11 =tl .broadcast_to (tmp10 ,[R0_BLOCK ])\n    tmp13 =triton_helpers .promote_to_tensor (tl .sum (tmp11 ,0 ))\n    tmp14 =256.0 \n    tmp15 =tmp13 /tmp14 \n    tmp16 =1e-05 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =libdevice .rsqrt (tmp17 )\n    tl .store (out_ptr2 +(x0 ),tmp18 ,None )\n    tl .store (out_ptr0 +(x0 ),tmp8 ,None )\n    tl .store (out_ptr1 +(x0 ),tmp13 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_native_group_norm_8 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3456 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =((xindex //36 )%6 )\n    x1 =((xindex //6 )%6 )\n    x0 =(xindex %6 )\n    x3 =xindex //216 \n    x8 =xindex \n    tmp0 =(-1 )+x2 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],4 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =(-1 )+x0 \n    tmp9 =tmp8 >=tmp1 \n    tmp10 =tmp8 <tmp3 \n    tmp11 =tmp2 &tmp4 \n    tmp12 =tmp11 &tmp6 \n    tmp13 =tmp12 &tmp7 \n    tmp14 =tmp13 &tmp9 \n    tmp15 =tmp14 &tmp10 \n    tmp16 =tl .load (in_ptr0 +((-21 )+x0 +4 *x1 +16 *x2 +64 *x3 ),tmp15 &xmask ,other =0.0 )\n    tmp17 =tl .load (in_ptr1 +(x3 //4 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp18 =tmp16 -tmp17 \n    tmp19 =tl .load (in_ptr2 +(x3 //4 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =256.0 \n    tmp21 =tmp19 /tmp20 \n    tmp22 =1e-05 \n    tmp23 =tmp21 +tmp22 \n    tmp24 =libdevice .rsqrt (tmp23 )\n    tmp25 =tmp18 *tmp24 \n    tmp26 =tl .load (in_ptr3 +(x3 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp27 =tmp25 *tmp26 \n    tmp28 =tl .load (in_ptr4 +(x3 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp29 =tmp27 +tmp28 \n    tmp30 =tl .full (tmp29 .shape ,0.0 ,tmp29 .dtype )\n    tmp31 =tl .where (tmp15 ,tmp29 ,tmp30 )\n    tl .store (out_ptr0 +(x8 ),tmp31 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(16 ,),(1 ,))\n    assert_size_stride (primals_2 ,(16 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,16 ,32 ,32 ,32 ),(524288 ,32768 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_4 ,(16 ,),(1 ,))\n    assert_size_stride (primals_5 ,(16 ,),(1 ,))\n    assert_size_stride (primals_6 ,(16 ,),(1 ,))\n    assert_size_stride (primals_7 ,(16 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,4 ,1 ,1 ,16 ),(64 ,16 ,64 ,64 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,4 ,1 ,1 ,16 ),(64 ,16 ,64 ,64 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,4 ,1 ,1 ,16 ),(64 ,16 ,64 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused_native_group_norm_0 [grid (64 )](primals_3 ,buf0 ,buf1 ,buf2 ,64 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,4 ,4 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,4 ,4 ),torch .float32 )\n        buf30 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,4 ,4 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_native_group_norm_1 [grid (4 )](buf0 ,buf1 ,buf2 ,buf3 ,buf4 ,buf30 ,4 ,16 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n        del buf1 \n        del buf2 \n        buf6 =empty_strided_cuda ((1 ,16 ,34 ,34 ,34 ),(644096 ,40256 ,1184 ,34 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_native_group_norm_2 [grid (628864 )](primals_3 ,buf3 ,buf4 ,primals_1 ,primals_2 ,buf6 ,628864 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del primals_1 \n        del primals_2 \n        buf7 =empty_strided_cuda ((3 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[3 ],out =buf7 )\n        buf8 =empty_strided_cuda ((1 ,16 ,3 ),(48 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_3 [grid (48 )](buf7 ,buf8 ,2 ,48 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n\n        buf9 =torch .ops .aten .fractional_max_pool3d .default (buf6 ,[2 ,2 ,2 ],[8 ,8 ,8 ],buf8 )\n        buf10 =buf9 [0 ]\n        buf11 =buf9 [1 ]\n        del buf9 \n        buf12 =buf4 ;del buf4 \n        buf13 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,4 ,4 ),torch .float32 )\n        buf15 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,4 ,4 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused_native_group_norm_4 [grid (4 )](buf10 ,buf12 ,buf13 ,buf15 ,4 ,2048 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf16 =empty_strided_cuda ((1 ,16 ,10 ,10 ,10 ),(16000 ,1000 ,100 ,10 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_native_group_norm_5 [grid (16000 )](buf10 ,buf12 ,buf13 ,primals_4 ,primals_5 ,buf16 ,16000 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_5 \n        buf17 =buf8 ;del buf8 \n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_6 [grid (48 )](buf7 ,buf17 ,1 ,48 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n\n        buf18 =torch .ops .aten .fractional_max_pool3d .default (buf16 ,[2 ,2 ,2 ],[4 ,4 ,4 ],buf17 )\n        buf19 =buf18 [0 ]\n        buf20 =buf18 [1 ]\n        del buf18 \n        buf21 =buf13 ;del buf13 \n        buf22 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,4 ,4 ),torch .float32 )\n        buf24 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,4 ,4 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_native_group_norm_7 [grid (4 )](buf19 ,buf21 ,buf22 ,buf24 ,4 ,256 ,num_warps =2 ,num_stages =1 )\n        buf25 =empty_strided_cuda ((1 ,16 ,6 ,6 ,6 ),(3456 ,216 ,36 ,6 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_native_group_norm_8 [grid (3456 )](buf19 ,buf21 ,buf22 ,primals_6 ,primals_7 ,buf25 ,3456 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf22 \n        del primals_7 \n        buf26 =buf17 ;del buf17 \n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_3 [grid (48 )](buf7 ,buf26 ,2 ,48 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del buf7 \n\n        buf27 =torch .ops .aten .fractional_max_pool3d .default (buf25 ,[2 ,2 ,2 ],[2 ,2 ,2 ],buf26 )\n        del buf26 \n        buf28 =buf27 [0 ]\n        buf29 =buf27 [1 ]\n        del buf27 \n    return (buf28 ,primals_3 ,primals_4 ,primals_6 ,buf6 ,buf10 ,buf11 ,reinterpret_tensor (buf12 ,(1 ,4 ),(4 ,1 ),0 ),reinterpret_tensor (buf15 ,(1 ,4 ),(4 ,1 ),0 ),buf16 ,buf19 ,buf20 ,reinterpret_tensor (buf21 ,(1 ,4 ),(4 ,1 ),0 ),reinterpret_tensor (buf24 ,(1 ,4 ),(4 ,1 ),0 ),buf25 ,buf29 ,reinterpret_tensor (buf3 ,(1 ,4 ,1 ),(4 ,1 ,1 ),0 ),reinterpret_tensor (buf30 ,(1 ,4 ,1 ),(4 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,16 ,32 ,32 ,32 ),(524288 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "1db04b39-4079-432c-b425-27aa04114d9e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardswish', 'HuberLoss', 'Sigmoid', 'ReplicationPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardswish = nn.Hardswish()\n        self.replication_pad3d = nn.ReplicationPad3d(1)\n        self.sigmoid = nn.Sigmoid()\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Apply ReplicationPad3d to the input\n        x = self.replication_pad3d(x)\n        \n        # Apply Hardswish activation\n        x = self.hardswish(x)\n        \n        # Apply Sigmoid activation\n        x = self.sigmoid(x)\n        \n        # Reshape the tensor to match the expected input shape for HuberLoss\n        # Assuming the target tensor is of the same shape as the input\n        target = torch.zeros_like(x)\n        \n        # Compute HuberLoss\n        loss = self.huber_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape for ReplicationPad3d\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_hardswish_huber_loss_replication_pad3d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =15 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp26 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 )\n        tmp1 =8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(ks3 *(((-1 )+ks2 )*(((-1 )+ks2 )<=(((0 )*((0 )>=((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(2 +ks3 ))%(2 +ks2 )))))+((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(2 +ks3 ))%(2 +ks2 ))))*(((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(2 +ks3 ))%(2 +ks2 ))))>(0 )))))+(((0 )*((0 )>=((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(2 +ks3 ))%(2 +ks2 )))))+((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(2 +ks3 ))%(2 +ks2 ))))*(((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(2 +ks3 ))%(2 +ks2 ))))>(0 ))))*((((0 )*((0 )>=((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(2 +ks3 ))%(2 +ks2 )))))+((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(2 +ks3 ))%(2 +ks2 ))))*(((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(2 +ks3 ))%(2 +ks2 ))))>(0 ))))<((-1 )+ks2 )))+ks2 *ks3 *(((-1 )+ks1 )*(((-1 )+ks1 )<=(((0 )*((0 )>=((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 )))))+((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 ))))*(((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 ))))>(0 )))))+(((0 )*((0 )>=((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 )))))+((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 ))))*(((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 ))))>(0 ))))*((((0 )*((0 )>=((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 )))))+((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 ))))*(((-1 )+((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 ))))>(0 ))))<((-1 )+ks1 )))+ks1 *ks2 *ks3 *((((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))//(8 +4 *ks1 +4 *ks2 +4 *ks3 +2 *ks1 *ks2 +2 *ks1 *ks3 +2 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+(((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))%(2 +ks3 )))))+((-1 )+(((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))%(2 +ks3 ))))*(((-1 )+(((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))%(2 +ks3 ))))>(0 )))))+(((0 )*((0 )>=((-1 )+(((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))%(2 +ks3 )))))+((-1 )+(((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))%(2 +ks3 ))))*(((-1 )+(((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))%(2 +ks3 ))))>(0 ))))*((((0 )*((0 )>=((-1 )+(((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))%(2 +ks3 )))))+((-1 )+(((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))%(2 +ks3 ))))*(((-1 )+(((r0_1 +x0 *((14 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//15 ))%(2 +ks3 ))))>(0 ))))<((-1 )+ks3 )))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =3.0 \n        tmp5 =tmp3 +tmp4 \n        tmp6 =0.0 \n        tmp7 =triton_helpers .maximum (tmp5 ,tmp6 )\n        tmp8 =6.0 \n        tmp9 =triton_helpers .minimum (tmp7 ,tmp8 )\n        tmp10 =tmp3 *tmp9 \n        tmp11 =0.16666666666666666 \n        tmp12 =tmp10 *tmp11 \n        tmp13 =tl .sigmoid (tmp12 )\n        tmp14 =tl_math .abs (tmp13 )\n        tmp15 =1.0 \n        tmp16 =tmp14 <tmp15 \n        tmp17 =0.5 \n        tmp18 =tmp14 *tmp17 \n        tmp19 =tmp18 *tmp14 \n        tmp20 =tmp14 -tmp17 \n        tmp21 =tmp20 *tmp15 \n        tmp22 =tl .where (tmp16 ,tmp19 ,tmp21 )\n        tmp23 =tl .full (tmp22 .shape ,0 ,tmp22 .dtype )\n        tmp24 =tl .where (tmp2 ,tmp22 ,tmp23 )\n        tmp25 =tl .broadcast_to (tmp24 ,[XBLOCK ,R0_BLOCK ])\n        tmp27 =_tmp26 +tmp25 \n        _tmp26 =tl .where (r0_mask &xmask ,tmp27 ,_tmp26 )\n    tmp26 =tl .sum (_tmp26 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp26 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_hardswish_huber_loss_replication_pad3d_1 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =15 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tmp4 /tmp6 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp7 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((15 ,),(1 ,),torch .float32 )\n\n        (14 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 )//15 \n        get_raw_stream (0 )\n        triton_red_fused_hardswish_huber_loss_replication_pad3d_0 [grid (15 )](arg4_1 ,buf0 ,3 ,32 ,32 ,32 ,15 ,7861 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg4_1 \n        buf1 =empty_strided_cuda ((),(),torch .float32 )\n        buf2 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_per_fused_hardswish_huber_loss_replication_pad3d_1 [grid (1 )](buf2 ,buf0 ,3 ,32 ,32 ,32 ,1 ,15 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "1ef7158e-b065-4b87-b139-61801895ff9a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CELU', 'TripletMarginWithDistanceLoss', 'Hardshrink', 'Hardswish', 'LeakyReLU', 'LSTM']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.celu = nn.CELU()\n        self.hardshrink = nn.Hardshrink()\n        self.hardswish = nn.Hardswish()\n        self.leaky_relu = nn.LeakyReLU()\n        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n        self.triplet_loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, sequence_length, feature_size)\n        batch_size, sequence_length, feature_size = x.shape\n        \n        # Apply CELU activation\n        x = self.celu(x)\n        \n        # Reshape for LSTM\n        x = x.view(batch_size, sequence_length, -1)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Apply Hardswish activation\n        x = self.hardswish(x)\n        \n        # Apply Hardshrink activation\n        x = self.hardshrink(x)\n        \n        # Apply LeakyReLU activation\n        x = self.leaky_relu(x)\n        \n        # Reshape for triplet loss\n        x = x.view(batch_size, -1)\n        \n        # Generate anchor, positive, and negative samples for triplet loss\n        anchor = x[:batch_size//2]\n        positive = x[batch_size//2:]\n        negative = torch.roll(x, shifts=1, dims=0)[:batch_size//2]\n        \n        # Compute triplet loss\n        loss = self.triplet_loss(anchor, positive, negative)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(32, 10, 128).cuda()  # (batch_size, sequence_length, feature_size)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_celu_view_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =libdevice .expm1 (tmp0 )\n    tmp4 =tl .where (tmp2 ,tmp0 ,tmp3 )\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(s0 ,s1 ,s2 ),(s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((s0 ,s1 ,s2 ),(s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_celu_view_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_celu_view_0 [grid (triton_poi_fused_celu_view_0_xnumel )](arg3_1 ,buf0 ,40960 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,s0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =32 \n    arg1_1 =10 \n    arg2_1 =128 \n    arg3_1 =rand_strided ((32 ,10 ,128 ),(1280 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "1f558a64-2c08-4f0b-874f-254327c0881e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxUnpool2d', 'HingeEmbeddingLoss', 'NLLLoss2d', 'Hardshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.hardshrink = nn.Hardshrink()\n        self.nll_loss_2d = nn.NLLLoss2d()\n        self.hinge_embedding_loss = nn.HingeEmbeddingLoss()\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        \n        # Apply Hardshrink\n        x = self.hardshrink(x)\n        \n        # Apply MaxUnpool2d (requires indices from a previous MaxPool2d operation)\n        # Since we don't have a MaxPool2d in the module list, we'll simulate it\n        pool_output, indices = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool(pool_output, indices, output_size=(height, width))\n        \n        # Apply NLLLoss2d (requires a target tensor)\n        # Since NLLLoss2d is a loss function, we'll simulate a target tensor\n        target = torch.randint(0, channels, (batch_size, height, width), device=x.device)\n        x = self.nll_loss_2d(F.log_softmax(x, dim=1), target)\n        \n        # Apply HingeEmbeddingLoss (requires a target tensor)\n        # Since HingeEmbeddingLoss is a loss function, we'll simulate a target tensor\n        target_hinge = torch.ones(batch_size, device=x.device)\n        x = self.hinge_embedding_loss(x, target_hinge)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_le_max_pool2d_with_indices_max_unpool2d_scalar_tensor_where_1 (in_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp6 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp23 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp49 =tl .load (in_ptr0 +(2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp53 =tl .load (in_ptr0 +(1 +2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp58 =tl .load (in_ptr0 +(ks4 +2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp63 =tl .load (in_ptr0 +(1 +ks4 +2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =0.5 \n    tmp3 =tmp1 <=tmp2 \n    tmp4 =0.0 \n    tmp5 =tl .where (tmp3 ,tmp4 ,tmp0 )\n    tmp7 =tl_math .abs (tmp6 )\n    tmp8 =tmp7 <=tmp2 \n    tmp9 =tl .where (tmp8 ,tmp4 ,tmp6 )\n    tmp10 =tmp9 >tmp5 \n    tmp11 =tl .full ([1 ],1 ,tl .int8 )\n    tmp12 =tl .full ([1 ],0 ,tl .int8 )\n    tmp13 =tl .where (tmp10 ,tmp11 ,tmp12 )\n    tmp14 =triton_helpers .maximum (tmp9 ,tmp5 )\n    tmp16 =tl_math .abs (tmp15 )\n    tmp17 =tmp16 <=tmp2 \n    tmp18 =tl .where (tmp17 ,tmp4 ,tmp15 )\n    tmp19 =tmp18 >tmp14 \n    tmp20 =tl .full ([1 ],2 ,tl .int8 )\n    tmp21 =tl .where (tmp19 ,tmp20 ,tmp13 )\n    tmp22 =triton_helpers .maximum (tmp18 ,tmp14 )\n    tmp24 =tl_math .abs (tmp23 )\n    tmp25 =tmp24 <=tmp2 \n    tmp26 =tl .where (tmp25 ,tmp4 ,tmp23 )\n    tmp27 =tmp26 >tmp22 \n    tmp28 =tl .full ([1 ],3 ,tl .int8 )\n    tmp29 =tl .where (tmp27 ,tmp28 ,tmp21 )\n    triton_helpers .maximum (tmp26 ,tmp22 )\n    tmp31 =tl .full ([1 ],2 ,tl .int32 )\n    tmp32 =tl .where ((tmp29 <0 )!=(tmp31 <0 ),tl .where (tmp29 %tmp31 !=0 ,tmp29 //tmp31 -1 ,tmp29 //tmp31 ),tmp29 //tmp31 )\n    tmp33 =tmp32 *tmp31 \n    tmp34 =tmp29 -tmp33 \n    tmp35 =2 *(((x3 //ks0 )%ks1 ))\n    tmp36 =tmp35 +tmp32 \n    tmp37 =2 *((x3 %ks0 ))\n    tmp38 =tmp37 +tmp34 \n    tmp39 =ks4 \n    tmp40 =tmp36 *tmp39 \n    tmp41 =tmp40 +tmp38 \n    tmp42 =ks3 *ks4 *(x3 //ks2 )\n    tmp43 =tmp41 +tmp42 \n    tmp44 =ks3 *ks4 *ks5 \n    tmp45 =tmp43 +tmp44 \n    tmp46 =tmp43 <0 \n    tmp47 =tl .where (tmp46 ,tmp45 ,tmp43 )\n    tl .device_assert (((0 <=tmp47 )&(tmp47 <ks3 *ks4 *ks5 ))|~(xmask ),\"index out of bounds: 0 <= tmp47 < ks3*ks4*ks5\")\n    tmp50 =tl_math .abs (tmp49 )\n    tmp51 =tmp50 <=tmp2 \n    tmp52 =tl .where (tmp51 ,tmp4 ,tmp49 )\n    tmp54 =tl_math .abs (tmp53 )\n    tmp55 =tmp54 <=tmp2 \n    tmp56 =tl .where (tmp55 ,tmp4 ,tmp53 )\n    tmp57 =triton_helpers .maximum (tmp56 ,tmp52 )\n    tmp59 =tl_math .abs (tmp58 )\n    tmp60 =tmp59 <=tmp2 \n    tmp61 =tl .where (tmp60 ,tmp4 ,tmp58 )\n    tmp62 =triton_helpers .maximum (tmp61 ,tmp57 )\n    tmp64 =tl_math .abs (tmp63 )\n    tmp65 =tmp64 <=tmp2 \n    tmp66 =tl .where (tmp65 ,tmp4 ,tmp63 )\n    tmp67 =triton_helpers .maximum (tmp66 ,tmp62 )\n    tl .store (out_ptr1 +(tl .broadcast_to ((tmp47 %(ks3 *ks4 *ks5 )),[XBLOCK ])),tmp67 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax_2 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =tl .load (in_ptr0 +(x0 +ks0 *x1 +ks0 *ks2 *((((x0 +ks0 *x1 +ks0 *ks2 *r0_2 )//(ks0 *ks2 ))%ks1 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =triton_helpers .maximum (_tmp2 ,tmp1 )\n        _tmp2 =tl .where (r0_mask &xmask ,tmp3 ,_tmp2 )\n    tmp2 =triton_helpers .max2 (_tmp2 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp4 =tl .load (in_ptr0 +(x0 +ks0 *x1 +ks0 *ks2 *((((x0 +ks0 *x1 +ks0 *ks2 *r0_2 )//(ks0 *ks2 ))%ks1 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tmp4 -tmp2 \n        tmp6 =tl_math .exp (tmp5 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =_tmp8 +tmp7 \n        _tmp8 =tl .where (r0_mask &xmask ,tmp9 ,_tmp8 )\n    tmp8 =tl .sum (_tmp8 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x3 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_fill_mean_ne_nll_loss2d_forward_randint_sub_where_zeros_like_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,load_seed_offset ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp22 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp26 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int64 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        r0_0 =(r0_index %ks2 )\n        r0_1 =r0_index //ks2 \n        tmp13 =tl .load (in_ptr2 +(r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp15 =tl .load (in_ptr3 +(r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =r0_2 \n        tmp2 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp3 =ks1 \n        tmp4 =triton_helpers .randint64 (tmp0 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp3 )\n        tmp5 =tl .full ([1 ,1 ],-100 ,tl .int64 )\n        tmp6 =tmp4 !=tmp5 \n        tmp7 =tl .where (tmp6 ,tmp4 ,tmp2 )\n        tmp8 =tmp7 +tmp3 \n        tmp9 =tmp7 <0 \n        tmp10 =tl .where (tmp9 ,tmp8 ,tmp7 )\n        tl .device_assert (((0 <=tmp10 )&(tmp10 <ks1 ))|~(r0_mask ),\"index out of bounds: 0 <= tmp10 < ks1\")\n        tmp12 =tl .load (in_ptr1 +(r0_0 +ks2 *r0_1 +ks2 *ks3 *((((r0_0 +ks2 *r0_1 +ks2 *ks3 *tmp10 )//(ks2 *ks3 ))%ks1 ))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp14 =tmp12 -tmp13 \n        tmp16 =tl_math .log (tmp15 )\n        tmp17 =tmp14 -tmp16 \n        tmp18 =-tmp17 \n        tmp19 =0.0 \n        tmp20 =tl .where (tmp6 ,tmp18 ,tmp19 )\n        tmp21 =tl .broadcast_to (tmp20 ,[XBLOCK ,R0_BLOCK ])\n        tmp23 =_tmp22 +tmp21 \n        _tmp22 =tl .where (r0_mask ,tmp23 ,_tmp22 )\n        tmp24 =tmp6 .to (tl .int64 )\n        tmp25 =tl .broadcast_to (tmp24 ,[XBLOCK ,R0_BLOCK ])\n        tmp27 =_tmp26 +tmp25 \n        _tmp26 =tl .where (r0_mask ,tmp27 ,_tmp26 )\n    tmp22 =tl .sum (_tmp22 ,1 )[:,None ]\n    tmp26 =tl .sum (_tmp26 ,1 )[:,None ]\n    tmp28 =tmp26 .to (tl .float32 )\n    tmp29 =tmp22 /tmp28 \n    tmp30 =1.0 \n    tmp31 =tmp30 -tmp29 \n    tmp32 =0.0 \n    tmp33 =triton_helpers .maximum (tmp31 ,tmp32 )\n    tmp34 =tl .full ([1 ,1 ],False ,tl .int1 )\n    tmp35 =tl .where (tmp34 ,tmp33 ,tmp32 )\n    tmp36 =tl .full ([1 ,1 ],True ,tl .int1 )\n    tmp37 =tl .where (tmp36 ,tmp29 ,tmp32 )\n    tmp38 =tmp35 +tmp37 \n    tmp39 =tmp38 /tmp30 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp39 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_0 [grid (triton_poi_fused_max_unpool2d_0_xnumel )](buf2 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        s2 //2 \n        s1 //2 \n        (s1 //2 )*(s2 //2 )\n\n        triton_poi_fused_abs_le_max_pool2d_with_indices_max_unpool2d_scalar_tensor_where_1_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_le_max_pool2d_with_indices_max_unpool2d_scalar_tensor_where_1 [grid (triton_poi_fused_abs_le_max_pool2d_with_indices_max_unpool2d_scalar_tensor_where_1_xnumel )](arg3_1 ,buf2 ,32 ,32 ,1024 ,64 ,64 ,3 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf1 )\n        buf4 =empty_strided_cuda ((1 ,1 ,s1 ,s2 ),(s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,1 ,s1 ,s2 ),(s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_red_fused__log_softmax_2_xnumel =s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__log_softmax_2 [grid (triton_red_fused__log_softmax_2_xnumel )](buf2 ,buf4 ,buf5 ,64 ,3 ,64 ,4096 ,3 ,XBLOCK =64 ,R0_BLOCK =4 ,num_warps =2 ,num_stages =1 )\n        buf6 =empty_strided_cuda ((),(),torch .float32 )\n        buf8 =buf6 ;del buf6 \n\n        s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_fill_mean_ne_nll_loss2d_forward_randint_sub_where_zeros_like_3 [grid (1 )](buf8 ,buf1 ,buf2 ,buf4 ,buf5 ,0 ,3 ,64 ,64 ,1 ,4096 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf1 \n        del buf2 \n        del buf4 \n        del buf5 \n    return (buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "1f566d3a-1401-4bb8-abc7-060e3304842a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CTCLoss', 'AvgPool2d', 'CrossMapLRN2d', 'ConvTranspose3d', 'InstanceNorm3d', 'BatchNorm1d', 'FeatureAlphaDropout', 'Softmin', 'AdaptiveAvgPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.avg_pool2d = nn.AvgPool2d(kernel_size=2)\n        self.cross_map_lrn2d = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.conv_transpose3d = nn.ConvTranspose3d(1, 10, kernel_size=3, stride=2, padding=1)\n        self.instance_norm3d = nn.InstanceNorm3d(10)\n        self.batch_norm1d = nn.BatchNorm1d(100)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.softmin = nn.Softmin(dim=1)\n        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=10)\n        self.ctc_loss = nn.CTCLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        x = self.avg_pool2d(x)  # Reduce spatial dimensions\n        x = self.cross_map_lrn2d(x)  # Apply local response normalization\n        \n        # Reshape for 3D operations\n        x = x.unsqueeze(1)  # Add a dummy dimension for 3D convolution\n        x = self.conv_transpose3d(x)  # Apply 3D transposed convolution\n        x = self.instance_norm3d(x)  # Apply instance normalization\n        \n        # Reshape for 1D operations\n        x = x.view(x.size(0), -1)  # Flatten to (batch_size, features)\n        x = self.batch_norm1d(x)  # Apply batch normalization\n        x = self.feature_alpha_dropout(x)  # Apply feature alpha dropout\n        \n        # Reshape for adaptive pooling\n        x = x.unsqueeze(1)  # Add a dummy dimension for 1D pooling\n        x = self.adaptive_avg_pool1d(x)  # Apply adaptive average pooling\n        x = self.softmin(x)  # Apply softmin\n        \n        # Compute CTC loss (assuming targets and input lengths are provided)\n        # For demonstration, we'll return the output before CTC loss\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tl .store (out_ptr0 +(x3 ),tmp8 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //2 \n        s1 //2 \n        (s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_0_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_0 [grid (triton_poi_fused_avg_pool2d_0_xnumel )](arg3_1 ,buf0 ,32 ,32 ,1024 ,64 ,64 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "215bb4b8-b450-4234-8e02-473b20be300c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Unflatten', 'SoftMarginLoss', 'ReLU6', 'LocalResponseNorm', 'LazyConv2d', 'UpsamplingBilinear2d', 'CrossMapLRN2d', 'RMSNorm', 'AdaptiveAvgPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.unflatten = nn.Unflatten(1, (1, 28, 28))  # Assuming input is flattened\n        self.lazy_conv2d = nn.LazyConv2d(out_channels=16, kernel_size=3, stride=1)\n        self.relu6 = nn.ReLU6()\n        self.local_response_norm = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=1.0)\n        self.upsampling_bilinear2d = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.cross_map_lrn2d = nn.CrossMapLRN2d(size=5, alpha=0.0001, beta=0.75, k=1.0)\n        self.rms_norm = nn.RMSNorm(16)\n        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=10)\n        self.soft_margin_loss = nn.SoftMarginLoss()\n\n    def forward(self, x):\n        # Unflatten the input\n        x = self.unflatten(x)\n        \n        # Apply LazyConv2d\n        x = self.lazy_conv2d(x)\n        \n        # Apply ReLU6\n        x = self.relu6(x)\n        \n        # Apply LocalResponseNorm\n        x = self.local_response_norm(x)\n        \n        # Apply UpsamplingBilinear2d\n        x = self.upsampling_bilinear2d(x)\n        \n        # Apply CrossMapLRN2d\n        x = self.cross_map_lrn2d(x)\n        \n        # Reshape for RMSNorm\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions\n        x = self.rms_norm(x)\n        \n        # Reshape for AdaptiveAvgPool1d\n        x = x.view(x.size(0), x.size(1), -1)  # Ensure correct shape for pooling\n        x = self.adaptive_avg_pool1d(x)\n        \n        # Flatten for SoftMarginLoss\n        x = x.view(x.size(0), -1)\n        \n        # Dummy target for SoftMarginLoss (assuming binary classification)\n        target = torch.ones_like(x)\n        \n        # Apply SoftMarginLoss\n        loss = self.soft_margin_loss(x, target)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 784).cuda()  # Assuming input is flattened (e.g., 28x28 image)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10816 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //676 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =13520 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //676 \n    x2 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],16 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-1352 )+x2 ),tmp5 &xmask ,other =0.0 )\n    tmp7 =0.0 \n    tmp8 =triton_helpers .maximum (tmp6 ,tmp7 )\n    tmp9 =6.0 \n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tmp11 =tmp10 *tmp10 \n    tmp12 =tl .full (tmp11 .shape ,0.0 ,tmp11 .dtype )\n    tmp13 =tl .where (tmp5 ,tmp11 ,tmp12 )\n    tl .store (out_ptr0 +(x2 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_avg_pool3d_div_hardtanh_mul_pow_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10816 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(676 +x0 ),xmask )\n    tmp3 =tl .load (in_ptr0 +(1352 +x0 ),xmask )\n    tmp5 =tl .load (in_ptr0 +(2028 +x0 ),xmask )\n    tmp7 =tl .load (in_ptr0 +(2704 +x0 ),xmask )\n    tmp11 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp8 =tmp7 +tmp6 \n    tmp9 =0.2 \n    tmp10 =tmp8 *tmp9 \n    tmp12 =0.0 \n    tmp13 =triton_helpers .maximum (tmp11 ,tmp12 )\n    tmp14 =6.0 \n    tmp15 =triton_helpers .minimum (tmp13 ,tmp14 )\n    tmp16 =0.0001 \n    tmp17 =tmp10 *tmp16 \n    tmp18 =1.0 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =0.75 \n    tmp21 =libdevice .pow (tmp19 ,tmp20 )\n    tmp22 =tmp15 /tmp21 \n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp22 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_3 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =52 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =x0 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.49019607843137253 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tl .store (out_ptr0 +(x0 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_clamp_4 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =52 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =x0 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.49019607843137253 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],25 ,tl .int64 )\n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_arange_clamp_mul_sub_5 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =52 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =x0 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.49019607843137253 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp5 -tmp7 \n    tmp9 =triton_helpers .maximum (tmp8 ,tmp4 )\n    tmp10 =1.0 \n    tmp11 =triton_helpers .minimum (tmp9 ,tmp10 )\n    tl .store (out_ptr0 +(x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__unsafe_index_add_mul_sub_6 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,in_ptr6 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =43264 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //52 )%52 )\n    x0 =(xindex %52 )\n    x2 =xindex //2704 \n    (xindex %2704 )\n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp16 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp19 =tl .load (in_ptr5 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr6 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([XBLOCK ],26 ,tl .int32 )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tmp0 <0 \n    tmp4 =tl .where (tmp3 ,tmp2 ,tmp0 )\n    tmp6 =tmp5 +tmp1 \n    tmp7 =tmp5 <0 \n    tmp8 =tl .where (tmp7 ,tmp6 ,tmp5 )\n    tmp9 =tl .load (in_ptr2 +(tmp8 +26 *tmp4 +676 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tmp10 +tmp1 \n    tmp12 =tmp10 <0 \n    tmp13 =tl .where (tmp12 ,tmp11 ,tmp10 )\n    tmp14 =tl .load (in_ptr2 +(tmp13 +26 *tmp4 +676 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tmp14 -tmp9 \n    tmp17 =tmp15 *tmp16 \n    tmp18 =tmp9 +tmp17 \n    tmp20 =tmp19 +tmp1 \n    tmp21 =tmp19 <0 \n    tmp22 =tl .where (tmp21 ,tmp20 ,tmp19 )\n    tmp23 =tl .load (in_ptr2 +(tmp8 +26 *tmp22 +676 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp24 =tl .load (in_ptr2 +(tmp13 +26 *tmp22 +676 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp25 =tmp24 -tmp23 \n    tmp26 =tmp25 *tmp16 \n    tmp27 =tmp23 +tmp26 \n    tmp28 =tmp27 -tmp18 \n    tmp30 =tmp28 *tmp29 \n    tmp31 =tmp18 +tmp30 \n    tl .store (out_ptr1 +(x4 ),tmp31 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,784 ),(784 ,1 ))\n    assert_size_stride (primals_2 ,(16 ,1 ,3 ,3 ),(9 ,9 ,3 ,1 ))\n    assert_size_stride (primals_3 ,(16 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (reinterpret_tensor (primals_1 ,(1 ,1 ,28 ,28 ),(784 ,784 ,28 ,1 ),0 ),primals_2 ,stride =(1 ,1 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =False ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,16 ,26 ,26 ),(10816 ,676 ,26 ,1 ))\n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_0 [grid (10816 )](buf1 ,primals_3 ,10816 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_3 \n        buf2 =empty_strided_cuda ((1 ,1 ,20 ,26 ,26 ),(13536 ,13536 ,676 ,26 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_1 [grid (13520 )](buf1 ,buf2 ,13520 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,1 ,16 ,26 ,26 ),(10816 ,10816 ,676 ,26 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,16 ,26 ,26 ),(10816 ,676 ,26 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_avg_pool3d_div_hardtanh_mul_pow_2 [grid (10816 )](buf2 ,buf1 ,buf3 ,buf4 ,10816 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((52 ,1 ),(1 ,1 ),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_3 [grid (52 )](buf5 ,52 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf6 =empty_strided_cuda ((52 ,1 ),(1 ,1 ),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_clamp_4 [grid (52 )](buf6 ,52 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf7 =empty_strided_cuda ((52 ,),(1 ,),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_3 [grid (52 )](buf7 ,52 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf8 =empty_strided_cuda ((52 ,),(1 ,),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_clamp_4 [grid (52 )](buf8 ,52 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf9 =empty_strided_cuda ((52 ,),(1 ,),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_arange_clamp_mul_sub_5 [grid (52 )](buf9 ,52 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf11 =empty_strided_cuda ((52 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_arange_clamp_mul_sub_5 [grid (52 )](buf11 ,52 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf12 =empty_strided_cuda ((1 ,16 ,52 ,52 ),(43264 ,2704 ,52 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__unsafe_index_add_mul_sub_6 [grid (43264 )](buf5 ,buf7 ,buf4 ,buf8 ,buf9 ,buf6 ,buf11 ,buf12 ,43264 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n    return (buf12 ,primals_2 ,reinterpret_tensor (primals_1 ,(1 ,1 ,28 ,28 ),(784 ,784 ,28 ,1 ),0 ),buf1 ,buf2 ,buf3 ,buf4 ,buf5 ,buf6 ,buf7 ,buf8 ,buf9 ,buf11 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,784 ),(784 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((16 ,1 ,3 ,3 ),(9 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "2228b19d-34df-4d79-aabc-64e00b47822f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveLogSoftmaxWithLoss', 'GRU', 'AvgPool3d', 'PixelUnshuffle', 'ReplicationPad2d', 'NLLLoss', 'TransformerDecoderLayer', 'MarginRankingLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pixel_unshuffle = nn.PixelUnshuffle(downscale_factor=2)\n        self.replication_pad2d = nn.ReplicationPad2d(padding=2)\n        self.avg_pool3d = nn.AvgPool3d(kernel_size=2, stride=2)\n        self.gru = nn.GRU(input_size=128, hidden_size=256, num_layers=2, batch_first=True)\n        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=256, nhead=8)\n        self.transformer_decoder = nn.TransformerDecoder(self.transformer_decoder_layer, num_layers=3)\n        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(in_features=256, n_classes=10, cutoffs=[2, 4])\n        self.nll_loss = nn.NLLLoss()\n        self.margin_ranking_loss = nn.MarginRankingLoss()\n\n    def forward(self, x):\n        # Assume input x is of shape (batch_size, channels, height, width)\n        x = self.pixel_unshuffle(x)  # Shape: (batch_size, channels * 4, height // 2, width // 2)\n        x = self.replication_pad2d(x)  # Shape: (batch_size, channels * 4, height // 2 + 4, width // 2 + 4)\n        \n        # Reshape for AvgPool3d\n        x = x.unsqueeze(1)  # Shape: (batch_size, 1, channels * 4, height // 2 + 4, width // 2 + 4)\n        x = self.avg_pool3d(x)  # Shape: (batch_size, 1, channels * 4, height // 4 + 2, width // 4 + 2)\n        \n        # Reshape for GRU\n        x = x.view(x.size(0), x.size(2), -1)  # Shape: (batch_size, channels * 4, (height // 4 + 2) * (width // 4 + 2))\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, (height // 4 + 2) * (width // 4 + 2), channels * 4)\n        \n        # GRU\n        x, _ = self.gru(x)  # Shape: (batch_size, (height // 4 + 2) * (width // 4 + 2), 256)\n        \n        # TransformerDecoder\n        memory = torch.zeros_like(x)  # Dummy memory for TransformerDecoder\n        x = self.transformer_decoder(x, memory)  # Shape: (batch_size, (height // 4 + 2) * (width // 4 + 2), 256)\n        \n        # AdaptiveLogSoftmaxWithLoss\n        x = x.mean(dim=1)  # Shape: (batch_size, 256)\n        output = self.adaptive_log_softmax.log_prob(x)  # Shape: (batch_size, 10)\n        \n        return output\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool3d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    tmp0 =tl .load (in_ptr0 +(2 *(((-1 )+(ks4 //2 ))*(((-1 )+(ks4 //2 ))<=(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))<((-1 )+(ks4 //2 ))))+2 *(ks4 //2 )*((x1 %2 ))+4 *(ks4 //2 )*(((-1 )+(ks3 //2 ))*(((-1 )+(ks3 //2 ))<=(((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 ))))<((-1 )+(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*(x1 //2 )+(((2 *x1 )%2 ))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(2 *(((-1 )+(ks4 //2 ))*(((-1 )+(ks4 //2 ))<=(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))<((-1 )+(ks4 //2 ))))+2 *(ks4 //2 )*((x1 %2 ))+4 *(ks4 //2 )*(((-1 )+(ks3 //2 ))*(((-1 )+(ks3 //2 ))<=(((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 ))))<((-1 )+(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*(x1 //2 )+(((2 *x1 )%2 ))),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 *(((-1 )+(ks4 //2 ))*(((-1 )+(ks4 //2 ))<=(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))<((-1 )+(ks4 //2 ))))+2 *(ks4 //2 )*((x1 %2 ))+4 *(ks4 //2 )*(((-1 )+(ks3 //2 ))*(((-1 )+(ks3 //2 ))<=(((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 ))))<((-1 )+(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*(x1 //2 )+(((2 *x1 )%2 ))),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(2 *(((-1 )+(ks4 //2 ))*(((-1 )+(ks4 //2 ))<=(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))<((-1 )+(ks4 //2 ))))+2 *(ks4 //2 )*((x1 %2 ))+4 *(ks4 //2 )*(((-1 )+(ks3 //2 ))*(((-1 )+(ks3 //2 ))<=(((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 ))))<((-1 )+(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*(x1 //2 )+(((2 *x1 )%2 ))),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(1 +2 *(((-1 )+(ks4 //2 ))*(((-1 )+(ks4 //2 ))<=(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))<((-1 )+(ks4 //2 ))))+2 *(ks4 //2 )*((((1 +2 *x1 )//2 )%2 ))+4 *(ks4 //2 )*(((-1 )+(ks3 //2 ))*(((-1 )+(ks3 //2 ))<=(((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 ))))<((-1 )+(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*((((1 +2 *x1 )//4 )%ks5 ))),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(1 +2 *(((-1 )+(ks4 //2 ))*(((-1 )+(ks4 //2 ))<=(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))<((-1 )+(ks4 //2 ))))+2 *(ks4 //2 )*((((1 +2 *x1 )//2 )%2 ))+4 *(ks4 //2 )*(((-1 )+(ks3 //2 ))*(((-1 )+(ks3 //2 ))<=(((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x2 ))+((-2 )+2 *x2 )*(((-2 )+2 *x2 )>(0 ))))<((-1 )+(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*((((1 +2 *x1 )//4 )%ks5 ))),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(1 +2 *(((-1 )+(ks4 //2 ))*(((-1 )+(ks4 //2 ))<=(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))<((-1 )+(ks4 //2 ))))+2 *(ks4 //2 )*((((1 +2 *x1 )//2 )%2 ))+4 *(ks4 //2 )*(((-1 )+(ks3 //2 ))*(((-1 )+(ks3 //2 ))<=(((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 ))))<((-1 )+(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*((((1 +2 *x1 )//4 )%ks5 ))),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(1 +2 *(((-1 )+(ks4 //2 ))*(((-1 )+(ks4 //2 ))<=(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))<((-1 )+(ks4 //2 ))))+2 *(ks4 //2 )*((((1 +2 *x1 )//2 )%2 ))+4 *(ks4 //2 )*(((-1 )+(ks3 //2 ))*(((-1 )+(ks3 //2 ))<=(((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x2 ))+((-1 )+2 *x2 )*(((-1 )+2 *x2 )>(0 ))))<((-1 )+(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*((((1 +2 *x1 )//4 )%ks5 ))),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp8 =tmp7 +tmp6 \n    tmp10 =tmp9 +tmp8 \n    tmp12 =tmp11 +tmp10 \n    tmp14 =tmp13 +tmp12 \n    tmp15 =0.125 \n    tmp16 =tmp14 *tmp15 \n    tl .store (out_ptr0 +(x0 +2 *x2 +4 *x1 +x2 *(ks4 //4 )+2 *x1 *(ks3 //4 )+2 *x1 *(ks4 //4 )+x1 *(ks3 //4 )*(ks4 //4 )),tmp16 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +(s2 //4 )\n        2 *s0 \n        4 *s0 +2 *s0 *(s2 //4 )\n        buf0 =empty_strided_cuda ((1 ,1 ,2 *s0 ,2 +(s1 //4 ),2 +(s2 //4 )),(8 *s0 +4 *s0 *(s1 //4 )+4 *s0 *(s2 //4 )+2 *s0 *(s1 //4 )*(s2 //4 ),8 *s0 +4 *s0 *(s1 //4 )+4 *s0 *(s2 //4 )+2 *s0 *(s1 //4 )*(s2 //4 ),4 +2 *(s1 //4 )+2 *(s2 //4 )+(s1 //4 )*(s2 //4 ),2 +(s2 //4 ),1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool3d_0_xnumel =8 *s0 +4 *s0 *(s1 //4 )+4 *s0 *(s2 //4 )+2 *s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool3d_0 [grid (triton_poi_fused_avg_pool3d_0_xnumel )](arg3_1 ,buf0 ,18 ,6 ,108 ,64 ,64 ,3 ,1944 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,(8 *s0 +4 *s0 *(s1 //4 )+4 *s0 *(s2 //4 )+2 *s0 *(s1 //4 )*(s2 //4 ))//((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//2 ),(s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//2 ),(((8 *s0 +4 *s0 *(s1 //4 )+4 *s0 *(s2 //4 )+2 *s0 *(s1 //4 )*(s2 //4 ))//((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//2 ))*((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//2 ),1 ,(8 *s0 +4 *s0 *(s1 //4 )+4 *s0 *(s2 //4 )+2 *s0 *(s1 //4 )*(s2 //4 ))//((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//2 )),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "22f4a2a3-8603-4f5c-957b-0dfb8d63760e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['BCELoss', 'TripletMarginLoss', 'LSTM', 'MaxPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.bce_loss = nn.BCELoss()\n        self.triplet_loss = nn.TripletMarginLoss(margin=1.0)\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        \n        # Apply MaxPool2d to reduce spatial dimensions\n        x = self.maxpool(x)  # Shape: (batch_size, channels, height//2, width//2)\n        \n        # Reshape for LSTM: (batch_size, sequence_length, feature_size)\n        x = x.view(batch_size, -1, 64)  # Assuming 64 features per time step\n        \n        # Pass through LSTM\n        lstm_out, _ = self.lstm(x)  # Shape: (batch_size, sequence_length, hidden_size)\n        \n        # Reshape back to original spatial dimensions (batch_size, channels, height, width)\n        lstm_out = lstm_out.view(batch_size, channels, height//2, width//2)\n        \n        # Apply BCELoss (assuming binary classification)\n        target = torch.randint(0, 2, (batch_size, channels, height//2, width//2)).float().to(x.device)\n        bce_loss = self.bce_loss(lstm_out.sigmoid(), target)\n        \n        # Apply TripletMarginLoss (assuming we have anchor, positive, and negative samples)\n        anchor = lstm_out[:, 0, :, :].unsqueeze(1)  # Anchor sample\n        positive = lstm_out[:, 1, :, :].unsqueeze(1)  # Positive sample\n        negative = lstm_out[:, 2, :, :].unsqueeze(1)  # Negative sample\n        triplet_loss = self.triplet_loss(anchor, positive, negative)\n        \n        # Return both losses (for demonstration purposes)\n        return bce_loss, triplet_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with batch_size=1, channels=3, height=64, width=64\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\n    tl .store (out_ptr0 +(x3 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_view_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %64 )\n    x1 =xindex //64 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(((x0 +64 *x1 )%(ks0 *ks1 *ks2 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //2 \n        s1 //2 \n        (s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_max_pool2d_with_indices_0_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_0 [grid (triton_poi_fused_max_pool2d_with_indices_0_xnumel )](arg3_1 ,buf0 ,32 ,32 ,1024 ,64 ,64 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((1 ,(s0 *(s1 //2 )*(s2 //2 ))//64 ,64 ),(64 *((s0 *(s1 //2 )*(s2 //2 ))//64 ),64 ,1 ),torch .float32 )\n\n        triton_poi_fused_max_pool2d_with_indices_view_1_xnumel =64 *((s0 *(s1 //2 )*(s2 //2 ))//64 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_view_1 [grid (triton_poi_fused_max_pool2d_with_indices_view_1_xnumel )](buf0 ,buf1 ,32 ,32 ,3 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n    return (buf1 ,s0 ,s1 ,s2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "23535807-40b6-417c-bca4-7fa6a41bcaa3",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['SoftMarginLoss', 'UpsamplingNearest2d', 'ReflectionPad3d', 'Module', 'Dropout', 'ReLU6', 'HingeEmbeddingLoss', 'HuberLoss', 'FeatureAlphaDropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample = nn.UpsamplingNearest2d(scale_factor=2)\n        self.pad = nn.ReflectionPad3d(padding=1)\n        self.dropout = nn.Dropout(p=0.5)\n        self.relu6 = nn.ReLU6()\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.soft_margin_loss = nn.SoftMarginLoss()\n        self.hinge_embedding_loss = nn.HingeEmbeddingLoss()\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Upsample the input\n        x = self.upsample(x)\n        \n        # Pad the input\n        x = self.pad(x.unsqueeze(1)).squeeze(1)\n        \n        # Apply Dropout\n        x = self.dropout(x)\n        \n        # Apply ReLU6\n        x = self.relu6(x)\n        \n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Compute SoftMarginLoss (requires a target, so we create a dummy target)\n        target = torch.ones_like(x)\n        soft_margin_loss = self.soft_margin_loss(x, target)\n        \n        # Compute HingeEmbeddingLoss (requires a target, so we create a dummy target)\n        hinge_embedding_loss = self.hinge_embedding_loss(x, target)\n        \n        # Compute HuberLoss (requires a target, so we create a dummy target)\n        huber_loss = self.huber_loss(x, target)\n        \n        # Return the losses as a tuple\n        return soft_margin_loss, hinge_embedding_loss, huber_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_1 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x3 =xindex \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x7 =xindex //ks4 \n    tmp0 =tl .load (in_out_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.5 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =tmp2 .to (tl .float32 )\n    tmp4 =tl .full ([1 ],2.0 ,tl .float64 )\n    tmp5 =ks0 \n    tmp6 =tmp5 .to (tl .float64 )\n    tmp7 =tmp4 *tmp6 \n    tmp8 =tmp6 /tmp7 \n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tl .where ((-1 )+((-1 )*tl_math .abs (1 +((-2 )*ks0 )+tl_math .abs ((-1 )+x1 )))+2 *ks0 <0 ,(-1 )+((-1 )*tl_math .abs (1 +((-2 )*ks0 )+tl_math .abs ((-1 )+x1 )))+4 *ks0 ,(-1 )+((-1 )*tl_math .abs (1 +((-2 )*ks0 )+tl_math .abs ((-1 )+x1 )))+2 *ks0 )\n    tmp11 =tmp10 .to (tl .float32 )\n    tmp12 =tmp11 *tmp9 \n    tmp13 =tmp12 .to (tl .int64 )\n    tmp14 =tmp13 +tmp5 \n    tmp15 =tmp13 <0 \n    tmp16 =tl .where (tmp15 ,tmp14 ,tmp13 )\n    tmp17 =ks3 \n    tmp18 =tmp17 .to (tl .float64 )\n    tmp19 =tmp4 *tmp18 \n    tmp20 =tmp18 /tmp19 \n    tmp21 =tmp20 .to (tl .float32 )\n    tmp22 =tl .where ((-1 )+((-1 )*tl_math .abs (1 +((-2 )*ks3 )+tl_math .abs ((-1 )+x0 )))+2 *ks3 <0 ,(-1 )+((-1 )*tl_math .abs (1 +((-2 )*ks3 )+tl_math .abs ((-1 )+x0 )))+4 *ks3 ,(-1 )+((-1 )*tl_math .abs (1 +((-2 )*ks3 )+tl_math .abs ((-1 )+x0 )))+2 *ks3 )\n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =tmp23 *tmp21 \n    tmp25 =tmp24 .to (tl .int64 )\n    tmp26 =tmp25 +tmp17 \n    tmp27 =tmp25 <0 \n    tmp28 =tl .where (tmp27 ,tmp26 ,tmp25 )\n    tmp29 =tl .load (in_ptr0 +(tmp28 +ks3 *tmp16 +ks0 *ks3 *(tl .where ((-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+x7 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+x7 )))+2 *ks5 ,(-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+x7 )))))),xmask ,eviction_policy ='evict_last')\n    tmp30 =tmp3 *tmp29 \n    tl .store (in_out_ptr0 +(x3 ),tmp30 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__to_copy_add_bernoulli_clamp_min_fill_hardtanh_mean_mul_native_dropout_ne_soft_margin_loss_sub_where_zeros_like_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp30 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp43 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 )\n        tmp1 =8 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(2 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks3 )%ks4 ))+4 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks5 )%(2 +ks0 )))+2 *ks2 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks3 )%ks4 ))+4 *ks1 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks5 )%(2 +ks0 )))+4 *ks2 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks5 )%(2 +ks0 )))+4 *ks1 *ks2 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks5 )%(2 +ks0 )))+(((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))%ks3 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =2.0 \n        tmp5 =tmp3 *tmp4 \n        tmp6 =0.0 \n        tmp7 =triton_helpers .maximum (tmp5 ,tmp6 )\n        tmp8 =6.0 \n        tmp9 =triton_helpers .minimum (tmp7 ,tmp8 )\n        tmp10 =tl .load (in_ptr1 +((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks5 )%(2 +ks0 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp11 =0.5 \n        tmp12 =tmp10 <tmp11 \n        tmp13 =tmp12 .to (tl .float32 )\n        tmp14 =0.8864048946659319 \n        tmp15 =tmp13 *tmp14 \n        tmp16 =tmp9 *tmp15 \n        tmp17 =-1.0 \n        tmp18 =tmp13 +tmp17 \n        tmp19 =1.558387861036063 \n        tmp20 =tmp18 *tmp19 \n        tmp21 =0.7791939305180315 \n        tmp22 =tmp20 +tmp21 \n        tmp23 =tmp16 +tmp22 \n        tmp24 =-tmp23 \n        tmp25 =tl_math .exp (tmp24 )\n        tmp26 =libdevice .log1p (tmp25 )\n        tmp27 =tl .full (tmp26 .shape ,0 ,tmp26 .dtype )\n        tmp28 =tl .where (tmp2 ,tmp26 ,tmp27 )\n        tmp29 =tl .broadcast_to (tmp28 ,[XBLOCK ,R0_BLOCK ])\n        tmp31 =_tmp30 +tmp29 \n        _tmp30 =tl .where (r0_mask &xmask ,tmp31 ,_tmp30 )\n        tmp32 =1.0 \n        tmp33 =tmp32 -tmp23 \n        tmp34 =triton_helpers .maximum (tmp33 ,tmp6 )\n        tmp35 =tl .full ([1 ,1 ],False ,tl .int1 )\n        tmp36 =tl .where (tmp35 ,tmp34 ,tmp6 )\n        tmp37 =tl .full ([1 ,1 ],True ,tl .int1 )\n        tmp38 =tl .where (tmp37 ,tmp23 ,tmp6 )\n        tmp39 =tmp36 +tmp38 \n        tmp40 =tl .full (tmp39 .shape ,0 ,tmp39 .dtype )\n        tmp41 =tl .where (tmp2 ,tmp39 ,tmp40 )\n        tmp42 =tl .broadcast_to (tmp41 ,[XBLOCK ,R0_BLOCK ])\n        tmp44 =_tmp43 +tmp42 \n        _tmp43 =tl .where (r0_mask &xmask ,tmp44 ,_tmp43 )\n    tmp30 =tl .sum (_tmp30 ,1 )[:,None ]\n    tmp43 =tl .sum (_tmp43 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp30 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp43 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__to_copy_add_bernoulli_hardtanh_mul_native_dropout_soft_margin_loss_4 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =3 \n    R0_BLOCK :tl .constexpr =4 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =8 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tmp4 /tmp6 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp7 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_hardtanh_huber_loss_mul_native_dropout_ones_like_5 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =2.0 \n    tmp2 =tmp0 *tmp1 \n    tmp3 =0.0 \n    tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n    tmp5 =6.0 \n    tmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\n    tmp8 =0.5 \n    tmp9 =tmp7 <tmp8 \n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =0.8864048946659319 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp6 *tmp12 \n    tmp14 =-1.0 \n    tmp15 =tmp10 +tmp14 \n    tmp16 =1.558387861036063 \n    tmp17 =tmp15 *tmp16 \n    tmp18 =0.7791939305180315 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =tmp13 +tmp19 \n    tmp21 =1.0 \n    tmp22 =tmp20 -tmp21 \n    tmp23 =tl_math .abs (tmp22 )\n    tmp24 =tmp23 <tmp21 \n    tmp25 =tmp23 *tmp8 \n    tmp26 =tmp25 *tmp23 \n    tmp27 =tmp23 -tmp8 \n    tmp28 =tmp27 *tmp21 \n    tmp29 =tl .where (tmp24 ,tmp26 ,tmp28 )\n    tl .store (in_out_ptr0 +(x2 ),tmp29 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_huber_loss_6 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp5 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 )\n        tmp1 =8 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(2 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks3 )%ks4 ))+4 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks5 )%(2 +ks0 )))+2 *ks2 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks3 )%ks4 ))+4 *ks1 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks5 )%(2 +ks0 )))+4 *ks2 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks5 )%(2 +ks0 )))+4 *ks1 *ks2 *((((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))//ks5 )%(2 +ks0 )))+(((r0_1 +x0 *((10 +4 *ks0 +8 *ks1 +8 *ks2 +4 *ks0 *ks1 +4 *ks0 *ks2 +8 *ks1 *ks2 +4 *ks0 *ks1 *ks2 )//3 ))%ks3 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n        tmp6 =_tmp5 +tmp4 \n        _tmp5 =tl .where (r0_mask &xmask ,tmp6 ,_tmp5 )\n    tmp5 =tl .sum (_tmp5 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp5 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,2 +s0 ,2 +2 *s1 ,2 +2 *s2 ),(8 +4 *s0 +8 *s1 +8 *s2 +4 *s0 *s1 +4 *s0 *s2 +8 *s1 *s2 +4 *s0 *s1 *s2 ,4 +4 *s1 +4 *s2 +4 *s1 *s2 ,2 +2 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_native_dropout_0_xnumel =8 +4 *s0 +8 *s1 +8 *s2 +4 *s0 *s1 +4 *s0 *s2 +8 *s1 *s2 +4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_0 [grid (triton_poi_fused_native_dropout_0_xnumel )](buf0 ,buf1 ,0 ,21780 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        2 +2 *s2 \n        2 +2 *s1 \n        4 +4 *s1 +4 *s2 +4 *s1 *s2 \n        buf2 =buf1 ;del buf1 \n\n        triton_poi_fused_native_dropout_1_xnumel =8 +4 *s0 +8 *s1 +8 *s2 +4 *s0 *s1 +4 *s0 *s2 +8 *s1 *s2 +4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_1 [grid (triton_poi_fused_native_dropout_1_xnumel )](buf2 ,arg3_1 ,32 ,66 ,66 ,32 ,4356 ,3 ,21780 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf3 =empty_strided_cuda ((1 ,2 +s0 ,1 ,1 ),(2 +s0 ,1 ,2 +s0 ,2 +s0 ),torch .float32 )\n\n        triton_poi_fused_bernoulli_2_xnumel =2 +s0 \n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_2 [grid (triton_poi_fused_bernoulli_2_xnumel )](buf0 ,buf3 ,1 ,5 ,XBLOCK =8 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf4 =empty_strided_cuda ((3 ,),(1 ,),torch .float32 )\n        buf6 =empty_strided_cuda ((3 ,),(1 ,),torch .float32 )\n\n        (10 +4 *s0 +8 *s1 +8 *s2 +4 *s0 *s1 +4 *s0 *s2 +8 *s1 *s2 +4 *s0 *s1 *s2 )//3 \n        get_raw_stream (0 )\n        triton_red_fused__to_copy_add_bernoulli_clamp_min_fill_hardtanh_mean_mul_native_dropout_ne_soft_margin_loss_sub_where_zeros_like_3 [grid (3 )](buf2 ,buf3 ,buf4 ,buf6 ,3 ,32 ,32 ,66 ,66 ,4356 ,3 ,7260 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((),(),torch .float32 )\n        buf11 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_per_fused__to_copy_add_bernoulli_hardtanh_mul_native_dropout_soft_margin_loss_4 [grid (1 )](buf11 ,buf4 ,3 ,32 ,32 ,1 ,3 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf4 \n        buf7 =empty_strided_cuda ((),(),torch .float32 )\n        buf12 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_per_fused__to_copy_add_bernoulli_hardtanh_mul_native_dropout_soft_margin_loss_4 [grid (1 )](buf12 ,buf6 ,3 ,32 ,32 ,1 ,3 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        4 +4 *s1 +4 *s2 +4 *s1 *s2 \n        buf8 =buf2 ;del buf2 \n\n        triton_poi_fused__to_copy_add_bernoulli_hardtanh_huber_loss_mul_native_dropout_ones_like_5_xnumel =8 +4 *s0 +8 *s1 +8 *s2 +4 *s0 *s1 +4 *s0 *s2 +8 *s1 *s2 +4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_hardtanh_huber_loss_mul_native_dropout_ones_like_5 [grid (triton_poi_fused__to_copy_add_bernoulli_hardtanh_huber_loss_mul_native_dropout_ones_like_5_xnumel )](buf8 ,buf3 ,4356 ,21780 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        buf9 =buf6 ;del buf6 \n\n        (10 +4 *s0 +8 *s1 +8 *s2 +4 *s0 *s1 +4 *s0 *s2 +8 *s1 *s2 +4 *s0 *s1 *s2 )//3 \n        get_raw_stream (0 )\n        triton_red_fused_huber_loss_6 [grid (3 )](buf8 ,buf9 ,3 ,32 ,32 ,66 ,66 ,4356 ,3 ,7260 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf8 \n        buf10 =empty_strided_cuda ((),(),torch .float32 )\n        buf13 =buf10 ;del buf10 \n\n        get_raw_stream (0 )\n        triton_per_fused__to_copy_add_bernoulli_hardtanh_mul_native_dropout_soft_margin_loss_4 [grid (1 )](buf13 ,buf9 ,3 ,32 ,32 ,1 ,3 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf9 \n    return (buf11 ,buf12 ,buf13 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "23ec8359-ae03-4ac7-8db0-60479298f55c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AlphaDropout', 'KLDivLoss', 'ReplicationPad2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.alpha_dropout1 = nn.AlphaDropout(p=0.5)\n        self.alpha_dropout2 = nn.AlphaDropout(p=0.5)\n        self.replication_pad = nn.ReplicationPad2d(2)\n        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, x):\n        # Apply ReplicationPad2d to the input\n        x = self.replication_pad(x)\n        \n        # Apply AlphaDropout twice\n        x = self.alpha_dropout1(x)\n        x = self.alpha_dropout2(x)\n        \n        # Compute KLDivLoss between the output and a target tensor (for demonstration purposes, we use a random target)\n        target = torch.rand_like(x)\n        loss = self.kl_div_loss(F.log_softmax(x, dim=1), F.softmax(target, dim=1))\n        \n        # Return the loss as part of the output (this is unusual but fits the requirement of using KLDivLoss)\n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_like_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(x0 +16 *r0_1 +4 *ks0 *r0_1 +4 *ks1 *r0_1 +ks0 *ks1 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =triton_helpers .maximum (_tmp2 ,tmp1 )\n        _tmp2 =tl .where (r0_mask &xmask ,tmp3 ,_tmp2 )\n    tmp2 =triton_helpers .max2 (_tmp2 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp4 =tl .load (in_ptr0 +(x0 +16 *r0_1 +4 *ks0 *r0_1 +4 *ks1 *r0_1 +ks0 *ks1 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp5 =tmp4 -tmp2 \n        tmp6 =tl_math .exp (tmp5 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =_tmp8 +tmp7 \n        _tmp8 =tl .where (r0_mask &xmask ,tmp9 ,_tmp8 )\n    tmp8 =tl .sum (_tmp8 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_mul_replication_pad2d_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks4 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 )))))+(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))*((((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *x2 +(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_out_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =0.5 \n    tmp3 =tmp1 <tmp2 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =0.8864048946659319 \n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp0 *tmp6 \n    tmp8 =-1.0 \n    tmp9 =tmp4 +tmp8 \n    tmp10 =1.558387861036063 \n    tmp11 =tmp9 *tmp10 \n    tmp12 =0.7791939305180315 \n    tmp13 =tmp11 +tmp12 \n    tmp14 =tmp7 +tmp13 \n    tmp16 =tmp15 <tmp2 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =tmp17 *tmp5 \n    tmp19 =tmp14 *tmp18 \n    tmp20 =tmp17 +tmp8 \n    tmp21 =tmp20 *tmp10 \n    tmp22 =tmp21 +tmp12 \n    tmp23 =tmp19 +tmp22 \n    tl .store (in_out_ptr0 +(x3 ),tmp23 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax__softmax_mul_sub_sum_xlogy_4 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    _tmp23 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =tl .load (in_ptr0 +(4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+16 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//(16 +4 *ks3 +4 *ks4 +ks3 *ks4 ))%ks2 ))+ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+4 *ks3 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//(16 +4 *ks3 +4 *ks4 +ks3 *ks4 ))%ks2 ))+4 *ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//(16 +4 *ks3 +4 *ks4 +ks3 *ks4 ))%ks2 ))+ks3 *ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//(16 +4 *ks3 +4 *ks4 +ks3 *ks4 ))%ks2 ))+(((r0_2 +ks2 *x0 )%ks0 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr1 +(4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+(((r0_2 +ks2 *x0 )%ks0 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr2 +(4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+(((r0_2 +ks2 *x0 )%ks0 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp14 =tl .load (in_ptr3 +(4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+16 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//(16 +4 *ks3 +4 *ks4 +ks3 *ks4 ))%ks2 ))+ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+4 *ks3 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//(16 +4 *ks3 +4 *ks4 +ks3 *ks4 ))%ks2 ))+4 *ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//(16 +4 *ks3 +4 *ks4 +ks3 *ks4 ))%ks2 ))+ks3 *ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//(16 +4 *ks3 +4 *ks4 +ks3 *ks4 ))%ks2 ))+(((r0_2 +ks2 *x0 )%ks0 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp15 =tl .load (in_ptr4 +(4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+(((r0_2 +ks2 *x0 )%ks0 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp17 =tl .load (in_ptr5 +(4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+ks4 *((((r0_2 +ks2 *x0 +4 *ks2 *x1 +ks2 *ks4 *x1 )//ks0 )%ks1 ))+(((r0_2 +ks2 *x0 )%ks0 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp3 =tl_math .exp (tmp2 )\n        tmp5 =tmp3 /tmp4 \n        tmp6 =libdevice .isnan (tmp5 ).to (tl .int1 )\n        tmp7 =0.0 \n        tmp8 =tmp5 ==tmp7 \n        tmp9 =tl_math .log (tmp5 )\n        tmp10 =tmp5 *tmp9 \n        tmp11 =tl .where (tmp8 ,tmp7 ,tmp10 )\n        tmp12 =float (\"nan\")\n        tmp13 =tl .where (tmp6 ,tmp12 ,tmp11 )\n        tmp16 =tmp14 -tmp15 \n        tmp18 =tl_math .log (tmp17 )\n        tmp19 =tmp16 -tmp18 \n        tmp20 =tmp5 *tmp19 \n        tmp21 =tmp13 -tmp20 \n        tmp22 =tl .broadcast_to (tmp21 ,[XBLOCK ,R0_BLOCK ])\n        tmp24 =_tmp23 +tmp22 \n        _tmp23 =tl .where (r0_mask &xmask ,tmp24 ,_tmp23 )\n    tmp23 =tl .sum (_tmp23 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp23 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax__softmax_div_mul_sub_sum_xlogy_5 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =_tmp2 +tmp1 \n        _tmp2 =tl .where (r0_mask ,tmp3 ,_tmp2 )\n    tmp2 =tl .sum (_tmp2 ,1 )[:,None ]\n    tmp4 =1.0 \n    tmp5 =tmp2 *tmp4 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp5 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((3 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[3 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_rand_like_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_rand_like_0 [grid (triton_poi_fused_rand_like_0_xnumel )](buf0 ,buf1 ,0 ,13872 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,1 ,4 +s1 ,4 +s2 ),(16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n        buf3 =empty_strided_cuda ((1 ,1 ,4 +s1 ,4 +s2 ),(16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_red_fused__softmax_1_xnumel =16 +4 *s1 +4 *s2 +s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_1 [grid (triton_red_fused__softmax_1_xnumel )](buf1 ,buf2 ,buf3 ,64 ,64 ,4624 ,3 ,XBLOCK =64 ,R0_BLOCK =4 ,num_warps =2 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_rand_like_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_rand_like_0 [grid (triton_poi_fused_rand_like_0_xnumel )](buf0 ,buf4 ,0 ,13872 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_bernoulli_2_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_2 [grid (triton_poi_fused_bernoulli_2_xnumel )](buf0 ,buf5 ,1 ,13872 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf6 =buf4 ;del buf4 \n\n        triton_poi_fused__to_copy_add_bernoulli_mul_replication_pad2d_3_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_mul_replication_pad2d_3 [grid (triton_poi_fused__to_copy_add_bernoulli_mul_replication_pad2d_3_xnumel )](buf6 ,arg3_1 ,buf5 ,68 ,68 ,4624 ,64 ,64 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf5 \n        buf7 =empty_strided_cuda ((1 ,1 ,4 +s1 ,4 +s2 ),(16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,1 ,4 +s1 ,4 +s2 ),(16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_red_fused__softmax_1_xnumel =16 +4 *s1 +4 *s2 +s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_1 [grid (triton_red_fused__softmax_1_xnumel )](buf6 ,buf7 ,buf8 ,64 ,64 ,4624 ,3 ,XBLOCK =64 ,R0_BLOCK =4 ,num_warps =2 ,num_stages =1 )\n        buf9 =empty_strided_cuda ((1 ,1 ,4 +s1 ,4 +s2 ),(16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_red_fused__log_softmax__softmax_mul_sub_sum_xlogy_4_xnumel =16 +4 *s1 +4 *s2 +s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__log_softmax__softmax_mul_sub_sum_xlogy_4 [grid (triton_red_fused__log_softmax__softmax_mul_sub_sum_xlogy_4_xnumel )](buf1 ,buf2 ,buf3 ,buf6 ,buf7 ,buf8 ,buf9 ,68 ,68 ,3 ,64 ,64 ,4624 ,3 ,XBLOCK =1 ,R0_BLOCK =4 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n        del buf2 \n        del buf3 \n        del buf7 \n        del buf8 \n        buf10 =empty_strided_cuda ((),(),torch .float32 )\n        buf11 =buf10 ;del buf10 \n\n        16 +4 *s1 +4 *s2 +s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__log_softmax__softmax_div_mul_sub_sum_xlogy_5 [grid (1 )](buf11 ,buf9 ,1 ,4624 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf9 \n    return (buf6 ,buf11 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "2514e870-9d03-4ab1-8897-7880fcfcdfcf",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Sigmoid', 'GaussianNLLLoss', 'Sequential', 'ConstantPad3d', 'Softshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad3d(padding=1, value=0)\n        self.sequential = nn.Sequential(\n            nn.Softshrink(lambd=0.5),\n            nn.Sigmoid(),\n            nn.Softshrink(lambd=0.5),\n            nn.Sigmoid(),\n            nn.Softshrink(lambd=0.5)\n        )\n        self.loss = nn.GaussianNLLLoss()\n\n    def forward(self, x):\n        # Apply padding to the input\n        x = self.pad(x)\n        \n        # Pass through the sequential layers\n        x = self.sequential(x)\n        \n        # Reshape the output to match the expected shape for GaussianNLLLoss\n        x = x.view(x.size(0), -1)\n        \n        # Generate a dummy target and variance for GaussianNLLLoss\n        target = torch.zeros_like(x)\n        var = torch.ones_like(x)\n        \n        # Compute the loss (this is just a placeholder, as the loss is typically used during training)\n        loss = self.loss(x, target, var)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64, 64).cuda()  # Example input with arbitrary shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_constant_pad_nd_gt_mul_sigmoid_sign_sub_where_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x6 =((xindex //ks0 )%ks1 )\n    x1 =((xindex //ks3 )%ks4 )\n    x0 =(xindex %ks3 )\n    x2 =((xindex //ks7 )%ks1 )\n    x3 =xindex //ks8 \n    x10 =xindex \n    tmp0 =(-1 )+x6 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks5 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-1 )+x0 \n    tmp10 =tmp9 >=tmp1 \n    tmp11 =ks6 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =tl .load (in_ptr0 +((-1 )+x0 +((-1 )*ks6 )+ks6 *x1 +((-1 )*ks5 *ks6 )+ks5 *ks6 *x2 +ks2 *ks5 *ks6 *x3 ),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =tl_math .abs (tmp18 )\n    tmp20 =0.5 \n    tmp21 =tmp19 >tmp20 \n    tmp22 =tl .full ([1 ],0 ,tl .int32 )\n    tmp23 =tmp22 <tmp18 \n    tmp24 =tmp23 .to (tl .int8 )\n    tmp25 =tmp18 <tmp22 \n    tmp26 =tmp25 .to (tl .int8 )\n    tmp27 =tmp24 -tmp26 \n    tmp28 =tmp27 .to (tmp18 .dtype )\n    tmp29 =tmp28 *tmp20 \n    tmp30 =tmp18 -tmp29 \n    tmp31 =0.0 \n    tmp32 =tmp18 *tmp31 \n    tmp33 =tl .where (tmp21 ,tmp30 ,tmp32 )\n    tmp34 =tl .sigmoid (tmp33 )\n    tmp35 =tl_math .abs (tmp34 )\n    tmp36 =tmp35 >tmp20 \n    tmp37 =tmp22 <tmp34 \n    tmp38 =tmp37 .to (tl .int8 )\n    tmp39 =tmp34 <tmp22 \n    tmp40 =tmp39 .to (tl .int8 )\n    tmp41 =tmp38 -tmp40 \n    tmp42 =tmp41 .to (tmp34 .dtype )\n    tmp43 =tmp42 *tmp20 \n    tmp44 =tmp34 -tmp43 \n    tmp45 =tmp34 *tmp31 \n    tmp46 =tl .where (tmp36 ,tmp44 ,tmp45 )\n    tmp47 =tl .sigmoid (tmp46 )\n    tl .store (out_ptr0 +(x10 ),tmp47 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_gt_mul_sign_sub_where_1 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =0.5 \n    tmp3 =tmp1 >tmp2 \n    tmp4 =tl .full ([1 ],0 ,tl .int32 )\n    tmp5 =tmp4 <tmp0 \n    tmp6 =tmp5 .to (tl .int8 )\n    tmp7 =tmp0 <tmp4 \n    tmp8 =tmp7 .to (tl .int8 )\n    tmp9 =tmp6 -tmp8 \n    tmp10 =tmp9 .to (tmp0 .dtype )\n    tmp11 =tmp10 *tmp2 \n    tmp12 =tmp0 -tmp11 \n    tmp13 =0.0 \n    tmp14 =tmp0 *tmp13 \n    tmp15 =tl .where (tmp3 ,tmp12 ,tmp14 )\n    tl .store (in_out_ptr0 +(x0 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_gt_mul_sign_sub_view_where_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *(((x0 //ks2 )%ks3 ))+4 *(((x0 //ks0 )%ks1 ))+8 *(x0 //(8 +4 *ks4 +4 *ks5 +4 *ks6 +2 *ks4 *ks5 +2 *ks4 *ks6 +2 *ks5 *ks6 +ks4 *ks5 *ks6 ))+ks6 *(((x0 //ks2 )%ks3 ))+2 *ks5 *(((x0 //ks0 )%ks1 ))+2 *ks6 *(((x0 //ks0 )%ks1 ))+4 *ks4 *(x0 //(8 +4 *ks4 +4 *ks5 +4 *ks6 +2 *ks4 *ks5 +2 *ks4 *ks6 +2 *ks5 *ks6 +ks4 *ks5 *ks6 ))+4 *ks5 *(x0 //(8 +4 *ks4 +4 *ks5 +4 *ks6 +2 *ks4 *ks5 +2 *ks4 *ks6 +2 *ks5 *ks6 +ks4 *ks5 *ks6 ))+4 *ks6 *(x0 //(8 +4 *ks4 +4 *ks5 +4 *ks6 +2 *ks4 *ks5 +2 *ks4 *ks6 +2 *ks5 *ks6 +ks4 *ks5 *ks6 ))+ks5 *ks6 *(((x0 //ks0 )%ks1 ))+2 *ks4 *ks5 *(x0 //(8 +4 *ks4 +4 *ks5 +4 *ks6 +2 *ks4 *ks5 +2 *ks4 *ks6 +2 *ks5 *ks6 +ks4 *ks5 *ks6 ))+2 *ks4 *ks6 *(x0 //(8 +4 *ks4 +4 *ks5 +4 *ks6 +2 *ks4 *ks5 +2 *ks4 *ks6 +2 *ks5 *ks6 +ks4 *ks5 *ks6 ))+2 *ks5 *ks6 *(x0 //(8 +4 *ks4 +4 *ks5 +4 *ks6 +2 *ks4 *ks5 +2 *ks4 *ks6 +2 *ks5 *ks6 +ks4 *ks5 *ks6 ))+ks4 *ks5 *ks6 *(x0 //(8 +4 *ks4 +4 *ks5 +4 *ks6 +2 *ks4 *ks5 +2 *ks4 *ks6 +2 *ks5 *ks6 +ks4 *ks5 *ks6 ))+((x0 %ks2 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_zeros_like_3 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_ones_like_4 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =1.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +2 *s2 +2 *s3 +s2 *s3 \n        2 +s1 \n        2 +s3 \n        2 +s2 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_constant_pad_nd_gt_mul_sigmoid_sign_sub_where_0_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_constant_pad_nd_gt_mul_sigmoid_sign_sub_where_0 [grid (triton_poi_fused_abs_constant_pad_nd_gt_mul_sigmoid_sign_sub_where_0_xnumel )](arg4_1 ,buf0 ,4356 ,66 ,64 ,66 ,66 ,64 ,64 ,4356 ,287496 ,862488 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del arg4_1 \n        buf1 =buf0 ;del buf0 \n\n        triton_poi_fused_abs_gt_mul_sign_sub_where_1_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_gt_mul_sign_sub_where_1 [grid (triton_poi_fused_abs_gt_mul_sign_sub_where_1_xnumel )](buf1 ,862488 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_gt_mul_sign_sub_view_where_2_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_gt_mul_sign_sub_view_where_2 [grid (triton_poi_fused_abs_gt_mul_sign_sub_view_where_2_xnumel )](buf1 ,buf2 ,4356 ,66 ,66 ,66 ,64 ,64 ,64 ,862488 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        buf3 =reinterpret_tensor (buf1 ,(1 ,8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,1 ),0 );del buf1 \n\n        triton_poi_fused_zeros_like_3_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_zeros_like_3 [grid (triton_poi_fused_zeros_like_3_xnumel )](buf3 ,862488 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((1 ,8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_ones_like_4_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_ones_like_4 [grid (triton_poi_fused_ones_like_4_xnumel )](buf4 ,862488 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n    return (buf2 ,buf3 ,buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =64 \n    arg4_1 =rand_strided ((1 ,3 ,64 ,64 ,64 ),(786432 ,262144 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "252b070b-1cd5-4fed-9323-0250d4acb1d5",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GLU', 'ConstantPad1d', 'AdaptiveAvgPool1d', 'TripletMarginWithDistanceLoss', 'ChannelShuffle']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.glu = nn.GLU(dim=1)\n        self.pad = nn.ConstantPad1d(padding=2, value=0)\n        self.avg_pool = nn.AdaptiveAvgPool1d(output_size=10)\n        self.loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n        self.channel_shuffle = nn.ChannelShuffle(groups=2)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, sequence_length)\n        x = self.glu(x)  # Apply GLU along the channel dimension\n        x = self.pad(x)  # Apply padding to the sequence dimension\n        x = self.avg_pool(x)  # Apply adaptive average pooling to reduce sequence length\n        x = self.channel_shuffle(x)  # Shuffle channels\n        \n        # For demonstration, create anchor, positive, and negative samples\n        anchor = x\n        positive = torch.roll(x, shifts=1, dims=2)  # Shifted version of x as positive sample\n        negative = torch.roll(x, shifts=2, dims=2)  # Further shifted version of x as negative sample\n        \n        # Compute triplet loss (not used for prediction, just for demonstration)\n        loss = self.loss(anchor, positive, negative)\n        \n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 4, 20).cuda()  # Example input shape (batch_size=1, channels=4, sequence_length=20)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_glu_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =48 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %24 )\n    x1 =xindex //24 \n    x2 =xindex \n    tmp0 =(-2 )+x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],20 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-2 )+x0 +20 *x1 ),tmp5 &xmask ,other =0.0 )\n    tmp7 =tl .load (in_ptr0 +(38 +x0 +20 *x1 ),tmp5 &xmask ,other =0.0 )\n    tmp8 =tl .sigmoid (tmp7 )\n    tmp9 =tmp6 *tmp8 \n    tmp10 =tl .full (tmp9 .shape ,0.0 ,tmp9 .dtype )\n    tmp11 =tl .where (tmp5 ,tmp9 ,tmp10 )\n    tl .store (out_ptr0 +(x2 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =20 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %10 )\n    x1 =xindex //10 \n    x2 =xindex \n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =(12 *x0 )//5 \n    tmp4 =(33 +24 *x0 )//10 \n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp2 &tmp5 \n    tmp7 =tl .load (in_ptr0 +(24 *x1 +((12 *x0 )//5 )),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =1 +((12 *x0 )//5 )\n    tmp9 =tmp8 <tmp4 \n    tmp10 =tmp2 &tmp9 \n    tmp11 =tl .load (in_ptr0 +(1 +24 *x1 +((12 *x0 )//5 )),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tmp11 +tmp7 \n    tmp13 =2 +((12 *x0 )//5 )\n    tmp14 =tmp13 <tmp4 \n    tmp15 =tmp2 &tmp14 \n    tmp16 =tl .load (in_ptr0 +(2 +24 *x1 +((12 *x0 )//5 )),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp17 =tmp16 +tmp12 \n    tmp18 =3 +((12 *x0 )//5 )\n    tmp19 =tmp18 <tmp4 \n    tmp20 =tmp2 &tmp19 \n    tmp21 =tl .load (in_ptr0 +(3 +24 *x1 +((12 *x0 )//5 )),tmp20 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tmp21 +tmp17 \n    tmp23 =1.0 \n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp6 ,tmp23 ,tmp24 )\n    tmp26 =1.0 \n    tmp27 =tl .full (tmp26 .shape ,0.0 ,tmp26 .dtype )\n    tmp28 =tl .where (tmp10 ,tmp26 ,tmp27 )\n    tmp29 =tmp28 +tmp25 \n    tmp30 =1.0 \n    tmp31 =tl .full (tmp30 .shape ,0.0 ,tmp30 .dtype )\n    tmp32 =tl .where (tmp15 ,tmp30 ,tmp31 )\n    tmp33 =tmp32 +tmp29 \n    tmp34 =1.0 \n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp20 ,tmp34 ,tmp35 )\n    tmp37 =tmp36 +tmp33 \n    tmp38 =tmp22 /tmp37 \n    tl .store (out_ptr0 +(x2 ),tmp38 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_norm_roll_sub_2 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +10 *x0 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr0 +(10 *x0 +(((9 +r0_1 )%10 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp10 =tl .load (in_ptr0 +(10 *x0 +(((8 +r0_1 )%10 ))),r0_mask &xmask ,other =0.0 )\n    tmp2 =tmp0 -tmp1 \n    tmp3 =1e-06 \n    tmp4 =tmp2 +tmp3 \n    tmp5 =tmp4 *tmp4 \n    tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n    tmp8 =tl .where (r0_mask &xmask ,tmp6 ,0 )\n    tmp9 =tl .sum (tmp8 ,1 )[:,None ]\n    tmp11 =tmp0 -tmp10 \n    tmp12 =tmp11 +tmp3 \n    tmp13 =tmp12 *tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tmp16 =tl .where (r0_mask &xmask ,tmp14 ,0 )\n    tmp17 =tl .sum (tmp16 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp9 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_clamp_min_mean_norm_sub_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp5 =tl .load (in_ptr1 +(0 ))\n    tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ])\n    tmp11 =tl .load (in_ptr0 +(1 ))\n    tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ])\n    tmp15 =tl .load (in_ptr1 +(1 ))\n    tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ])\n    tmp2 =libdevice .sqrt (tmp1 )\n    tmp3 =1.0 \n    tmp4 =tmp2 +tmp3 \n    tmp7 =libdevice .sqrt (tmp6 )\n    tmp8 =tmp4 -tmp7 \n    tmp9 =0.0 \n    tmp10 =triton_helpers .maximum (tmp8 ,tmp9 )\n    tmp13 =libdevice .sqrt (tmp12 )\n    tmp14 =tmp13 +tmp3 \n    tmp17 =libdevice .sqrt (tmp16 )\n    tmp18 =tmp14 -tmp17 \n    tmp19 =triton_helpers .maximum (tmp18 ,tmp9 )\n    tmp20 =tmp10 +tmp19 \n    tmp21 =2.0 \n    tmp22 =tmp20 /tmp21 \n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp22 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    assert_size_stride (arg2_1 ,(1 ,4 ,20 ),(80 ,20 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,2 ,24 ),(48 ,24 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_glu_0 [grid (48 )](arg2_1 ,buf0 ,48 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del arg2_1 \n        buf1 =empty_strided_cuda ((1 ,2 ,1 ,10 ),(20 ,10 ,10 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_1 [grid (20 )](buf0 ,buf1 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n        buf3 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_norm_roll_sub_2 [grid (2 )](buf1 ,buf2 ,buf3 ,2 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_clamp_min_mean_norm_sub_3 [grid (1 )](buf2 ,buf3 ,buf4 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf2 \n        del buf3 \n    return (reinterpret_tensor (buf1 ,(1 ,2 ,10 ),(20 ,10 ,1 ),0 ),buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =4 \n    arg1_1 =20 \n    arg2_1 =rand_strided ((1 ,4 ,20 ),(80 ,20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "2691c4fb-a35a-4894-b64e-f1c32187ac1a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardshrink', 'LPPool3d', 'ReflectionPad1d', 'CrossMapLRN2d', 'TransformerDecoderLayer', 'Identity']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardshrink = nn.Hardshrink()\n        self.lppool3d = nn.LPPool3d(norm_type=2, kernel_size=3, stride=2)\n        self.reflection_pad1d = nn.ReflectionPad1d(padding=2)\n        self.cross_map_lrn2d = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n        self.identity = nn.Identity()\n\n    def forward(self, x):\n        # Apply Hardshrink\n        x = self.hardshrink(x)\n        \n        # Reshape for LPPool3d (assuming input is 4D, adding a dummy dimension)\n        x = x.unsqueeze(1)  # Add a dummy dimension to make it 5D\n        x = self.lppool3d(x)\n        x = x.squeeze(1)  # Remove the dummy dimension\n        \n        # Reshape for ReflectionPad1d (assuming input is 3D, adding a dummy dimension)\n        x = x.unsqueeze(1)  # Add a dummy dimension to make it 4D\n        x = self.reflection_pad1d(x)\n        x = x.squeeze(1)  # Remove the dummy dimension\n        \n        # Reshape for CrossMapLRN2d (assuming input is 4D)\n        x = x.unsqueeze(1)  # Add a dummy dimension to make it 5D\n        x = self.cross_map_lrn2d(x)\n        x = x.squeeze(1)  # Remove the dummy dimension\n        \n        # Reshape for TransformerDecoderLayer (assuming input is 3D, adding a dummy dimension)\n        x = x.unsqueeze(1)  # Add a dummy dimension to make it 4D\n        x = self.transformer_decoder_layer(x, x)  # Using the same tensor as memory for simplicity\n        x = x.squeeze(1)  # Remove the dummy dimension\n        \n        # Apply Identity\n        x = self.identity(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 64, 64).cuda()  # Arbitrary input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_pow_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =0.5 \n    tmp3 =tmp1 <=tmp2 \n    tmp4 =0.0 \n    tmp5 =tl .where (tmp3 ,tmp4 ,tmp0 )\n    tmp6 =tmp5 *tmp5 \n    tl .store (out_ptr0 +(x0 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_mul_pow_relu_sign_1 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tmp1 <tmp0 \n    tmp3 =tmp2 .to (tl .int8 )\n    tmp4 =tmp0 <tmp1 \n    tmp5 =tmp4 .to (tl .int8 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =tmp6 .to (tmp0 .dtype )\n    tmp8 =tl_math .abs (tmp0 )\n    tmp9 =triton_helpers .maximum (tmp1 ,tmp8 )\n    tmp10 =tmp7 *tmp9 \n    tmp11 =27.0 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =libdevice .sqrt (tmp12 )\n    tl .store (in_out_ptr0 +(x0 ),tmp13 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_pow_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_pow_0 [grid (triton_poi_fused_pow_0_xnumel )](arg3_1 ,buf0 ,262144 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg3_1 \n\n        buf1 =torch .ops .aten .avg_pool3d .default (buf0 ,[3 ,3 ,3 ],[2 ,2 ,2 ],[0 ,0 ,0 ],False ,True ,None )\n        del buf0 \n        buf2 =buf1 \n        del buf1 \n        buf3 =reinterpret_tensor (buf2 ,(1 ,1 ,1 +(((-3 )+s0 )//2 ),1 +(((-3 )+s1 )//2 ),1 +(((-3 )+s2 )//2 )),(1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 ,1 +(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 +(((-3 )+s2 )//2 ),1 ),0 );del buf2 \n\n        triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel =1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_mul_pow_relu_sign_1 [grid (triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel )](buf3 ,29791 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n    return (reinterpret_tensor (buf3 ,(1 ,1 ,1 +(((-3 )+s0 )//2 ),1 +(((-3 )+s1 )//2 ),1 +(((-3 )+s2 )//2 )),(1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 +(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 +(((-3 )+s2 )//2 ),1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,64 ,64 ,64 ),(262144 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "282d5fff-8869-4125-9e14-0083941137ee",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FractionalMaxPool3d', 'LocalResponseNorm', 'Dropout3d', 'Tanh', 'Conv3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv3d(1, 10, kernel_size=3, stride=1, padding=1)\n        self.fractional_max_pool = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))\n        self.local_response_norm = nn.LocalResponseNorm(size=5)\n        self.dropout3d = nn.Dropout3d(p=0.5)\n        self.conv2 = nn.Conv3d(10, 20, kernel_size=3, stride=1, padding=1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.fractional_max_pool(x)\n        x = self.local_response_norm(x)\n        x = self.dropout3d(x)\n        x = self.conv2(x)\n        x = self.tanh(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 16, 16, 16).cuda()  # Example input shape\n    return [x]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =xindex \n    x1 =xindex //4096 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),None )\n    tmp1 =tl .load (in_ptr0 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =30 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_2 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =7168 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //512 \n    x2 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],10 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-1024 )+x2 ),tmp5 &xmask ,other =0.0 )\n    tmp7 =tmp6 *tmp6 \n    tmp8 =tl .full (tmp7 .shape ,0.0 ,tmp7 .dtype )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tl .store (out_ptr0 +(x2 ),tmp9 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_3 (in_ptr0 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_avg_pool3d_div_mul_pow_4 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =5120 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    x2 =xindex //512 \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(512 +x0 ),xmask )\n    tmp3 =tl .load (in_ptr0 +(1024 +x0 ),xmask )\n    tmp5 =tl .load (in_ptr0 +(1536 +x0 ),xmask )\n    tmp7 =tl .load (in_ptr0 +(2048 +x0 ),xmask )\n    tmp11 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp19 =tl .load (in_ptr2 +(x2 ),xmask ,eviction_policy ='evict_last').to (tl .int1 )\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp8 =tmp7 +tmp6 \n    tmp9 =0.2 \n    tmp10 =tmp8 *tmp9 \n    tmp12 =0.0001 \n    tmp13 =tmp10 *tmp12 \n    tmp14 =1.0 \n    tmp15 =tmp13 +tmp14 \n    tmp16 =0.75 \n    tmp17 =libdevice .pow (tmp15 ,tmp16 )\n    tmp18 =tmp11 /tmp17 \n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =2.0 \n    tmp22 =tmp20 *tmp21 \n    tmp23 =tmp18 *tmp22 \n    tmp24 =tmp18 /tmp17 \n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp23 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp24 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_tanh_5 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10240 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //512 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =libdevice .tanh (tmp2 )\n    tl .store (in_out_ptr0 +(x2 ),tmp3 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(10 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_2 ,(10 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,1 ,16 ,16 ,16 ),(4096 ,4096 ,256 ,16 ,1 ))\n    assert_size_stride (primals_4 ,(20 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_5 ,(20 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,1 ,1 ),padding =(1 ,1 ,1 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,10 ,16 ,16 ,16 ),(40960 ,4096 ,256 ,16 ,1 ))\n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_0 [grid (40960 )](buf1 ,primals_2 ,40960 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n        buf2 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,10 ,3 ),(30 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_1 [grid (30 )](buf2 ,buf3 ,0 ,30 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n\n        buf4 =torch .ops .aten .fractional_max_pool3d .default (buf1 ,[2 ,2 ,2 ],[8 ,8 ,8 ],buf3 )\n        del buf3 \n        buf5 =buf4 [0 ]\n        buf6 =buf4 [1 ]\n        del buf4 \n        buf7 =empty_strided_cuda ((1 ,1 ,14 ,8 ,64 ),(7168 ,7168 ,512 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_2 [grid (7168 )](buf5 ,buf7 ,7168 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf10 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,1 ,1 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_3 [grid (10 )](buf2 ,buf10 ,1 ,10 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf2 \n        buf8 =empty_strided_cuda ((1 ,1 ,10 ,8 ,64 ),(5120 ,5120 ,512 ,64 ,1 ),torch .float32 )\n        buf11 =empty_strided_cuda ((1 ,10 ,8 ,8 ,8 ),(5120 ,512 ,64 ,8 ,1 ),torch .float32 )\n        buf14 =empty_strided_cuda ((1 ,10 ,8 ,8 ,8 ),(5120 ,512 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_avg_pool3d_div_mul_pow_4 [grid (5120 )](buf7 ,buf5 ,buf10 ,buf8 ,buf11 ,buf14 ,5120 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n\n        buf12 =extern_kernels .convolution (buf11 ,primals_4 ,stride =(1 ,1 ,1 ),padding =(1 ,1 ,1 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf12 ,(1 ,20 ,8 ,8 ,8 ),(10240 ,512 ,64 ,8 ,1 ))\n        buf13 =buf12 ;del buf12 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_tanh_5 [grid (10240 )](buf13 ,primals_5 ,10240 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_5 \n    return (buf13 ,primals_1 ,primals_3 ,primals_4 ,buf1 ,buf5 ,buf6 ,buf7 ,buf8 ,buf10 ,buf11 ,buf13 ,buf14 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((10 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,1 ,16 ,16 ,16 ),(4096 ,4096 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((20 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "2833b1f0-3c02-4ee7-ac0f-0385ee32c84f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LazyLinear', 'Sigmoid', 'Container']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lazy_linear1 = nn.LazyLinear(128)\n        self.lazy_linear2 = nn.LazyLinear(64)\n        self.lazy_linear3 = nn.LazyLinear(32)\n        self.sigmoid = nn.Sigmoid()\n        self.container = nn.Sequential(\n            nn.LazyLinear(16),\n            nn.Sigmoid(),\n            nn.LazyLinear(8)\n        )\n\n    def forward(self, x):\n        # Flatten the input to make it compatible with LazyLinear\n        x = x.view(x.size(0), -1)\n        \n        # Apply the first LazyLinear layer followed by Sigmoid\n        x = self.sigmoid(self.lazy_linear1(x))\n        \n        # Apply the second LazyLinear layer followed by Sigmoid\n        x = self.sigmoid(self.lazy_linear2(x))\n        \n        # Apply the third LazyLinear layer followed by Sigmoid\n        x = self.sigmoid(self.lazy_linear3(x))\n        \n        # Pass through the container (Sequential) module\n        x = self.container(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input with shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_sigmoid_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .sigmoid (tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp3 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_sigmoid_1 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .sigmoid (tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp3 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_sigmoid_2 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .sigmoid (tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp3 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_sigmoid_3 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .sigmoid (tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp3 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_2 ,(128 ,3072 ),(3072 ,1 ))\n    assert_size_stride (primals_3 ,(128 ,),(1 ,))\n    assert_size_stride (primals_4 ,(64 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_5 ,(64 ,),(1 ,))\n    assert_size_stride (primals_6 ,(32 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_7 ,(32 ,),(1 ,))\n    assert_size_stride (primals_8 ,(16 ,32 ),(32 ,1 ))\n    assert_size_stride (primals_9 ,(16 ,),(1 ,))\n    assert_size_stride (primals_10 ,(8 ,16 ),(16 ,1 ))\n    assert_size_stride (primals_11 ,(8 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,3072 ),(3072 ,1 ),0 ),reinterpret_tensor (primals_2 ,(3072 ,128 ),(1 ,3072 ),0 ),out =buf0 )\n        del primals_2 \n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_sigmoid_0 [grid (128 )](buf1 ,primals_3 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_3 \n        buf2 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf1 ,reinterpret_tensor (primals_4 ,(128 ,64 ),(1 ,128 ),0 ),out =buf2 )\n        buf3 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_sigmoid_1 [grid (64 )](buf3 ,primals_5 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del primals_5 \n        buf4 =empty_strided_cuda ((1 ,32 ),(32 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf3 ,reinterpret_tensor (primals_6 ,(64 ,32 ),(1 ,64 ),0 ),out =buf4 )\n        buf5 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_sigmoid_2 [grid (32 )](buf5 ,primals_7 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del primals_7 \n        buf6 =empty_strided_cuda ((1 ,16 ),(16 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf5 ,reinterpret_tensor (primals_8 ,(32 ,16 ),(1 ,32 ),0 ),out =buf6 )\n        buf7 =buf6 ;del buf6 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_sigmoid_3 [grid (16 )](buf7 ,primals_9 ,16 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del primals_9 \n        buf8 =empty_strided_cuda ((1 ,8 ),(8 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_11 ,buf7 ,reinterpret_tensor (primals_10 ,(16 ,8 ),(1 ,16 ),0 ),alpha =1 ,beta =1 ,out =buf8 )\n        del primals_11 \n    return (buf8 ,reinterpret_tensor (primals_1 ,(1 ,3072 ),(3072 ,1 ),0 ),buf1 ,buf3 ,buf5 ,buf7 ,primals_10 ,primals_8 ,primals_6 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((128 ,3072 ),(3072 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((64 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((32 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((16 ,32 ),(32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((8 ,16 ),(16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((8 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "288e6cdf-2a7a-4d1c-bcfe-0a3920703c30",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['NLLLoss', 'Dropout1d', 'TripletMarginLoss', 'ReflectionPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout1d = nn.Dropout1d(p=0.5)\n        self.reflection_pad1d = nn.ReflectionPad1d(padding=2)\n        self.nll_loss = nn.NLLLoss()\n        self.triplet_margin_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, channels, sequence_length)\n        x = self.reflection_pad1d(x)  # Apply ReflectionPad1d\n        x = self.dropout1d(x)  # Apply Dropout1d\n        \n        # For NLLLoss, we need a target tensor. Let's assume a dummy target for demonstration.\n        # NLLLoss expects log probabilities as input, so we apply log_softmax.\n        log_probs = F.log_softmax(x, dim=1)\n        target = torch.randint(0, x.size(1), (x.size(0), x.size(2)), device=x.device)\n        nll_loss = self.nll_loss(log_probs, target)\n        \n        # For TripletMarginLoss, we need anchor, positive, and negative tensors.\n        # Let's create dummy tensors for demonstration.\n        anchor = x[:, :, :x.size(2)//2]\n        positive = x[:, :, x.size(2)//2:]\n        negative = torch.flip(x, dims=[2])[:, :, :x.size(2)//2]\n        triplet_loss = self.triplet_margin_loss(anchor, positive, negative)\n        \n        # Return both losses for demonstration purposes\n        return nll_loss, triplet_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 32).cuda()  # Example input shape: (batch_size, channels, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_bernoulli_norm_sub_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,load_seed_offset ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    _tmp17 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp25 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp3 =tl .load (in_ptr1 +(ks1 *x0 +(tl .where ((-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-2 )+r0_1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-2 )+r0_1 )))+2 *ks1 ,(-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-2 )+r0_1 )))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp10 =tl .load (in_ptr1 +(ks1 *x0 +(tl .where ((-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs (r0_1 +(ks1 //2 ))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs (r0_1 +(ks1 //2 ))))+2 *ks1 ,(-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs (r0_1 +(ks1 //2 ))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp19 =tl .load (in_ptr1 +(ks1 *x0 +(tl .where ((-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs (1 +ks1 +((-1 )*r0_1 ))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs (1 +ks1 +((-1 )*r0_1 ))))+2 *ks1 ,(-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs (1 +ks1 +((-1 )*r0_1 ))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =0.5 \n        tmp5 =tmp2 <tmp4 \n        tmp6 =tmp5 .to (tl .float32 )\n        tmp7 =2.0 \n        tmp8 =tmp6 *tmp7 \n        tmp9 =tmp3 *tmp8 \n        tmp11 =tmp10 *tmp8 \n        tmp12 =tmp9 -tmp11 \n        tmp13 =1e-06 \n        tmp14 =tmp12 +tmp13 \n        tmp15 =tmp14 *tmp14 \n        tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ,R0_BLOCK ])\n        tmp18 =_tmp17 +tmp16 \n        _tmp17 =tl .where (r0_mask &xmask ,tmp18 ,_tmp17 )\n        tmp20 =tmp19 *tmp8 \n        tmp21 =tmp9 -tmp20 \n        tmp22 =tmp21 +tmp13 \n        tmp23 =tmp22 *tmp22 \n        tmp24 =tl .broadcast_to (tmp23 ,[XBLOCK ,R0_BLOCK ])\n        tmp26 =_tmp25 +tmp24 \n        _tmp25 =tl .where (r0_mask &xmask ,tmp26 ,_tmp25 )\n    tmp17 =tl .sum (_tmp17 ,1 )[:,None ]\n    tmp25 =tl .sum (_tmp25 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp17 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp25 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_mean_norm_sub_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp10 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =libdevice .sqrt (tmp0 )\n        tmp2 =1.0 \n        tmp3 =tmp1 +tmp2 \n        tmp5 =libdevice .sqrt (tmp4 )\n        tmp6 =tmp3 -tmp5 \n        tmp7 =0.0 \n        tmp8 =triton_helpers .maximum (tmp6 ,tmp7 )\n        tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n        tmp11 =_tmp10 +tmp9 \n        _tmp10 =tl .where (r0_mask ,tmp11 ,_tmp10 )\n    tmp10 =tl .sum (_tmp10 ,1 )[:,None ]\n    tmp12 =ks0 \n    tmp13 =tmp12 .to (tl .float32 )\n    tmp14 =tmp10 /tmp13 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp14 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax__to_copy_bernoulli_div_mul_nll_loss2d_forward_reflection_pad1d_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,ks0 ,load_seed_offset ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp9 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(ks0 *r0_1 +(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+x0 )))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+x0 )))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr1 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =0.5 \n        tmp3 =tmp1 <tmp2 \n        tmp4 =tmp3 .to (tl .float32 )\n        tmp5 =2.0 \n        tmp6 =tmp4 *tmp5 \n        tmp7 =tmp0 *tmp6 \n        tmp8 =tl .broadcast_to (tmp7 ,[XBLOCK ,R0_BLOCK ])\n        tmp10 =triton_helpers .maximum (_tmp9 ,tmp8 )\n        _tmp9 =tl .where (r0_mask &xmask ,tmp10 ,_tmp9 )\n    tmp9 =triton_helpers .max2 (_tmp9 ,1 )[:,None ]\n    _tmp22 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp11 =tl .load (in_ptr0 +(ks0 *r0_1 +(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+x0 )))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+x0 )))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp12 =tl .load (in_ptr1 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp13 =0.5 \n        tmp14 =tmp12 <tmp13 \n        tmp15 =tmp14 .to (tl .float32 )\n        tmp16 =2.0 \n        tmp17 =tmp15 *tmp16 \n        tmp18 =tmp11 *tmp17 \n        tmp19 =tmp18 -tmp9 \n        tmp20 =tl_math .exp (tmp19 )\n        tmp21 =tl .broadcast_to (tmp20 ,[XBLOCK ,R0_BLOCK ])\n        tmp23 =_tmp22 +tmp21 \n        _tmp22 =tl .where (r0_mask &xmask ,tmp23 ,_tmp22 )\n    tmp22 =tl .sum (_tmp22 ,1 )[:,None ]\n    tmp24 =tl .load (in_ptr2 +load_seed_offset )\n    tmp25 =x0 \n    tmp26 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp27 =ks2 \n    tmp28 =triton_helpers .randint64 (tmp24 ,(tmp25 ).to (tl .uint32 ),tmp26 ,tmp27 )\n    tmp29 =tl .full ([1 ,1 ],-100 ,tl .int64 )\n    tmp30 =tmp28 !=tmp29 \n    tmp31 =tl .where (tmp30 ,tmp28 ,tmp26 )\n    tmp32 =tmp31 +tmp27 \n    tmp33 =tmp31 <0 \n    tmp34 =tl .where (tmp33 ,tmp32 ,tmp31 )\n    tl .device_assert (((0 <=tmp34 )&(tmp34 <ks2 ))|~(xmask ),\"index out of bounds: 0 <= tmp34 < ks2\")\n    tmp36 =tl .load (in_ptr0 +(ks0 *tmp34 +(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+x0 )))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr1 +(tmp34 ),xmask ,eviction_policy ='evict_last')\n    tmp38 =0.5 \n    tmp39 =tmp37 <tmp38 \n    tmp40 =tmp39 .to (tl .float32 )\n    tmp41 =2.0 \n    tmp42 =tmp40 *tmp41 \n    tmp43 =tmp36 *tmp42 \n    tmp44 =tmp43 -tmp9 \n    tmp45 =tl_math .log (tmp22 )\n    tmp46 =tmp44 -tmp45 \n    tmp47 =-tmp46 \n    tmp48 =0.0 \n    tmp49 =tl .where (tmp30 ,tmp47 ,tmp48 )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp49 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_nll_loss2d_forward_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,load_seed_offset ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =_tmp2 +tmp1 \n        _tmp2 =tl .where (r0_mask ,tmp3 ,_tmp2 )\n    tmp2 =tl .sum (_tmp2 ,1 )[:,None ]\n    _tmp13 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int64 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp4 =tl .load (in_ptr1 +load_seed_offset )\n        tmp5 =r0_0 \n        tmp6 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp7 =ks1 \n        tmp8 =triton_helpers .randint64 (tmp4 ,(tmp5 ).to (tl .uint32 ),tmp6 ,tmp7 )\n        tmp9 =tl .full ([1 ,1 ],-100 ,tl .int64 )\n        tmp10 =tmp8 !=tmp9 \n        tmp11 =tmp10 .to (tl .int64 )\n        tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n        tmp14 =_tmp13 +tmp12 \n        _tmp13 =tl .where (r0_mask ,tmp14 ,_tmp13 )\n    tmp13 =tl .sum (_tmp13 ,1 )[:,None ]\n    tmp15 =tmp13 .to (tl .float32 )\n    tmp16 =tmp2 /tmp15 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp16 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ),(s0 ,1 ,s0 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,s0 ),(s0 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,s0 ),(s0 ,1 ),torch .float32 )\n\n        2 +(s1 //2 )\n        get_raw_stream (0 )\n        triton_red_fused_add_bernoulli_norm_sub_0 [grid (s0 )](buf0 ,arg2_1 ,buf1 ,buf7 ,buf8 ,0 ,32 ,10 ,18 ,XBLOCK =1 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n        buf9 =empty_strided_cuda ((),(),torch .float32 )\n        buf11 =buf9 ;del buf9 \n\n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_mean_norm_sub_1 [grid (1 )](buf11 ,buf7 ,buf8 ,10 ,1 ,10 ,XBLOCK =1 ,R0_BLOCK =16 ,num_warps =2 ,num_stages =1 )\n        del buf7 \n        del buf8 \n        buf2 =empty_strided_cuda ((1 ,1 ,4 +s1 ),(4 +s1 ,4 +s1 ,1 ),torch .float32 )\n        buf4 =buf2 ;del buf2 \n\n        triton_red_fused__log_softmax__to_copy_bernoulli_div_mul_nll_loss2d_forward_reflection_pad1d_2_xnumel =4 +s1 \n        get_raw_stream (0 )\n        triton_red_fused__log_softmax__to_copy_bernoulli_div_mul_nll_loss2d_forward_reflection_pad1d_2 [grid (triton_red_fused__log_softmax__to_copy_bernoulli_div_mul_nll_loss2d_forward_reflection_pad1d_2_xnumel )](buf4 ,arg2_1 ,buf1 ,buf0 ,32 ,1 ,10 ,36 ,10 ,XBLOCK =1 ,R0_BLOCK =16 ,num_warps =2 ,num_stages =1 )\n        del arg2_1 \n        del buf1 \n        buf5 =empty_strided_cuda ((),(),torch .float32 )\n        buf10 =buf5 ;del buf5 \n\n        4 +s1 \n        get_raw_stream (0 )\n        triton_red_fused_nll_loss2d_forward_3 [grid (1 )](buf10 ,buf4 ,buf0 ,1 ,10 ,1 ,36 ,XBLOCK =1 ,R0_BLOCK =64 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n        del buf4 \n    return (buf10 ,buf11 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =32 \n    arg2_1 =rand_strided ((1 ,10 ,32 ),(320 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "297c17d6-8a5a-4c26-97b2-5895446ea47a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad3d', 'ReplicationPad3d', 'PoissonNLLLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ConstantPad3d(padding=1, value=0.5)\n        self.pad2 = nn.ReplicationPad3d(padding=1)\n        self.pad3 = nn.ConstantPad3d(padding=2, value=0.25)\n        self.pad4 = nn.ReplicationPad3d(padding=2)\n        self.pad5 = nn.ConstantPad3d(padding=3, value=0.75)\n        self.loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Apply padding layers\n        x = self.pad1(x)\n        x = self.pad2(x)\n        x = self.pad3(x)\n        x = self.pad4(x)\n        x = self.pad5(x)\n        \n        # Compute the PoissonNLLLoss between the padded input and a target tensor\n        # For simplicity, we assume the target tensor is a tensor of ones with the same shape as x\n        target = torch.ones_like(x)\n        loss = self.loss(x, target)\n        \n        # Return the loss as the output\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input with shape (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_replication_pad3d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x6 =((xindex //ks0 )%ks1 )\n    x1 =((xindex //ks3 )%ks4 )\n    x0 =(xindex %ks3 )\n    x2 =((xindex //ks7 )%ks1 )\n    x3 =xindex //ks8 \n    x8 =xindex \n    tmp0 =(-2 )+x6 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =4 +ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =4 +ks5 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-2 )+x0 \n    tmp10 =tmp9 >=tmp1 \n    tmp11 =4 +ks6 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =(-1 )+((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-3 )+x6 ))+((-3 )+x6 )*(((-3 )+x6 )>(0 )))))+(((0 )*((0 )>=((-3 )+x6 ))+((-3 )+x6 )*(((-3 )+x6 )>(0 ))))*((((0 )*((0 )>=((-3 )+x6 ))+((-3 )+x6 )*(((-3 )+x6 )>(0 ))))<(1 +ks2 )))\n    tmp19 =tl .full ([1 ],0 ,tl .int64 )\n    tmp20 =tmp18 >=tmp19 \n    tmp21 =tl .broadcast_to (ks2 ,[XBLOCK ])\n    tmp22 =tmp18 <tmp21 \n    tmp23 =(-1 )+((1 +ks5 )*((1 +ks5 )<=(((0 )*((0 )>=((-3 )+x1 ))+((-3 )+x1 )*(((-3 )+x1 )>(0 )))))+(((0 )*((0 )>=((-3 )+x1 ))+((-3 )+x1 )*(((-3 )+x1 )>(0 ))))*((((0 )*((0 )>=((-3 )+x1 ))+((-3 )+x1 )*(((-3 )+x1 )>(0 ))))<(1 +ks5 )))\n    tmp24 =tmp23 >=tmp19 \n    tmp25 =tl .broadcast_to (ks5 ,[XBLOCK ])\n    tmp26 =tmp23 <tmp25 \n    tmp27 =(-1 )+((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-3 )+x0 ))+((-3 )+x0 )*(((-3 )+x0 )>(0 )))))+(((0 )*((0 )>=((-3 )+x0 ))+((-3 )+x0 )*(((-3 )+x0 )>(0 ))))*((((0 )*((0 )>=((-3 )+x0 ))+((-3 )+x0 )*(((-3 )+x0 )>(0 ))))<(1 +ks6 )))\n    tmp28 =tmp27 >=tmp19 \n    tmp29 =tl .broadcast_to (ks6 ,[XBLOCK ])\n    tmp30 =tmp27 <tmp29 \n    tmp31 =tmp20 &tmp22 \n    tmp32 =tmp31 &tmp24 \n    tmp33 =tmp32 &tmp26 \n    tmp34 =tmp33 &tmp28 \n    tmp35 =tmp34 &tmp30 \n    tmp36 =tmp35 &tmp17 \n    tmp37 =tl .load (in_ptr0 +((-1 )+((-1 )*ks6 )+ks6 *((1 +ks5 )*((1 +ks5 )<=(((0 )*((0 )>=((-3 )+x1 ))+((-3 )+x1 )*(((-3 )+x1 )>(0 )))))+(((0 )*((0 )>=((-3 )+x1 ))+((-3 )+x1 )*(((-3 )+x1 )>(0 ))))*((((0 )*((0 )>=((-3 )+x1 ))+((-3 )+x1 )*(((-3 )+x1 )>(0 ))))<(1 +ks5 )))+((-1 )*ks5 *ks6 )+ks5 *ks6 *((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-3 )+x2 ))+((-3 )+x2 )*(((-3 )+x2 )>(0 )))))+(((0 )*((0 )>=((-3 )+x2 ))+((-3 )+x2 )*(((-3 )+x2 )>(0 ))))*((((0 )*((0 )>=((-3 )+x2 ))+((-3 )+x2 )*(((-3 )+x2 )>(0 ))))<(1 +ks2 )))+ks2 *ks5 *ks6 *x3 +((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-3 )+x0 ))+((-3 )+x0 )*(((-3 )+x0 )>(0 )))))+(((0 )*((0 )>=((-3 )+x0 ))+((-3 )+x0 )*(((-3 )+x0 )>(0 ))))*((((0 )*((0 )>=((-3 )+x0 ))+((-3 )+x0 )*(((-3 )+x0 )>(0 ))))<(1 +ks6 )))),tmp36 &xmask ,eviction_policy ='evict_last',other =0.5 )\n    tmp38 =tl .full (tmp37 .shape ,0.25 ,tmp37 .dtype )\n    tmp39 =tl .where (tmp17 ,tmp37 ,tmp38 )\n    tl .store (out_ptr0 +(x8 ),tmp39 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_replication_pad3d_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x6 =((xindex //ks0 )%ks1 )\n    x1 =((xindex //ks3 )%ks4 )\n    x0 =(xindex %ks3 )\n    x2 =((xindex //ks7 )%ks1 )\n    x3 =xindex //ks8 \n    x8 =xindex \n    tmp0 =(-3 )+x6 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =12 +ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-3 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =12 +ks5 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-3 )+x0 \n    tmp10 =tmp9 >=tmp1 \n    tmp11 =12 +ks6 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =tl .load (in_ptr0 +(8 *((7 +ks5 )*((7 +ks5 )<=(((0 )*((0 )>=((-5 )+x1 ))+((-5 )+x1 )*(((-5 )+x1 )>(0 )))))+(((0 )*((0 )>=((-5 )+x1 ))+((-5 )+x1 )*(((-5 )+x1 )>(0 ))))*((((0 )*((0 )>=((-5 )+x1 ))+((-5 )+x1 )*(((-5 )+x1 )>(0 ))))<(7 +ks5 )))+64 *((7 +ks2 )*((7 +ks2 )<=(((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 )))))+(((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 ))))*((((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 ))))<(7 +ks2 )))+512 *x3 +ks6 *((7 +ks5 )*((7 +ks5 )<=(((0 )*((0 )>=((-5 )+x1 ))+((-5 )+x1 )*(((-5 )+x1 )>(0 )))))+(((0 )*((0 )>=((-5 )+x1 ))+((-5 )+x1 )*(((-5 )+x1 )>(0 ))))*((((0 )*((0 )>=((-5 )+x1 ))+((-5 )+x1 )*(((-5 )+x1 )>(0 ))))<(7 +ks5 )))+8 *ks5 *((7 +ks2 )*((7 +ks2 )<=(((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 )))))+(((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 ))))*((((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 ))))<(7 +ks2 )))+8 *ks6 *((7 +ks2 )*((7 +ks2 )<=(((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 )))))+(((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 ))))*((((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 ))))<(7 +ks2 )))+64 *ks2 *x3 +64 *ks5 *x3 +64 *ks6 *x3 +ks5 *ks6 *((7 +ks2 )*((7 +ks2 )<=(((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 )))))+(((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 ))))*((((0 )*((0 )>=((-5 )+x2 ))+((-5 )+x2 )*(((-5 )+x2 )>(0 ))))<(7 +ks2 )))+8 *ks2 *ks5 *x3 +8 *ks2 *ks6 *x3 +8 *ks5 *ks6 *x3 +ks2 *ks5 *ks6 *x3 +((7 +ks6 )*((7 +ks6 )<=(((0 )*((0 )>=((-5 )+x0 ))+((-5 )+x0 )*(((-5 )+x0 )>(0 )))))+(((0 )*((0 )>=((-5 )+x0 ))+((-5 )+x0 )*(((-5 )+x0 )>(0 ))))*((((0 )*((0 )>=((-5 )+x0 ))+((-5 )+x0 )*(((-5 )+x0 )>(0 ))))<(7 +ks6 )))),tmp17 &xmask ,eviction_policy ='evict_last',other =0.75 )\n    tl .store (out_ptr0 +(x8 ),tmp18 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_exp_mean_mul_ones_like_sub_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =46 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 )\n        tmp1 =5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(18 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//ks6 )%ks7 ))+324 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//ks4 )%ks5 ))+5832 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//(5832 +324 *ks1 +324 *ks2 +324 *ks3 +18 *ks1 *ks2 +18 *ks1 *ks3 +18 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+ks3 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//ks6 )%ks7 ))+18 *ks2 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//ks4 )%ks5 ))+18 *ks3 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//ks4 )%ks5 ))+324 *ks1 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//(5832 +324 *ks1 +324 *ks2 +324 *ks3 +18 *ks1 *ks2 +18 *ks1 *ks3 +18 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+324 *ks2 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//(5832 +324 *ks1 +324 *ks2 +324 *ks3 +18 *ks1 *ks2 +18 *ks1 *ks3 +18 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+324 *ks3 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//(5832 +324 *ks1 +324 *ks2 +324 *ks3 +18 *ks1 *ks2 +18 *ks1 *ks3 +18 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+ks2 *ks3 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//ks4 )%ks5 ))+18 *ks1 *ks2 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//(5832 +324 *ks1 +324 *ks2 +324 *ks3 +18 *ks1 *ks2 +18 *ks1 *ks3 +18 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+18 *ks1 *ks3 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//(5832 +324 *ks1 +324 *ks2 +324 *ks3 +18 *ks1 *ks2 +18 *ks1 *ks3 +18 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+18 *ks2 *ks3 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//(5832 +324 *ks1 +324 *ks2 +324 *ks3 +18 *ks1 *ks2 +18 *ks1 *ks3 +18 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+ks1 *ks2 *ks3 *((((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))//(5832 +324 *ks1 +324 *ks2 +324 *ks3 +18 *ks1 *ks2 +18 *ks1 *ks3 +18 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+(((r0_1 +x0 *((45 +5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//46 ))%ks6 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl_math .exp (tmp3 )\n        tmp5 =1.0 \n        tmp6 =tmp5 *tmp3 \n        tmp7 =tmp4 -tmp6 \n        tmp8 =tl .full (tmp7 .shape ,0 ,tmp7 .dtype )\n        tmp9 =tl .where (tmp2 ,tmp7 ,tmp8 )\n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =_tmp11 +tmp10 \n        _tmp11 =tl .where (r0_mask &xmask ,tmp12 ,_tmp11 )\n    tmp11 =tl .sum (_tmp11 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_exp_mean_mul_ones_like_sub_3 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =46 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =5832 *ks0 +324 *ks0 *ks1 +324 *ks0 *ks2 +324 *ks0 *ks3 +18 *ks0 *ks1 *ks2 +18 *ks0 *ks1 *ks3 +18 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tmp4 /tmp6 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp7 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        64 +8 *s2 +8 *s3 +s2 *s3 \n        8 +s1 \n        8 +s3 \n        8 +s2 \n        64 +8 *s2 +8 *s3 +s2 *s3 \n        512 +64 *s1 +64 *s2 +64 *s3 +8 *s1 *s2 +8 *s1 *s3 +8 *s2 *s3 +s1 *s2 *s3 \n        buf0 =empty_strided_cuda ((1 ,s0 ,8 +s1 ,8 +s2 ,8 +s3 ),(512 *s0 +64 *s0 *s1 +64 *s0 *s2 +64 *s0 *s3 +8 *s0 *s1 *s2 +8 *s0 *s1 *s3 +8 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,512 +64 *s1 +64 *s2 +64 *s3 +8 *s1 *s2 +8 *s1 *s3 +8 *s2 *s3 +s1 *s2 *s3 ,64 +8 *s2 +8 *s3 +s2 *s3 ,8 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_replication_pad3d_0_xnumel =512 *s0 +64 *s0 *s1 +64 *s0 *s2 +64 *s0 *s3 +8 *s0 *s1 *s2 +8 *s0 *s1 *s3 +8 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_replication_pad3d_0 [grid (triton_poi_fused_constant_pad_nd_replication_pad3d_0_xnumel )](arg4_1 ,buf0 ,1600 ,40 ,32 ,40 ,40 ,32 ,32 ,1600 ,64000 ,192000 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg4_1 \n        324 +18 *s2 +18 *s3 +s2 *s3 \n        18 +s1 \n        18 +s3 \n        18 +s2 \n        324 +18 *s2 +18 *s3 +s2 *s3 \n        5832 +324 *s1 +324 *s2 +324 *s3 +18 *s1 *s2 +18 *s1 *s3 +18 *s2 *s3 +s1 *s2 *s3 \n        buf1 =empty_strided_cuda ((1 ,s0 ,18 +s1 ,18 +s2 ,18 +s3 ),(5832 *s0 +324 *s0 *s1 +324 *s0 *s2 +324 *s0 *s3 +18 *s0 *s1 *s2 +18 *s0 *s1 *s3 +18 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,5832 +324 *s1 +324 *s2 +324 *s3 +18 *s1 *s2 +18 *s1 *s3 +18 *s2 *s3 +s1 *s2 *s3 ,324 +18 *s2 +18 *s3 +s2 *s3 ,18 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_replication_pad3d_1_xnumel =5832 *s0 +324 *s0 *s1 +324 *s0 *s2 +324 *s0 *s3 +18 *s0 *s1 *s2 +18 *s0 *s1 *s3 +18 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_replication_pad3d_1 [grid (triton_poi_fused_constant_pad_nd_replication_pad3d_1_xnumel )](buf0 ,buf1 ,2500 ,50 ,32 ,50 ,50 ,32 ,32 ,2500 ,125000 ,375000 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((46 ,),(1 ,),torch .float32 )\n\n        (45 +5832 *s0 +324 *s0 *s1 +324 *s0 *s2 +324 *s0 *s3 +18 *s0 *s1 *s2 +18 *s0 *s1 *s3 +18 *s0 *s2 *s3 +s0 *s1 *s2 *s3 )//46 \n        get_raw_stream (0 )\n        triton_red_fused_exp_mean_mul_ones_like_sub_2 [grid (46 )](buf1 ,buf2 ,3 ,32 ,32 ,32 ,2500 ,50 ,50 ,50 ,46 ,8153 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf1 \n        buf3 =empty_strided_cuda ((),(),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        get_raw_stream (0 )\n        triton_per_fused_exp_mean_mul_ones_like_sub_3 [grid (1 )](buf4 ,buf2 ,3 ,32 ,32 ,32 ,1 ,46 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf2 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "2c81cbd5-a1ff-4300-a2a2-2ccff662b11c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['BCEWithLogitsLoss', 'ZeroPad2d', 'ReflectionPad2d', 'Tanh', 'Mish', 'Softsign', 'SiLU', 'Tanhshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad = nn.ZeroPad2d(2)\n        self.reflection_pad = nn.ReflectionPad2d(2)\n        self.tanh = nn.Tanh()\n        self.mish = nn.Mish()\n        self.softsign = nn.Softsign()\n        self.silu = nn.SiLU()\n        self.tanhshrink = nn.Tanhshrink()\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        # Apply ZeroPad2d\n        x = self.zero_pad(x)\n        \n        # Apply ReflectionPad2d\n        x = self.reflection_pad(x)\n        \n        # Apply Tanh\n        x = self.tanh(x)\n        \n        # Apply Mish\n        x = self.mish(x)\n        \n        # Apply Softsign\n        x = self.softsign(x)\n        \n        # Apply SiLU\n        x = self.silu(x)\n        \n        # Apply Tanhshrink\n        x = self.tanhshrink(x)\n        \n        # Reshape for BCEWithLogitsLoss\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        target = torch.ones_like(x)  # Dummy target for BCEWithLogitsLoss\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with arbitrary shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_mish_reflection_pad2d_tanh_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-2 )+(tl .where (3 +ks2 +((-1 )*tl_math .abs (3 +ks2 +((-1 )*tl_math .abs ((-2 )+x1 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks2 +((-1 )*tl_math .abs ((-2 )+x1 ))))+2 *ks2 ,3 +ks2 +((-1 )*tl_math .abs (3 +ks2 +((-1 )*tl_math .abs ((-2 )+x1 ))))))\n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+(tl .where (3 +ks3 +((-1 )*tl_math .abs (3 +ks3 +((-1 )*tl_math .abs ((-2 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks3 +((-1 )*tl_math .abs ((-2 )+x0 ))))+2 *ks3 ,3 +ks3 +((-1 )*tl_math .abs (3 +ks3 +((-1 )*tl_math .abs ((-2 )+x0 ))))))\n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+((-2 )*ks3 )+ks3 *(tl .where (3 +ks2 +((-1 )*tl_math .abs (3 +ks2 +((-1 )*tl_math .abs ((-2 )+x1 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks2 +((-1 )*tl_math .abs ((-2 )+x1 ))))+2 *ks2 ,3 +ks2 +((-1 )*tl_math .abs (3 +ks2 +((-1 )*tl_math .abs ((-2 )+x1 ))))))+ks2 *ks3 *x2 +(tl .where (3 +ks3 +((-1 )*tl_math .abs (3 +ks3 +((-1 )*tl_math .abs ((-2 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks3 +((-1 )*tl_math .abs ((-2 )+x0 ))))+2 *ks3 ,3 +ks3 +((-1 )*tl_math .abs (3 +ks3 +((-1 )*tl_math .abs ((-2 )+x0 ))))))),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =libdevice .tanh (tmp12 )\n    tmp14 =20.0 \n    tmp15 =tmp13 >tmp14 \n    tmp16 =tl_math .exp (tmp13 )\n    tmp17 =libdevice .log1p (tmp16 )\n    tmp18 =tl .where (tmp15 ,tmp13 ,tmp17 )\n    tmp19 =libdevice .tanh (tmp18 )\n    tmp20 =tmp13 *tmp19 \n    tl .store (out_ptr0 +(x4 ),tmp20 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_binary_cross_entropy_with_logits_ones_like_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp24 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+32 *ks0 *x0 +4 *ks0 *ks1 *x0 +4 *ks0 *ks2 *x0 \n        tmp1 =64 *ks0 +8 *ks0 *ks1 +8 *ks0 *ks2 +ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(8 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+32 *ks0 *x0 +4 *ks0 *ks1 *x0 +4 *ks0 *ks2 *x0 )//ks3 )%ks4 ))+64 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+32 *ks0 *x0 +4 *ks0 *ks1 *x0 +4 *ks0 *ks2 *x0 )//(64 +8 *ks1 +8 *ks2 +ks1 *ks2 ))%ks0 ))+ks2 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+32 *ks0 *x0 +4 *ks0 *ks1 *x0 +4 *ks0 *ks2 *x0 )//ks3 )%ks4 ))+8 *ks1 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+32 *ks0 *x0 +4 *ks0 *ks1 *x0 +4 *ks0 *ks2 *x0 )//(64 +8 *ks1 +8 *ks2 +ks1 *ks2 ))%ks0 ))+8 *ks2 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+32 *ks0 *x0 +4 *ks0 *ks1 *x0 +4 *ks0 *ks2 *x0 )//(64 +8 *ks1 +8 *ks2 +ks1 *ks2 ))%ks0 ))+ks1 *ks2 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+32 *ks0 *x0 +4 *ks0 *ks1 *x0 +4 *ks0 *ks2 *x0 )//(64 +8 *ks1 +8 *ks2 +ks1 *ks2 ))%ks0 ))+(((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+32 *ks0 *x0 +4 *ks0 *ks1 *x0 +4 *ks0 *ks2 *x0 )%ks3 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl_math .abs (tmp3 )\n        tmp5 =1.0 \n        tmp6 =tmp4 +tmp5 \n        tmp7 =tmp3 /tmp6 \n        tmp8 =tl .sigmoid (tmp7 )\n        tmp9 =tmp7 *tmp8 \n        tmp10 =libdevice .tanh (tmp9 )\n        tmp11 =tmp9 -tmp10 \n        tmp12 =0.0 \n        tmp13 =tmp12 *tmp11 \n        tmp14 =triton_helpers .minimum (tmp12 ,tmp11 )\n        tmp15 =tl_math .abs (tmp11 )\n        tmp16 =-tmp15 \n        tmp17 =tl_math .exp (tmp16 )\n        tmp18 =libdevice .log1p (tmp17 )\n        tmp19 =tmp14 -tmp18 \n        tmp20 =tmp13 -tmp19 \n        tmp21 =tl .full (tmp20 .shape ,0 ,tmp20 .dtype )\n        tmp22 =tl .where (tmp2 ,tmp20 ,tmp21 )\n        tmp23 =tl .broadcast_to (tmp22 ,[XBLOCK ,R0_BLOCK ])\n        tmp25 =_tmp24 +tmp23 \n        _tmp24 =tl .where (r0_mask &xmask ,tmp25 ,_tmp24 )\n    tmp24 =tl .sum (_tmp24 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp24 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_binary_cross_entropy_with_logits_ones_like_2 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =64 *ks0 +8 *ks0 *ks1 +8 *ks0 *ks2 +ks0 *ks1 *ks2 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp6 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        8 +s2 \n        8 +s1 \n        64 +8 *s1 +8 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,8 +s1 ,8 +s2 ),(64 *s0 +8 *s0 *s1 +8 *s0 *s2 +s0 *s1 *s2 ,64 +8 *s1 +8 *s2 +s1 *s2 ,8 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_mish_reflection_pad2d_tanh_0_xnumel =64 *s0 +8 *s0 *s1 +8 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_mish_reflection_pad2d_tanh_0 [grid (triton_poi_fused_constant_pad_nd_mish_reflection_pad2d_tanh_0_xnumel )](arg3_1 ,buf0 ,72 ,72 ,64 ,64 ,5184 ,15552 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((2 ,),(1 ,),torch .float32 )\n\n        32 *s0 +4 *s0 *s1 +4 *s0 *s2 +((1 +s0 *s1 *s2 )//2 )\n        get_raw_stream (0 )\n        triton_red_fused_binary_cross_entropy_with_logits_ones_like_1 [grid (2 )](buf0 ,buf1 ,3 ,64 ,64 ,72 ,72 ,2 ,7776 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((),(),torch .float32 )\n        buf3 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_per_fused_binary_cross_entropy_with_logits_ones_like_2 [grid (1 )](buf3 ,buf1 ,3 ,64 ,64 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "2d433465-a878-4bd7-a5bc-f39fd6f5dc23",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool2d', 'CosineSimilarity', 'CELU', 'CircularPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.celu1 = nn.CELU()\n        self.celu2 = nn.CELU()\n        self.circular_pad1 = nn.CircularPad1d(padding=1)\n        self.circular_pad2 = nn.CircularPad1d(padding=1)\n\n    def forward(self, x):\n        # Apply MaxPool2d\n        x = self.maxpool1(x)\n        x = self.maxpool2(x)\n        \n        # Reshape for 1D operations\n        x = x.view(x.size(0), -1)  # Flatten to 1D\n        \n        # Apply CircularPad1d\n        x = x.unsqueeze(1)  # Add a dummy dimension for 1D padding\n        x = self.circular_pad1(x)\n        x = self.circular_pad2(x)\n        x = x.squeeze(1)  # Remove the dummy dimension\n        \n        # Apply CELU\n        x = self.celu1(x)\n        x = self.celu2(x)\n        \n        # Reshape back to 2D for CosineSimilarity\n        x = x.view(x.size(0), -1, 1)  # Reshape to (batch_size, features, 1)\n        y = torch.ones_like(x)  # Create a tensor of ones for CosineSimilarity\n        x = F.cosine_similarity(x, y, dim=1)  # Apply CosineSimilarity\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels and 64x64 spatial dimensions\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\n    tl .store (out_ptr0 +(x3 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =x0 +ks0 *(ks1 //4 )*(ks2 //4 )\n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .broadcast_to (1 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ])\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =tl .load (in_ptr0 +(2 *((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))%(ks2 //4 )))+2 *ks3 *(((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))//(ks2 //4 ))%(ks1 //4 )))+ks3 *ks4 *(((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))//((ks1 //4 )*(ks2 //4 )))%ks0 ))),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp11 =tl .load (in_ptr0 +(1 +2 *((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))%(ks2 //4 )))+2 *ks3 *(((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))//(ks2 //4 ))%(ks1 //4 )))+ks3 *ks4 *(((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))//((ks1 //4 )*(ks2 //4 )))%ks0 ))),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\n    tmp13 =tl .load (in_ptr0 +(ks3 +2 *((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))%(ks2 //4 )))+2 *ks3 *(((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))//(ks2 //4 ))%(ks1 //4 )))+ks3 *ks4 *(((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))//((ks1 //4 )*(ks2 //4 )))%ks0 ))),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp14 =triton_helpers .maximum (tmp13 ,tmp12 )\n    tmp15 =tl .load (in_ptr0 +(1 +ks3 +2 *((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))%(ks2 //4 )))+2 *ks3 *(((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))//(ks2 //4 ))%(ks1 //4 )))+ks3 *ks4 *(((((-1 )+x0 +ks0 *(ks1 //4 )*(ks2 //4 ))//((ks1 //4 )*(ks2 //4 )))%ks0 ))),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp16 =triton_helpers .maximum (tmp15 ,tmp14 )\n    tmp17 =tl .full (tmp16 .shape ,0.0 ,tmp16 .dtype )\n    tmp18 =tl .where (tmp9 ,tmp16 ,tmp17 )\n    tmp19 =float (\"nan\")\n    tmp20 =tl .where (tmp8 ,tmp18 ,tmp19 )\n    tmp21 =tl .full (tmp20 .shape ,0.0 ,tmp20 .dtype )\n    tmp22 =tl .where (tmp2 ,tmp20 ,tmp21 )\n    tmp23 =tmp0 >=tmp1 \n    tmp24 =1 +ks0 *(ks1 //4 )*(ks2 //4 )\n    tmp25 =tmp0 <tmp24 \n    tmp26 =tmp23 &tmp25 \n    tmp27 =tl .load (in_ptr0 +(2 *((((-1 )+x0 )%(ks2 //4 )))+2 *ks3 *(((((-1 )+x0 )//(ks2 //4 ))%(ks1 //4 )))+ks3 *ks4 *(((((-1 )+x0 )//((ks1 //4 )*(ks2 //4 )))%ks0 ))),tmp26 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp28 =tl .load (in_ptr0 +(1 +2 *((((-1 )+x0 )%(ks2 //4 )))+2 *ks3 *(((((-1 )+x0 )//(ks2 //4 ))%(ks1 //4 )))+ks3 *ks4 *(((((-1 )+x0 )//((ks1 //4 )*(ks2 //4 )))%ks0 ))),tmp26 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp29 =triton_helpers .maximum (tmp28 ,tmp27 )\n    tmp30 =tl .load (in_ptr0 +(ks3 +2 *((((-1 )+x0 )%(ks2 //4 )))+2 *ks3 *(((((-1 )+x0 )//(ks2 //4 ))%(ks1 //4 )))+ks3 *ks4 *(((((-1 )+x0 )//((ks1 //4 )*(ks2 //4 )))%ks0 ))),tmp26 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp31 =triton_helpers .maximum (tmp30 ,tmp29 )\n    tmp32 =tl .load (in_ptr0 +(1 +ks3 +2 *((((-1 )+x0 )%(ks2 //4 )))+2 *ks3 *(((((-1 )+x0 )//(ks2 //4 ))%(ks1 //4 )))+ks3 *ks4 *(((((-1 )+x0 )//((ks1 //4 )*(ks2 //4 )))%ks0 ))),tmp26 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =triton_helpers .maximum (tmp32 ,tmp31 )\n    tmp34 =tl .full (tmp33 .shape ,0.0 ,tmp33 .dtype )\n    tmp35 =tl .where (tmp26 ,tmp33 ,tmp34 )\n    tmp36 =float (\"nan\")\n    tmp37 =tl .where (tmp26 ,tmp35 ,tmp36 )\n    tmp38 =tl .where (tmp2 ,tmp22 ,tmp37 )\n    tl .store (out_ptr0 +(x0 ),tmp38 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_clamp_min_copy_div_linalg_vector_norm_mul_ones_like_sum_2 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp86 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp97 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =r0_0 \n        tmp1 =3 +ks0 *(ks1 //4 )*(ks2 //4 )\n        tmp2 =tmp0 >=tmp1 \n        tmp3 =tl .broadcast_to ((-2 )+r0_0 +((-1 )*ks0 *(ks1 //4 )*(ks2 //4 )),[XBLOCK ,R0_BLOCK ])\n        tmp4 =tl .full ([1 ,1 ],1 ,tl .int64 )\n        tmp5 =tmp3 <tmp4 \n        tmp6 =tmp5 &tmp2 \n        tmp7 =tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])\n        tmp8 =tl .full ([1 ,1 ],1 ,tl .int64 )\n        tmp9 =tmp7 >=tmp8 \n        tmp10 =tl .broadcast_to (3 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])\n        tmp11 =tmp7 <tmp10 \n        tmp12 =tmp9 &tmp11 \n        tmp13 =tmp12 &tmp6 \n        tmp14 =tl .broadcast_to ((-1 )+r0_0 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =tl .broadcast_to (1 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])\n        tmp16 =tmp14 >=tmp15 \n        tmp17 =tmp16 &tmp13 \n        tmp18 =tl .load (in_ptr0 +(tl .full ([XBLOCK ,R0_BLOCK ],1 ,tl .int32 )),tmp17 ,eviction_policy ='evict_last',other =0.0 )\n        tmp19 =tl .load (in_ptr0 +(tl .broadcast_to ((-1 )+r0_0 ,[XBLOCK ,R0_BLOCK ])),r0_mask &tmp13 ,eviction_policy ='evict_last',other =0.0 )\n        tmp20 =tl .where (tmp16 ,tmp18 ,tmp19 )\n        tmp21 =tl .full (tmp20 .shape ,0.0 ,tmp20 .dtype )\n        tmp22 =tl .where (tmp13 ,tmp20 ,tmp21 )\n        tmp23 =float (\"nan\")\n        tmp24 =tl .where (tmp12 ,tmp22 ,tmp23 )\n        tmp25 =tl .full (tmp24 .shape ,0.0 ,tmp24 .dtype )\n        tmp26 =tl .where (tmp6 ,tmp24 ,tmp25 )\n        tmp27 =tmp3 >=tmp4 \n        tmp28 =tl .broadcast_to (3 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])\n        tmp29 =tmp3 <tmp28 \n        tmp30 =tmp27 &tmp29 \n        tmp31 =tmp30 &tmp2 \n        tmp32 =tl .broadcast_to ((-3 )+r0_0 +((-1 )*ks0 *(ks1 //4 )*(ks2 //4 )),[XBLOCK ,R0_BLOCK ])\n        tmp33 =tl .broadcast_to (1 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])\n        tmp34 =tmp32 >=tmp33 \n        tmp35 =tmp34 &tmp31 \n        tmp36 =tl .load (in_ptr0 +(tl .full ([XBLOCK ,R0_BLOCK ],1 ,tl .int32 )),tmp35 ,eviction_policy ='evict_last',other =0.0 )\n        tmp37 =tl .load (in_ptr0 +(tl .broadcast_to ((-3 )+r0_0 +((-1 )*ks0 *(ks1 //4 )*(ks2 //4 )),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp31 ,eviction_policy ='evict_last',other =0.0 )\n        tmp38 =tl .where (tmp34 ,tmp36 ,tmp37 )\n        tmp39 =tl .full (tmp38 .shape ,0.0 ,tmp38 .dtype )\n        tmp40 =tl .where (tmp31 ,tmp38 ,tmp39 )\n        tmp41 =float (\"nan\")\n        tmp42 =tl .where (tmp30 ,tmp40 ,tmp41 )\n        tmp43 =tl .where (tmp5 ,tmp26 ,tmp42 )\n        tmp44 =tl .full (tmp43 .shape ,0.0 ,tmp43 .dtype )\n        tmp45 =tl .where (tmp2 ,tmp43 ,tmp44 )\n        tmp46 =tl .full ([1 ,1 ],1 ,tl .int64 )\n        tmp47 =tmp0 <tmp46 \n        tmp48 =tl .broadcast_to (2 +r0_0 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])\n        tmp49 =tl .full ([1 ,1 ],1 ,tl .int64 )\n        tmp50 =tmp48 >=tmp49 \n        tmp51 =tl .broadcast_to (3 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])\n        tmp52 =tmp48 <tmp51 \n        tmp53 =tmp50 &tmp52 \n        tmp54 =tmp53 &tmp47 \n        tmp55 =tl .broadcast_to (1 +r0_0 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])\n        tmp56 =tl .broadcast_to (1 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])\n        tmp57 =tmp55 >=tmp56 \n        tmp58 =tmp57 &tmp54 \n        tmp59 =tl .load (in_ptr0 +(tl .full ([XBLOCK ,R0_BLOCK ],1 ,tl .int32 )),tmp58 ,eviction_policy ='evict_last',other =0.0 )\n        tmp60 =tl .load (in_ptr0 +(tl .broadcast_to (1 +r0_0 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp54 ,eviction_policy ='evict_last',other =0.0 )\n        tmp61 =tl .where (tmp57 ,tmp59 ,tmp60 )\n        tmp62 =tl .full (tmp61 .shape ,0.0 ,tmp61 .dtype )\n        tmp63 =tl .where (tmp54 ,tmp61 ,tmp62 )\n        tmp64 =float (\"nan\")\n        tmp65 =tl .where (tmp53 ,tmp63 ,tmp64 )\n        tmp66 =tl .full (tmp65 .shape ,0.0 ,tmp65 .dtype )\n        tmp67 =tl .where (tmp47 ,tmp65 ,tmp66 )\n        tmp68 =tmp0 >=tmp46 \n        tmp69 =tmp0 <tmp1 \n        tmp70 =tmp68 &tmp69 \n        tmp71 =tl .broadcast_to ((-1 )+r0_0 ,[XBLOCK ,R0_BLOCK ])\n        tmp72 =tl .broadcast_to (1 +ks0 *(ks1 //4 )*(ks2 //4 ),[XBLOCK ,R0_BLOCK ])\n        tmp73 =tmp71 >=tmp72 \n        tmp74 =tmp73 &tmp70 \n        tmp75 =tl .load (in_ptr0 +(tl .full ([XBLOCK ,R0_BLOCK ],1 ,tl .int32 )),tmp74 ,eviction_policy ='evict_last',other =0.0 )\n        tmp76 =tl .load (in_ptr0 +(tl .broadcast_to ((-1 )+r0_0 ,[XBLOCK ,R0_BLOCK ])),r0_mask &tmp70 ,eviction_policy ='evict_first',other =0.0 )\n        tmp77 =tl .where (tmp73 ,tmp75 ,tmp76 )\n        tmp78 =tl .full (tmp77 .shape ,0.0 ,tmp77 .dtype )\n        tmp79 =tl .where (tmp70 ,tmp77 ,tmp78 )\n        tmp80 =float (\"nan\")\n        tmp81 =tl .where (tmp70 ,tmp79 ,tmp80 )\n        tmp82 =tl .where (tmp47 ,tmp67 ,tmp81 )\n        tmp83 =tl .where (tmp2 ,tmp45 ,tmp82 )\n        tmp84 =1.0 \n        tmp85 =tl .broadcast_to (tmp84 ,[XBLOCK ,R0_BLOCK ])\n        tmp87 =_tmp86 +tmp85 \n        _tmp86 =tl .where (r0_mask ,tmp87 ,_tmp86 )\n        tmp88 =0.0 \n        tmp89 =tmp83 >tmp88 \n        tmp90 =libdevice .expm1 (tmp83 )\n        tmp91 =tl .where (tmp89 ,tmp83 ,tmp90 )\n        tmp92 =tmp91 >tmp88 \n        tmp93 =libdevice .expm1 (tmp91 )\n        tmp94 =tl .where (tmp92 ,tmp91 ,tmp93 )\n        tmp95 =tmp94 *tmp94 \n        tmp96 =tl .broadcast_to (tmp95 ,[XBLOCK ,R0_BLOCK ])\n        tmp98 =_tmp97 +tmp96 \n        _tmp97 =tl .where (r0_mask ,tmp98 ,_tmp97 )\n        tl .store (out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp83 ,r0_mask )\n    tmp86 =tl .sum (_tmp86 ,1 )[:,None ]\n    tmp97 =tl .sum (_tmp97 ,1 )[:,None ]\n    _tmp117 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp99 =tl .load (out_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp100 =0.0 \n        tmp101 =tmp99 >tmp100 \n        tmp102 =libdevice .expm1 (tmp99 )\n        tmp103 =tl .where (tmp101 ,tmp99 ,tmp102 )\n        tmp104 =tmp103 >tmp100 \n        tmp105 =libdevice .expm1 (tmp103 )\n        tmp106 =tl .where (tmp104 ,tmp103 ,tmp105 )\n        tmp107 =libdevice .sqrt (tmp97 )\n        tmp108 =1e-08 \n        tmp109 =triton_helpers .maximum (tmp107 ,tmp108 )\n        tmp110 =tmp106 /tmp109 \n        tmp111 =libdevice .sqrt (tmp86 )\n        tmp112 =triton_helpers .maximum (tmp111 ,tmp108 )\n        tmp113 =1.0 \n        tmp114 =tmp113 /tmp112 \n        tmp115 =tmp110 *tmp114 \n        tmp116 =tl .broadcast_to (tmp115 ,[XBLOCK ,R0_BLOCK ])\n        tmp118 =_tmp117 +tmp116 \n        _tmp117 =tl .where (r0_mask ,tmp118 ,_tmp117 )\n    tmp117 =tl .sum (_tmp117 ,1 )[:,None ]\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp117 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //2 \n        s1 //2 \n        (s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_max_pool2d_with_indices_0_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_0 [grid (triton_poi_fused_max_pool2d_with_indices_0_xnumel )](arg3_1 ,buf0 ,32 ,32 ,1024 ,64 ,64 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf2 =empty_strided_cuda ((1 ,1 ,2 +s0 *(s1 //4 )*(s2 //4 )),(2 +s0 *(s1 //4 )*(s2 //4 ),2 +s0 *(s1 //4 )*(s2 //4 ),1 ),torch .float32 )\n\n        triton_poi_fused_copy_1_xnumel =2 +s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_copy_1 [grid (triton_poi_fused_copy_1_xnumel )](buf0 ,buf2 ,3 ,64 ,64 ,32 ,32 ,770 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        buf4 =empty_strided_cuda ((1 ,1 ,4 +s0 *(s1 //4 )*(s2 //4 )),(4 +s0 *(s1 //4 )*(s2 //4 ),4 +s0 *(s1 //4 )*(s2 //4 ),1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n        buf7 =reinterpret_tensor (buf5 ,(1 ,1 ),(1 ,1 ),0 );del buf5 \n\n        4 +s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_red_fused_clamp_min_copy_div_linalg_vector_norm_mul_ones_like_sum_2 [grid (1 )](buf7 ,buf2 ,buf4 ,3 ,64 ,64 ,1 ,772 ,XBLOCK =1 ,R0_BLOCK =1024 ,num_warps =8 ,num_stages =1 )\n        del buf2 \n        del buf4 \n    return (buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "2d565129-b376-4b15-a957-514c8b964636",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MarginRankingLoss', 'Softmax2d', 'HingeEmbeddingLoss', 'HuberLoss', 'AlphaDropout', 'LayerNorm']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.alpha_dropout = nn.AlphaDropout(p=0.5)\n        self.layer_norm = nn.LayerNorm([10, 10])\n        self.softmax2d = nn.Softmax2d()\n        self.margin_ranking_loss = nn.MarginRankingLoss()\n        self.hinge_embedding_loss = nn.HingeEmbeddingLoss()\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Apply AlphaDropout\n        x = self.alpha_dropout(x)\n        \n        # Reshape to fit LayerNorm input shape\n        x = x.view(-1, 10, 10)\n        x = self.layer_norm(x)\n        \n        # Reshape to fit Softmax2d input shape\n        x = x.view(-1, 1, 10, 10)\n        x = self.softmax2d(x)\n        \n        # Generate random tensors for loss functions\n        input1 = torch.randn_like(x)\n        input2 = torch.randn_like(x)\n        target = torch.randn_like(x)\n        \n        # Apply MarginRankingLoss\n        margin_loss = self.margin_ranking_loss(input1, input2, target)\n        \n        # Apply HingeEmbeddingLoss\n        hinge_loss = self.hinge_embedding_loss(input1, target)\n        \n        # Apply HuberLoss\n        huber_loss = self.huber_loss(input1, target)\n        \n        # Combine losses (just for demonstration)\n        total_loss = margin_loss + hinge_loss + huber_loss\n        \n        return total_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 100).cuda()  # Arbitrary input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_clamp_min_fill_huber_loss_mean_mul_ne_neg_randn_like_sub_where_zeros_like_0 (in_out_ptr0 ,in_ptr0 ,load_seed_offset ,load_seed_offset1 ,load_seed_offset2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =100 \n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_0 \n    tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =tl .load (in_ptr0 +load_seed_offset1 )\n    tmp4 =tl .randn (tmp3 ,(tmp1 ).to (tl .uint32 ))\n    tmp5 =tl .load (in_ptr0 +load_seed_offset2 )\n    tmp6 =tl .randn (tmp5 ,(tmp1 ).to (tl .uint32 ))\n    tmp7 =-tmp2 \n    tmp8 =tmp4 -tmp6 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =0.0 \n    tmp11 =tmp9 +tmp10 \n    tmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n    tmp15 =tl .where (r0_mask ,tmp13 ,0 )\n    tmp16 =tl .sum (tmp15 ,1 )[:,None ]\n    tmp17 =1.0 \n    tmp18 =tmp2 !=tmp17 \n    tmp19 =tmp17 -tmp4 \n    tmp20 =triton_helpers .maximum (tmp19 ,tmp10 )\n    tmp21 =tl .where (tmp18 ,tmp20 ,tmp10 )\n    tmp22 =-1.0 \n    tmp23 =tmp2 !=tmp22 \n    tmp24 =tl .where (tmp23 ,tmp4 ,tmp10 )\n    tmp25 =tmp21 +tmp24 \n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (r0_mask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =tmp4 -tmp2 \n    tmp31 =tl_math .abs (tmp30 )\n    tmp32 =tmp31 <tmp17 \n    tmp33 =0.5 \n    tmp34 =tmp31 *tmp33 \n    tmp35 =tmp34 *tmp31 \n    tmp36 =tmp31 -tmp33 \n    tmp37 =tmp36 *tmp17 \n    tmp38 =tl .where (tmp32 ,tmp35 ,tmp37 )\n    tmp39 =tl .broadcast_to (tmp38 ,[XBLOCK ,R0_BLOCK ])\n    tmp41 =tl .where (r0_mask ,tmp39 ,0 )\n    tmp42 =tl .sum (tmp41 ,1 )[:,None ]\n    tmp43 =100.0 \n    tmp44 =tmp16 /tmp43 \n    tmp45 =tmp29 /tmp43 \n    tmp46 =tmp44 +tmp45 \n    tmp47 =tmp42 /tmp43 \n    tmp48 =tmp46 +tmp47 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp48 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    assert_size_stride (arg1_1 ,(1 ,100 ),(100 ,1 ))\n    assert_size_stride (arg2_1 ,(10 ,10 ),(10 ,1 ))\n    assert_size_stride (arg3_1 ,(10 ,10 ),(10 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((4 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[4 ],out =buf0 )\n        buf5 =empty_strided_cuda ((),(),torch .float32 )\n        buf8 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_clamp_min_fill_huber_loss_mean_mul_ne_neg_randn_like_sub_where_zeros_like_0 [grid (1 )](buf8 ,buf0 ,3 ,1 ,2 ,1 ,100 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n    return (buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =100 \n    arg1_1 =rand_strided ((1 ,100 ),(100 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    arg2_1 =rand_strided ((10 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    arg3_1 =rand_strided ((10 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "2f7012a5-8b2c-4a75-a27a-6b6ec12518ed",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LPPool2d', 'SoftMarginLoss', 'Dropout3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lppool1 = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)\n        self.dropout3d1 = nn.Dropout3d(p=0.5)\n        self.lppool2 = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)\n        self.dropout3d2 = nn.Dropout3d(p=0.5)\n        self.softmarginloss = nn.SoftMarginLoss()\n\n    def forward(self, x):\n        # Assuming input is 4D (batch, channels, height, width)\n        x = self.lppool1(x)\n        x = x.unsqueeze(1)  # Add a dimension to make it 5D for Dropout3d\n        x = self.dropout3d1(x)\n        x = x.squeeze(1)  # Remove the added dimension\n        x = self.lppool2(x)\n        x = x.unsqueeze(1)  # Add a dimension to make it 5D for Dropout3d\n        x = self.dropout3d2(x)\n        x = x.squeeze(1)  # Remove the added dimension\n        # For SoftMarginLoss, we need a target tensor of the same shape as input\n        # Here we assume the target is a tensor of ones for demonstration\n        target = torch.ones_like(x)\n        loss = self.softmarginloss(x, target)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_pow_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp26 =tl .load (in_ptr1 +(0 ))\n    tmp27 =tl .broadcast_to (tmp26 ,[XBLOCK ])\n    tmp1 =tmp0 *tmp0 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tmp3 +tmp1 \n    tmp6 =tmp5 *tmp5 \n    tmp7 =tmp6 +tmp4 \n    tmp9 =tmp8 *tmp8 \n    tmp10 =tmp9 +tmp7 \n    tmp11 =0.25 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tl .full ([1 ],0 ,tl .int32 )\n    tmp14 =tmp13 <tmp12 \n    tmp15 =tmp14 .to (tl .int8 )\n    tmp16 =tmp12 <tmp13 \n    tmp17 =tmp16 .to (tl .int8 )\n    tmp18 =tmp15 -tmp17 \n    tmp19 =tmp18 .to (tmp12 .dtype )\n    tmp20 =tl_math .abs (tmp12 )\n    tmp21 =triton_helpers .maximum (tmp13 ,tmp20 )\n    tmp22 =tmp19 *tmp21 \n    tmp23 =4.0 \n    tmp24 =tmp22 *tmp23 \n    tmp25 =libdevice .sqrt (tmp24 )\n    tmp28 =0.5 \n    tmp29 =tmp27 <tmp28 \n    tmp30 =tmp29 .to (tl .float32 )\n    tmp31 =2.0 \n    tmp32 =tmp30 *tmp31 \n    tmp33 =tmp25 *tmp32 \n    tmp34 =tmp33 *tmp33 \n    tl .store (out_ptr0 +(x3 ),tmp34 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_bernoulli_ones_like_soft_margin_loss_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,load_seed_offset ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    _tmp36 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =(r0_index %ks1 )\n        r0_1 =((r0_index //ks1 )%ks2 )\n        r0_2 =r0_index //ks3 \n        tmp3 =tl .load (in_ptr1 +(2 *r0_0 +2 *ks4 *r0_1 +ks4 *ks5 *r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +(1 +2 *r0_0 +2 *ks4 *r0_1 +ks4 *ks5 *r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp6 =tl .load (in_ptr1 +(ks4 +2 *r0_0 +2 *ks4 *r0_1 +ks4 *ks5 *r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp8 =tl .load (in_ptr1 +(1 +ks4 +2 *r0_0 +2 *ks4 *r0_1 +ks4 *ks5 *r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tmp4 +tmp3 \n        tmp7 =tmp6 +tmp5 \n        tmp9 =tmp8 +tmp7 \n        tmp10 =0.25 \n        tmp11 =tmp9 *tmp10 \n        tmp12 =tmp1 <tmp11 \n        tmp13 =tmp12 .to (tl .int8 )\n        tmp14 =tmp11 <tmp1 \n        tmp15 =tmp14 .to (tl .int8 )\n        tmp16 =tmp13 -tmp15 \n        tmp17 =tmp16 .to (tmp11 .dtype )\n        tmp18 =tl_math .abs (tmp11 )\n        tmp19 =triton_helpers .maximum (tmp1 ,tmp18 )\n        tmp20 =tmp17 *tmp19 \n        tmp21 =4.0 \n        tmp22 =tmp20 *tmp21 \n        tmp23 =libdevice .sqrt (tmp22 )\n        tmp24 =0.5 \n        tmp25 =tmp2 <tmp24 \n        tmp26 =tmp25 .to (tl .float32 )\n        tmp27 =2.0 \n        tmp28 =tmp26 *tmp27 \n        tmp29 =tmp23 *tmp28 \n        tmp30 =-tmp29 \n        tmp31 =1.0 \n        tmp32 =tmp30 *tmp31 \n        tmp33 =tl_math .exp (tmp32 )\n        tmp34 =libdevice .log1p (tmp33 )\n        tmp35 =tl .broadcast_to (tmp34 ,[XBLOCK ,R0_BLOCK ])\n        tmp37 =_tmp36 +tmp35 \n        _tmp36 =tl .where (r0_mask ,tmp37 ,_tmp36 )\n    tmp36 =tl .sum (_tmp36 ,1 )[:,None ]\n    tmp38 =3 *ks1 *ks2 \n    tmp39 =tmp38 .to (tl .float32 )\n    tmp40 =tmp36 /tmp39 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp40 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,3 ,s0 ,s1 ),(3 *s0 *s1 ,s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,1 ,1 ,1 ,1 ),(1 ,1 ,1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (1 )](buf0 ,buf1 ,0 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        s1 //2 \n        s0 //2 \n        (s0 //2 )*(s1 //2 )\n        buf2 =empty_strided_cuda ((1 ,3 ,s0 //2 ,s1 //2 ),(3 *(s0 //2 )*(s1 //2 ),(s0 //2 )*(s1 //2 ),s1 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_pow_1_xnumel =3 *(s0 //2 )*(s1 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_pow_1 [grid (triton_poi_fused_pow_1_xnumel )](arg2_1 ,buf1 ,buf2 ,32 ,32 ,1024 ,64 ,64 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        s1 //4 \n        s0 //4 \n        (s0 //4 )*(s1 //4 )\n        buf3 =buf1 ;del buf1 \n        buf4 =reinterpret_tensor (buf3 ,(),(),0 );del buf3 \n        buf5 =buf4 ;del buf4 \n\n        3 *(s0 //4 )*(s1 //4 )\n        get_raw_stream (0 )\n        triton_red_fused_bernoulli_ones_like_soft_margin_loss_2 [grid (1 )](buf5 ,buf0 ,buf2 ,1 ,16 ,16 ,256 ,32 ,32 ,1 ,768 ,XBLOCK =1 ,R0_BLOCK =1024 ,num_warps =8 ,num_stages =1 )\n        del buf0 \n        del buf2 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "301fb261-a75c-4f40-af64-9b5068d8ceeb",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxUnpool3d', 'ConstantPad2d', 'AdaptiveAvgPool3d', 'ModuleDict', 'HuberLoss', 'ReflectionPad2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.max_unpool3d = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.constant_pad2d = nn.ConstantPad2d(padding=1, value=0)\n        self.adaptive_avg_pool3d = nn.AdaptiveAvgPool3d(output_size=(8, 8, 8))\n        self.module_dict = nn.ModuleDict({\n            'conv1': nn.Conv3d(1, 10, kernel_size=3),\n            'conv2': nn.Conv3d(10, 20, kernel_size=3),\n        })\n        self.huber_loss = nn.HuberLoss()\n        self.reflection_pad2d = nn.ReflectionPad2d(padding=1)\n\n    def forward(self, x):\n        # Assuming x is a 5D tensor (batch, channels, depth, height, width)\n        # Apply MaxUnpool3d\n        x, indices = F.max_pool3d_with_indices(x, kernel_size=2, stride=2)\n        x = self.max_unpool3d(x, indices)\n\n        # Reshape to 4D tensor for ConstantPad2d and ReflectionPad2d\n        x = x.view(x.size(0), x.size(1), x.size(2), -1)  # Flatten depth dimension\n        x = self.constant_pad2d(x)\n        x = self.reflection_pad2d(x)\n\n        # Reshape back to 5D tensor for AdaptiveAvgPool3d\n        x = x.view(x.size(0), x.size(1), x.size(2), x.size(3), -1)  # Unflatten depth dimension\n        x = self.adaptive_avg_pool3d(x)\n\n        # Apply ModuleDict layers\n        x = self.module_dict['conv1'](x)\n        x = F.relu(x)\n        x = self.module_dict['conv2'](x)\n        x = F.relu(x)\n\n        # Compute Huber loss (assuming target is a tensor of zeros for demonstration)\n        target = torch.zeros_like(x)\n        loss = self.huber_loss(x, target)\n\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 16, 16, 16).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp8 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =8 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )\n    tmp4 =tmp2 +tmp3 \n    tmp5 =tmp2 <0 \n    tmp6 =tl .where (tmp5 ,tmp4 ,tmp2 )\n    tl .device_assert (((0 <=tmp6 )&(tmp6 <8 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )))|~(xmask ),\"index out of bounds: 0 <= tmp6 < 8*(ks0 // 2)*(ks1 // 2)*(ks2 // 2)\")\n    tl .store (out_ptr0 +(tl .broadcast_to ((tmp6 %(8 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 ))),[XBLOCK ])),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_reflection_pad2d_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //ks0 \n    x0 =(xindex %ks0 )\n    x2 =xindex \n    tmp0 =(-1 )+(tl .where (1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x1 ))+2 *(ks1 //2 )))+2 *(ks1 //2 )<0 ,3 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x1 ))+2 *(ks1 //2 )))+4 *(ks1 //2 ),1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x1 ))+2 *(ks1 //2 )))+2 *(ks1 //2 )))\n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =2 *(ks1 //2 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+(tl .where (1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 )<0 ,3 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+8 *(ks2 //2 )*(ks3 //2 ),1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 )))\n    tmp6 =tmp5 >=tmp1 \n    tmp7 =4 *(ks2 //2 )*(ks3 //2 )\n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +(2 *(ks3 //2 )*((((2 *(ks3 //2 )*(((((-1 )+(tl .where (1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 )<0 ,3 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+8 *(ks2 //2 )*(ks3 //2 ),1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 ))))//(2 *(ks3 //2 )))%(2 *(ks2 //2 ))))+((((-1 )+(tl .where (1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 )<0 ,3 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+8 *(ks2 //2 )*(ks3 //2 ),1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 ))))%(2 *(ks3 //2 )))))//(2 *(ks3 //2 )))%(2 *(ks2 //2 ))))+4 *(ks2 //2 )*(ks3 //2 )*((((((-4 )*(ks2 //2 )*(ks3 //2 ))+2 *(ks3 //2 )*(((((-1 )+(tl .where (1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 )<0 ,3 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+8 *(ks2 //2 )*(ks3 //2 ),1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 ))))//(2 *(ks3 //2 )))%(2 *(ks2 //2 ))))+4 *(ks2 //2 )*(ks3 //2 )*(tl .where (1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x1 ))+2 *(ks1 //2 )))+2 *(ks1 //2 )<0 ,3 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x1 ))+2 *(ks1 //2 )))+4 *(ks1 //2 ),1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x1 ))+2 *(ks1 //2 )))+2 *(ks1 //2 )))+((((-1 )+(tl .where (1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 )<0 ,3 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+8 *(ks2 //2 )*(ks3 //2 ),1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 ))))%(2 *(ks3 //2 )))))//(4 *(ks2 //2 )*(ks3 //2 )))%(2 *(ks1 //2 ))))+((((((-1 )+(tl .where (1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 )<0 ,3 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+8 *(ks2 //2 )*(ks3 //2 ),1 +((-1 )*tl_math .abs (1 +((-1 )*tl_math .abs ((-1 )+x0 ))+4 *(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 ))))%(2 *(ks3 //2 ))))%(2 *(ks3 //2 ))))),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tl .store (out_ptr0 +(x2 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_relu_3 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =2160 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //216 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x2 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_convolution_huber_loss_huber_loss_backward_4 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    r0_numel =1280 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp15 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        r0_1 =r0_index //64 \n        tmp0 =tl .load (in_out_ptr0 +(r0_2 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp0 +tmp1 \n        tmp3 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n        tmp5 =tl_math .abs (tmp4 )\n        tmp6 =1.0 \n        tmp7 =tmp5 <tmp6 \n        tmp8 =0.5 \n        tmp9 =tmp5 *tmp8 \n        tmp10 =tmp9 *tmp5 \n        tmp11 =tmp5 -tmp8 \n        tmp12 =tmp11 *tmp6 \n        tmp13 =tl .where (tmp7 ,tmp10 ,tmp12 )\n        tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n        tmp16 =_tmp15 +tmp14 \n        _tmp15 =tl .where (r0_mask ,tmp16 ,_tmp15 )\n        tl .store (in_out_ptr0 +(tl .broadcast_to (r0_2 ,[XBLOCK ,R0_BLOCK ])),tmp2 ,r0_mask )\n    tmp15 =tl .sum (_tmp15 ,1 )[:,None ]\n    tmp17 =1280.0 \n    tmp18 =tmp15 /tmp17 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp18 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 =args \n    args .clear ()\n    s0 =primals_1 \n    s1 =primals_2 \n    s2 =primals_3 \n    assert_size_stride (primals_4 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    assert_size_stride (primals_5 ,(10 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_6 ,(10 ,),(1 ,))\n    assert_size_stride (primals_7 ,(20 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_8 ,(20 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .max_pool3d_with_indices .default (primals_4 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del primals_4 \n        buf1 =buf0 [0 ]\n        buf2 =buf0 [1 ]\n        del buf0 \n        buf3 =empty_strided_cuda ((1 ,1 ,2 *(s0 //2 ),2 *(s1 //2 ),2 *(s2 //2 )),(8 *(s0 //2 )*(s1 //2 )*(s2 //2 ),8 *(s0 //2 )*(s1 //2 )*(s2 //2 ),4 *(s1 //2 )*(s2 //2 ),2 *(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool3d_0_xnumel =8 *(s0 //2 )*(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_0 [grid (triton_poi_fused_max_unpool3d_0_xnumel )](buf3 ,4096 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool3d_1_xnumel =(s0 //2 )*(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_1 [grid (triton_poi_fused_max_unpool3d_1_xnumel )](buf2 ,buf1 ,buf3 ,16 ,16 ,16 ,512 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        del buf2 \n        4 +4 *(s1 //2 )*(s2 //2 )\n        buf5 =empty_strided_cuda ((1 ,1 ,4 +2 *(s0 //2 ),4 +4 *(s1 //2 )*(s2 //2 )),(16 +8 *(s0 //2 )+16 *(s1 //2 )*(s2 //2 )+8 *(s0 //2 )*(s1 //2 )*(s2 //2 ),1 ,4 +4 *(s1 //2 )*(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_reflection_pad2d_2_xnumel =16 +8 *(s0 //2 )+16 *(s1 //2 )*(s2 //2 )+8 *(s0 //2 )*(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_reflection_pad2d_2 [grid (triton_poi_fused_constant_pad_nd_reflection_pad2d_2_xnumel )](buf3 ,buf5 ,260 ,16 ,16 ,16 ,5200 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n\n        buf6 =torch .ops .aten ._adaptive_avg_pool3d .default (reinterpret_tensor (buf5 ,(1 ,1 ,4 +2 *(s0 //2 ),4 +4 *(s1 //2 )*(s2 //2 ),1 ),(0 ,0 ,4 +4 *(s1 //2 )*(s2 //2 ),1 ,0 ),0 ),[8 ,8 ,8 ])\n        del buf5 \n        buf7 =buf6 \n        del buf6 \n\n        buf8 =extern_kernels .convolution (buf7 ,primals_5 ,stride =(1 ,1 ,1 ),padding =(0 ,0 ,0 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf8 ,(1 ,10 ,6 ,6 ,6 ),(2160 ,216 ,36 ,6 ,1 ))\n        buf9 =buf8 ;del buf8 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_relu_3 [grid (2160 )](buf9 ,primals_6 ,2160 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_6 \n\n        buf10 =extern_kernels .convolution (buf9 ,primals_7 ,stride =(1 ,1 ,1 ),padding =(0 ,0 ,0 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf10 ,(1 ,20 ,4 ,4 ,4 ),(1280 ,64 ,16 ,4 ,1 ))\n        buf11 =buf10 ;del buf10 \n        buf12 =empty_strided_cuda ((),(),torch .float32 )\n        buf13 =buf12 ;del buf12 \n\n        get_raw_stream (0 )\n        triton_red_fused_convolution_huber_loss_huber_loss_backward_4 [grid (1 )](buf11 ,buf13 ,primals_8 ,1 ,1280 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del primals_8 \n    return (buf13 ,primals_5 ,primals_7 ,buf7 ,buf9 ,buf11 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =16 \n    primals_2 =16 \n    primals_3 =16 \n    primals_4 =rand_strided ((1 ,1 ,16 ,16 ,16 ),(4096 ,4096 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((10 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((20 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "30af2d96-7cd9-4f8b-aa63-59e13012fa94",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softsign', 'AvgPool2d', 'LocalResponseNorm', 'LSTM']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softsign = nn.Softsign()\n        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.local_response_norm = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=1.0)\n        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.fc = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # Apply Softsign\n        x = self.softsign(x)\n        \n        # Apply AvgPool2d twice\n        x = self.avgpool1(x)\n        x = self.avgpool2(x)\n        \n        # Apply LocalResponseNorm\n        x = self.local_response_norm(x)\n        \n        # Reshape for LSTM\n        batch_size, channels, height, width = x.size()\n        x = x.view(batch_size, channels * height, width)  # Reshape to (batch_size, seq_len, input_size)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Take the last output of the LSTM\n        x = x[:, -1, :]\n        \n        # Apply a fully connected layer\n        x = self.fc(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels, 64x64 height and width\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_avg_pool2d_div_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =1.0 \n    tmp3 =tmp1 +tmp2 \n    tmp4 =tmp0 /tmp3 \n    tmp6 =tl_math .abs (tmp5 )\n    tmp7 =tmp6 +tmp2 \n    tmp8 =tmp5 /tmp7 \n    tmp9 =tmp8 +tmp4 \n    tmp11 =tl_math .abs (tmp10 )\n    tmp12 =tmp11 +tmp2 \n    tmp13 =tmp10 /tmp12 \n    tmp14 =tmp13 +tmp9 \n    tmp16 =tl_math .abs (tmp15 )\n    tmp17 =tmp16 +tmp2 \n    tmp18 =tmp15 /tmp17 \n    tmp19 =tmp18 +tmp14 \n    tmp20 =0.25 \n    tmp21 =tmp19 *tmp20 \n    tl .store (out_ptr0 +(x3 ),tmp21 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex //ks0 \n    x0 =(xindex %ks2 )\n    x1 =((xindex //ks2 )%ks3 )\n    x4 =xindex \n    tmp0 =(-2 )+x2 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +(2 *x0 +((-2 )*ks4 *ks5 )+2 *ks4 *x1 +ks4 *ks5 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tl .load (in_ptr0 +(1 +2 *x0 +((-2 )*ks4 *ks5 )+2 *ks4 *x1 +ks4 *ks5 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =tmp7 +tmp6 \n    tmp9 =tl .load (in_ptr0 +(ks4 +2 *x0 +((-2 )*ks4 *ks5 )+2 *ks4 *x1 +ks4 *ks5 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp10 =tmp9 +tmp8 \n    tmp11 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +((-2 )*ks4 *ks5 )+2 *ks4 *x1 +ks4 *ks5 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tmp11 +tmp10 \n    tmp13 =0.25 \n    tmp14 =tmp12 *tmp13 \n    tmp15 =tmp14 *tmp14 \n    tmp16 =tl .full (tmp15 .shape ,0.0 ,tmp15 .dtype )\n    tmp17 =tl .where (tmp5 ,tmp15 ,tmp16 )\n    tl .store (out_ptr0 +(x4 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_avg_pool2d_div_mul_pow_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks3 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks3 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks3 +2 *x0 +2 *ks3 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +ks3 +2 *x0 +2 *ks3 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr1 +(ks2 +x3 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr1 +(x3 +2 *ks0 *ks1 ),xmask ,eviction_policy ='evict_last')\n    tmp14 =tl .load (in_ptr1 +(x3 +3 *ks0 *ks1 ),xmask ,eviction_policy ='evict_last')\n    tmp16 =tl .load (in_ptr1 +(x3 +4 *ks0 *ks1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tmp11 =tmp10 +tmp9 \n    tmp13 =tmp12 +tmp11 \n    tmp15 =tmp14 +tmp13 \n    tmp17 =tmp16 +tmp15 \n    tmp18 =0.2 \n    tmp19 =tmp17 *tmp18 \n    tmp20 =0.0001 \n    tmp21 =tmp19 *tmp20 \n    tmp22 =1.0 \n    tmp23 =tmp21 +tmp22 \n    tmp24 =0.75 \n    tmp25 =libdevice .pow (tmp23 ,tmp24 )\n    tmp26 =tmp8 /tmp25 \n    tl .store (out_ptr0 +(x3 ),tmp26 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //2 \n        s1 //2 \n        (s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_add_avg_pool2d_div_0_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_avg_pool2d_div_0 [grid (triton_poi_fused_abs_add_avg_pool2d_div_0_xnumel )](arg3_1 ,buf0 ,32 ,32 ,1024 ,64 ,64 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        (s1 //4 )*(s2 //4 )\n        s2 //4 \n        s1 //4 \n        buf1 =empty_strided_cuda ((1 ,1 ,4 +s0 ,s1 //4 ,s2 //4 ),(4 *(s1 //4 )*(s2 //4 )+s0 *(s1 //4 )*(s2 //4 ),4 *(s1 //4 )*(s2 //4 )+s0 *(s1 //4 )*(s2 //4 ),(s1 //4 )*(s2 //4 ),s2 //4 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_1_xnumel =4 *(s1 //4 )*(s2 //4 )+s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_1 [grid (triton_poi_fused_constant_pad_nd_1_xnumel )](buf0 ,buf1 ,256 ,3 ,16 ,16 ,32 ,32 ,1792 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,s1 //4 ,s2 //4 ),(s0 *(s1 //4 )*(s2 //4 ),(s1 //4 )*(s2 //4 ),s2 //4 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_add_avg_pool2d_div_mul_pow_2_xnumel =s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_avg_pool2d_div_mul_pow_2 [grid (triton_poi_fused_abs_add_avg_pool2d_div_mul_pow_2_xnumel )](buf0 ,buf1 ,buf2 ,16 ,16 ,256 ,32 ,32 ,768 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        del buf1 \n    return (reinterpret_tensor (buf2 ,(1 ,s0 *(s1 //4 ),s2 //4 ),(s0 *(s1 //4 )*(s2 //4 ),s2 //4 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "31353e50-e489-41f9-8f1f-eec2b44d0f5a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool1d', 'TransformerEncoderLayer', 'GELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n        self.transformer_encoder1 = nn.TransformerEncoderLayer(d_model=64, nhead=8)\n        self.gelu1 = nn.GELU()\n        self.transformer_encoder2 = nn.TransformerEncoderLayer(d_model=64, nhead=8)\n        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n        self.gelu2 = nn.GELU()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, sequence_length, feature_dim)\n        x = x.permute(0, 2, 1)  # Reshape to (batch_size, feature_dim, sequence_length) for MaxPool1d\n        x = self.maxpool1(x)\n        x = x.permute(0, 2, 1)  # Reshape back to (batch_size, sequence_length, feature_dim) for TransformerEncoderLayer\n        x = self.transformer_encoder1(x)\n        x = self.gelu1(x)\n        x = self.transformer_encoder2(x)\n        x = x.permute(0, 2, 1)  # Reshape to (batch_size, feature_dim, sequence_length) for MaxPool1d\n        x = self.maxpool2(x)\n        x = x.permute(0, 2, 1)  # Reshape back to (batch_size, sequence_length, feature_dim)\n        x = self.gelu2(x)\n        return x\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 32, 64).cuda()  # Example input shape (batch_size=1, sequence_length=32, feature_dim=64)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1024 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %64 )\n    x1 =xindex //64 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +128 *x1 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(64 +x0 +128 *x1 ),xmask )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tl .store (out_ptr0 +(x2 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3072 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %64 )\n    x1 =((xindex //64 )%16 )\n    x2 =xindex //1024 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +64 *x2 +192 *x1 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 +64 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_2 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1024 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask )\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1024 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(1024 +x2 ),xmask )\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_4 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1024 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2048 +x2 ),xmask )\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_5 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp7 =tl .load (in_out_ptr0 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp8 =tl .load (in_ptr2 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +64 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp14 ,0 )\n    tmp17 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp19 =tl .where (xmask ,tmp17 ,0 )\n    tmp20 =tl .sum (tmp19 ,1 )[:,None ]\n    tmp21 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 /tmp22 \n    tmp24 =tmp14 -tmp23 \n    tmp25 =tmp24 *tmp24 \n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (xmask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =tmp13 -tmp23 \n    tmp31 =64.0 \n    tmp32 =tmp29 /tmp31 \n    tmp33 =1e-05 \n    tmp34 =tmp32 +tmp33 \n    tmp35 =libdevice .rsqrt (tmp34 )\n    tmp36 =tmp30 *tmp35 \n    tmp38 =tmp36 *tmp37 \n    tmp40 =tmp38 +tmp39 \n    tmp41 =0.015625 \n    tmp42 =tmp35 *tmp41 \n    tl .store (out_ptr1 +(r0_1 +64 *x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(r0_1 +64 *x0 ),tmp36 ,xmask )\n    tl .store (out_ptr4 +(r0_1 +64 *x0 ),tmp40 ,xmask )\n    tl .store (out_ptr5 +(x0 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_threshold_backward_6 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    x1 =(xindex %2048 )\n    tmp6 =tl .load (in_ptr1 +(x0 ),None )\n    tmp7 =tl .load (in_ptr2 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.0 \n    tmp15 =tmp10 <=tmp14 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,None )\n    tl .store (out_ptr2 +(x0 ),tmp13 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp15 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_gelu_native_dropout_native_layer_norm_native_layer_norm_backward_7 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp7 =tl .load (in_out_ptr0 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp8 =tl .load (in_ptr2 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +64 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp14 ,0 )\n    tmp17 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp19 =tl .where (xmask ,tmp17 ,0 )\n    tmp20 =tl .sum (tmp19 ,1 )[:,None ]\n    tmp21 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 /tmp22 \n    tmp24 =tmp14 -tmp23 \n    tmp25 =tmp24 *tmp24 \n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (xmask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =tmp13 -tmp23 \n    tmp31 =64.0 \n    tmp32 =tmp29 /tmp31 \n    tmp33 =1e-05 \n    tmp34 =tmp32 +tmp33 \n    tmp35 =libdevice .rsqrt (tmp34 )\n    tmp36 =tmp30 *tmp35 \n    tmp38 =tmp36 *tmp37 \n    tmp40 =tmp38 +tmp39 \n    tmp41 =0.5 \n    tmp42 =tmp40 *tmp41 \n    tmp43 =0.7071067811865476 \n    tmp44 =tmp40 *tmp43 \n    tmp45 =libdevice .erf (tmp44 )\n    tmp46 =1.0 \n    tmp47 =tmp45 +tmp46 \n    tmp48 =tmp42 *tmp47 \n    tmp49 =0.015625 \n    tmp50 =tmp35 *tmp49 \n    tl .store (out_ptr1 +(r0_1 +64 *x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(r0_1 +64 *x0 ),tmp36 ,xmask )\n    tl .store (out_ptr4 +(r0_1 +64 *x0 ),tmp48 ,xmask )\n    tl .store (out_ptr5 +(x0 ),tmp50 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_threshold_backward_8 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    x1 =(xindex %2048 )\n    tmp6 =tl .load (in_ptr1 +(x0 ),None )\n    tmp7 =tl .load (in_ptr2 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.0 \n    tmp15 =tmp10 <=tmp14 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,None )\n    tl .store (out_ptr2 +(x0 ),tmp13 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp15 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_gelu_gelu_backward_max_pool2d_with_indices_9 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =512 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %64 )\n    x1 =xindex //64 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +128 *x1 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(64 +x0 +128 *x1 ),xmask )\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp7 =0.5 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =0.7071067811865476 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =libdevice .erf (tmp10 )\n    tmp12 =1.0 \n    tmp13 =tmp11 +tmp12 \n    tmp14 =tmp8 *tmp13 \n    tmp15 =tmp13 *tmp7 \n    tmp16 =tmp6 *tmp6 \n    tmp17 =-0.5 \n    tmp18 =tmp16 *tmp17 \n    tmp19 =tl_math .exp (tmp18 )\n    tmp20 =0.3989422804014327 \n    tmp21 =tmp19 *tmp20 \n    tmp22 =tmp6 *tmp21 \n    tmp23 =tmp15 +tmp22 \n    tl .store (out_ptr0 +(x2 ),tmp5 ,xmask )\n    tl .store (out_ptr1 +(x2 ),tmp14 ,xmask )\n    tl .store (out_ptr2 +(x2 ),tmp23 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,32 ,64 ),(2048 ,64 ,1 ))\n    assert_size_stride (primals_2 ,(192 ,),(1 ,))\n    assert_size_stride (primals_3 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_4 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_5 ,(64 ,),(1 ,))\n    assert_size_stride (primals_6 ,(64 ,),(1 ,))\n    assert_size_stride (primals_7 ,(64 ,),(1 ,))\n    assert_size_stride (primals_8 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_9 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_10 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_11 ,(64 ,),(1 ,))\n    assert_size_stride (primals_12 ,(64 ,),(1 ,))\n    assert_size_stride (primals_13 ,(64 ,),(1 ,))\n    assert_size_stride (primals_14 ,(192 ,),(1 ,))\n    assert_size_stride (primals_15 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_16 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_17 ,(64 ,),(1 ,))\n    assert_size_stride (primals_18 ,(64 ,),(1 ,))\n    assert_size_stride (primals_19 ,(64 ,),(1 ,))\n    assert_size_stride (primals_20 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_21 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_22 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_23 ,(64 ,),(1 ,))\n    assert_size_stride (primals_24 ,(64 ,),(1 ,))\n    assert_size_stride (primals_25 ,(64 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,64 ,1 ,16 ),(1024 ,1 ,1024 ,64 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_0 [grid (1024 )](primals_1 ,buf0 ,1024 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_1 \n        buf1 =empty_strided_cuda ((16 ,192 ),(192 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf0 ,(16 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf1 )\n        del primals_3 \n        buf2 =empty_strided_cuda ((3 ,1 ,16 ,64 ),(1024 ,1 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_1 [grid (3072 )](buf1 ,primals_2 ,buf2 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n        buf3 =empty_strided_cuda ((16 ,8 ,1 ,8 ),(64 ,8 ,1024 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (1024 )](buf2 ,buf3 ,1024 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((16 ,8 ,1 ,8 ),(64 ,8 ,1024 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (1024 )](buf2 ,buf4 ,1024 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((16 ,8 ,1 ,8 ),(64 ,8 ,1024 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_4 [grid (1024 )](buf2 ,buf5 ,1024 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        buf6 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (buf3 ,buf4 ,buf5 ,None ,True ,0.1 )\n        buf7 =buf6 [0 ]\n        buf8 =buf6 [1 ]\n        buf9 =buf6 [2 ]\n        buf10 =buf6 [3 ]\n        del buf6 \n        buf11 =empty_strided_cuda ((16 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf7 ,(16 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_4 ,(64 ,64 ),(1 ,64 ),0 ),out =buf11 )\n        buf12 =empty_strided_cuda ((6 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[6 ],out =buf12 )\n        buf14 =empty_strided_cuda ((1 ,16 ,64 ),(1024 ,64 ,1 ),torch .bool )\n        buf18 =reinterpret_tensor (buf11 ,(1 ,16 ,64 ),(1024 ,64 ,1 ),0 );del buf11 \n        buf19 =empty_strided_cuda ((1 ,16 ,64 ),(1024 ,64 ,1 ),torch .float32 )\n        buf70 =empty_strided_cuda ((1 ,16 ,1 ),(16 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_5 [grid (16 )](buf18 ,buf12 ,buf0 ,primals_5 ,primals_6 ,primals_7 ,buf14 ,buf19 ,buf70 ,5 ,16 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_5 \n        del primals_7 \n        buf20 =empty_strided_cuda ((16 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf19 ,(16 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_8 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf20 )\n        buf22 =empty_strided_cuda ((1 ,16 ,2048 ),(32768 ,2048 ,1 ),torch .bool )\n        buf23 =empty_strided_cuda ((1 ,16 ,2048 ),(32768 ,2048 ,1 ),torch .float32 )\n        buf69 =empty_strided_cuda ((1 ,16 ,2048 ),(32768 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_6 [grid (32768 )](buf12 ,buf20 ,primals_9 ,buf22 ,buf23 ,buf69 ,1 ,32768 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_9 \n        buf24 =empty_strided_cuda ((16 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf23 ,(16 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_10 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf24 )\n        buf26 =empty_strided_cuda ((1 ,16 ,64 ),(1024 ,64 ,1 ),torch .bool )\n        buf30 =reinterpret_tensor (buf24 ,(1 ,16 ,64 ),(1024 ,64 ,1 ),0 );del buf24 \n        buf31 =empty_strided_cuda ((1 ,16 ,64 ),(1024 ,64 ,1 ),torch .float32 )\n        buf68 =empty_strided_cuda ((1 ,16 ,1 ),(16 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_gelu_native_dropout_native_layer_norm_native_layer_norm_backward_7 [grid (16 )](buf30 ,buf12 ,buf19 ,primals_11 ,primals_12 ,primals_13 ,buf26 ,buf31 ,buf68 ,2 ,16 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_11 \n        buf32 =reinterpret_tensor (buf2 ,(16 ,192 ),(192 ,1 ),0 );del buf2 \n\n        extern_kernels .mm (reinterpret_tensor (buf31 ,(16 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_15 ,(64 ,192 ),(1 ,64 ),0 ),out =buf32 )\n        buf33 =reinterpret_tensor (buf1 ,(3 ,1 ,16 ,64 ),(1024 ,1 ,64 ,1 ),0 );del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_1 [grid (3072 )](buf32 ,primals_14 ,buf33 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf32 \n        del primals_14 \n        buf34 =empty_strided_cuda ((16 ,8 ,1 ,8 ),(64 ,8 ,1024 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (1024 )](buf33 ,buf34 ,1024 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf35 =empty_strided_cuda ((16 ,8 ,1 ,8 ),(64 ,8 ,1024 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (1024 )](buf33 ,buf35 ,1024 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf36 =empty_strided_cuda ((16 ,8 ,1 ,8 ),(64 ,8 ,1024 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_4 [grid (1024 )](buf33 ,buf36 ,1024 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf33 \n\n        buf37 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (buf34 ,buf35 ,buf36 ,None ,True ,0.1 )\n        buf38 =buf37 [0 ]\n        buf39 =buf37 [1 ]\n        buf40 =buf37 [2 ]\n        buf41 =buf37 [3 ]\n        del buf37 \n        buf42 =empty_strided_cuda ((16 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf38 ,(16 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_16 ,(64 ,64 ),(1 ,64 ),0 ),out =buf42 )\n        buf44 =empty_strided_cuda ((1 ,16 ,64 ),(1024 ,64 ,1 ),torch .bool )\n        buf48 =reinterpret_tensor (buf42 ,(1 ,16 ,64 ),(1024 ,64 ,1 ),0 );del buf42 \n        buf49 =empty_strided_cuda ((1 ,16 ,64 ),(1024 ,64 ,1 ),torch .float32 )\n        buf67 =empty_strided_cuda ((1 ,16 ,1 ),(16 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_5 [grid (16 )](buf48 ,buf12 ,buf31 ,primals_17 ,primals_18 ,primals_19 ,buf44 ,buf49 ,buf67 ,5 ,16 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_17 \n        del primals_19 \n        buf50 =buf20 ;del buf20 \n\n        extern_kernels .mm (reinterpret_tensor (buf49 ,(16 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_20 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf50 )\n        buf52 =empty_strided_cuda ((1 ,16 ,2048 ),(32768 ,2048 ,1 ),torch .bool )\n        buf53 =empty_strided_cuda ((1 ,16 ,2048 ),(32768 ,2048 ,1 ),torch .float32 )\n        buf66 =empty_strided_cuda ((1 ,16 ,2048 ),(32768 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_8 [grid (32768 )](buf12 ,buf50 ,primals_21 ,buf52 ,buf53 ,buf66 ,4 ,32768 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf50 \n        del primals_21 \n        buf54 =empty_strided_cuda ((16 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf53 ,(16 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_22 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf54 )\n        buf56 =empty_strided_cuda ((1 ,16 ,64 ),(1024 ,64 ,1 ),torch .bool )\n        buf60 =reinterpret_tensor (buf54 ,(1 ,16 ,64 ),(1024 ,64 ,1 ),0 );del buf54 \n        buf61 =empty_strided_cuda ((1 ,16 ,64 ),(1024 ,64 ,1 ),torch .float32 )\n        buf65 =empty_strided_cuda ((1 ,16 ,1 ),(16 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_5 [grid (16 )](buf60 ,buf12 ,buf49 ,primals_23 ,primals_24 ,primals_25 ,buf56 ,buf61 ,buf65 ,5 ,16 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf12 \n        del primals_23 \n        del primals_25 \n        buf62 =empty_strided_cuda ((1 ,64 ,1 ,8 ),(512 ,1 ,512 ,64 ),torch .int8 )\n        buf63 =empty_strided_cuda ((1 ,8 ,64 ),(512 ,64 ,1 ),torch .float32 )\n        buf64 =empty_strided_cuda ((1 ,8 ,64 ),(512 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_gelu_gelu_backward_max_pool2d_with_indices_9 [grid (512 )](buf61 ,buf62 ,buf63 ,buf64 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n    return (buf63 ,primals_6 ,primals_12 ,primals_13 ,primals_18 ,primals_24 ,reinterpret_tensor (buf0 ,(16 ,64 ),(64 ,1 ),0 ),buf3 ,buf4 ,buf5 ,buf7 ,buf8 ,buf9 ,buf10 ,buf14 ,buf18 ,reinterpret_tensor (buf19 ,(16 ,64 ),(64 ,1 ),0 ),buf22 ,reinterpret_tensor (buf23 ,(16 ,2048 ),(2048 ,1 ),0 ),buf26 ,buf30 ,reinterpret_tensor (buf31 ,(16 ,64 ),(64 ,1 ),0 ),buf34 ,buf35 ,buf36 ,buf38 ,buf39 ,buf40 ,buf41 ,buf44 ,buf48 ,reinterpret_tensor (buf49 ,(16 ,64 ),(64 ,1 ),0 ),buf52 ,reinterpret_tensor (buf53 ,(16 ,2048 ),(2048 ,1 ),0 ),buf56 ,buf60 ,reinterpret_tensor (buf61 ,(1 ,64 ,1 ,16 ),(1024 ,1 ,1024 ,64 ),0 ),buf62 ,buf64 ,buf65 ,primals_22 ,buf66 ,primals_20 ,buf67 ,primals_16 ,primals_15 ,buf68 ,primals_10 ,buf69 ,primals_8 ,buf70 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,32 ,64 ),(2048 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_18 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_22 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_23 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_24 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_25 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "31411e41-bc42-42f8-8e72-3813e579b9bf",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FeatureAlphaDropout', 'Softsign', 'Linear', 'GLU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.linear1 = nn.Linear(128, 256)\n        self.glu1 = nn.GLU(dim=1)\n        self.linear2 = nn.Linear(128, 64)\n        self.softsign = nn.Softsign()\n        self.linear3 = nn.Linear(64, 32)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.linear4 = nn.Linear(32, 10)\n\n    def forward(self, x):\n        # Flatten the input to a 2D tensor\n        x = x.view(x.size(0), -1)\n        \n        # Apply the first linear layer\n        x = self.linear1(x)\n        \n        # Apply GLU\n        x = self.glu1(x)\n        \n        # Apply the second linear layer\n        x = self.linear2(x)\n        \n        # Apply Softsign activation\n        x = self.softsign(x)\n        \n        # Apply the third linear layer\n        x = self.linear3(x)\n        \n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Apply the final linear layer\n        x = self.linear4(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 128).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_glu_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(128 +x0 ),xmask )\n    tmp2 =tl .sigmoid (tmp1 )\n    tmp3 =tmp0 *tmp2 \n    tl .store (out_ptr0 +(x0 ),tmp3 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_div_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =1.0 \n    tmp3 =tmp1 +tmp2 \n    tmp4 =tmp0 /tmp3 \n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_addmm_bernoulli_mul_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp5 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp6 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =tmp4 .to (tl .float32 )\n    tmp9 =0.8864048946659319 \n    tmp10 =tmp8 *tmp9 \n    tmp11 =tmp7 *tmp10 \n    tmp12 =-1.0 \n    tmp13 =tmp8 +tmp12 \n    tmp14 =1.558387861036063 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =0.7791939305180315 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =tmp11 +tmp17 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(x0 ),tmp18 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_2 ,(256 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_3 ,(256 ,),(1 ,))\n    assert_size_stride (primals_4 ,(64 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_5 ,(64 ,),(1 ,))\n    assert_size_stride (primals_6 ,(32 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_7 ,(32 ,),(1 ,))\n    assert_size_stride (primals_8 ,(10 ,32 ),(32 ,1 ))\n    assert_size_stride (primals_9 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_3 ,primals_1 ,reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf0 )\n        del primals_2 \n        del primals_3 \n        buf1 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_glu_0 [grid (128 )](buf0 ,buf1 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_5 ,buf1 ,reinterpret_tensor (primals_4 ,(128 ,64 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf2 )\n        del primals_5 \n        buf3 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_div_1 [grid (64 )](buf2 ,buf3 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((1 ,32 ),(32 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf3 ,reinterpret_tensor (primals_6 ,(64 ,32 ),(1 ,64 ),0 ),out =buf4 )\n        buf5 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf5 )\n        buf7 =empty_strided_cuda ((1 ,32 ),(32 ,1 ),torch .bool )\n        buf8 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_addmm_bernoulli_mul_2 [grid (32 )](buf8 ,buf5 ,primals_7 ,buf7 ,0 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del buf5 \n        del primals_7 \n        buf9 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_9 ,buf8 ,reinterpret_tensor (primals_8 ,(32 ,10 ),(1 ,32 ),0 ),alpha =1 ,beta =1 ,out =buf9 )\n        del primals_9 \n    return (buf9 ,primals_1 ,buf0 ,buf1 ,buf2 ,buf3 ,buf7 ,buf8 ,primals_8 ,primals_6 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((256 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((64 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((32 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((10 ,32 ),(32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "31b0f61e-09d0-438e-9a92-e931b143045a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveAvgPool1d', 'MultiheadAttention', 'LSTMCell', 'ReplicationPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool1d(64)\n        self.multihead_attention = nn.MultiheadAttention(embed_dim=64, num_heads=8)\n        self.lstm_cell1 = nn.LSTMCell(input_size=64, hidden_size=128)\n        self.lstm_cell2 = nn.LSTMCell(input_size=128, hidden_size=64)\n        self.replication_pad = nn.ReplicationPad3d(padding=(1, 1, 1, 1, 1, 1))\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, sequence_length)\n        # Reshape and apply AdaptiveAvgPool1d\n        x = x.permute(0, 2, 1)  # (batch_size, sequence_length, channels)\n        x = self.adaptive_avg_pool(x)  # (batch_size, sequence_length, 64)\n        x = x.permute(1, 0, 2)  # (sequence_length, batch_size, 64) for MultiheadAttention\n\n        # Apply MultiheadAttention\n        x, _ = self.multihead_attention(x, x, x)  # (sequence_length, batch_size, 64)\n\n        # Reshape for LSTMCell\n        x = x.permute(1, 0, 2)  # (batch_size, sequence_length, 64)\n        batch_size, seq_len, _ = x.size()\n        hx1 = torch.zeros(batch_size, 128).to(x.device)\n        cx1 = torch.zeros(batch_size, 128).to(x.device)\n        hx2 = torch.zeros(batch_size, 64).to(x.device)\n        cx2 = torch.zeros(batch_size, 64).to(x.device)\n\n        # Apply LSTMCell\n        for i in range(seq_len):\n            hx1, cx1 = self.lstm_cell1(x[:, i, :], (hx1, cx1))\n            hx2, cx2 = self.lstm_cell2(hx1, (hx2, cx2))\n\n        # Reshape for ReplicationPad3d\n        x = hx2.view(batch_size, 8, 8, 1, 1)  # (batch_size, 8, 8, 1, 1)\n        x = self.replication_pad(x)  # (batch_size, 10, 10, 3, 3)\n\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 128).cuda()  # (batch_size, channels, sequence_length)\n    return [x]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x1 =xindex //128 \n    x0 =(xindex %128 )\n    x2 =xindex \n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =(3 *x1 )//64 \n    tmp4 =(66 +3 *x1 )//64 \n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp2 &tmp5 \n    tmp7 =tl .load (in_ptr0 +(x0 +128 *((3 *x1 )//64 )),tmp6 ,other =0.0 )\n    tmp8 =1 +((3 *x1 )//64 )\n    tmp9 =tmp8 <tmp4 \n    tmp10 =tmp2 &tmp9 \n    tmp11 =tl .load (in_ptr0 +(128 +x0 +128 *((3 *x1 )//64 )),tmp10 ,other =0.0 )\n    tmp12 =tmp11 +tmp7 \n    tmp13 =1.0 \n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =1.0 \n    tmp17 =tl .full (tmp16 .shape ,0.0 ,tmp16 .dtype )\n    tmp18 =tl .where (tmp10 ,tmp16 ,tmp17 )\n    tmp19 =tmp18 +tmp15 \n    tmp20 =tmp12 /tmp19 \n    tl .store (out_ptr0 +(x2 ),tmp20 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =(xindex %64 )\n    x1 =((xindex //64 )%128 )\n    x2 =xindex //8192 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +64 *x2 +192 *x1 ),None )\n    tmp1 =tl .load (in_ptr1 +(x0 +64 *x2 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (out_ptr0 +(x3 ),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_mul_2 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),None )\n    tmp1 =0.3535533905932738 \n    tmp2 =tmp0 *tmp1 \n    tl .store (out_ptr0 +(x0 ),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_3 (in_out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =1024 \n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(r0_1 +128 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (xmask ,tmp1 ,float (\"-inf\"))\n    tmp4 =triton_helpers .max2 (tmp3 ,1 )[:,None ]\n    tmp5 =tmp0 -tmp4 \n    tmp6 =tl_math .exp (tmp5 )\n    tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n    tmp9 =tl .where (xmask ,tmp7 ,0 )\n    tmp10 =tl .sum (tmp9 ,1 )[:,None ]\n    tmp11 =tmp6 /tmp10 \n    tl .store (in_out_ptr0 +(r0_1 +128 *x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_4 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =(xindex %8 )\n    x1 =((xindex //8 )%8 )\n    x2 =xindex //64 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +8 *x2 +1024 *x1 ),None )\n    tl .store (out_ptr0 +(x3 ),tmp0 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_5 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_6 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_replication_pad3d_7 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =720 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //9 )%10 )\n    x2 =xindex //90 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(8 *x2 +((7 )*((7 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<(7 )))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x3 ),tmp0 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 =args \n    args .clear ()\n    assert_size_stride (primals_2 ,(1 ,3 ,128 ),(384 ,128 ,1 ))\n    assert_size_stride (primals_3 ,(192 ,),(1 ,))\n    assert_size_stride (primals_4 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_5 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_6 ,(64 ,),(1 ,))\n    assert_size_stride (primals_7 ,(512 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_8 ,(512 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_9 ,(512 ,),(1 ,))\n    assert_size_stride (primals_10 ,(512 ,),(1 ,))\n    assert_size_stride (primals_11 ,(256 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_12 ,(256 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_13 ,(256 ,),(1 ,))\n    assert_size_stride (primals_14 ,(256 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,128 ,1 ,64 ),(8192 ,1 ,8192 ,128 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_0 [grid (8192 )](primals_2 ,buf0 ,8192 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n        buf1 =empty_strided_cuda ((128 ,192 ),(192 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf0 ,(128 ,64 ),(1 ,128 ),0 ),reinterpret_tensor (primals_4 ,(64 ,192 ),(1 ,64 ),0 ),out =buf1 )\n        del primals_4 \n        buf2 =empty_strided_cuda ((3 ,128 ,1 ,64 ),(8192 ,64 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_1 [grid (24576 )](buf1 ,primals_3 ,buf2 ,24576 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        del primals_3 \n        buf3 =empty_strided_cuda ((8 ,128 ,8 ),(8 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_mul_2 [grid (8192 )](buf2 ,buf3 ,8192 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((8 ,128 ,128 ),(16384 ,128 ,1 ),torch .float32 )\n\n        extern_kernels .bmm (buf3 ,reinterpret_tensor (buf2 ,(8 ,8 ,128 ),(8 ,1 ,64 ),8192 ),out =buf4 )\n        buf7 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_3 [grid (1024 )](buf7 ,1024 ,128 ,XBLOCK =8 ,num_warps =8 ,num_stages =1 )\n        buf8 =empty_strided_cuda ((8 ,128 ,8 ),(1024 ,8 ,1 ),torch .float32 )\n\n        extern_kernels .bmm (buf7 ,reinterpret_tensor (buf2 ,(8 ,128 ,8 ),(8 ,64 ,1 ),16384 ),out =buf8 )\n        buf9 =empty_strided_cuda ((128 ,8 ,8 ),(64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_4 [grid (8192 )](buf8 ,buf9 ,8192 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf10 =reinterpret_tensor (buf8 ,(128 ,64 ),(64 ,1 ),0 );del buf8 \n\n        extern_kernels .addmm (primals_6 ,reinterpret_tensor (buf9 ,(128 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_5 ,(64 ,64 ),(1 ,64 ),0 ),alpha =1 ,beta =1 ,out =buf10 )\n        del primals_6 \n        buf11 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_5 [grid (128 )](buf11 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf12 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_6 [grid (64 )](buf12 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf13 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf13 )\n        buf14 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf11 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf14 )\n\n        buf15 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf13 ,buf14 ,buf11 ,primals_9 ,primals_10 )\n        buf16 =buf15 [0 ]\n        buf17 =buf15 [1 ]\n        buf18 =buf15 [2 ]\n        del buf15 \n        buf19 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf16 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf19 )\n        buf20 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf12 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf20 )\n\n        buf21 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf19 ,buf20 ,buf12 ,primals_13 ,primals_14 )\n        buf22 =buf21 [0 ]\n        buf23 =buf21 [1 ]\n        buf24 =buf21 [2 ]\n        del buf21 \n        buf25 =buf14 ;del buf14 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),64 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf25 )\n        buf26 =buf13 ;del buf13 \n\n        extern_kernels .mm (buf16 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf26 )\n\n        buf27 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf25 ,buf26 ,buf17 ,primals_9 ,primals_10 )\n        buf28 =buf27 [0 ]\n        buf29 =buf27 [1 ]\n        buf30 =buf27 [2 ]\n        del buf27 \n        buf31 =buf20 ;del buf20 \n\n        extern_kernels .mm (buf28 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf31 )\n        buf32 =buf19 ;del buf19 \n\n        extern_kernels .mm (buf22 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf32 )\n\n        buf33 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf31 ,buf32 ,buf23 ,primals_13 ,primals_14 )\n        buf34 =buf33 [0 ]\n        buf35 =buf33 [1 ]\n        buf36 =buf33 [2 ]\n        del buf33 \n        buf37 =buf26 ;del buf26 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),128 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf37 )\n        buf38 =buf25 ;del buf25 \n\n        extern_kernels .mm (buf28 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf38 )\n\n        buf39 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf37 ,buf38 ,buf29 ,primals_9 ,primals_10 )\n        buf40 =buf39 [0 ]\n        buf41 =buf39 [1 ]\n        buf42 =buf39 [2 ]\n        del buf39 \n        buf43 =buf32 ;del buf32 \n\n        extern_kernels .mm (buf40 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf43 )\n        buf44 =buf31 ;del buf31 \n\n        extern_kernels .mm (buf34 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf44 )\n\n        buf45 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf43 ,buf44 ,buf35 ,primals_13 ,primals_14 )\n        buf46 =buf45 [0 ]\n        buf47 =buf45 [1 ]\n        buf48 =buf45 [2 ]\n        del buf45 \n        buf49 =buf38 ;del buf38 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),192 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf49 )\n        buf50 =buf37 ;del buf37 \n\n        extern_kernels .mm (buf40 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf50 )\n\n        buf51 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf49 ,buf50 ,buf41 ,primals_9 ,primals_10 )\n        buf52 =buf51 [0 ]\n        buf53 =buf51 [1 ]\n        buf54 =buf51 [2 ]\n        del buf51 \n        buf55 =buf44 ;del buf44 \n\n        extern_kernels .mm (buf52 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf55 )\n        buf56 =buf43 ;del buf43 \n\n        extern_kernels .mm (buf46 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf56 )\n\n        buf57 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf55 ,buf56 ,buf47 ,primals_13 ,primals_14 )\n        buf58 =buf57 [0 ]\n        buf59 =buf57 [1 ]\n        buf60 =buf57 [2 ]\n        del buf57 \n        buf61 =buf50 ;del buf50 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),256 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf61 )\n        buf62 =buf49 ;del buf49 \n\n        extern_kernels .mm (buf52 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf62 )\n\n        buf63 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf61 ,buf62 ,buf53 ,primals_9 ,primals_10 )\n        buf64 =buf63 [0 ]\n        buf65 =buf63 [1 ]\n        buf66 =buf63 [2 ]\n        del buf63 \n        buf67 =buf56 ;del buf56 \n\n        extern_kernels .mm (buf64 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf67 )\n        buf68 =buf55 ;del buf55 \n\n        extern_kernels .mm (buf58 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf68 )\n\n        buf69 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf67 ,buf68 ,buf59 ,primals_13 ,primals_14 )\n        buf70 =buf69 [0 ]\n        buf71 =buf69 [1 ]\n        buf72 =buf69 [2 ]\n        del buf69 \n        buf73 =buf62 ;del buf62 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),320 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf73 )\n        buf74 =buf61 ;del buf61 \n\n        extern_kernels .mm (buf64 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf74 )\n\n        buf75 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf73 ,buf74 ,buf65 ,primals_9 ,primals_10 )\n        buf76 =buf75 [0 ]\n        buf77 =buf75 [1 ]\n        buf78 =buf75 [2 ]\n        del buf75 \n        buf79 =buf68 ;del buf68 \n\n        extern_kernels .mm (buf76 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf79 )\n        buf80 =buf67 ;del buf67 \n\n        extern_kernels .mm (buf70 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf80 )\n\n        buf81 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf79 ,buf80 ,buf71 ,primals_13 ,primals_14 )\n        buf82 =buf81 [0 ]\n        buf83 =buf81 [1 ]\n        buf84 =buf81 [2 ]\n        del buf81 \n        buf85 =buf74 ;del buf74 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),384 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf85 )\n        buf86 =buf73 ;del buf73 \n\n        extern_kernels .mm (buf76 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf86 )\n\n        buf87 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf85 ,buf86 ,buf77 ,primals_9 ,primals_10 )\n        buf88 =buf87 [0 ]\n        buf89 =buf87 [1 ]\n        buf90 =buf87 [2 ]\n        del buf87 \n        buf91 =buf80 ;del buf80 \n\n        extern_kernels .mm (buf88 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf91 )\n        buf92 =buf79 ;del buf79 \n\n        extern_kernels .mm (buf82 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf92 )\n\n        buf93 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf91 ,buf92 ,buf83 ,primals_13 ,primals_14 )\n        buf94 =buf93 [0 ]\n        buf95 =buf93 [1 ]\n        buf96 =buf93 [2 ]\n        del buf93 \n        buf97 =buf86 ;del buf86 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),448 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf97 )\n        buf98 =buf85 ;del buf85 \n\n        extern_kernels .mm (buf88 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf98 )\n\n        buf99 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf97 ,buf98 ,buf89 ,primals_9 ,primals_10 )\n        buf100 =buf99 [0 ]\n        buf101 =buf99 [1 ]\n        buf102 =buf99 [2 ]\n        del buf99 \n        buf103 =buf92 ;del buf92 \n\n        extern_kernels .mm (buf100 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf103 )\n        buf104 =buf91 ;del buf91 \n\n        extern_kernels .mm (buf94 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf104 )\n\n        buf105 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf103 ,buf104 ,buf95 ,primals_13 ,primals_14 )\n        buf106 =buf105 [0 ]\n        buf107 =buf105 [1 ]\n        buf108 =buf105 [2 ]\n        del buf105 \n        buf109 =buf98 ;del buf98 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),512 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf109 )\n        buf110 =buf97 ;del buf97 \n\n        extern_kernels .mm (buf100 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf110 )\n\n        buf111 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf109 ,buf110 ,buf101 ,primals_9 ,primals_10 )\n        buf112 =buf111 [0 ]\n        buf113 =buf111 [1 ]\n        buf114 =buf111 [2 ]\n        del buf111 \n        buf115 =buf104 ;del buf104 \n\n        extern_kernels .mm (buf112 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf115 )\n        buf116 =buf103 ;del buf103 \n\n        extern_kernels .mm (buf106 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf116 )\n\n        buf117 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf115 ,buf116 ,buf107 ,primals_13 ,primals_14 )\n        buf118 =buf117 [0 ]\n        buf119 =buf117 [1 ]\n        buf120 =buf117 [2 ]\n        del buf117 \n        buf121 =buf110 ;del buf110 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),576 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf121 )\n        buf122 =buf109 ;del buf109 \n\n        extern_kernels .mm (buf112 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf122 )\n\n        buf123 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf121 ,buf122 ,buf113 ,primals_9 ,primals_10 )\n        buf124 =buf123 [0 ]\n        buf125 =buf123 [1 ]\n        buf126 =buf123 [2 ]\n        del buf123 \n        buf127 =buf116 ;del buf116 \n\n        extern_kernels .mm (buf124 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf127 )\n        buf128 =buf115 ;del buf115 \n\n        extern_kernels .mm (buf118 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf128 )\n\n        buf129 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf127 ,buf128 ,buf119 ,primals_13 ,primals_14 )\n        buf130 =buf129 [0 ]\n        buf131 =buf129 [1 ]\n        buf132 =buf129 [2 ]\n        del buf129 \n        buf133 =buf122 ;del buf122 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),640 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf133 )\n        buf134 =buf121 ;del buf121 \n\n        extern_kernels .mm (buf124 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf134 )\n\n        buf135 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf133 ,buf134 ,buf125 ,primals_9 ,primals_10 )\n        buf136 =buf135 [0 ]\n        buf137 =buf135 [1 ]\n        buf138 =buf135 [2 ]\n        del buf135 \n        buf139 =buf128 ;del buf128 \n\n        extern_kernels .mm (buf136 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf139 )\n        buf140 =buf127 ;del buf127 \n\n        extern_kernels .mm (buf130 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf140 )\n\n        buf141 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf139 ,buf140 ,buf131 ,primals_13 ,primals_14 )\n        buf142 =buf141 [0 ]\n        buf143 =buf141 [1 ]\n        buf144 =buf141 [2 ]\n        del buf141 \n        buf145 =buf134 ;del buf134 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),704 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf145 )\n        buf146 =buf133 ;del buf133 \n\n        extern_kernels .mm (buf136 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf146 )\n\n        buf147 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf145 ,buf146 ,buf137 ,primals_9 ,primals_10 )\n        buf148 =buf147 [0 ]\n        buf149 =buf147 [1 ]\n        buf150 =buf147 [2 ]\n        del buf147 \n        buf151 =buf140 ;del buf140 \n\n        extern_kernels .mm (buf148 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf151 )\n        buf152 =buf139 ;del buf139 \n\n        extern_kernels .mm (buf142 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf152 )\n\n        buf153 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf151 ,buf152 ,buf143 ,primals_13 ,primals_14 )\n        buf154 =buf153 [0 ]\n        buf155 =buf153 [1 ]\n        buf156 =buf153 [2 ]\n        del buf153 \n        buf157 =buf146 ;del buf146 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),768 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf157 )\n        buf158 =buf145 ;del buf145 \n\n        extern_kernels .mm (buf148 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf158 )\n\n        buf159 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf157 ,buf158 ,buf149 ,primals_9 ,primals_10 )\n        buf160 =buf159 [0 ]\n        buf161 =buf159 [1 ]\n        buf162 =buf159 [2 ]\n        del buf159 \n        buf163 =buf152 ;del buf152 \n\n        extern_kernels .mm (buf160 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf163 )\n        buf164 =buf151 ;del buf151 \n\n        extern_kernels .mm (buf154 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf164 )\n\n        buf165 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf163 ,buf164 ,buf155 ,primals_13 ,primals_14 )\n        buf166 =buf165 [0 ]\n        buf167 =buf165 [1 ]\n        buf168 =buf165 [2 ]\n        del buf165 \n        buf169 =buf158 ;del buf158 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),832 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf169 )\n        buf170 =buf157 ;del buf157 \n\n        extern_kernels .mm (buf160 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf170 )\n\n        buf171 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf169 ,buf170 ,buf161 ,primals_9 ,primals_10 )\n        buf172 =buf171 [0 ]\n        buf173 =buf171 [1 ]\n        buf174 =buf171 [2 ]\n        del buf171 \n        buf175 =buf164 ;del buf164 \n\n        extern_kernels .mm (buf172 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf175 )\n        buf176 =buf163 ;del buf163 \n\n        extern_kernels .mm (buf166 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf176 )\n\n        buf177 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf175 ,buf176 ,buf167 ,primals_13 ,primals_14 )\n        buf178 =buf177 [0 ]\n        buf179 =buf177 [1 ]\n        buf180 =buf177 [2 ]\n        del buf177 \n        buf181 =buf170 ;del buf170 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),896 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf181 )\n        buf182 =buf169 ;del buf169 \n\n        extern_kernels .mm (buf172 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf182 )\n\n        buf183 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf181 ,buf182 ,buf173 ,primals_9 ,primals_10 )\n        buf184 =buf183 [0 ]\n        buf185 =buf183 [1 ]\n        buf186 =buf183 [2 ]\n        del buf183 \n        buf187 =buf176 ;del buf176 \n\n        extern_kernels .mm (buf184 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf187 )\n        buf188 =buf175 ;del buf175 \n\n        extern_kernels .mm (buf178 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf188 )\n\n        buf189 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf187 ,buf188 ,buf179 ,primals_13 ,primals_14 )\n        buf190 =buf189 [0 ]\n        buf191 =buf189 [1 ]\n        buf192 =buf189 [2 ]\n        del buf189 \n        buf193 =buf182 ;del buf182 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),960 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf193 )\n        buf194 =buf181 ;del buf181 \n\n        extern_kernels .mm (buf184 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf194 )\n\n        buf195 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf193 ,buf194 ,buf185 ,primals_9 ,primals_10 )\n        buf196 =buf195 [0 ]\n        buf197 =buf195 [1 ]\n        buf198 =buf195 [2 ]\n        del buf195 \n        buf199 =buf188 ;del buf188 \n\n        extern_kernels .mm (buf196 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf199 )\n        buf200 =buf187 ;del buf187 \n\n        extern_kernels .mm (buf190 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf200 )\n\n        buf201 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf199 ,buf200 ,buf191 ,primals_13 ,primals_14 )\n        buf202 =buf201 [0 ]\n        buf203 =buf201 [1 ]\n        buf204 =buf201 [2 ]\n        del buf201 \n        buf205 =buf194 ;del buf194 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1024 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf205 )\n        buf206 =buf193 ;del buf193 \n\n        extern_kernels .mm (buf196 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf206 )\n\n        buf207 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf205 ,buf206 ,buf197 ,primals_9 ,primals_10 )\n        buf208 =buf207 [0 ]\n        buf209 =buf207 [1 ]\n        buf210 =buf207 [2 ]\n        del buf207 \n        buf211 =buf200 ;del buf200 \n\n        extern_kernels .mm (buf208 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf211 )\n        buf212 =buf199 ;del buf199 \n\n        extern_kernels .mm (buf202 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf212 )\n\n        buf213 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf211 ,buf212 ,buf203 ,primals_13 ,primals_14 )\n        buf214 =buf213 [0 ]\n        buf215 =buf213 [1 ]\n        buf216 =buf213 [2 ]\n        del buf213 \n        buf217 =buf206 ;del buf206 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1088 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf217 )\n        buf218 =buf205 ;del buf205 \n\n        extern_kernels .mm (buf208 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf218 )\n\n        buf219 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf217 ,buf218 ,buf209 ,primals_9 ,primals_10 )\n        buf220 =buf219 [0 ]\n        buf221 =buf219 [1 ]\n        buf222 =buf219 [2 ]\n        del buf219 \n        buf223 =buf212 ;del buf212 \n\n        extern_kernels .mm (buf220 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf223 )\n        buf224 =buf211 ;del buf211 \n\n        extern_kernels .mm (buf214 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf224 )\n\n        buf225 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf223 ,buf224 ,buf215 ,primals_13 ,primals_14 )\n        buf226 =buf225 [0 ]\n        buf227 =buf225 [1 ]\n        buf228 =buf225 [2 ]\n        del buf225 \n        buf229 =buf218 ;del buf218 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1152 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf229 )\n        buf230 =buf217 ;del buf217 \n\n        extern_kernels .mm (buf220 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf230 )\n\n        buf231 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf229 ,buf230 ,buf221 ,primals_9 ,primals_10 )\n        buf232 =buf231 [0 ]\n        buf233 =buf231 [1 ]\n        buf234 =buf231 [2 ]\n        del buf231 \n        buf235 =buf224 ;del buf224 \n\n        extern_kernels .mm (buf232 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf235 )\n        buf236 =buf223 ;del buf223 \n\n        extern_kernels .mm (buf226 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf236 )\n\n        buf237 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf235 ,buf236 ,buf227 ,primals_13 ,primals_14 )\n        buf238 =buf237 [0 ]\n        buf239 =buf237 [1 ]\n        buf240 =buf237 [2 ]\n        del buf237 \n        buf241 =buf230 ;del buf230 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1216 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf241 )\n        buf242 =buf229 ;del buf229 \n\n        extern_kernels .mm (buf232 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf242 )\n\n        buf243 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf241 ,buf242 ,buf233 ,primals_9 ,primals_10 )\n        buf244 =buf243 [0 ]\n        buf245 =buf243 [1 ]\n        buf246 =buf243 [2 ]\n        del buf243 \n        buf247 =buf236 ;del buf236 \n\n        extern_kernels .mm (buf244 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf247 )\n        buf248 =buf235 ;del buf235 \n\n        extern_kernels .mm (buf238 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf248 )\n\n        buf249 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf247 ,buf248 ,buf239 ,primals_13 ,primals_14 )\n        buf250 =buf249 [0 ]\n        buf251 =buf249 [1 ]\n        buf252 =buf249 [2 ]\n        del buf249 \n        buf253 =buf242 ;del buf242 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1280 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf253 )\n        buf254 =buf241 ;del buf241 \n\n        extern_kernels .mm (buf244 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf254 )\n\n        buf255 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf253 ,buf254 ,buf245 ,primals_9 ,primals_10 )\n        buf256 =buf255 [0 ]\n        buf257 =buf255 [1 ]\n        buf258 =buf255 [2 ]\n        del buf255 \n        buf259 =buf248 ;del buf248 \n\n        extern_kernels .mm (buf256 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf259 )\n        buf260 =buf247 ;del buf247 \n\n        extern_kernels .mm (buf250 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf260 )\n\n        buf261 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf259 ,buf260 ,buf251 ,primals_13 ,primals_14 )\n        buf262 =buf261 [0 ]\n        buf263 =buf261 [1 ]\n        buf264 =buf261 [2 ]\n        del buf261 \n        buf265 =buf254 ;del buf254 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1344 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf265 )\n        buf266 =buf253 ;del buf253 \n\n        extern_kernels .mm (buf256 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf266 )\n\n        buf267 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf265 ,buf266 ,buf257 ,primals_9 ,primals_10 )\n        buf268 =buf267 [0 ]\n        buf269 =buf267 [1 ]\n        buf270 =buf267 [2 ]\n        del buf267 \n        buf271 =buf260 ;del buf260 \n\n        extern_kernels .mm (buf268 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf271 )\n        buf272 =buf259 ;del buf259 \n\n        extern_kernels .mm (buf262 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf272 )\n\n        buf273 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf271 ,buf272 ,buf263 ,primals_13 ,primals_14 )\n        buf274 =buf273 [0 ]\n        buf275 =buf273 [1 ]\n        buf276 =buf273 [2 ]\n        del buf273 \n        buf277 =buf266 ;del buf266 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1408 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf277 )\n        buf278 =buf265 ;del buf265 \n\n        extern_kernels .mm (buf268 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf278 )\n\n        buf279 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf277 ,buf278 ,buf269 ,primals_9 ,primals_10 )\n        buf280 =buf279 [0 ]\n        buf281 =buf279 [1 ]\n        buf282 =buf279 [2 ]\n        del buf279 \n        buf283 =buf272 ;del buf272 \n\n        extern_kernels .mm (buf280 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf283 )\n        buf284 =buf271 ;del buf271 \n\n        extern_kernels .mm (buf274 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf284 )\n\n        buf285 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf283 ,buf284 ,buf275 ,primals_13 ,primals_14 )\n        buf286 =buf285 [0 ]\n        buf287 =buf285 [1 ]\n        buf288 =buf285 [2 ]\n        del buf285 \n        buf289 =buf278 ;del buf278 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1472 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf289 )\n        buf290 =buf277 ;del buf277 \n\n        extern_kernels .mm (buf280 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf290 )\n\n        buf291 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf289 ,buf290 ,buf281 ,primals_9 ,primals_10 )\n        buf292 =buf291 [0 ]\n        buf293 =buf291 [1 ]\n        buf294 =buf291 [2 ]\n        del buf291 \n        buf295 =buf284 ;del buf284 \n\n        extern_kernels .mm (buf292 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf295 )\n        buf296 =buf283 ;del buf283 \n\n        extern_kernels .mm (buf286 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf296 )\n\n        buf297 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf295 ,buf296 ,buf287 ,primals_13 ,primals_14 )\n        buf298 =buf297 [0 ]\n        buf299 =buf297 [1 ]\n        buf300 =buf297 [2 ]\n        del buf297 \n        buf301 =buf290 ;del buf290 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1536 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf301 )\n        buf302 =buf289 ;del buf289 \n\n        extern_kernels .mm (buf292 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf302 )\n\n        buf303 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf301 ,buf302 ,buf293 ,primals_9 ,primals_10 )\n        buf304 =buf303 [0 ]\n        buf305 =buf303 [1 ]\n        buf306 =buf303 [2 ]\n        del buf303 \n        buf307 =buf296 ;del buf296 \n\n        extern_kernels .mm (buf304 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf307 )\n        buf308 =buf295 ;del buf295 \n\n        extern_kernels .mm (buf298 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf308 )\n\n        buf309 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf307 ,buf308 ,buf299 ,primals_13 ,primals_14 )\n        buf310 =buf309 [0 ]\n        buf311 =buf309 [1 ]\n        buf312 =buf309 [2 ]\n        del buf309 \n        buf313 =buf302 ;del buf302 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1600 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf313 )\n        buf314 =buf301 ;del buf301 \n\n        extern_kernels .mm (buf304 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf314 )\n\n        buf315 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf313 ,buf314 ,buf305 ,primals_9 ,primals_10 )\n        buf316 =buf315 [0 ]\n        buf317 =buf315 [1 ]\n        buf318 =buf315 [2 ]\n        del buf315 \n        buf319 =buf308 ;del buf308 \n\n        extern_kernels .mm (buf316 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf319 )\n        buf320 =buf307 ;del buf307 \n\n        extern_kernels .mm (buf310 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf320 )\n\n        buf321 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf319 ,buf320 ,buf311 ,primals_13 ,primals_14 )\n        buf322 =buf321 [0 ]\n        buf323 =buf321 [1 ]\n        buf324 =buf321 [2 ]\n        del buf321 \n        buf325 =buf314 ;del buf314 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1664 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf325 )\n        buf326 =buf313 ;del buf313 \n\n        extern_kernels .mm (buf316 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf326 )\n\n        buf327 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf325 ,buf326 ,buf317 ,primals_9 ,primals_10 )\n        buf328 =buf327 [0 ]\n        buf329 =buf327 [1 ]\n        buf330 =buf327 [2 ]\n        del buf327 \n        buf331 =buf320 ;del buf320 \n\n        extern_kernels .mm (buf328 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf331 )\n        buf332 =buf319 ;del buf319 \n\n        extern_kernels .mm (buf322 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf332 )\n\n        buf333 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf331 ,buf332 ,buf323 ,primals_13 ,primals_14 )\n        buf334 =buf333 [0 ]\n        buf335 =buf333 [1 ]\n        buf336 =buf333 [2 ]\n        del buf333 \n        buf337 =buf326 ;del buf326 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1728 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf337 )\n        buf338 =buf325 ;del buf325 \n\n        extern_kernels .mm (buf328 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf338 )\n\n        buf339 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf337 ,buf338 ,buf329 ,primals_9 ,primals_10 )\n        buf340 =buf339 [0 ]\n        buf341 =buf339 [1 ]\n        buf342 =buf339 [2 ]\n        del buf339 \n        buf343 =buf332 ;del buf332 \n\n        extern_kernels .mm (buf340 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf343 )\n        buf344 =buf331 ;del buf331 \n\n        extern_kernels .mm (buf334 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf344 )\n\n        buf345 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf343 ,buf344 ,buf335 ,primals_13 ,primals_14 )\n        buf346 =buf345 [0 ]\n        buf347 =buf345 [1 ]\n        buf348 =buf345 [2 ]\n        del buf345 \n        buf349 =buf338 ;del buf338 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1792 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf349 )\n        buf350 =buf337 ;del buf337 \n\n        extern_kernels .mm (buf340 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf350 )\n\n        buf351 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf349 ,buf350 ,buf341 ,primals_9 ,primals_10 )\n        buf352 =buf351 [0 ]\n        buf353 =buf351 [1 ]\n        buf354 =buf351 [2 ]\n        del buf351 \n        buf355 =buf344 ;del buf344 \n\n        extern_kernels .mm (buf352 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf355 )\n        buf356 =buf343 ;del buf343 \n\n        extern_kernels .mm (buf346 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf356 )\n\n        buf357 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf355 ,buf356 ,buf347 ,primals_13 ,primals_14 )\n        buf358 =buf357 [0 ]\n        buf359 =buf357 [1 ]\n        buf360 =buf357 [2 ]\n        del buf357 \n        buf361 =buf350 ;del buf350 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1856 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf361 )\n        buf362 =buf349 ;del buf349 \n\n        extern_kernels .mm (buf352 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf362 )\n\n        buf363 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf361 ,buf362 ,buf353 ,primals_9 ,primals_10 )\n        buf364 =buf363 [0 ]\n        buf365 =buf363 [1 ]\n        buf366 =buf363 [2 ]\n        del buf363 \n        buf367 =buf356 ;del buf356 \n\n        extern_kernels .mm (buf364 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf367 )\n        buf368 =buf355 ;del buf355 \n\n        extern_kernels .mm (buf358 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf368 )\n\n        buf369 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf367 ,buf368 ,buf359 ,primals_13 ,primals_14 )\n        buf370 =buf369 [0 ]\n        buf371 =buf369 [1 ]\n        buf372 =buf369 [2 ]\n        del buf369 \n        buf373 =buf362 ;del buf362 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1920 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf373 )\n        buf374 =buf361 ;del buf361 \n\n        extern_kernels .mm (buf364 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf374 )\n\n        buf375 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf373 ,buf374 ,buf365 ,primals_9 ,primals_10 )\n        buf376 =buf375 [0 ]\n        buf377 =buf375 [1 ]\n        buf378 =buf375 [2 ]\n        del buf375 \n        buf379 =buf368 ;del buf368 \n\n        extern_kernels .mm (buf376 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf379 )\n        buf380 =buf367 ;del buf367 \n\n        extern_kernels .mm (buf370 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf380 )\n\n        buf381 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf379 ,buf380 ,buf371 ,primals_13 ,primals_14 )\n        buf382 =buf381 [0 ]\n        buf383 =buf381 [1 ]\n        buf384 =buf381 [2 ]\n        del buf381 \n        buf385 =buf374 ;del buf374 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1984 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf385 )\n        buf386 =buf373 ;del buf373 \n\n        extern_kernels .mm (buf376 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf386 )\n\n        buf387 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf385 ,buf386 ,buf377 ,primals_9 ,primals_10 )\n        buf388 =buf387 [0 ]\n        buf389 =buf387 [1 ]\n        buf390 =buf387 [2 ]\n        del buf387 \n        buf391 =buf380 ;del buf380 \n\n        extern_kernels .mm (buf388 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf391 )\n        buf392 =buf379 ;del buf379 \n\n        extern_kernels .mm (buf382 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf392 )\n\n        buf393 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf391 ,buf392 ,buf383 ,primals_13 ,primals_14 )\n        buf394 =buf393 [0 ]\n        buf395 =buf393 [1 ]\n        buf396 =buf393 [2 ]\n        del buf393 \n        buf397 =buf386 ;del buf386 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2048 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf397 )\n        buf398 =buf385 ;del buf385 \n\n        extern_kernels .mm (buf388 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf398 )\n\n        buf399 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf397 ,buf398 ,buf389 ,primals_9 ,primals_10 )\n        buf400 =buf399 [0 ]\n        buf401 =buf399 [1 ]\n        buf402 =buf399 [2 ]\n        del buf399 \n        buf403 =buf392 ;del buf392 \n\n        extern_kernels .mm (buf400 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf403 )\n        buf404 =buf391 ;del buf391 \n\n        extern_kernels .mm (buf394 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf404 )\n\n        buf405 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf403 ,buf404 ,buf395 ,primals_13 ,primals_14 )\n        buf406 =buf405 [0 ]\n        buf407 =buf405 [1 ]\n        buf408 =buf405 [2 ]\n        del buf405 \n        buf409 =buf398 ;del buf398 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2112 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf409 )\n        buf410 =buf397 ;del buf397 \n\n        extern_kernels .mm (buf400 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf410 )\n\n        buf411 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf409 ,buf410 ,buf401 ,primals_9 ,primals_10 )\n        buf412 =buf411 [0 ]\n        buf413 =buf411 [1 ]\n        buf414 =buf411 [2 ]\n        del buf411 \n        buf415 =buf404 ;del buf404 \n\n        extern_kernels .mm (buf412 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf415 )\n        buf416 =buf403 ;del buf403 \n\n        extern_kernels .mm (buf406 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf416 )\n\n        buf417 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf415 ,buf416 ,buf407 ,primals_13 ,primals_14 )\n        buf418 =buf417 [0 ]\n        buf419 =buf417 [1 ]\n        buf420 =buf417 [2 ]\n        del buf417 \n        buf421 =buf410 ;del buf410 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2176 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf421 )\n        buf422 =buf409 ;del buf409 \n\n        extern_kernels .mm (buf412 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf422 )\n\n        buf423 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf421 ,buf422 ,buf413 ,primals_9 ,primals_10 )\n        buf424 =buf423 [0 ]\n        buf425 =buf423 [1 ]\n        buf426 =buf423 [2 ]\n        del buf423 \n        buf427 =buf416 ;del buf416 \n\n        extern_kernels .mm (buf424 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf427 )\n        buf428 =buf415 ;del buf415 \n\n        extern_kernels .mm (buf418 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf428 )\n\n        buf429 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf427 ,buf428 ,buf419 ,primals_13 ,primals_14 )\n        buf430 =buf429 [0 ]\n        buf431 =buf429 [1 ]\n        buf432 =buf429 [2 ]\n        del buf429 \n        buf433 =buf422 ;del buf422 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2240 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf433 )\n        buf434 =buf421 ;del buf421 \n\n        extern_kernels .mm (buf424 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf434 )\n\n        buf435 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf433 ,buf434 ,buf425 ,primals_9 ,primals_10 )\n        buf436 =buf435 [0 ]\n        buf437 =buf435 [1 ]\n        buf438 =buf435 [2 ]\n        del buf435 \n        buf439 =buf428 ;del buf428 \n\n        extern_kernels .mm (buf436 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf439 )\n        buf440 =buf427 ;del buf427 \n\n        extern_kernels .mm (buf430 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf440 )\n\n        buf441 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf439 ,buf440 ,buf431 ,primals_13 ,primals_14 )\n        buf442 =buf441 [0 ]\n        buf443 =buf441 [1 ]\n        buf444 =buf441 [2 ]\n        del buf441 \n        buf445 =buf434 ;del buf434 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2304 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf445 )\n        buf446 =buf433 ;del buf433 \n\n        extern_kernels .mm (buf436 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf446 )\n\n        buf447 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf445 ,buf446 ,buf437 ,primals_9 ,primals_10 )\n        buf448 =buf447 [0 ]\n        buf449 =buf447 [1 ]\n        buf450 =buf447 [2 ]\n        del buf447 \n        buf451 =buf440 ;del buf440 \n\n        extern_kernels .mm (buf448 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf451 )\n        buf452 =buf439 ;del buf439 \n\n        extern_kernels .mm (buf442 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf452 )\n\n        buf453 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf451 ,buf452 ,buf443 ,primals_13 ,primals_14 )\n        buf454 =buf453 [0 ]\n        buf455 =buf453 [1 ]\n        buf456 =buf453 [2 ]\n        del buf453 \n        buf457 =buf446 ;del buf446 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2368 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf457 )\n        buf458 =buf445 ;del buf445 \n\n        extern_kernels .mm (buf448 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf458 )\n\n        buf459 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf457 ,buf458 ,buf449 ,primals_9 ,primals_10 )\n        buf460 =buf459 [0 ]\n        buf461 =buf459 [1 ]\n        buf462 =buf459 [2 ]\n        del buf459 \n        buf463 =buf452 ;del buf452 \n\n        extern_kernels .mm (buf460 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf463 )\n        buf464 =buf451 ;del buf451 \n\n        extern_kernels .mm (buf454 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf464 )\n\n        buf465 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf463 ,buf464 ,buf455 ,primals_13 ,primals_14 )\n        buf466 =buf465 [0 ]\n        buf467 =buf465 [1 ]\n        buf468 =buf465 [2 ]\n        del buf465 \n        buf469 =buf458 ;del buf458 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2432 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf469 )\n        buf470 =buf457 ;del buf457 \n\n        extern_kernels .mm (buf460 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf470 )\n\n        buf471 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf469 ,buf470 ,buf461 ,primals_9 ,primals_10 )\n        buf472 =buf471 [0 ]\n        buf473 =buf471 [1 ]\n        buf474 =buf471 [2 ]\n        del buf471 \n        buf475 =buf464 ;del buf464 \n\n        extern_kernels .mm (buf472 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf475 )\n        buf476 =buf463 ;del buf463 \n\n        extern_kernels .mm (buf466 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf476 )\n\n        buf477 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf475 ,buf476 ,buf467 ,primals_13 ,primals_14 )\n        buf478 =buf477 [0 ]\n        buf479 =buf477 [1 ]\n        buf480 =buf477 [2 ]\n        del buf477 \n        buf481 =buf470 ;del buf470 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2496 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf481 )\n        buf482 =buf469 ;del buf469 \n\n        extern_kernels .mm (buf472 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf482 )\n\n        buf483 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf481 ,buf482 ,buf473 ,primals_9 ,primals_10 )\n        buf484 =buf483 [0 ]\n        buf485 =buf483 [1 ]\n        buf486 =buf483 [2 ]\n        del buf483 \n        buf487 =buf476 ;del buf476 \n\n        extern_kernels .mm (buf484 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf487 )\n        buf488 =buf475 ;del buf475 \n\n        extern_kernels .mm (buf478 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf488 )\n\n        buf489 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf487 ,buf488 ,buf479 ,primals_13 ,primals_14 )\n        buf490 =buf489 [0 ]\n        buf491 =buf489 [1 ]\n        buf492 =buf489 [2 ]\n        del buf489 \n        buf493 =buf482 ;del buf482 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2560 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf493 )\n        buf494 =buf481 ;del buf481 \n\n        extern_kernels .mm (buf484 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf494 )\n\n        buf495 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf493 ,buf494 ,buf485 ,primals_9 ,primals_10 )\n        buf496 =buf495 [0 ]\n        buf497 =buf495 [1 ]\n        buf498 =buf495 [2 ]\n        del buf495 \n        buf499 =buf488 ;del buf488 \n\n        extern_kernels .mm (buf496 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf499 )\n        buf500 =buf487 ;del buf487 \n\n        extern_kernels .mm (buf490 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf500 )\n\n        buf501 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf499 ,buf500 ,buf491 ,primals_13 ,primals_14 )\n        buf502 =buf501 [0 ]\n        buf503 =buf501 [1 ]\n        buf504 =buf501 [2 ]\n        del buf501 \n        buf505 =buf494 ;del buf494 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2624 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf505 )\n        buf506 =buf493 ;del buf493 \n\n        extern_kernels .mm (buf496 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf506 )\n\n        buf507 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf505 ,buf506 ,buf497 ,primals_9 ,primals_10 )\n        buf508 =buf507 [0 ]\n        buf509 =buf507 [1 ]\n        buf510 =buf507 [2 ]\n        del buf507 \n        buf511 =buf500 ;del buf500 \n\n        extern_kernels .mm (buf508 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf511 )\n        buf512 =buf499 ;del buf499 \n\n        extern_kernels .mm (buf502 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf512 )\n\n        buf513 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf511 ,buf512 ,buf503 ,primals_13 ,primals_14 )\n        buf514 =buf513 [0 ]\n        buf515 =buf513 [1 ]\n        buf516 =buf513 [2 ]\n        del buf513 \n        buf517 =buf506 ;del buf506 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2688 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf517 )\n        buf518 =buf505 ;del buf505 \n\n        extern_kernels .mm (buf508 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf518 )\n\n        buf519 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf517 ,buf518 ,buf509 ,primals_9 ,primals_10 )\n        buf520 =buf519 [0 ]\n        buf521 =buf519 [1 ]\n        buf522 =buf519 [2 ]\n        del buf519 \n        buf523 =buf512 ;del buf512 \n\n        extern_kernels .mm (buf520 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf523 )\n        buf524 =buf511 ;del buf511 \n\n        extern_kernels .mm (buf514 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf524 )\n\n        buf525 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf523 ,buf524 ,buf515 ,primals_13 ,primals_14 )\n        buf526 =buf525 [0 ]\n        buf527 =buf525 [1 ]\n        buf528 =buf525 [2 ]\n        del buf525 \n        buf529 =buf518 ;del buf518 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2752 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf529 )\n        buf530 =buf517 ;del buf517 \n\n        extern_kernels .mm (buf520 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf530 )\n\n        buf531 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf529 ,buf530 ,buf521 ,primals_9 ,primals_10 )\n        buf532 =buf531 [0 ]\n        buf533 =buf531 [1 ]\n        buf534 =buf531 [2 ]\n        del buf531 \n        buf535 =buf524 ;del buf524 \n\n        extern_kernels .mm (buf532 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf535 )\n        buf536 =buf523 ;del buf523 \n\n        extern_kernels .mm (buf526 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf536 )\n\n        buf537 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf535 ,buf536 ,buf527 ,primals_13 ,primals_14 )\n        buf538 =buf537 [0 ]\n        buf539 =buf537 [1 ]\n        buf540 =buf537 [2 ]\n        del buf537 \n        buf541 =buf530 ;del buf530 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2816 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf541 )\n        buf542 =buf529 ;del buf529 \n\n        extern_kernels .mm (buf532 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf542 )\n\n        buf543 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf541 ,buf542 ,buf533 ,primals_9 ,primals_10 )\n        buf544 =buf543 [0 ]\n        buf545 =buf543 [1 ]\n        buf546 =buf543 [2 ]\n        del buf543 \n        buf547 =buf536 ;del buf536 \n\n        extern_kernels .mm (buf544 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf547 )\n        buf548 =buf535 ;del buf535 \n\n        extern_kernels .mm (buf538 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf548 )\n\n        buf549 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf547 ,buf548 ,buf539 ,primals_13 ,primals_14 )\n        buf550 =buf549 [0 ]\n        buf551 =buf549 [1 ]\n        buf552 =buf549 [2 ]\n        del buf549 \n        buf553 =buf542 ;del buf542 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2880 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf553 )\n        buf554 =buf541 ;del buf541 \n\n        extern_kernels .mm (buf544 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf554 )\n\n        buf555 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf553 ,buf554 ,buf545 ,primals_9 ,primals_10 )\n        buf556 =buf555 [0 ]\n        buf557 =buf555 [1 ]\n        buf558 =buf555 [2 ]\n        del buf555 \n        buf559 =buf548 ;del buf548 \n\n        extern_kernels .mm (buf556 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf559 )\n        buf560 =buf547 ;del buf547 \n\n        extern_kernels .mm (buf550 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf560 )\n\n        buf561 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf559 ,buf560 ,buf551 ,primals_13 ,primals_14 )\n        buf562 =buf561 [0 ]\n        buf563 =buf561 [1 ]\n        buf564 =buf561 [2 ]\n        del buf561 \n        buf565 =buf554 ;del buf554 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2944 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf565 )\n        buf566 =buf553 ;del buf553 \n\n        extern_kernels .mm (buf556 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf566 )\n\n        buf567 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf565 ,buf566 ,buf557 ,primals_9 ,primals_10 )\n        buf568 =buf567 [0 ]\n        buf569 =buf567 [1 ]\n        buf570 =buf567 [2 ]\n        del buf567 \n        buf571 =buf560 ;del buf560 \n\n        extern_kernels .mm (buf568 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf571 )\n        buf572 =buf559 ;del buf559 \n\n        extern_kernels .mm (buf562 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf572 )\n\n        buf573 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf571 ,buf572 ,buf563 ,primals_13 ,primals_14 )\n        buf574 =buf573 [0 ]\n        buf575 =buf573 [1 ]\n        buf576 =buf573 [2 ]\n        del buf573 \n        buf577 =buf566 ;del buf566 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3008 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf577 )\n        buf578 =buf565 ;del buf565 \n\n        extern_kernels .mm (buf568 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf578 )\n\n        buf579 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf577 ,buf578 ,buf569 ,primals_9 ,primals_10 )\n        buf580 =buf579 [0 ]\n        buf581 =buf579 [1 ]\n        buf582 =buf579 [2 ]\n        del buf579 \n        buf583 =buf572 ;del buf572 \n\n        extern_kernels .mm (buf580 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf583 )\n        buf584 =buf571 ;del buf571 \n\n        extern_kernels .mm (buf574 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf584 )\n\n        buf585 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf583 ,buf584 ,buf575 ,primals_13 ,primals_14 )\n        buf586 =buf585 [0 ]\n        buf587 =buf585 [1 ]\n        buf588 =buf585 [2 ]\n        del buf585 \n        buf589 =buf578 ;del buf578 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3072 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf589 )\n        buf590 =buf577 ;del buf577 \n\n        extern_kernels .mm (buf580 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf590 )\n\n        buf591 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf589 ,buf590 ,buf581 ,primals_9 ,primals_10 )\n        buf592 =buf591 [0 ]\n        buf593 =buf591 [1 ]\n        buf594 =buf591 [2 ]\n        del buf591 \n        buf595 =buf584 ;del buf584 \n\n        extern_kernels .mm (buf592 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf595 )\n        buf596 =buf583 ;del buf583 \n\n        extern_kernels .mm (buf586 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf596 )\n\n        buf597 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf595 ,buf596 ,buf587 ,primals_13 ,primals_14 )\n        buf598 =buf597 [0 ]\n        buf599 =buf597 [1 ]\n        buf600 =buf597 [2 ]\n        del buf597 \n        buf601 =buf590 ;del buf590 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3136 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf601 )\n        buf602 =buf589 ;del buf589 \n\n        extern_kernels .mm (buf592 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf602 )\n\n        buf603 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf601 ,buf602 ,buf593 ,primals_9 ,primals_10 )\n        buf604 =buf603 [0 ]\n        buf605 =buf603 [1 ]\n        buf606 =buf603 [2 ]\n        del buf603 \n        buf607 =buf596 ;del buf596 \n\n        extern_kernels .mm (buf604 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf607 )\n        buf608 =buf595 ;del buf595 \n\n        extern_kernels .mm (buf598 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf608 )\n\n        buf609 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf607 ,buf608 ,buf599 ,primals_13 ,primals_14 )\n        buf610 =buf609 [0 ]\n        buf611 =buf609 [1 ]\n        buf612 =buf609 [2 ]\n        del buf609 \n        buf613 =buf602 ;del buf602 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3200 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf613 )\n        buf614 =buf601 ;del buf601 \n\n        extern_kernels .mm (buf604 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf614 )\n\n        buf615 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf613 ,buf614 ,buf605 ,primals_9 ,primals_10 )\n        buf616 =buf615 [0 ]\n        buf617 =buf615 [1 ]\n        buf618 =buf615 [2 ]\n        del buf615 \n        buf619 =buf608 ;del buf608 \n\n        extern_kernels .mm (buf616 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf619 )\n        buf620 =buf607 ;del buf607 \n\n        extern_kernels .mm (buf610 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf620 )\n\n        buf621 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf619 ,buf620 ,buf611 ,primals_13 ,primals_14 )\n        buf622 =buf621 [0 ]\n        buf623 =buf621 [1 ]\n        buf624 =buf621 [2 ]\n        del buf621 \n        buf625 =buf614 ;del buf614 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3264 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf625 )\n        buf626 =buf613 ;del buf613 \n\n        extern_kernels .mm (buf616 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf626 )\n\n        buf627 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf625 ,buf626 ,buf617 ,primals_9 ,primals_10 )\n        buf628 =buf627 [0 ]\n        buf629 =buf627 [1 ]\n        buf630 =buf627 [2 ]\n        del buf627 \n        buf631 =buf620 ;del buf620 \n\n        extern_kernels .mm (buf628 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf631 )\n        buf632 =buf619 ;del buf619 \n\n        extern_kernels .mm (buf622 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf632 )\n\n        buf633 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf631 ,buf632 ,buf623 ,primals_13 ,primals_14 )\n        buf634 =buf633 [0 ]\n        buf635 =buf633 [1 ]\n        buf636 =buf633 [2 ]\n        del buf633 \n        buf637 =buf626 ;del buf626 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3328 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf637 )\n        buf638 =buf625 ;del buf625 \n\n        extern_kernels .mm (buf628 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf638 )\n\n        buf639 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf637 ,buf638 ,buf629 ,primals_9 ,primals_10 )\n        buf640 =buf639 [0 ]\n        buf641 =buf639 [1 ]\n        buf642 =buf639 [2 ]\n        del buf639 \n        buf643 =buf632 ;del buf632 \n\n        extern_kernels .mm (buf640 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf643 )\n        buf644 =buf631 ;del buf631 \n\n        extern_kernels .mm (buf634 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf644 )\n\n        buf645 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf643 ,buf644 ,buf635 ,primals_13 ,primals_14 )\n        buf646 =buf645 [0 ]\n        buf647 =buf645 [1 ]\n        buf648 =buf645 [2 ]\n        del buf645 \n        buf649 =buf638 ;del buf638 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3392 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf649 )\n        buf650 =buf637 ;del buf637 \n\n        extern_kernels .mm (buf640 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf650 )\n\n        buf651 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf649 ,buf650 ,buf641 ,primals_9 ,primals_10 )\n        buf652 =buf651 [0 ]\n        buf653 =buf651 [1 ]\n        buf654 =buf651 [2 ]\n        del buf651 \n        buf655 =buf644 ;del buf644 \n\n        extern_kernels .mm (buf652 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf655 )\n        buf656 =buf643 ;del buf643 \n\n        extern_kernels .mm (buf646 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf656 )\n\n        buf657 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf655 ,buf656 ,buf647 ,primals_13 ,primals_14 )\n        buf658 =buf657 [0 ]\n        buf659 =buf657 [1 ]\n        buf660 =buf657 [2 ]\n        del buf657 \n        buf661 =buf650 ;del buf650 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3456 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf661 )\n        buf662 =buf649 ;del buf649 \n\n        extern_kernels .mm (buf652 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf662 )\n\n        buf663 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf661 ,buf662 ,buf653 ,primals_9 ,primals_10 )\n        buf664 =buf663 [0 ]\n        buf665 =buf663 [1 ]\n        buf666 =buf663 [2 ]\n        del buf663 \n        buf667 =buf656 ;del buf656 \n\n        extern_kernels .mm (buf664 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf667 )\n        buf668 =buf655 ;del buf655 \n\n        extern_kernels .mm (buf658 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf668 )\n\n        buf669 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf667 ,buf668 ,buf659 ,primals_13 ,primals_14 )\n        buf670 =buf669 [0 ]\n        buf671 =buf669 [1 ]\n        buf672 =buf669 [2 ]\n        del buf669 \n        buf673 =buf662 ;del buf662 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3520 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf673 )\n        buf674 =buf661 ;del buf661 \n\n        extern_kernels .mm (buf664 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf674 )\n\n        buf675 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf673 ,buf674 ,buf665 ,primals_9 ,primals_10 )\n        buf676 =buf675 [0 ]\n        buf677 =buf675 [1 ]\n        buf678 =buf675 [2 ]\n        del buf675 \n        buf679 =buf668 ;del buf668 \n\n        extern_kernels .mm (buf676 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf679 )\n        buf680 =buf667 ;del buf667 \n\n        extern_kernels .mm (buf670 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf680 )\n\n        buf681 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf679 ,buf680 ,buf671 ,primals_13 ,primals_14 )\n        buf682 =buf681 [0 ]\n        buf683 =buf681 [1 ]\n        buf684 =buf681 [2 ]\n        del buf681 \n        buf685 =buf674 ;del buf674 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3584 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf685 )\n        buf686 =buf673 ;del buf673 \n\n        extern_kernels .mm (buf676 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf686 )\n\n        buf687 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf685 ,buf686 ,buf677 ,primals_9 ,primals_10 )\n        buf688 =buf687 [0 ]\n        buf689 =buf687 [1 ]\n        buf690 =buf687 [2 ]\n        del buf687 \n        buf691 =buf680 ;del buf680 \n\n        extern_kernels .mm (buf688 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf691 )\n        buf692 =buf679 ;del buf679 \n\n        extern_kernels .mm (buf682 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf692 )\n\n        buf693 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf691 ,buf692 ,buf683 ,primals_13 ,primals_14 )\n        buf694 =buf693 [0 ]\n        buf695 =buf693 [1 ]\n        buf696 =buf693 [2 ]\n        del buf693 \n        buf697 =buf686 ;del buf686 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3648 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf697 )\n        buf698 =buf685 ;del buf685 \n\n        extern_kernels .mm (buf688 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf698 )\n\n        buf699 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf697 ,buf698 ,buf689 ,primals_9 ,primals_10 )\n        buf700 =buf699 [0 ]\n        buf701 =buf699 [1 ]\n        buf702 =buf699 [2 ]\n        del buf699 \n        buf703 =buf692 ;del buf692 \n\n        extern_kernels .mm (buf700 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf703 )\n        buf704 =buf691 ;del buf691 \n\n        extern_kernels .mm (buf694 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf704 )\n\n        buf705 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf703 ,buf704 ,buf695 ,primals_13 ,primals_14 )\n        buf706 =buf705 [0 ]\n        buf707 =buf705 [1 ]\n        buf708 =buf705 [2 ]\n        del buf705 \n        buf709 =buf698 ;del buf698 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3712 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf709 )\n        buf710 =buf697 ;del buf697 \n\n        extern_kernels .mm (buf700 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf710 )\n\n        buf711 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf709 ,buf710 ,buf701 ,primals_9 ,primals_10 )\n        buf712 =buf711 [0 ]\n        buf713 =buf711 [1 ]\n        buf714 =buf711 [2 ]\n        del buf711 \n        buf715 =buf704 ;del buf704 \n\n        extern_kernels .mm (buf712 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf715 )\n        buf716 =buf703 ;del buf703 \n\n        extern_kernels .mm (buf706 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf716 )\n\n        buf717 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf715 ,buf716 ,buf707 ,primals_13 ,primals_14 )\n        buf718 =buf717 [0 ]\n        buf719 =buf717 [1 ]\n        buf720 =buf717 [2 ]\n        del buf717 \n        buf721 =buf710 ;del buf710 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3776 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf721 )\n        buf722 =buf709 ;del buf709 \n\n        extern_kernels .mm (buf712 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf722 )\n\n        buf723 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf721 ,buf722 ,buf713 ,primals_9 ,primals_10 )\n        buf724 =buf723 [0 ]\n        buf725 =buf723 [1 ]\n        buf726 =buf723 [2 ]\n        del buf723 \n        buf727 =buf716 ;del buf716 \n\n        extern_kernels .mm (buf724 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf727 )\n        buf728 =buf715 ;del buf715 \n\n        extern_kernels .mm (buf718 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf728 )\n\n        buf729 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf727 ,buf728 ,buf719 ,primals_13 ,primals_14 )\n        buf730 =buf729 [0 ]\n        buf731 =buf729 [1 ]\n        buf732 =buf729 [2 ]\n        del buf729 \n        buf733 =buf722 ;del buf722 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3840 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf733 )\n        buf734 =buf721 ;del buf721 \n\n        extern_kernels .mm (buf724 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf734 )\n\n        buf735 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf733 ,buf734 ,buf725 ,primals_9 ,primals_10 )\n        buf736 =buf735 [0 ]\n        buf737 =buf735 [1 ]\n        buf738 =buf735 [2 ]\n        del buf735 \n        buf739 =buf728 ;del buf728 \n\n        extern_kernels .mm (buf736 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf739 )\n        buf740 =buf727 ;del buf727 \n\n        extern_kernels .mm (buf730 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf740 )\n\n        buf741 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf739 ,buf740 ,buf731 ,primals_13 ,primals_14 )\n        buf742 =buf741 [0 ]\n        buf743 =buf741 [1 ]\n        buf744 =buf741 [2 ]\n        del buf741 \n        buf745 =buf734 ;del buf734 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3904 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf745 )\n        buf746 =buf733 ;del buf733 \n\n        extern_kernels .mm (buf736 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf746 )\n\n        buf747 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf745 ,buf746 ,buf737 ,primals_9 ,primals_10 )\n        buf748 =buf747 [0 ]\n        buf749 =buf747 [1 ]\n        buf750 =buf747 [2 ]\n        del buf747 \n        buf751 =buf740 ;del buf740 \n\n        extern_kernels .mm (buf748 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf751 )\n        buf752 =buf739 ;del buf739 \n\n        extern_kernels .mm (buf742 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf752 )\n\n        buf753 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf751 ,buf752 ,buf743 ,primals_13 ,primals_14 )\n        buf754 =buf753 [0 ]\n        buf755 =buf753 [1 ]\n        buf756 =buf753 [2 ]\n        del buf753 \n        buf757 =buf746 ;del buf746 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3968 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf757 )\n        buf758 =buf745 ;del buf745 \n\n        extern_kernels .mm (buf748 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf758 )\n\n        buf759 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf757 ,buf758 ,buf749 ,primals_9 ,primals_10 )\n        buf760 =buf759 [0 ]\n        buf761 =buf759 [1 ]\n        buf762 =buf759 [2 ]\n        del buf759 \n        buf763 =buf752 ;del buf752 \n\n        extern_kernels .mm (buf760 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf763 )\n        buf764 =buf751 ;del buf751 \n\n        extern_kernels .mm (buf754 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf764 )\n\n        buf765 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf763 ,buf764 ,buf755 ,primals_13 ,primals_14 )\n        buf766 =buf765 [0 ]\n        buf767 =buf765 [1 ]\n        buf768 =buf765 [2 ]\n        del buf765 \n        buf769 =buf758 ;del buf758 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4032 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf769 )\n        buf770 =buf757 ;del buf757 \n\n        extern_kernels .mm (buf760 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf770 )\n\n        buf771 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf769 ,buf770 ,buf761 ,primals_9 ,primals_10 )\n        buf772 =buf771 [0 ]\n        buf773 =buf771 [1 ]\n        buf774 =buf771 [2 ]\n        del buf771 \n        buf775 =buf764 ;del buf764 \n\n        extern_kernels .mm (buf772 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf775 )\n        buf776 =buf763 ;del buf763 \n\n        extern_kernels .mm (buf766 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf776 )\n\n        buf777 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf775 ,buf776 ,buf767 ,primals_13 ,primals_14 )\n        buf778 =buf777 [0 ]\n        buf779 =buf777 [1 ]\n        buf780 =buf777 [2 ]\n        del buf777 \n        buf781 =buf770 ;del buf770 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4096 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf781 )\n        buf782 =buf769 ;del buf769 \n\n        extern_kernels .mm (buf772 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf782 )\n\n        buf783 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf781 ,buf782 ,buf773 ,primals_9 ,primals_10 )\n        buf784 =buf783 [0 ]\n        buf785 =buf783 [1 ]\n        buf786 =buf783 [2 ]\n        del buf783 \n        buf787 =buf776 ;del buf776 \n\n        extern_kernels .mm (buf784 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf787 )\n        buf788 =buf775 ;del buf775 \n\n        extern_kernels .mm (buf778 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf788 )\n\n        buf789 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf787 ,buf788 ,buf779 ,primals_13 ,primals_14 )\n        buf790 =buf789 [0 ]\n        buf791 =buf789 [1 ]\n        buf792 =buf789 [2 ]\n        del buf789 \n        buf793 =buf782 ;del buf782 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4160 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf793 )\n        buf794 =buf781 ;del buf781 \n\n        extern_kernels .mm (buf784 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf794 )\n\n        buf795 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf793 ,buf794 ,buf785 ,primals_9 ,primals_10 )\n        buf796 =buf795 [0 ]\n        buf797 =buf795 [1 ]\n        buf798 =buf795 [2 ]\n        del buf795 \n        buf799 =buf788 ;del buf788 \n\n        extern_kernels .mm (buf796 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf799 )\n        buf800 =buf787 ;del buf787 \n\n        extern_kernels .mm (buf790 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf800 )\n\n        buf801 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf799 ,buf800 ,buf791 ,primals_13 ,primals_14 )\n        buf802 =buf801 [0 ]\n        buf803 =buf801 [1 ]\n        buf804 =buf801 [2 ]\n        del buf801 \n        buf805 =buf794 ;del buf794 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4224 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf805 )\n        buf806 =buf793 ;del buf793 \n\n        extern_kernels .mm (buf796 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf806 )\n\n        buf807 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf805 ,buf806 ,buf797 ,primals_9 ,primals_10 )\n        buf808 =buf807 [0 ]\n        buf809 =buf807 [1 ]\n        buf810 =buf807 [2 ]\n        del buf807 \n        buf811 =buf800 ;del buf800 \n\n        extern_kernels .mm (buf808 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf811 )\n        buf812 =buf799 ;del buf799 \n\n        extern_kernels .mm (buf802 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf812 )\n\n        buf813 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf811 ,buf812 ,buf803 ,primals_13 ,primals_14 )\n        buf814 =buf813 [0 ]\n        buf815 =buf813 [1 ]\n        buf816 =buf813 [2 ]\n        del buf813 \n        buf817 =buf806 ;del buf806 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4288 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf817 )\n        buf818 =buf805 ;del buf805 \n\n        extern_kernels .mm (buf808 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf818 )\n\n        buf819 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf817 ,buf818 ,buf809 ,primals_9 ,primals_10 )\n        buf820 =buf819 [0 ]\n        buf821 =buf819 [1 ]\n        buf822 =buf819 [2 ]\n        del buf819 \n        buf823 =buf812 ;del buf812 \n\n        extern_kernels .mm (buf820 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf823 )\n        buf824 =buf811 ;del buf811 \n\n        extern_kernels .mm (buf814 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf824 )\n\n        buf825 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf823 ,buf824 ,buf815 ,primals_13 ,primals_14 )\n        buf826 =buf825 [0 ]\n        buf827 =buf825 [1 ]\n        buf828 =buf825 [2 ]\n        del buf825 \n        buf829 =buf818 ;del buf818 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4352 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf829 )\n        buf830 =buf817 ;del buf817 \n\n        extern_kernels .mm (buf820 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf830 )\n\n        buf831 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf829 ,buf830 ,buf821 ,primals_9 ,primals_10 )\n        buf832 =buf831 [0 ]\n        buf833 =buf831 [1 ]\n        buf834 =buf831 [2 ]\n        del buf831 \n        buf835 =buf824 ;del buf824 \n\n        extern_kernels .mm (buf832 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf835 )\n        buf836 =buf823 ;del buf823 \n\n        extern_kernels .mm (buf826 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf836 )\n\n        buf837 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf835 ,buf836 ,buf827 ,primals_13 ,primals_14 )\n        buf838 =buf837 [0 ]\n        buf839 =buf837 [1 ]\n        buf840 =buf837 [2 ]\n        del buf837 \n        buf841 =buf830 ;del buf830 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4416 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf841 )\n        buf842 =buf829 ;del buf829 \n\n        extern_kernels .mm (buf832 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf842 )\n\n        buf843 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf841 ,buf842 ,buf833 ,primals_9 ,primals_10 )\n        buf844 =buf843 [0 ]\n        buf845 =buf843 [1 ]\n        buf846 =buf843 [2 ]\n        del buf843 \n        buf847 =buf836 ;del buf836 \n\n        extern_kernels .mm (buf844 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf847 )\n        buf848 =buf835 ;del buf835 \n\n        extern_kernels .mm (buf838 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf848 )\n\n        buf849 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf847 ,buf848 ,buf839 ,primals_13 ,primals_14 )\n        buf850 =buf849 [0 ]\n        buf851 =buf849 [1 ]\n        buf852 =buf849 [2 ]\n        del buf849 \n        buf853 =buf842 ;del buf842 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4480 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf853 )\n        buf854 =buf841 ;del buf841 \n\n        extern_kernels .mm (buf844 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf854 )\n\n        buf855 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf853 ,buf854 ,buf845 ,primals_9 ,primals_10 )\n        buf856 =buf855 [0 ]\n        buf857 =buf855 [1 ]\n        buf858 =buf855 [2 ]\n        del buf855 \n        buf859 =buf848 ;del buf848 \n\n        extern_kernels .mm (buf856 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf859 )\n        buf860 =buf847 ;del buf847 \n\n        extern_kernels .mm (buf850 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf860 )\n\n        buf861 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf859 ,buf860 ,buf851 ,primals_13 ,primals_14 )\n        buf862 =buf861 [0 ]\n        buf863 =buf861 [1 ]\n        buf864 =buf861 [2 ]\n        del buf861 \n        buf865 =buf854 ;del buf854 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4544 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf865 )\n        buf866 =buf853 ;del buf853 \n\n        extern_kernels .mm (buf856 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf866 )\n\n        buf867 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf865 ,buf866 ,buf857 ,primals_9 ,primals_10 )\n        buf868 =buf867 [0 ]\n        buf869 =buf867 [1 ]\n        buf870 =buf867 [2 ]\n        del buf867 \n        buf871 =buf860 ;del buf860 \n\n        extern_kernels .mm (buf868 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf871 )\n        buf872 =buf859 ;del buf859 \n\n        extern_kernels .mm (buf862 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf872 )\n\n        buf873 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf871 ,buf872 ,buf863 ,primals_13 ,primals_14 )\n        buf874 =buf873 [0 ]\n        buf875 =buf873 [1 ]\n        buf876 =buf873 [2 ]\n        del buf873 \n        buf877 =buf866 ;del buf866 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4608 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf877 )\n        buf878 =buf865 ;del buf865 \n\n        extern_kernels .mm (buf868 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf878 )\n\n        buf879 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf877 ,buf878 ,buf869 ,primals_9 ,primals_10 )\n        buf880 =buf879 [0 ]\n        buf881 =buf879 [1 ]\n        buf882 =buf879 [2 ]\n        del buf879 \n        buf883 =buf872 ;del buf872 \n\n        extern_kernels .mm (buf880 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf883 )\n        buf884 =buf871 ;del buf871 \n\n        extern_kernels .mm (buf874 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf884 )\n\n        buf885 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf883 ,buf884 ,buf875 ,primals_13 ,primals_14 )\n        buf886 =buf885 [0 ]\n        buf887 =buf885 [1 ]\n        buf888 =buf885 [2 ]\n        del buf885 \n        buf889 =buf878 ;del buf878 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4672 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf889 )\n        buf890 =buf877 ;del buf877 \n\n        extern_kernels .mm (buf880 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf890 )\n\n        buf891 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf889 ,buf890 ,buf881 ,primals_9 ,primals_10 )\n        buf892 =buf891 [0 ]\n        buf893 =buf891 [1 ]\n        buf894 =buf891 [2 ]\n        del buf891 \n        buf895 =buf884 ;del buf884 \n\n        extern_kernels .mm (buf892 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf895 )\n        buf896 =buf883 ;del buf883 \n\n        extern_kernels .mm (buf886 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf896 )\n\n        buf897 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf895 ,buf896 ,buf887 ,primals_13 ,primals_14 )\n        buf898 =buf897 [0 ]\n        buf899 =buf897 [1 ]\n        buf900 =buf897 [2 ]\n        del buf897 \n        buf901 =buf890 ;del buf890 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4736 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf901 )\n        buf902 =buf889 ;del buf889 \n\n        extern_kernels .mm (buf892 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf902 )\n\n        buf903 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf901 ,buf902 ,buf893 ,primals_9 ,primals_10 )\n        buf904 =buf903 [0 ]\n        buf905 =buf903 [1 ]\n        buf906 =buf903 [2 ]\n        del buf903 \n        buf907 =buf896 ;del buf896 \n\n        extern_kernels .mm (buf904 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf907 )\n        buf908 =buf895 ;del buf895 \n\n        extern_kernels .mm (buf898 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf908 )\n\n        buf909 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf907 ,buf908 ,buf899 ,primals_13 ,primals_14 )\n        buf910 =buf909 [0 ]\n        buf911 =buf909 [1 ]\n        buf912 =buf909 [2 ]\n        del buf909 \n        buf913 =buf902 ;del buf902 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4800 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf913 )\n        buf914 =buf901 ;del buf901 \n\n        extern_kernels .mm (buf904 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf914 )\n\n        buf915 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf913 ,buf914 ,buf905 ,primals_9 ,primals_10 )\n        buf916 =buf915 [0 ]\n        buf917 =buf915 [1 ]\n        buf918 =buf915 [2 ]\n        del buf915 \n        buf919 =buf908 ;del buf908 \n\n        extern_kernels .mm (buf916 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf919 )\n        buf920 =buf907 ;del buf907 \n\n        extern_kernels .mm (buf910 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf920 )\n\n        buf921 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf919 ,buf920 ,buf911 ,primals_13 ,primals_14 )\n        buf922 =buf921 [0 ]\n        buf923 =buf921 [1 ]\n        buf924 =buf921 [2 ]\n        del buf921 \n        buf925 =buf914 ;del buf914 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4864 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf925 )\n        buf926 =buf913 ;del buf913 \n\n        extern_kernels .mm (buf916 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf926 )\n\n        buf927 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf925 ,buf926 ,buf917 ,primals_9 ,primals_10 )\n        buf928 =buf927 [0 ]\n        buf929 =buf927 [1 ]\n        buf930 =buf927 [2 ]\n        del buf927 \n        buf931 =buf920 ;del buf920 \n\n        extern_kernels .mm (buf928 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf931 )\n        buf932 =buf919 ;del buf919 \n\n        extern_kernels .mm (buf922 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf932 )\n\n        buf933 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf931 ,buf932 ,buf923 ,primals_13 ,primals_14 )\n        buf934 =buf933 [0 ]\n        buf935 =buf933 [1 ]\n        buf936 =buf933 [2 ]\n        del buf933 \n        buf937 =buf926 ;del buf926 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4928 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf937 )\n        buf938 =buf925 ;del buf925 \n\n        extern_kernels .mm (buf928 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf938 )\n\n        buf939 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf937 ,buf938 ,buf929 ,primals_9 ,primals_10 )\n        buf940 =buf939 [0 ]\n        buf941 =buf939 [1 ]\n        buf942 =buf939 [2 ]\n        del buf939 \n        buf943 =buf932 ;del buf932 \n\n        extern_kernels .mm (buf940 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf943 )\n        buf944 =buf931 ;del buf931 \n\n        extern_kernels .mm (buf934 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf944 )\n\n        buf945 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf943 ,buf944 ,buf935 ,primals_13 ,primals_14 )\n        buf946 =buf945 [0 ]\n        buf947 =buf945 [1 ]\n        buf948 =buf945 [2 ]\n        del buf945 \n        buf949 =buf938 ;del buf938 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4992 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf949 )\n        buf950 =buf937 ;del buf937 \n\n        extern_kernels .mm (buf940 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf950 )\n\n        buf951 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf949 ,buf950 ,buf941 ,primals_9 ,primals_10 )\n        buf952 =buf951 [0 ]\n        buf953 =buf951 [1 ]\n        buf954 =buf951 [2 ]\n        del buf951 \n        buf955 =buf944 ;del buf944 \n\n        extern_kernels .mm (buf952 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf955 )\n        buf956 =buf943 ;del buf943 \n\n        extern_kernels .mm (buf946 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf956 )\n\n        buf957 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf955 ,buf956 ,buf947 ,primals_13 ,primals_14 )\n        buf958 =buf957 [0 ]\n        buf959 =buf957 [1 ]\n        buf960 =buf957 [2 ]\n        del buf957 \n        buf961 =buf950 ;del buf950 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5056 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf961 )\n        buf962 =buf949 ;del buf949 \n\n        extern_kernels .mm (buf952 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf962 )\n\n        buf963 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf961 ,buf962 ,buf953 ,primals_9 ,primals_10 )\n        buf964 =buf963 [0 ]\n        buf965 =buf963 [1 ]\n        buf966 =buf963 [2 ]\n        del buf963 \n        buf967 =buf956 ;del buf956 \n\n        extern_kernels .mm (buf964 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf967 )\n        buf968 =buf955 ;del buf955 \n\n        extern_kernels .mm (buf958 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf968 )\n\n        buf969 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf967 ,buf968 ,buf959 ,primals_13 ,primals_14 )\n        buf970 =buf969 [0 ]\n        buf971 =buf969 [1 ]\n        buf972 =buf969 [2 ]\n        del buf969 \n        buf973 =buf962 ;del buf962 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5120 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf973 )\n        buf974 =buf961 ;del buf961 \n\n        extern_kernels .mm (buf964 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf974 )\n\n        buf975 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf973 ,buf974 ,buf965 ,primals_9 ,primals_10 )\n        buf976 =buf975 [0 ]\n        buf977 =buf975 [1 ]\n        buf978 =buf975 [2 ]\n        del buf975 \n        buf979 =buf968 ;del buf968 \n\n        extern_kernels .mm (buf976 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf979 )\n        buf980 =buf967 ;del buf967 \n\n        extern_kernels .mm (buf970 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf980 )\n\n        buf981 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf979 ,buf980 ,buf971 ,primals_13 ,primals_14 )\n        buf982 =buf981 [0 ]\n        buf983 =buf981 [1 ]\n        buf984 =buf981 [2 ]\n        del buf981 \n        buf985 =buf974 ;del buf974 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5184 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf985 )\n        buf986 =buf973 ;del buf973 \n\n        extern_kernels .mm (buf976 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf986 )\n\n        buf987 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf985 ,buf986 ,buf977 ,primals_9 ,primals_10 )\n        buf988 =buf987 [0 ]\n        buf989 =buf987 [1 ]\n        buf990 =buf987 [2 ]\n        del buf987 \n        buf991 =buf980 ;del buf980 \n\n        extern_kernels .mm (buf988 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf991 )\n        buf992 =buf979 ;del buf979 \n\n        extern_kernels .mm (buf982 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf992 )\n\n        buf993 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf991 ,buf992 ,buf983 ,primals_13 ,primals_14 )\n        buf994 =buf993 [0 ]\n        buf995 =buf993 [1 ]\n        buf996 =buf993 [2 ]\n        del buf993 \n        buf997 =buf986 ;del buf986 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5248 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf997 )\n        buf998 =buf985 ;del buf985 \n\n        extern_kernels .mm (buf988 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf998 )\n\n        buf999 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf997 ,buf998 ,buf989 ,primals_9 ,primals_10 )\n        buf1000 =buf999 [0 ]\n        buf1001 =buf999 [1 ]\n        buf1002 =buf999 [2 ]\n        del buf999 \n        buf1003 =buf992 ;del buf992 \n\n        extern_kernels .mm (buf1000 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1003 )\n        buf1004 =buf991 ;del buf991 \n\n        extern_kernels .mm (buf994 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1004 )\n\n        buf1005 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1003 ,buf1004 ,buf995 ,primals_13 ,primals_14 )\n        buf1006 =buf1005 [0 ]\n        buf1007 =buf1005 [1 ]\n        buf1008 =buf1005 [2 ]\n        del buf1005 \n        buf1009 =buf998 ;del buf998 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5312 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1009 )\n        buf1010 =buf997 ;del buf997 \n\n        extern_kernels .mm (buf1000 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1010 )\n\n        buf1011 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1009 ,buf1010 ,buf1001 ,primals_9 ,primals_10 )\n        buf1012 =buf1011 [0 ]\n        buf1013 =buf1011 [1 ]\n        buf1014 =buf1011 [2 ]\n        del buf1011 \n        buf1015 =buf1004 ;del buf1004 \n\n        extern_kernels .mm (buf1012 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1015 )\n        buf1016 =buf1003 ;del buf1003 \n\n        extern_kernels .mm (buf1006 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1016 )\n\n        buf1017 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1015 ,buf1016 ,buf1007 ,primals_13 ,primals_14 )\n        buf1018 =buf1017 [0 ]\n        buf1019 =buf1017 [1 ]\n        buf1020 =buf1017 [2 ]\n        del buf1017 \n        buf1021 =buf1010 ;del buf1010 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5376 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1021 )\n        buf1022 =buf1009 ;del buf1009 \n\n        extern_kernels .mm (buf1012 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1022 )\n\n        buf1023 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1021 ,buf1022 ,buf1013 ,primals_9 ,primals_10 )\n        buf1024 =buf1023 [0 ]\n        buf1025 =buf1023 [1 ]\n        buf1026 =buf1023 [2 ]\n        del buf1023 \n        buf1027 =buf1016 ;del buf1016 \n\n        extern_kernels .mm (buf1024 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1027 )\n        buf1028 =buf1015 ;del buf1015 \n\n        extern_kernels .mm (buf1018 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1028 )\n\n        buf1029 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1027 ,buf1028 ,buf1019 ,primals_13 ,primals_14 )\n        buf1030 =buf1029 [0 ]\n        buf1031 =buf1029 [1 ]\n        buf1032 =buf1029 [2 ]\n        del buf1029 \n        buf1033 =buf1022 ;del buf1022 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5440 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1033 )\n        buf1034 =buf1021 ;del buf1021 \n\n        extern_kernels .mm (buf1024 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1034 )\n\n        buf1035 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1033 ,buf1034 ,buf1025 ,primals_9 ,primals_10 )\n        buf1036 =buf1035 [0 ]\n        buf1037 =buf1035 [1 ]\n        buf1038 =buf1035 [2 ]\n        del buf1035 \n        buf1039 =buf1028 ;del buf1028 \n\n        extern_kernels .mm (buf1036 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1039 )\n        buf1040 =buf1027 ;del buf1027 \n\n        extern_kernels .mm (buf1030 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1040 )\n\n        buf1041 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1039 ,buf1040 ,buf1031 ,primals_13 ,primals_14 )\n        buf1042 =buf1041 [0 ]\n        buf1043 =buf1041 [1 ]\n        buf1044 =buf1041 [2 ]\n        del buf1041 \n        buf1045 =buf1034 ;del buf1034 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5504 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1045 )\n        buf1046 =buf1033 ;del buf1033 \n\n        extern_kernels .mm (buf1036 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1046 )\n\n        buf1047 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1045 ,buf1046 ,buf1037 ,primals_9 ,primals_10 )\n        buf1048 =buf1047 [0 ]\n        buf1049 =buf1047 [1 ]\n        buf1050 =buf1047 [2 ]\n        del buf1047 \n        buf1051 =buf1040 ;del buf1040 \n\n        extern_kernels .mm (buf1048 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1051 )\n        buf1052 =buf1039 ;del buf1039 \n\n        extern_kernels .mm (buf1042 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1052 )\n\n        buf1053 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1051 ,buf1052 ,buf1043 ,primals_13 ,primals_14 )\n        buf1054 =buf1053 [0 ]\n        buf1055 =buf1053 [1 ]\n        buf1056 =buf1053 [2 ]\n        del buf1053 \n        buf1057 =buf1046 ;del buf1046 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5568 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1057 )\n        buf1058 =buf1045 ;del buf1045 \n\n        extern_kernels .mm (buf1048 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1058 )\n\n        buf1059 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1057 ,buf1058 ,buf1049 ,primals_9 ,primals_10 )\n        buf1060 =buf1059 [0 ]\n        buf1061 =buf1059 [1 ]\n        buf1062 =buf1059 [2 ]\n        del buf1059 \n        buf1063 =buf1052 ;del buf1052 \n\n        extern_kernels .mm (buf1060 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1063 )\n        buf1064 =buf1051 ;del buf1051 \n\n        extern_kernels .mm (buf1054 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1064 )\n\n        buf1065 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1063 ,buf1064 ,buf1055 ,primals_13 ,primals_14 )\n        buf1066 =buf1065 [0 ]\n        buf1067 =buf1065 [1 ]\n        buf1068 =buf1065 [2 ]\n        del buf1065 \n        buf1069 =buf1058 ;del buf1058 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5632 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1069 )\n        buf1070 =buf1057 ;del buf1057 \n\n        extern_kernels .mm (buf1060 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1070 )\n\n        buf1071 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1069 ,buf1070 ,buf1061 ,primals_9 ,primals_10 )\n        buf1072 =buf1071 [0 ]\n        buf1073 =buf1071 [1 ]\n        buf1074 =buf1071 [2 ]\n        del buf1071 \n        buf1075 =buf1064 ;del buf1064 \n\n        extern_kernels .mm (buf1072 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1075 )\n        buf1076 =buf1063 ;del buf1063 \n\n        extern_kernels .mm (buf1066 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1076 )\n\n        buf1077 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1075 ,buf1076 ,buf1067 ,primals_13 ,primals_14 )\n        buf1078 =buf1077 [0 ]\n        buf1079 =buf1077 [1 ]\n        buf1080 =buf1077 [2 ]\n        del buf1077 \n        buf1081 =buf1070 ;del buf1070 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5696 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1081 )\n        buf1082 =buf1069 ;del buf1069 \n\n        extern_kernels .mm (buf1072 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1082 )\n\n        buf1083 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1081 ,buf1082 ,buf1073 ,primals_9 ,primals_10 )\n        buf1084 =buf1083 [0 ]\n        buf1085 =buf1083 [1 ]\n        buf1086 =buf1083 [2 ]\n        del buf1083 \n        buf1087 =buf1076 ;del buf1076 \n\n        extern_kernels .mm (buf1084 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1087 )\n        buf1088 =buf1075 ;del buf1075 \n\n        extern_kernels .mm (buf1078 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1088 )\n\n        buf1089 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1087 ,buf1088 ,buf1079 ,primals_13 ,primals_14 )\n        buf1090 =buf1089 [0 ]\n        buf1091 =buf1089 [1 ]\n        buf1092 =buf1089 [2 ]\n        del buf1089 \n        buf1093 =buf1082 ;del buf1082 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5760 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1093 )\n        buf1094 =buf1081 ;del buf1081 \n\n        extern_kernels .mm (buf1084 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1094 )\n\n        buf1095 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1093 ,buf1094 ,buf1085 ,primals_9 ,primals_10 )\n        buf1096 =buf1095 [0 ]\n        buf1097 =buf1095 [1 ]\n        buf1098 =buf1095 [2 ]\n        del buf1095 \n        buf1099 =buf1088 ;del buf1088 \n\n        extern_kernels .mm (buf1096 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1099 )\n        buf1100 =buf1087 ;del buf1087 \n\n        extern_kernels .mm (buf1090 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1100 )\n\n        buf1101 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1099 ,buf1100 ,buf1091 ,primals_13 ,primals_14 )\n        buf1102 =buf1101 [0 ]\n        buf1103 =buf1101 [1 ]\n        buf1104 =buf1101 [2 ]\n        del buf1101 \n        buf1105 =buf1094 ;del buf1094 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5824 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1105 )\n        buf1106 =buf1093 ;del buf1093 \n\n        extern_kernels .mm (buf1096 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1106 )\n\n        buf1107 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1105 ,buf1106 ,buf1097 ,primals_9 ,primals_10 )\n        buf1108 =buf1107 [0 ]\n        buf1109 =buf1107 [1 ]\n        buf1110 =buf1107 [2 ]\n        del buf1107 \n        buf1111 =buf1100 ;del buf1100 \n\n        extern_kernels .mm (buf1108 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1111 )\n        buf1112 =buf1099 ;del buf1099 \n\n        extern_kernels .mm (buf1102 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1112 )\n\n        buf1113 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1111 ,buf1112 ,buf1103 ,primals_13 ,primals_14 )\n        buf1114 =buf1113 [0 ]\n        buf1115 =buf1113 [1 ]\n        buf1116 =buf1113 [2 ]\n        del buf1113 \n        buf1117 =buf1106 ;del buf1106 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5888 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1117 )\n        buf1118 =buf1105 ;del buf1105 \n\n        extern_kernels .mm (buf1108 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1118 )\n\n        buf1119 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1117 ,buf1118 ,buf1109 ,primals_9 ,primals_10 )\n        buf1120 =buf1119 [0 ]\n        buf1121 =buf1119 [1 ]\n        buf1122 =buf1119 [2 ]\n        del buf1119 \n        buf1123 =buf1112 ;del buf1112 \n\n        extern_kernels .mm (buf1120 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1123 )\n        buf1124 =buf1111 ;del buf1111 \n\n        extern_kernels .mm (buf1114 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1124 )\n\n        buf1125 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1123 ,buf1124 ,buf1115 ,primals_13 ,primals_14 )\n        buf1126 =buf1125 [0 ]\n        buf1127 =buf1125 [1 ]\n        buf1128 =buf1125 [2 ]\n        del buf1125 \n        buf1129 =buf1118 ;del buf1118 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5952 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1129 )\n        buf1130 =buf1117 ;del buf1117 \n\n        extern_kernels .mm (buf1120 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1130 )\n\n        buf1131 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1129 ,buf1130 ,buf1121 ,primals_9 ,primals_10 )\n        buf1132 =buf1131 [0 ]\n        buf1133 =buf1131 [1 ]\n        buf1134 =buf1131 [2 ]\n        del buf1131 \n        buf1135 =buf1124 ;del buf1124 \n\n        extern_kernels .mm (buf1132 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1135 )\n        buf1136 =buf1123 ;del buf1123 \n\n        extern_kernels .mm (buf1126 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1136 )\n\n        buf1137 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1135 ,buf1136 ,buf1127 ,primals_13 ,primals_14 )\n        buf1138 =buf1137 [0 ]\n        buf1139 =buf1137 [1 ]\n        buf1140 =buf1137 [2 ]\n        del buf1137 \n        buf1141 =buf1130 ;del buf1130 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6016 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1141 )\n        buf1142 =buf1129 ;del buf1129 \n\n        extern_kernels .mm (buf1132 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1142 )\n\n        buf1143 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1141 ,buf1142 ,buf1133 ,primals_9 ,primals_10 )\n        buf1144 =buf1143 [0 ]\n        buf1145 =buf1143 [1 ]\n        buf1146 =buf1143 [2 ]\n        del buf1143 \n        buf1147 =buf1136 ;del buf1136 \n\n        extern_kernels .mm (buf1144 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1147 )\n        buf1148 =buf1135 ;del buf1135 \n\n        extern_kernels .mm (buf1138 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1148 )\n\n        buf1149 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1147 ,buf1148 ,buf1139 ,primals_13 ,primals_14 )\n        buf1150 =buf1149 [0 ]\n        buf1151 =buf1149 [1 ]\n        buf1152 =buf1149 [2 ]\n        del buf1149 \n        buf1153 =buf1142 ;del buf1142 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6080 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1153 )\n        buf1154 =buf1141 ;del buf1141 \n\n        extern_kernels .mm (buf1144 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1154 )\n\n        buf1155 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1153 ,buf1154 ,buf1145 ,primals_9 ,primals_10 )\n        buf1156 =buf1155 [0 ]\n        buf1157 =buf1155 [1 ]\n        buf1158 =buf1155 [2 ]\n        del buf1155 \n        buf1159 =buf1148 ;del buf1148 \n\n        extern_kernels .mm (buf1156 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1159 )\n        buf1160 =buf1147 ;del buf1147 \n\n        extern_kernels .mm (buf1150 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1160 )\n\n        buf1161 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1159 ,buf1160 ,buf1151 ,primals_13 ,primals_14 )\n        buf1162 =buf1161 [0 ]\n        buf1163 =buf1161 [1 ]\n        buf1164 =buf1161 [2 ]\n        del buf1161 \n        buf1165 =buf1154 ;del buf1154 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6144 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1165 )\n        buf1166 =buf1153 ;del buf1153 \n\n        extern_kernels .mm (buf1156 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1166 )\n\n        buf1167 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1165 ,buf1166 ,buf1157 ,primals_9 ,primals_10 )\n        buf1168 =buf1167 [0 ]\n        buf1169 =buf1167 [1 ]\n        buf1170 =buf1167 [2 ]\n        del buf1167 \n        buf1171 =buf1160 ;del buf1160 \n\n        extern_kernels .mm (buf1168 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1171 )\n        buf1172 =buf1159 ;del buf1159 \n\n        extern_kernels .mm (buf1162 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1172 )\n\n        buf1173 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1171 ,buf1172 ,buf1163 ,primals_13 ,primals_14 )\n        buf1174 =buf1173 [0 ]\n        buf1175 =buf1173 [1 ]\n        buf1176 =buf1173 [2 ]\n        del buf1173 \n        buf1177 =buf1166 ;del buf1166 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6208 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1177 )\n        buf1178 =buf1165 ;del buf1165 \n\n        extern_kernels .mm (buf1168 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1178 )\n\n        buf1179 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1177 ,buf1178 ,buf1169 ,primals_9 ,primals_10 )\n        buf1180 =buf1179 [0 ]\n        buf1181 =buf1179 [1 ]\n        buf1182 =buf1179 [2 ]\n        del buf1179 \n        buf1183 =buf1172 ;del buf1172 \n\n        extern_kernels .mm (buf1180 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1183 )\n        buf1184 =buf1171 ;del buf1171 \n\n        extern_kernels .mm (buf1174 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1184 )\n\n        buf1185 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1183 ,buf1184 ,buf1175 ,primals_13 ,primals_14 )\n        buf1186 =buf1185 [0 ]\n        buf1187 =buf1185 [1 ]\n        buf1188 =buf1185 [2 ]\n        del buf1185 \n        buf1189 =buf1178 ;del buf1178 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6272 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1189 )\n        buf1190 =buf1177 ;del buf1177 \n\n        extern_kernels .mm (buf1180 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1190 )\n\n        buf1191 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1189 ,buf1190 ,buf1181 ,primals_9 ,primals_10 )\n        buf1192 =buf1191 [0 ]\n        buf1193 =buf1191 [1 ]\n        buf1194 =buf1191 [2 ]\n        del buf1191 \n        buf1195 =buf1184 ;del buf1184 \n\n        extern_kernels .mm (buf1192 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1195 )\n        buf1196 =buf1183 ;del buf1183 \n\n        extern_kernels .mm (buf1186 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1196 )\n\n        buf1197 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1195 ,buf1196 ,buf1187 ,primals_13 ,primals_14 )\n        buf1198 =buf1197 [0 ]\n        buf1199 =buf1197 [1 ]\n        buf1200 =buf1197 [2 ]\n        del buf1197 \n        buf1201 =buf1190 ;del buf1190 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6336 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1201 )\n        buf1202 =buf1189 ;del buf1189 \n\n        extern_kernels .mm (buf1192 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1202 )\n\n        buf1203 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1201 ,buf1202 ,buf1193 ,primals_9 ,primals_10 )\n        buf1204 =buf1203 [0 ]\n        buf1205 =buf1203 [1 ]\n        buf1206 =buf1203 [2 ]\n        del buf1203 \n        buf1207 =buf1196 ;del buf1196 \n\n        extern_kernels .mm (buf1204 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1207 )\n        buf1208 =buf1195 ;del buf1195 \n\n        extern_kernels .mm (buf1198 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1208 )\n\n        buf1209 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1207 ,buf1208 ,buf1199 ,primals_13 ,primals_14 )\n        buf1210 =buf1209 [0 ]\n        buf1211 =buf1209 [1 ]\n        buf1212 =buf1209 [2 ]\n        del buf1209 \n        buf1213 =buf1202 ;del buf1202 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6400 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1213 )\n        buf1214 =buf1201 ;del buf1201 \n\n        extern_kernels .mm (buf1204 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1214 )\n\n        buf1215 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1213 ,buf1214 ,buf1205 ,primals_9 ,primals_10 )\n        buf1216 =buf1215 [0 ]\n        buf1217 =buf1215 [1 ]\n        buf1218 =buf1215 [2 ]\n        del buf1215 \n        buf1219 =buf1208 ;del buf1208 \n\n        extern_kernels .mm (buf1216 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1219 )\n        buf1220 =buf1207 ;del buf1207 \n\n        extern_kernels .mm (buf1210 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1220 )\n\n        buf1221 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1219 ,buf1220 ,buf1211 ,primals_13 ,primals_14 )\n        buf1222 =buf1221 [0 ]\n        buf1223 =buf1221 [1 ]\n        buf1224 =buf1221 [2 ]\n        del buf1221 \n        buf1225 =buf1214 ;del buf1214 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6464 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1225 )\n        buf1226 =buf1213 ;del buf1213 \n\n        extern_kernels .mm (buf1216 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1226 )\n\n        buf1227 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1225 ,buf1226 ,buf1217 ,primals_9 ,primals_10 )\n        buf1228 =buf1227 [0 ]\n        buf1229 =buf1227 [1 ]\n        buf1230 =buf1227 [2 ]\n        del buf1227 \n        buf1231 =buf1220 ;del buf1220 \n\n        extern_kernels .mm (buf1228 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1231 )\n        buf1232 =buf1219 ;del buf1219 \n\n        extern_kernels .mm (buf1222 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1232 )\n\n        buf1233 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1231 ,buf1232 ,buf1223 ,primals_13 ,primals_14 )\n        buf1234 =buf1233 [0 ]\n        buf1235 =buf1233 [1 ]\n        buf1236 =buf1233 [2 ]\n        del buf1233 \n        buf1237 =buf1226 ;del buf1226 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6528 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1237 )\n        buf1238 =buf1225 ;del buf1225 \n\n        extern_kernels .mm (buf1228 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1238 )\n\n        buf1239 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1237 ,buf1238 ,buf1229 ,primals_9 ,primals_10 )\n        buf1240 =buf1239 [0 ]\n        buf1241 =buf1239 [1 ]\n        buf1242 =buf1239 [2 ]\n        del buf1239 \n        buf1243 =buf1232 ;del buf1232 \n\n        extern_kernels .mm (buf1240 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1243 )\n        buf1244 =buf1231 ;del buf1231 \n\n        extern_kernels .mm (buf1234 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1244 )\n\n        buf1245 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1243 ,buf1244 ,buf1235 ,primals_13 ,primals_14 )\n        buf1246 =buf1245 [0 ]\n        buf1247 =buf1245 [1 ]\n        buf1248 =buf1245 [2 ]\n        del buf1245 \n        buf1249 =buf1238 ;del buf1238 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6592 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1249 )\n        buf1250 =buf1237 ;del buf1237 \n\n        extern_kernels .mm (buf1240 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1250 )\n\n        buf1251 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1249 ,buf1250 ,buf1241 ,primals_9 ,primals_10 )\n        buf1252 =buf1251 [0 ]\n        buf1253 =buf1251 [1 ]\n        buf1254 =buf1251 [2 ]\n        del buf1251 \n        buf1255 =buf1244 ;del buf1244 \n\n        extern_kernels .mm (buf1252 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1255 )\n        buf1256 =buf1243 ;del buf1243 \n\n        extern_kernels .mm (buf1246 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1256 )\n\n        buf1257 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1255 ,buf1256 ,buf1247 ,primals_13 ,primals_14 )\n        buf1258 =buf1257 [0 ]\n        buf1259 =buf1257 [1 ]\n        buf1260 =buf1257 [2 ]\n        del buf1257 \n        buf1261 =buf1250 ;del buf1250 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6656 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1261 )\n        buf1262 =buf1249 ;del buf1249 \n\n        extern_kernels .mm (buf1252 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1262 )\n\n        buf1263 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1261 ,buf1262 ,buf1253 ,primals_9 ,primals_10 )\n        buf1264 =buf1263 [0 ]\n        buf1265 =buf1263 [1 ]\n        buf1266 =buf1263 [2 ]\n        del buf1263 \n        buf1267 =buf1256 ;del buf1256 \n\n        extern_kernels .mm (buf1264 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1267 )\n        buf1268 =buf1255 ;del buf1255 \n\n        extern_kernels .mm (buf1258 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1268 )\n\n        buf1269 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1267 ,buf1268 ,buf1259 ,primals_13 ,primals_14 )\n        buf1270 =buf1269 [0 ]\n        buf1271 =buf1269 [1 ]\n        buf1272 =buf1269 [2 ]\n        del buf1269 \n        buf1273 =buf1262 ;del buf1262 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6720 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1273 )\n        buf1274 =buf1261 ;del buf1261 \n\n        extern_kernels .mm (buf1264 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1274 )\n\n        buf1275 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1273 ,buf1274 ,buf1265 ,primals_9 ,primals_10 )\n        buf1276 =buf1275 [0 ]\n        buf1277 =buf1275 [1 ]\n        buf1278 =buf1275 [2 ]\n        del buf1275 \n        buf1279 =buf1268 ;del buf1268 \n\n        extern_kernels .mm (buf1276 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1279 )\n        buf1280 =buf1267 ;del buf1267 \n\n        extern_kernels .mm (buf1270 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1280 )\n\n        buf1281 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1279 ,buf1280 ,buf1271 ,primals_13 ,primals_14 )\n        buf1282 =buf1281 [0 ]\n        buf1283 =buf1281 [1 ]\n        buf1284 =buf1281 [2 ]\n        del buf1281 \n        buf1285 =buf1274 ;del buf1274 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6784 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1285 )\n        buf1286 =buf1273 ;del buf1273 \n\n        extern_kernels .mm (buf1276 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1286 )\n\n        buf1287 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1285 ,buf1286 ,buf1277 ,primals_9 ,primals_10 )\n        buf1288 =buf1287 [0 ]\n        buf1289 =buf1287 [1 ]\n        buf1290 =buf1287 [2 ]\n        del buf1287 \n        buf1291 =buf1280 ;del buf1280 \n\n        extern_kernels .mm (buf1288 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1291 )\n        buf1292 =buf1279 ;del buf1279 \n\n        extern_kernels .mm (buf1282 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1292 )\n\n        buf1293 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1291 ,buf1292 ,buf1283 ,primals_13 ,primals_14 )\n        buf1294 =buf1293 [0 ]\n        buf1295 =buf1293 [1 ]\n        buf1296 =buf1293 [2 ]\n        del buf1293 \n        buf1297 =buf1286 ;del buf1286 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6848 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1297 )\n        buf1298 =buf1285 ;del buf1285 \n\n        extern_kernels .mm (buf1288 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1298 )\n\n        buf1299 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1297 ,buf1298 ,buf1289 ,primals_9 ,primals_10 )\n        buf1300 =buf1299 [0 ]\n        buf1301 =buf1299 [1 ]\n        buf1302 =buf1299 [2 ]\n        del buf1299 \n        buf1303 =buf1292 ;del buf1292 \n\n        extern_kernels .mm (buf1300 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1303 )\n        buf1304 =buf1291 ;del buf1291 \n\n        extern_kernels .mm (buf1294 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1304 )\n\n        buf1305 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1303 ,buf1304 ,buf1295 ,primals_13 ,primals_14 )\n        buf1306 =buf1305 [0 ]\n        buf1307 =buf1305 [1 ]\n        buf1308 =buf1305 [2 ]\n        del buf1305 \n        buf1309 =buf1298 ;del buf1298 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6912 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1309 )\n        buf1310 =buf1297 ;del buf1297 \n\n        extern_kernels .mm (buf1300 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1310 )\n\n        buf1311 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1309 ,buf1310 ,buf1301 ,primals_9 ,primals_10 )\n        buf1312 =buf1311 [0 ]\n        buf1313 =buf1311 [1 ]\n        buf1314 =buf1311 [2 ]\n        del buf1311 \n        buf1315 =buf1304 ;del buf1304 \n\n        extern_kernels .mm (buf1312 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1315 )\n        buf1316 =buf1303 ;del buf1303 \n\n        extern_kernels .mm (buf1306 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1316 )\n\n        buf1317 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1315 ,buf1316 ,buf1307 ,primals_13 ,primals_14 )\n        buf1318 =buf1317 [0 ]\n        buf1319 =buf1317 [1 ]\n        buf1320 =buf1317 [2 ]\n        del buf1317 \n        buf1321 =buf1310 ;del buf1310 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6976 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1321 )\n        buf1322 =buf1309 ;del buf1309 \n\n        extern_kernels .mm (buf1312 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1322 )\n\n        buf1323 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1321 ,buf1322 ,buf1313 ,primals_9 ,primals_10 )\n        buf1324 =buf1323 [0 ]\n        buf1325 =buf1323 [1 ]\n        buf1326 =buf1323 [2 ]\n        del buf1323 \n        buf1327 =buf1316 ;del buf1316 \n\n        extern_kernels .mm (buf1324 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1327 )\n        buf1328 =buf1315 ;del buf1315 \n\n        extern_kernels .mm (buf1318 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1328 )\n\n        buf1329 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1327 ,buf1328 ,buf1319 ,primals_13 ,primals_14 )\n        buf1330 =buf1329 [0 ]\n        buf1331 =buf1329 [1 ]\n        buf1332 =buf1329 [2 ]\n        del buf1329 \n        buf1333 =buf1322 ;del buf1322 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7040 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1333 )\n        buf1334 =buf1321 ;del buf1321 \n\n        extern_kernels .mm (buf1324 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1334 )\n\n        buf1335 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1333 ,buf1334 ,buf1325 ,primals_9 ,primals_10 )\n        buf1336 =buf1335 [0 ]\n        buf1337 =buf1335 [1 ]\n        buf1338 =buf1335 [2 ]\n        del buf1335 \n        buf1339 =buf1328 ;del buf1328 \n\n        extern_kernels .mm (buf1336 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1339 )\n        buf1340 =buf1327 ;del buf1327 \n\n        extern_kernels .mm (buf1330 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1340 )\n\n        buf1341 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1339 ,buf1340 ,buf1331 ,primals_13 ,primals_14 )\n        buf1342 =buf1341 [0 ]\n        buf1343 =buf1341 [1 ]\n        buf1344 =buf1341 [2 ]\n        del buf1341 \n        buf1345 =buf1334 ;del buf1334 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7104 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1345 )\n        buf1346 =buf1333 ;del buf1333 \n\n        extern_kernels .mm (buf1336 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1346 )\n\n        buf1347 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1345 ,buf1346 ,buf1337 ,primals_9 ,primals_10 )\n        buf1348 =buf1347 [0 ]\n        buf1349 =buf1347 [1 ]\n        buf1350 =buf1347 [2 ]\n        del buf1347 \n        buf1351 =buf1340 ;del buf1340 \n\n        extern_kernels .mm (buf1348 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1351 )\n        buf1352 =buf1339 ;del buf1339 \n\n        extern_kernels .mm (buf1342 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1352 )\n\n        buf1353 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1351 ,buf1352 ,buf1343 ,primals_13 ,primals_14 )\n        buf1354 =buf1353 [0 ]\n        buf1355 =buf1353 [1 ]\n        buf1356 =buf1353 [2 ]\n        del buf1353 \n        buf1357 =buf1346 ;del buf1346 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7168 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1357 )\n        buf1358 =buf1345 ;del buf1345 \n\n        extern_kernels .mm (buf1348 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1358 )\n\n        buf1359 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1357 ,buf1358 ,buf1349 ,primals_9 ,primals_10 )\n        buf1360 =buf1359 [0 ]\n        buf1361 =buf1359 [1 ]\n        buf1362 =buf1359 [2 ]\n        del buf1359 \n        buf1363 =buf1352 ;del buf1352 \n\n        extern_kernels .mm (buf1360 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1363 )\n        buf1364 =buf1351 ;del buf1351 \n\n        extern_kernels .mm (buf1354 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1364 )\n\n        buf1365 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1363 ,buf1364 ,buf1355 ,primals_13 ,primals_14 )\n        buf1366 =buf1365 [0 ]\n        buf1367 =buf1365 [1 ]\n        buf1368 =buf1365 [2 ]\n        del buf1365 \n        buf1369 =buf1358 ;del buf1358 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7232 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1369 )\n        buf1370 =buf1357 ;del buf1357 \n\n        extern_kernels .mm (buf1360 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1370 )\n\n        buf1371 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1369 ,buf1370 ,buf1361 ,primals_9 ,primals_10 )\n        buf1372 =buf1371 [0 ]\n        buf1373 =buf1371 [1 ]\n        buf1374 =buf1371 [2 ]\n        del buf1371 \n        buf1375 =buf1364 ;del buf1364 \n\n        extern_kernels .mm (buf1372 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1375 )\n        buf1376 =buf1363 ;del buf1363 \n\n        extern_kernels .mm (buf1366 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1376 )\n\n        buf1377 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1375 ,buf1376 ,buf1367 ,primals_13 ,primals_14 )\n        buf1378 =buf1377 [0 ]\n        buf1379 =buf1377 [1 ]\n        buf1380 =buf1377 [2 ]\n        del buf1377 \n        buf1381 =buf1370 ;del buf1370 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7296 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1381 )\n        buf1382 =buf1369 ;del buf1369 \n\n        extern_kernels .mm (buf1372 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1382 )\n\n        buf1383 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1381 ,buf1382 ,buf1373 ,primals_9 ,primals_10 )\n        buf1384 =buf1383 [0 ]\n        buf1385 =buf1383 [1 ]\n        buf1386 =buf1383 [2 ]\n        del buf1383 \n        buf1387 =buf1376 ;del buf1376 \n\n        extern_kernels .mm (buf1384 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1387 )\n        buf1388 =buf1375 ;del buf1375 \n\n        extern_kernels .mm (buf1378 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1388 )\n\n        buf1389 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1387 ,buf1388 ,buf1379 ,primals_13 ,primals_14 )\n        buf1390 =buf1389 [0 ]\n        buf1391 =buf1389 [1 ]\n        buf1392 =buf1389 [2 ]\n        del buf1389 \n        buf1393 =buf1382 ;del buf1382 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7360 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1393 )\n        buf1394 =buf1381 ;del buf1381 \n\n        extern_kernels .mm (buf1384 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1394 )\n\n        buf1395 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1393 ,buf1394 ,buf1385 ,primals_9 ,primals_10 )\n        buf1396 =buf1395 [0 ]\n        buf1397 =buf1395 [1 ]\n        buf1398 =buf1395 [2 ]\n        del buf1395 \n        buf1399 =buf1388 ;del buf1388 \n\n        extern_kernels .mm (buf1396 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1399 )\n        buf1400 =buf1387 ;del buf1387 \n\n        extern_kernels .mm (buf1390 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1400 )\n\n        buf1401 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1399 ,buf1400 ,buf1391 ,primals_13 ,primals_14 )\n        buf1402 =buf1401 [0 ]\n        buf1403 =buf1401 [1 ]\n        buf1404 =buf1401 [2 ]\n        del buf1401 \n        buf1405 =buf1394 ;del buf1394 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7424 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1405 )\n        buf1406 =buf1393 ;del buf1393 \n\n        extern_kernels .mm (buf1396 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1406 )\n\n        buf1407 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1405 ,buf1406 ,buf1397 ,primals_9 ,primals_10 )\n        buf1408 =buf1407 [0 ]\n        buf1409 =buf1407 [1 ]\n        buf1410 =buf1407 [2 ]\n        del buf1407 \n        buf1411 =buf1400 ;del buf1400 \n\n        extern_kernels .mm (buf1408 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1411 )\n        buf1412 =buf1399 ;del buf1399 \n\n        extern_kernels .mm (buf1402 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1412 )\n\n        buf1413 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1411 ,buf1412 ,buf1403 ,primals_13 ,primals_14 )\n        buf1414 =buf1413 [0 ]\n        buf1415 =buf1413 [1 ]\n        buf1416 =buf1413 [2 ]\n        del buf1413 \n        buf1417 =buf1406 ;del buf1406 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7488 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1417 )\n        buf1418 =buf1405 ;del buf1405 \n\n        extern_kernels .mm (buf1408 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1418 )\n\n        buf1419 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1417 ,buf1418 ,buf1409 ,primals_9 ,primals_10 )\n        buf1420 =buf1419 [0 ]\n        buf1421 =buf1419 [1 ]\n        buf1422 =buf1419 [2 ]\n        del buf1419 \n        buf1423 =buf1412 ;del buf1412 \n\n        extern_kernels .mm (buf1420 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1423 )\n        buf1424 =buf1411 ;del buf1411 \n\n        extern_kernels .mm (buf1414 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1424 )\n\n        buf1425 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1423 ,buf1424 ,buf1415 ,primals_13 ,primals_14 )\n        buf1426 =buf1425 [0 ]\n        buf1427 =buf1425 [1 ]\n        buf1428 =buf1425 [2 ]\n        del buf1425 \n        buf1429 =buf1418 ;del buf1418 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7552 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1429 )\n        buf1430 =buf1417 ;del buf1417 \n\n        extern_kernels .mm (buf1420 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1430 )\n\n        buf1431 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1429 ,buf1430 ,buf1421 ,primals_9 ,primals_10 )\n        buf1432 =buf1431 [0 ]\n        buf1433 =buf1431 [1 ]\n        buf1434 =buf1431 [2 ]\n        del buf1431 \n        buf1435 =buf1424 ;del buf1424 \n\n        extern_kernels .mm (buf1432 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1435 )\n        buf1436 =buf1423 ;del buf1423 \n\n        extern_kernels .mm (buf1426 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1436 )\n\n        buf1437 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1435 ,buf1436 ,buf1427 ,primals_13 ,primals_14 )\n        buf1438 =buf1437 [0 ]\n        buf1439 =buf1437 [1 ]\n        buf1440 =buf1437 [2 ]\n        del buf1437 \n        buf1441 =buf1430 ;del buf1430 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7616 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1441 )\n        buf1442 =buf1429 ;del buf1429 \n\n        extern_kernels .mm (buf1432 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1442 )\n\n        buf1443 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1441 ,buf1442 ,buf1433 ,primals_9 ,primals_10 )\n        buf1444 =buf1443 [0 ]\n        buf1445 =buf1443 [1 ]\n        buf1446 =buf1443 [2 ]\n        del buf1443 \n        buf1447 =buf1436 ;del buf1436 \n\n        extern_kernels .mm (buf1444 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1447 )\n        buf1448 =buf1435 ;del buf1435 \n\n        extern_kernels .mm (buf1438 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1448 )\n\n        buf1449 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1447 ,buf1448 ,buf1439 ,primals_13 ,primals_14 )\n        buf1450 =buf1449 [0 ]\n        buf1451 =buf1449 [1 ]\n        buf1452 =buf1449 [2 ]\n        del buf1449 \n        buf1453 =buf1442 ;del buf1442 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7680 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1453 )\n        buf1454 =buf1441 ;del buf1441 \n\n        extern_kernels .mm (buf1444 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1454 )\n\n        buf1455 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1453 ,buf1454 ,buf1445 ,primals_9 ,primals_10 )\n        buf1456 =buf1455 [0 ]\n        buf1457 =buf1455 [1 ]\n        buf1458 =buf1455 [2 ]\n        del buf1455 \n        buf1459 =buf1448 ;del buf1448 \n\n        extern_kernels .mm (buf1456 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1459 )\n        buf1460 =buf1447 ;del buf1447 \n\n        extern_kernels .mm (buf1450 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1460 )\n\n        buf1461 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1459 ,buf1460 ,buf1451 ,primals_13 ,primals_14 )\n        buf1462 =buf1461 [0 ]\n        buf1463 =buf1461 [1 ]\n        buf1464 =buf1461 [2 ]\n        del buf1461 \n        buf1465 =buf1454 ;del buf1454 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7744 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1465 )\n        buf1466 =buf1453 ;del buf1453 \n\n        extern_kernels .mm (buf1456 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1466 )\n\n        buf1467 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1465 ,buf1466 ,buf1457 ,primals_9 ,primals_10 )\n        buf1468 =buf1467 [0 ]\n        buf1469 =buf1467 [1 ]\n        buf1470 =buf1467 [2 ]\n        del buf1467 \n        buf1471 =buf1460 ;del buf1460 \n\n        extern_kernels .mm (buf1468 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1471 )\n        buf1472 =buf1459 ;del buf1459 \n\n        extern_kernels .mm (buf1462 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1472 )\n\n        buf1473 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1471 ,buf1472 ,buf1463 ,primals_13 ,primals_14 )\n        buf1474 =buf1473 [0 ]\n        buf1475 =buf1473 [1 ]\n        buf1476 =buf1473 [2 ]\n        del buf1473 \n        buf1477 =buf1466 ;del buf1466 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7808 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1477 )\n        buf1478 =buf1465 ;del buf1465 \n\n        extern_kernels .mm (buf1468 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1478 )\n\n        buf1479 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1477 ,buf1478 ,buf1469 ,primals_9 ,primals_10 )\n        buf1480 =buf1479 [0 ]\n        buf1481 =buf1479 [1 ]\n        buf1482 =buf1479 [2 ]\n        del buf1479 \n        buf1483 =buf1472 ;del buf1472 \n\n        extern_kernels .mm (buf1480 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1483 )\n        buf1484 =buf1471 ;del buf1471 \n\n        extern_kernels .mm (buf1474 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1484 )\n\n        buf1485 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1483 ,buf1484 ,buf1475 ,primals_13 ,primals_14 )\n        buf1486 =buf1485 [0 ]\n        buf1487 =buf1485 [1 ]\n        buf1488 =buf1485 [2 ]\n        del buf1485 \n        buf1489 =buf1478 ;del buf1478 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7872 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1489 )\n        buf1490 =buf1477 ;del buf1477 \n\n        extern_kernels .mm (buf1480 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1490 )\n\n        buf1491 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1489 ,buf1490 ,buf1481 ,primals_9 ,primals_10 )\n        buf1492 =buf1491 [0 ]\n        buf1493 =buf1491 [1 ]\n        buf1494 =buf1491 [2 ]\n        del buf1491 \n        buf1495 =buf1484 ;del buf1484 \n\n        extern_kernels .mm (buf1492 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1495 )\n        buf1496 =buf1483 ;del buf1483 \n\n        extern_kernels .mm (buf1486 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1496 )\n\n        buf1497 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1495 ,buf1496 ,buf1487 ,primals_13 ,primals_14 )\n        buf1498 =buf1497 [0 ]\n        buf1499 =buf1497 [1 ]\n        buf1500 =buf1497 [2 ]\n        del buf1497 \n        buf1501 =buf1490 ;del buf1490 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7936 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1501 )\n        buf1502 =buf1489 ;del buf1489 \n\n        extern_kernels .mm (buf1492 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1502 )\n\n        buf1503 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1501 ,buf1502 ,buf1493 ,primals_9 ,primals_10 )\n        buf1504 =buf1503 [0 ]\n        buf1505 =buf1503 [1 ]\n        buf1506 =buf1503 [2 ]\n        del buf1503 \n        buf1507 =buf1496 ;del buf1496 \n\n        extern_kernels .mm (buf1504 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1507 )\n        buf1508 =buf1495 ;del buf1495 \n\n        extern_kernels .mm (buf1498 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1508 )\n\n        buf1509 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1507 ,buf1508 ,buf1499 ,primals_13 ,primals_14 )\n        buf1510 =buf1509 [0 ]\n        buf1511 =buf1509 [1 ]\n        buf1512 =buf1509 [2 ]\n        del buf1509 \n        buf1513 =buf1502 ;del buf1502 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),8000 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1513 )\n        buf1514 =buf1501 ;del buf1501 \n\n        extern_kernels .mm (buf1504 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1514 )\n\n        buf1515 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1513 ,buf1514 ,buf1505 ,primals_9 ,primals_10 )\n        buf1516 =buf1515 [0 ]\n        buf1517 =buf1515 [1 ]\n        buf1518 =buf1515 [2 ]\n        del buf1515 \n        buf1519 =buf1508 ;del buf1508 \n\n        extern_kernels .mm (buf1516 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1519 )\n        buf1520 =buf1507 ;del buf1507 \n\n        extern_kernels .mm (buf1510 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1520 )\n\n        buf1521 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1519 ,buf1520 ,buf1511 ,primals_13 ,primals_14 )\n        buf1522 =buf1521 [0 ]\n        buf1523 =buf1521 [1 ]\n        buf1524 =buf1521 [2 ]\n        del buf1521 \n        buf1525 =buf1514 ;del buf1514 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),8064 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1525 )\n        buf1526 =buf1513 ;del buf1513 \n\n        extern_kernels .mm (buf1516 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1526 )\n\n        buf1527 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1525 ,buf1526 ,buf1517 ,primals_9 ,primals_10 )\n        buf1528 =buf1527 [0 ]\n        buf1529 =buf1527 [1 ]\n        buf1530 =buf1527 [2 ]\n        del buf1527 \n        buf1531 =buf1520 ;del buf1520 \n\n        extern_kernels .mm (buf1528 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1531 )\n        buf1532 =buf1519 ;del buf1519 \n\n        extern_kernels .mm (buf1522 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1532 )\n\n        buf1533 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1531 ,buf1532 ,buf1523 ,primals_13 ,primals_14 )\n        buf1534 =buf1533 [0 ]\n        buf1535 =buf1533 [1 ]\n        buf1536 =buf1533 [2 ]\n        del buf1533 \n        buf1537 =buf1526 ;del buf1526 \n\n        extern_kernels .mm (reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),8128 ),reinterpret_tensor (primals_7 ,(64 ,512 ),(1 ,64 ),0 ),out =buf1537 )\n        buf1538 =buf1525 ;del buf1525 \n\n        extern_kernels .mm (buf1528 ,reinterpret_tensor (primals_8 ,(128 ,512 ),(1 ,128 ),0 ),out =buf1538 )\n\n        buf1539 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1537 ,buf1538 ,buf1529 ,primals_9 ,primals_10 )\n        del buf1537 \n        del buf1538 \n        del primals_10 \n        del primals_9 \n        buf1540 =buf1539 [0 ]\n        buf1541 =buf1539 [1 ]\n        buf1542 =buf1539 [2 ]\n        del buf1539 \n        buf1543 =buf1532 ;del buf1532 \n\n        extern_kernels .mm (buf1540 ,reinterpret_tensor (primals_11 ,(128 ,256 ),(1 ,128 ),0 ),out =buf1543 )\n        buf1544 =buf1531 ;del buf1531 \n\n        extern_kernels .mm (buf1534 ,reinterpret_tensor (primals_12 ,(64 ,256 ),(1 ,64 ),0 ),out =buf1544 )\n\n        buf1545 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf1543 ,buf1544 ,buf1535 ,primals_13 ,primals_14 )\n        del buf1543 \n        del buf1544 \n        del primals_13 \n        del primals_14 \n        buf1546 =buf1545 [0 ]\n        buf1547 =buf1545 [1 ]\n        buf1548 =buf1545 [2 ]\n        del buf1545 \n        buf1549 =empty_strided_cuda ((1 ,8 ,10 ,3 ,3 ),(720 ,90 ,9 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_replication_pad3d_7 [grid (720 )](buf1546 ,buf1549 ,720 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n    return (buf1549 ,reinterpret_tensor (buf0 ,(128 ,64 ),(1 ,128 ),0 ),buf7 ,reinterpret_tensor (buf9 ,(128 ,64 ),(64 ,1 ),0 ),buf11 ,buf12 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),0 ),buf16 ,buf17 ,buf18 ,buf22 ,buf23 ,buf24 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),64 ),buf28 ,buf29 ,buf30 ,buf34 ,buf35 ,buf36 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),128 ),buf40 ,buf41 ,buf42 ,buf46 ,buf47 ,buf48 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),192 ),buf52 ,buf53 ,buf54 ,buf58 ,buf59 ,buf60 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),256 ),buf64 ,buf65 ,buf66 ,buf70 ,buf71 ,buf72 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),320 ),buf76 ,buf77 ,buf78 ,buf82 ,buf83 ,buf84 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),384 ),buf88 ,buf89 ,buf90 ,buf94 ,buf95 ,buf96 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),448 ),buf100 ,buf101 ,buf102 ,buf106 ,buf107 ,buf108 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),512 ),buf112 ,buf113 ,buf114 ,buf118 ,buf119 ,buf120 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),576 ),buf124 ,buf125 ,buf126 ,buf130 ,buf131 ,buf132 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),640 ),buf136 ,buf137 ,buf138 ,buf142 ,buf143 ,buf144 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),704 ),buf148 ,buf149 ,buf150 ,buf154 ,buf155 ,buf156 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),768 ),buf160 ,buf161 ,buf162 ,buf166 ,buf167 ,buf168 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),832 ),buf172 ,buf173 ,buf174 ,buf178 ,buf179 ,buf180 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),896 ),buf184 ,buf185 ,buf186 ,buf190 ,buf191 ,buf192 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),960 ),buf196 ,buf197 ,buf198 ,buf202 ,buf203 ,buf204 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1024 ),buf208 ,buf209 ,buf210 ,buf214 ,buf215 ,buf216 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1088 ),buf220 ,buf221 ,buf222 ,buf226 ,buf227 ,buf228 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1152 ),buf232 ,buf233 ,buf234 ,buf238 ,buf239 ,buf240 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1216 ),buf244 ,buf245 ,buf246 ,buf250 ,buf251 ,buf252 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1280 ),buf256 ,buf257 ,buf258 ,buf262 ,buf263 ,buf264 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1344 ),buf268 ,buf269 ,buf270 ,buf274 ,buf275 ,buf276 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1408 ),buf280 ,buf281 ,buf282 ,buf286 ,buf287 ,buf288 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1472 ),buf292 ,buf293 ,buf294 ,buf298 ,buf299 ,buf300 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1536 ),buf304 ,buf305 ,buf306 ,buf310 ,buf311 ,buf312 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1600 ),buf316 ,buf317 ,buf318 ,buf322 ,buf323 ,buf324 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1664 ),buf328 ,buf329 ,buf330 ,buf334 ,buf335 ,buf336 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1728 ),buf340 ,buf341 ,buf342 ,buf346 ,buf347 ,buf348 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1792 ),buf352 ,buf353 ,buf354 ,buf358 ,buf359 ,buf360 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1856 ),buf364 ,buf365 ,buf366 ,buf370 ,buf371 ,buf372 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1920 ),buf376 ,buf377 ,buf378 ,buf382 ,buf383 ,buf384 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),1984 ),buf388 ,buf389 ,buf390 ,buf394 ,buf395 ,buf396 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2048 ),buf400 ,buf401 ,buf402 ,buf406 ,buf407 ,buf408 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2112 ),buf412 ,buf413 ,buf414 ,buf418 ,buf419 ,buf420 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2176 ),buf424 ,buf425 ,buf426 ,buf430 ,buf431 ,buf432 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2240 ),buf436 ,buf437 ,buf438 ,buf442 ,buf443 ,buf444 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2304 ),buf448 ,buf449 ,buf450 ,buf454 ,buf455 ,buf456 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2368 ),buf460 ,buf461 ,buf462 ,buf466 ,buf467 ,buf468 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2432 ),buf472 ,buf473 ,buf474 ,buf478 ,buf479 ,buf480 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2496 ),buf484 ,buf485 ,buf486 ,buf490 ,buf491 ,buf492 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2560 ),buf496 ,buf497 ,buf498 ,buf502 ,buf503 ,buf504 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2624 ),buf508 ,buf509 ,buf510 ,buf514 ,buf515 ,buf516 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2688 ),buf520 ,buf521 ,buf522 ,buf526 ,buf527 ,buf528 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2752 ),buf532 ,buf533 ,buf534 ,buf538 ,buf539 ,buf540 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2816 ),buf544 ,buf545 ,buf546 ,buf550 ,buf551 ,buf552 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2880 ),buf556 ,buf557 ,buf558 ,buf562 ,buf563 ,buf564 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),2944 ),buf568 ,buf569 ,buf570 ,buf574 ,buf575 ,buf576 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3008 ),buf580 ,buf581 ,buf582 ,buf586 ,buf587 ,buf588 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3072 ),buf592 ,buf593 ,buf594 ,buf598 ,buf599 ,buf600 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3136 ),buf604 ,buf605 ,buf606 ,buf610 ,buf611 ,buf612 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3200 ),buf616 ,buf617 ,buf618 ,buf622 ,buf623 ,buf624 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3264 ),buf628 ,buf629 ,buf630 ,buf634 ,buf635 ,buf636 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3328 ),buf640 ,buf641 ,buf642 ,buf646 ,buf647 ,buf648 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3392 ),buf652 ,buf653 ,buf654 ,buf658 ,buf659 ,buf660 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3456 ),buf664 ,buf665 ,buf666 ,buf670 ,buf671 ,buf672 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3520 ),buf676 ,buf677 ,buf678 ,buf682 ,buf683 ,buf684 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3584 ),buf688 ,buf689 ,buf690 ,buf694 ,buf695 ,buf696 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3648 ),buf700 ,buf701 ,buf702 ,buf706 ,buf707 ,buf708 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3712 ),buf712 ,buf713 ,buf714 ,buf718 ,buf719 ,buf720 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3776 ),buf724 ,buf725 ,buf726 ,buf730 ,buf731 ,buf732 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3840 ),buf736 ,buf737 ,buf738 ,buf742 ,buf743 ,buf744 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3904 ),buf748 ,buf749 ,buf750 ,buf754 ,buf755 ,buf756 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),3968 ),buf760 ,buf761 ,buf762 ,buf766 ,buf767 ,buf768 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4032 ),buf772 ,buf773 ,buf774 ,buf778 ,buf779 ,buf780 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4096 ),buf784 ,buf785 ,buf786 ,buf790 ,buf791 ,buf792 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4160 ),buf796 ,buf797 ,buf798 ,buf802 ,buf803 ,buf804 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4224 ),buf808 ,buf809 ,buf810 ,buf814 ,buf815 ,buf816 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4288 ),buf820 ,buf821 ,buf822 ,buf826 ,buf827 ,buf828 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4352 ),buf832 ,buf833 ,buf834 ,buf838 ,buf839 ,buf840 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4416 ),buf844 ,buf845 ,buf846 ,buf850 ,buf851 ,buf852 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4480 ),buf856 ,buf857 ,buf858 ,buf862 ,buf863 ,buf864 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4544 ),buf868 ,buf869 ,buf870 ,buf874 ,buf875 ,buf876 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4608 ),buf880 ,buf881 ,buf882 ,buf886 ,buf887 ,buf888 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4672 ),buf892 ,buf893 ,buf894 ,buf898 ,buf899 ,buf900 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4736 ),buf904 ,buf905 ,buf906 ,buf910 ,buf911 ,buf912 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4800 ),buf916 ,buf917 ,buf918 ,buf922 ,buf923 ,buf924 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4864 ),buf928 ,buf929 ,buf930 ,buf934 ,buf935 ,buf936 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4928 ),buf940 ,buf941 ,buf942 ,buf946 ,buf947 ,buf948 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),4992 ),buf952 ,buf953 ,buf954 ,buf958 ,buf959 ,buf960 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5056 ),buf964 ,buf965 ,buf966 ,buf970 ,buf971 ,buf972 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5120 ),buf976 ,buf977 ,buf978 ,buf982 ,buf983 ,buf984 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5184 ),buf988 ,buf989 ,buf990 ,buf994 ,buf995 ,buf996 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5248 ),buf1000 ,buf1001 ,buf1002 ,buf1006 ,buf1007 ,buf1008 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5312 ),buf1012 ,buf1013 ,buf1014 ,buf1018 ,buf1019 ,buf1020 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5376 ),buf1024 ,buf1025 ,buf1026 ,buf1030 ,buf1031 ,buf1032 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5440 ),buf1036 ,buf1037 ,buf1038 ,buf1042 ,buf1043 ,buf1044 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5504 ),buf1048 ,buf1049 ,buf1050 ,buf1054 ,buf1055 ,buf1056 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5568 ),buf1060 ,buf1061 ,buf1062 ,buf1066 ,buf1067 ,buf1068 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5632 ),buf1072 ,buf1073 ,buf1074 ,buf1078 ,buf1079 ,buf1080 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5696 ),buf1084 ,buf1085 ,buf1086 ,buf1090 ,buf1091 ,buf1092 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5760 ),buf1096 ,buf1097 ,buf1098 ,buf1102 ,buf1103 ,buf1104 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5824 ),buf1108 ,buf1109 ,buf1110 ,buf1114 ,buf1115 ,buf1116 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5888 ),buf1120 ,buf1121 ,buf1122 ,buf1126 ,buf1127 ,buf1128 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),5952 ),buf1132 ,buf1133 ,buf1134 ,buf1138 ,buf1139 ,buf1140 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6016 ),buf1144 ,buf1145 ,buf1146 ,buf1150 ,buf1151 ,buf1152 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6080 ),buf1156 ,buf1157 ,buf1158 ,buf1162 ,buf1163 ,buf1164 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6144 ),buf1168 ,buf1169 ,buf1170 ,buf1174 ,buf1175 ,buf1176 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6208 ),buf1180 ,buf1181 ,buf1182 ,buf1186 ,buf1187 ,buf1188 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6272 ),buf1192 ,buf1193 ,buf1194 ,buf1198 ,buf1199 ,buf1200 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6336 ),buf1204 ,buf1205 ,buf1206 ,buf1210 ,buf1211 ,buf1212 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6400 ),buf1216 ,buf1217 ,buf1218 ,buf1222 ,buf1223 ,buf1224 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6464 ),buf1228 ,buf1229 ,buf1230 ,buf1234 ,buf1235 ,buf1236 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6528 ),buf1240 ,buf1241 ,buf1242 ,buf1246 ,buf1247 ,buf1248 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6592 ),buf1252 ,buf1253 ,buf1254 ,buf1258 ,buf1259 ,buf1260 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6656 ),buf1264 ,buf1265 ,buf1266 ,buf1270 ,buf1271 ,buf1272 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6720 ),buf1276 ,buf1277 ,buf1278 ,buf1282 ,buf1283 ,buf1284 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6784 ),buf1288 ,buf1289 ,buf1290 ,buf1294 ,buf1295 ,buf1296 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6848 ),buf1300 ,buf1301 ,buf1302 ,buf1306 ,buf1307 ,buf1308 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6912 ),buf1312 ,buf1313 ,buf1314 ,buf1318 ,buf1319 ,buf1320 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),6976 ),buf1324 ,buf1325 ,buf1326 ,buf1330 ,buf1331 ,buf1332 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7040 ),buf1336 ,buf1337 ,buf1338 ,buf1342 ,buf1343 ,buf1344 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7104 ),buf1348 ,buf1349 ,buf1350 ,buf1354 ,buf1355 ,buf1356 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7168 ),buf1360 ,buf1361 ,buf1362 ,buf1366 ,buf1367 ,buf1368 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7232 ),buf1372 ,buf1373 ,buf1374 ,buf1378 ,buf1379 ,buf1380 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7296 ),buf1384 ,buf1385 ,buf1386 ,buf1390 ,buf1391 ,buf1392 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7360 ),buf1396 ,buf1397 ,buf1398 ,buf1402 ,buf1403 ,buf1404 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7424 ),buf1408 ,buf1409 ,buf1410 ,buf1414 ,buf1415 ,buf1416 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7488 ),buf1420 ,buf1421 ,buf1422 ,buf1426 ,buf1427 ,buf1428 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7552 ),buf1432 ,buf1433 ,buf1434 ,buf1438 ,buf1439 ,buf1440 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7616 ),buf1444 ,buf1445 ,buf1446 ,buf1450 ,buf1451 ,buf1452 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7680 ),buf1456 ,buf1457 ,buf1458 ,buf1462 ,buf1463 ,buf1464 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7744 ),buf1468 ,buf1469 ,buf1470 ,buf1474 ,buf1475 ,buf1476 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7808 ),buf1480 ,buf1481 ,buf1482 ,buf1486 ,buf1487 ,buf1488 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7872 ),buf1492 ,buf1493 ,buf1494 ,buf1498 ,buf1499 ,buf1500 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),7936 ),buf1504 ,buf1505 ,buf1506 ,buf1510 ,buf1511 ,buf1512 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),8000 ),buf1516 ,buf1517 ,buf1518 ,buf1522 ,buf1523 ,buf1524 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),8064 ),buf1528 ,buf1529 ,buf1530 ,buf1534 ,buf1535 ,buf1536 ,reinterpret_tensor (buf10 ,(1 ,64 ),(64 ,1 ),8128 ),buf1540 ,buf1541 ,buf1542 ,buf1547 ,buf1548 ,reinterpret_tensor (buf1546 ,(1 ,8 ,8 ,1 ,1 ),(64 ,8 ,1 ,1 ,1 ),0 ),primals_12 ,primals_11 ,primals_8 ,primals_7 ,primals_5 ,reinterpret_tensor (buf2 ,(8 ,8 ,128 ),(8 ,1 ,64 ),16384 ),reinterpret_tensor (buf3 ,(8 ,8 ,128 ),(8 ,1 ,64 ),0 ),reinterpret_tensor (buf2 ,(8 ,128 ,8 ),(8 ,64 ,1 ),8192 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =3 \n    primals_2 =rand_strided ((1 ,3 ,128 ),(384 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((512 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((512 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((256 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((256 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "325b8684-13ef-43c8-9091-8f1e1685531f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardsigmoid', 'TripletMarginLoss', 'Unfold', 'BCELoss', 'CELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.unfold = nn.Unfold(kernel_size=(3, 3), stride=(1, 1))\n        self.celu = nn.CELU()\n        self.bce_loss = nn.BCELoss()\n        self.triplet_margin_loss = nn.TripletMarginLoss()\n\n    def forward(self, x):\n        # Apply Unfold to extract patches\n        x = self.unfold(x)\n        \n        # Reshape to a 2D tensor for further processing\n        x = x.view(x.size(0), -1, x.size(2))\n        \n        # Apply CELU activation\n        x = self.celu(x)\n        \n        # Apply Hardsigmoid activation\n        x = self.hardsigmoid(x)\n        \n        # Reshape back to a 4D tensor for loss computation\n        x = x.view(x.size(0), -1, 3, 3)\n        \n        # Generate random anchor, positive, and negative samples for TripletMarginLoss\n        anchor = x[:, 0, :, :].unsqueeze(1)\n        positive = x[:, 1, :, :].unsqueeze(1)\n        negative = x[:, 2, :, :].unsqueeze(1)\n        \n        # Compute TripletMarginLoss\n        triplet_loss = self.triplet_margin_loss(anchor, positive, negative)\n        \n        # Generate random target for BCELoss\n        target = torch.rand_like(x[:, 0, :, :].unsqueeze(1))\n        \n        # Compute BCELoss\n        bce_loss = self.bce_loss(x[:, 0, :, :].unsqueeze(1), target)\n        \n        # Return the sum of the losses\n        return triplet_loss + bce_loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_binary_cross_entropy_rand_like_sub_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,out_ptr3 ,ks0 ,ks1 ,ks2 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =9 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =(r0_index %3 )\n    r0_1 =r0_index //3 \n    r0_2 =r0_index \n    tl .device_assert (((0 <=tl .where ((triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))+((((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 )))<0 ,ks0 +(triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))+((((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 ))),(triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))+((((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 )))))&(tl .where ((triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))+((((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 )))<0 ,ks0 +(triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))+((((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 ))),(triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))+((((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 ))))<ks0 ))|~(r0_mask ),\"index out of bounds: 0 <= tl.where((triton_helpers.div_floor_integer(r0_0 + 3*r0_1,  12 + ((-6)*ks0) + ((-6)*ks1) + 3*ks0*ks1)) + ((((((r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) // ((-2) + ks1)) % ((-2) + ks0))) < 0, ks0 + (triton_helpers.div_floor_integer(r0_0 + 3*r0_1,  12 + ((-6)*ks0) + ((-6)*ks1) + 3*ks0*ks1)) + ((((((r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) // ((-2) + ks1)) % ((-2) + ks0))), (triton_helpers.div_floor_integer(r0_0 + 3*r0_1,  12 + ((-6)*ks0) + ((-6)*ks1) + 3*ks0*ks1)) + ((((((r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) // ((-2) + ks1)) % ((-2) + ks0)))) < ks0\")\n    tl .device_assert (((0 <=tl .where (((((r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 )))<0 ,ks1 +((((r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 ))),((((r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 )))))&(tl .where (((((r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 )))<0 ,ks1 +((((r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 ))),((((r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 ))))<ks1 ))|~(r0_mask ),\"index out of bounds: 0 <= tl.where(((((r0_0 + 3*r0_1) // (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1)) % 3)) + (((((r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) % ((-2) + ks1))) < 0, ks1 + ((((r0_0 + 3*r0_1) // (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1)) % 3)) + (((((r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) % ((-2) + ks1))), ((((r0_0 + 3*r0_1) // (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1)) % 3)) + (((((r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) % ((-2) + ks1)))) < ks1\")\n    tmp2 =tl .load (in_ptr0 +(ks1 *(tl .where ((triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))+((((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 )))<0 ,ks0 +(triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))+((((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 ))),(triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))+((((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 )))))+ks0 *ks1 *(triton_helpers .div_floor_integer (r0_0 +3 *r0_1 ,36 +((-18 )*ks0 )+((-18 )*ks1 )+9 *ks0 *ks1 ))+(tl .where (((((r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 )))<0 ,ks1 +((((r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 ))),((((r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 )))))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tl .device_assert ((((((9 +r0_0 +3 *r0_1 )//(12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))%3 ))+((((((9 +r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 )))<ks0 )|~(r0_mask ),\"index out of bounds: ((((9 + r0_0 + 3*r0_1) // (12 + ((-6)*ks0) + ((-6)*ks1) + 3*ks0*ks1)) % 3)) + ((((((9 + r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) // ((-2) + ks1)) % ((-2) + ks0))) < ks0\")\n    tl .device_assert ((((((9 +r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((9 +r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 )))<ks1 )|~(r0_mask ),\"index out of bounds: ((((9 + r0_0 + 3*r0_1) // (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1)) % 3)) + (((((9 + r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) % ((-2) + ks1))) < ks1\")\n    tmp16 =tl .load (in_ptr0 +(ks1 *((((9 +r0_0 +3 *r0_1 )//(12 +((-6 )*ks0 )+((-6 )*ks1 )+3 *ks0 *ks1 ))%3 ))+ks1 *((((((9 +r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 )))+ks0 *ks1 *(triton_helpers .div_floor_integer (9 +r0_0 +3 *r0_1 ,36 +((-18 )*ks0 )+((-18 )*ks1 )+9 *ks0 *ks1 ))+((((9 +r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%3 ))+(((((9 +r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tl .device_assert ((((((((18 +r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 )))+(((((((18 +r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%(9 *ks2 )))//3 )%3 ))<ks0 )|~(r0_mask ),\"index out of bounds: ((((((18 + r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) // ((-2) + ks1)) % ((-2) + ks0))) + (((((((18 + r0_0 + 3*r0_1) // (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1)) % (9*ks2))) // 3) % 3)) < ks0\")\n    tl .device_assert (((((((18 +r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 )))+((((((18 +r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%(9 *ks2 )))%3 ))<ks1 )|~(r0_mask ),\"index out of bounds: (((((18 + r0_0 + 3*r0_1) % (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1))) % ((-2) + ks1))) + ((((((18 + r0_0 + 3*r0_1) // (4 + ((-2)*ks0) + ((-2)*ks1) + ks0*ks1)) % (9*ks2))) % 3)) < ks1\")\n    tmp27 =tl .load (in_ptr0 +(ks1 *((((((18 +r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))//((-2 )+ks1 ))%((-2 )+ks0 )))+ks1 *(((((((18 +r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%(9 *ks2 )))//3 )%3 ))+ks0 *ks1 *(((((((18 +r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%(9 *ks2 )))//9 )%ks2 ))+(((((18 +r0_0 +3 *r0_1 )%(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 )))%((-2 )+ks1 )))+((((((18 +r0_0 +3 *r0_1 )//(4 +((-2 )*ks0 )+((-2 )*ks1 )+ks0 *ks1 ))%(9 *ks2 )))%3 ))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp3 =0.0 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =libdevice .expm1 (tmp2 )\n    tmp6 =tl .where (tmp4 ,tmp2 ,tmp5 )\n    tmp7 =3.0 \n    tmp8 =tmp6 +tmp7 \n    tmp9 =triton_helpers .maximum (tmp8 ,tmp3 )\n    tmp10 =6.0 \n    tmp11 =triton_helpers .minimum (tmp9 ,tmp10 )\n    tmp12 =0.16666666666666666 \n    tmp13 =tmp11 *tmp12 \n    tmp17 =tmp16 >tmp3 \n    tmp18 =libdevice .expm1 (tmp16 )\n    tmp19 =tl .where (tmp17 ,tmp16 ,tmp18 )\n    tmp20 =tmp19 +tmp7 \n    tmp21 =triton_helpers .maximum (tmp20 ,tmp3 )\n    tmp22 =triton_helpers .minimum (tmp21 ,tmp10 )\n    tmp23 =tmp22 *tmp12 \n    tmp24 =tmp13 -tmp23 \n    tmp28 =tmp27 >tmp3 \n    tmp29 =libdevice .expm1 (tmp27 )\n    tmp30 =tl .where (tmp28 ,tmp27 ,tmp29 )\n    tmp31 =tmp30 +tmp7 \n    tmp32 =triton_helpers .maximum (tmp31 ,tmp3 )\n    tmp33 =triton_helpers .minimum (tmp32 ,tmp10 )\n    tmp34 =tmp33 *tmp12 \n    tmp35 =tmp13 -tmp34 \n    tmp36 =tl .load (in_ptr1 +load_seed_offset )\n    tmp37 =r0_2 \n    tmp38 =tl .rand (tmp36 ,(tmp37 ).to (tl .uint32 ))\n    tmp39 =1.0 \n    tmp40 =tmp38 -tmp39 \n    tmp41 =-tmp13 \n    tmp42 =libdevice .log1p (tmp41 )\n    tmp43 =-100.0 \n    tmp44 =triton_helpers .maximum (tmp42 ,tmp43 )\n    tmp45 =tmp40 *tmp44 \n    tmp46 =tl_math .log (tmp13 )\n    tmp47 =triton_helpers .maximum (tmp46 ,tmp43 )\n    tmp48 =tmp38 *tmp47 \n    tmp49 =tmp45 -tmp48 \n    tmp50 =tl .broadcast_to (tmp49 ,[XBLOCK ,R0_BLOCK ])\n    tmp52 =tl .where (r0_mask ,tmp50 ,0 )\n    tmp53 =tl .sum (tmp52 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .broadcast_to (r0_2 ,[XBLOCK ,R0_BLOCK ])),tmp24 ,r0_mask )\n    tl .store (out_ptr1 +(tl .broadcast_to (r0_2 ,[XBLOCK ,R0_BLOCK ])),tmp35 ,r0_mask )\n    tl .store (out_ptr3 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp53 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_binary_cross_entropy_clamp_min_mean_norm_sub_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp5 =tl .load (in_ptr0 +(1 ))\n    tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ])\n    tmp10 =tl .load (in_ptr0 +(2 ))\n    tmp11 =tl .broadcast_to (tmp10 ,[XBLOCK ])\n    tmp18 =tl .load (in_ptr1 +(0 ))\n    tmp19 =tl .broadcast_to (tmp18 ,[XBLOCK ])\n    tmp22 =tl .load (in_ptr1 +(1 ))\n    tmp23 =tl .broadcast_to (tmp22 ,[XBLOCK ])\n    tmp27 =tl .load (in_ptr1 +(2 ))\n    tmp28 =tl .broadcast_to (tmp27 ,[XBLOCK ])\n    tmp36 =tl .load (in_ptr0 +(3 ))\n    tmp37 =tl .broadcast_to (tmp36 ,[XBLOCK ])\n    tmp40 =tl .load (in_ptr0 +(4 ))\n    tmp41 =tl .broadcast_to (tmp40 ,[XBLOCK ])\n    tmp45 =tl .load (in_ptr0 +(5 ))\n    tmp46 =tl .broadcast_to (tmp45 ,[XBLOCK ])\n    tmp52 =tl .load (in_ptr1 +(3 ))\n    tmp53 =tl .broadcast_to (tmp52 ,[XBLOCK ])\n    tmp56 =tl .load (in_ptr1 +(4 ))\n    tmp57 =tl .broadcast_to (tmp56 ,[XBLOCK ])\n    tmp61 =tl .load (in_ptr1 +(5 ))\n    tmp62 =tl .broadcast_to (tmp61 ,[XBLOCK ])\n    tmp70 =tl .load (in_ptr0 +(6 ))\n    tmp71 =tl .broadcast_to (tmp70 ,[XBLOCK ])\n    tmp74 =tl .load (in_ptr0 +(7 ))\n    tmp75 =tl .broadcast_to (tmp74 ,[XBLOCK ])\n    tmp79 =tl .load (in_ptr0 +(8 ))\n    tmp80 =tl .broadcast_to (tmp79 ,[XBLOCK ])\n    tmp86 =tl .load (in_ptr1 +(6 ))\n    tmp87 =tl .broadcast_to (tmp86 ,[XBLOCK ])\n    tmp90 =tl .load (in_ptr1 +(7 ))\n    tmp91 =tl .broadcast_to (tmp90 ,[XBLOCK ])\n    tmp95 =tl .load (in_ptr1 +(8 ))\n    tmp96 =tl .broadcast_to (tmp95 ,[XBLOCK ])\n    tmp106 =tl .load (in_out_ptr0 +(0 ))\n    tmp107 =tl .broadcast_to (tmp106 ,[XBLOCK ])\n    tmp2 =1e-06 \n    tmp3 =tmp1 +tmp2 \n    tmp4 =tmp3 *tmp3 \n    tmp7 =tmp6 +tmp2 \n    tmp8 =tmp7 *tmp7 \n    tmp9 =tmp4 +tmp8 \n    tmp12 =tmp11 +tmp2 \n    tmp13 =tmp12 *tmp12 \n    tmp14 =tmp9 +tmp13 \n    tmp15 =libdevice .sqrt (tmp14 )\n    tmp16 =1.0 \n    tmp17 =tmp15 +tmp16 \n    tmp20 =tmp19 +tmp2 \n    tmp21 =tmp20 *tmp20 \n    tmp24 =tmp23 +tmp2 \n    tmp25 =tmp24 *tmp24 \n    tmp26 =tmp21 +tmp25 \n    tmp29 =tmp28 +tmp2 \n    tmp30 =tmp29 *tmp29 \n    tmp31 =tmp26 +tmp30 \n    tmp32 =libdevice .sqrt (tmp31 )\n    tmp33 =tmp17 -tmp32 \n    tmp34 =0.0 \n    tmp35 =triton_helpers .maximum (tmp33 ,tmp34 )\n    tmp38 =tmp37 +tmp2 \n    tmp39 =tmp38 *tmp38 \n    tmp42 =tmp41 +tmp2 \n    tmp43 =tmp42 *tmp42 \n    tmp44 =tmp39 +tmp43 \n    tmp47 =tmp46 +tmp2 \n    tmp48 =tmp47 *tmp47 \n    tmp49 =tmp44 +tmp48 \n    tmp50 =libdevice .sqrt (tmp49 )\n    tmp51 =tmp50 +tmp16 \n    tmp54 =tmp53 +tmp2 \n    tmp55 =tmp54 *tmp54 \n    tmp58 =tmp57 +tmp2 \n    tmp59 =tmp58 *tmp58 \n    tmp60 =tmp55 +tmp59 \n    tmp63 =tmp62 +tmp2 \n    tmp64 =tmp63 *tmp63 \n    tmp65 =tmp60 +tmp64 \n    tmp66 =libdevice .sqrt (tmp65 )\n    tmp67 =tmp51 -tmp66 \n    tmp68 =triton_helpers .maximum (tmp67 ,tmp34 )\n    tmp69 =tmp35 +tmp68 \n    tmp72 =tmp71 +tmp2 \n    tmp73 =tmp72 *tmp72 \n    tmp76 =tmp75 +tmp2 \n    tmp77 =tmp76 *tmp76 \n    tmp78 =tmp73 +tmp77 \n    tmp81 =tmp80 +tmp2 \n    tmp82 =tmp81 *tmp81 \n    tmp83 =tmp78 +tmp82 \n    tmp84 =libdevice .sqrt (tmp83 )\n    tmp85 =tmp84 +tmp16 \n    tmp88 =tmp87 +tmp2 \n    tmp89 =tmp88 *tmp88 \n    tmp92 =tmp91 +tmp2 \n    tmp93 =tmp92 *tmp92 \n    tmp94 =tmp89 +tmp93 \n    tmp97 =tmp96 +tmp2 \n    tmp98 =tmp97 *tmp97 \n    tmp99 =tmp94 +tmp98 \n    tmp100 =libdevice .sqrt (tmp99 )\n    tmp101 =tmp85 -tmp100 \n    tmp102 =triton_helpers .maximum (tmp101 ,tmp34 )\n    tmp103 =tmp69 +tmp102 \n    tmp104 =3.0 \n    tmp105 =tmp103 /tmp104 \n    tmp108 =9.0 \n    tmp109 =tmp107 /tmp108 \n    tmp110 =tmp105 +tmp109 \n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp110 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf2 )\n        buf0 =empty_strided_cuda ((1 ,1 ,3 ,3 ),(9 ,9 ,3 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,1 ,3 ,3 ),(9 ,9 ,3 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_binary_cross_entropy_rand_like_sub_0 [grid (1 )](arg3_1 ,buf2 ,buf0 ,buf1 ,buf4 ,64 ,64 ,3 ,0 ,1 ,9 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del arg3_1 \n        del buf2 \n        buf5 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_binary_cross_entropy_clamp_min_mean_norm_sub_1 [grid (1 )](buf5 ,buf0 ,buf1 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        del buf1 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "32e260af-3515-4ef3-a9d6-2e82da198304",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softplus', 'NLLLoss2d', 'LPPool1d', 'ReflectionPad2d', 'PixelUnshuffle', 'Softmin', 'MaxUnpool3d', 'ReLU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reflection_pad = nn.ReflectionPad2d(2)\n        self.pixel_unshuffle = nn.PixelUnshuffle(2)\n        self.lp_pool1d = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.softplus = nn.Softplus()\n        self.softmin = nn.Softmin(dim=1)\n        self.max_unpool3d = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n        self.nll_loss2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Apply ReflectionPad2d\n        x = self.reflection_pad(x)\n        \n        # Apply PixelUnshuffle\n        x = self.pixel_unshuffle(x)\n        \n        # Reshape for LPPool1d\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels * height, width)\n        x = self.lp_pool1d(x)\n        \n        # Reshape back to 4D\n        x = x.view(batch_size, channels, height, -1)\n        \n        # Apply Softplus\n        x = self.softplus(x)\n        \n        # Apply Softmin\n        x = self.softmin(x)\n        \n        # Reshape for MaxUnpool3d\n        x = x.unsqueeze(1)  # Add a dummy dimension for 3D\n        x, indices = F.max_pool3d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool3d(x, indices)\n        \n        # Remove the dummy dimension\n        x = x.squeeze(1)\n        \n        # Apply ReLU\n        x = self.relu(x)\n        \n        # Apply NLLLoss2d (assuming a target tensor is provided)\n        # Note: NLLLoss2d is typically used in the loss function, not in the forward pass.\n        # For demonstration purposes, we will skip applying it here.\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks3 *(tl .where ((-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-2 )+2 *((x1 %(2 +(ks2 //2 ))))+((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//2 )%2 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-2 )+2 *((x1 %(2 +(ks2 //2 ))))+((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//2 )%2 )))))+2 *ks2 ,(-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-2 )+2 *((x1 %(2 +(ks2 //2 ))))+((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//2 )%2 )))))))+ks2 *ks3 *((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//4 )%ks1 ))+(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+4 *x0 +(((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))%2 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+4 *x0 +(((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))%2 )))))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+4 *x0 +(((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))%2 )))))))),xmask ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(ks3 *(tl .where ((-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-2 )+2 *((x1 %(2 +(ks2 //2 ))))+((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//2 )%2 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-2 )+2 *((x1 %(2 +(ks2 //2 ))))+((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//2 )%2 )))))+2 *ks2 ,(-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-2 )+2 *((x1 %(2 +(ks2 //2 ))))+((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//2 )%2 )))))))+ks2 *ks3 *((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//4 )%ks1 ))+(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs (4 *x0 +(((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))%2 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs (4 *x0 +(((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))%2 )))))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs (4 *x0 +(((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))%2 )))))))),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(ks3 *(tl .where ((-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-2 )+2 *((x1 %(2 +(ks2 //2 ))))+((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//2 )%2 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-2 )+2 *((x1 %(2 +(ks2 //2 ))))+((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//2 )%2 )))))+2 *ks2 ,(-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-2 )+2 *((x1 %(2 +(ks2 //2 ))))+((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//2 )%2 )))))))+ks2 *ks3 *((((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))//4 )%ks1 ))+(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs (2 +4 *x0 +(((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))%2 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs (2 +4 *x0 +(((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))%2 )))))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs (2 +4 *x0 +(((((x1 //(2 +(ks2 //2 )))%(4 *ks1 )))%2 )))))))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tmp0 *tmp0 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tmp3 +tmp1 \n    tmp6 =tmp5 *tmp5 \n    tmp7 =tmp6 +tmp4 \n    tmp8 =0.3333333333333333 \n    tmp9 =tmp7 *tmp8 \n    tl .store (out_ptr0 +(x2 ),tmp9 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_neg_softplus_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp24 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(x0 +2 *ks0 *r0_1 +ks0 *r0_1 *(ks1 //2 )),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp2 =tmp1 <tmp0 \n        tmp3 =tmp2 .to (tl .int8 )\n        tmp4 =tmp0 <tmp1 \n        tmp5 =tmp4 .to (tl .int8 )\n        tmp6 =tmp3 -tmp5 \n        tmp7 =tmp6 .to (tmp0 .dtype )\n        tmp8 =tl_math .abs (tmp0 )\n        tmp9 =triton_helpers .maximum (tmp1 ,tmp8 )\n        tmp10 =tmp7 *tmp9 \n        tmp11 =3.0 \n        tmp12 =tmp10 *tmp11 \n        tmp13 =libdevice .sqrt (tmp12 )\n        tmp14 =1.0 \n        tmp15 =tmp13 *tmp14 \n        tmp16 =20.0 \n        tmp17 =tmp15 >tmp16 \n        tmp18 =tl_math .exp (tmp15 )\n        tmp19 =libdevice .log1p (tmp18 )\n        tmp20 =tmp19 *tmp14 \n        tmp21 =tl .where (tmp17 ,tmp13 ,tmp20 )\n        tmp22 =-tmp21 \n        tmp23 =tl .broadcast_to (tmp22 ,[XBLOCK ,R0_BLOCK ])\n        tmp25 =triton_helpers .maximum (_tmp24 ,tmp23 )\n        _tmp24 =tl .where (r0_mask &xmask ,tmp25 ,_tmp24 )\n    tmp24 =triton_helpers .max2 (_tmp24 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp24 ,xmask )\n    _tmp52 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp26 =tl .load (in_ptr0 +(x0 +2 *ks0 *r0_1 +ks0 *r0_1 *(ks1 //2 )),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp27 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp28 =tmp27 <tmp26 \n        tmp29 =tmp28 .to (tl .int8 )\n        tmp30 =tmp26 <tmp27 \n        tmp31 =tmp30 .to (tl .int8 )\n        tmp32 =tmp29 -tmp31 \n        tmp33 =tmp32 .to (tmp26 .dtype )\n        tmp34 =tl_math .abs (tmp26 )\n        tmp35 =triton_helpers .maximum (tmp27 ,tmp34 )\n        tmp36 =tmp33 *tmp35 \n        tmp37 =3.0 \n        tmp38 =tmp36 *tmp37 \n        tmp39 =libdevice .sqrt (tmp38 )\n        tmp40 =1.0 \n        tmp41 =tmp39 *tmp40 \n        tmp42 =20.0 \n        tmp43 =tmp41 >tmp42 \n        tmp44 =tl_math .exp (tmp41 )\n        tmp45 =libdevice .log1p (tmp44 )\n        tmp46 =tmp45 *tmp40 \n        tmp47 =tl .where (tmp43 ,tmp39 ,tmp46 )\n        tmp48 =-tmp47 \n        tmp49 =tmp48 -tmp24 \n        tmp50 =tl_math .exp (tmp49 )\n        tmp51 =tl .broadcast_to (tmp50 ,[XBLOCK ,R0_BLOCK ])\n        tmp53 =_tmp52 +tmp51 \n        _tmp52 =tl .where (r0_mask &xmask ,tmp53 ,_tmp52 )\n    tmp52 =tl .sum (_tmp52 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp52 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_neg_softplus_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %ks0 )\n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp23 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp26 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tmp1 <tmp0 \n    tmp3 =tmp2 .to (tl .int8 )\n    tmp4 =tmp0 <tmp1 \n    tmp5 =tmp4 .to (tl .int8 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =tmp6 .to (tmp0 .dtype )\n    tmp8 =tl_math .abs (tmp0 )\n    tmp9 =triton_helpers .maximum (tmp1 ,tmp8 )\n    tmp10 =tmp7 *tmp9 \n    tmp11 =3.0 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =libdevice .sqrt (tmp12 )\n    tmp14 =1.0 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =20.0 \n    tmp17 =tmp15 >tmp16 \n    tmp18 =tl_math .exp (tmp15 )\n    tmp19 =libdevice .log1p (tmp18 )\n    tmp20 =tmp19 *tmp14 \n    tmp21 =tl .where (tmp17 ,tmp13 ,tmp20 )\n    tmp22 =-tmp21 \n    tmp24 =tmp22 -tmp23 \n    tmp25 =tl_math .exp (tmp24 )\n    tmp27 =tmp25 /tmp26 \n    tl .store (in_out_ptr0 +(x2 ),tmp27 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_3 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_4 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +((triton_helpers .div_floor_integer (x0 ,(ks0 //4 )*(triton_helpers .div_floor_integer (1 +(ks1 //2 ),4 ))+(triton_helpers .div_floor_integer (1 +(ks1 //2 ),4 ))))*(triton_helpers .div_floor_integer (1 +(ks1 //2 ),4 ))+(ks0 //4 )*(triton_helpers .div_floor_integer (x0 ,(ks0 //4 )*(triton_helpers .div_floor_integer (1 +(ks1 //2 ),4 ))+(triton_helpers .div_floor_integer (1 +(ks1 //2 ),4 ))))*(triton_helpers .div_floor_integer (1 +(ks1 //2 ),4 ))+((x0 %((ks0 //4 )*(triton_helpers .div_floor_integer (1 +(ks1 //2 ),4 ))+(triton_helpers .div_floor_integer (1 +(ks1 //2 ),4 )))))),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =8 *(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 ))+8 *(ks0 //4 )*(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 ))+8 *(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 ))*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))+8 *(ks0 //4 )*(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 ))*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))\n    tmp4 =tmp2 +tmp3 \n    tmp5 =tmp2 <0 \n    tmp6 =tl .where (tmp5 ,tmp4 ,tmp2 )\n    tl .device_assert (((0 <=tmp6 )&(tmp6 <8 *(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 ))+8 *(ks0 //4 )*(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 ))+8 *(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 ))*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))+8 *(ks0 //4 )*(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 ))*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))))|~(xmask ),\"index out of bounds: 0 <= tmp6 < 8*(triton_helpers.div_floor_integer(ks2*(triton_helpers.div_floor_integer(4 + ks0,  2 + (ks0 // 2)))*(triton_helpers.div_floor_integer(4 + ks1,  2 + (ks1 // 2))),  2)) + 8*(ks0 // 4)*(triton_helpers.div_floor_integer(ks2*(triton_helpers.div_floor_integer(4 + ks0,  2 + (ks0 // 2)))*(triton_helpers.div_floor_integer(4 + ks1,  2 + (ks1 // 2))),  2)) + 8*(triton_helpers.div_floor_integer(ks2*(triton_helpers.div_floor_integer(4 + ks0,  2 + (ks0 // 2)))*(triton_helpers.div_floor_integer(4 + ks1,  2 + (ks1 // 2))),  2))*(triton_helpers.div_floor_integer((-1) + (triton_helpers.div_floor_integer((-1) + (ks1 // 2),  2)),  2)) + 8*(ks0 // 4)*(triton_helpers.div_floor_integer(ks2*(triton_helpers.div_floor_integer(4 + ks0,  2 + (ks0 // 2)))*(triton_helpers.div_floor_integer(4 + ks1,  2 + (ks1 // 2))),  2))*(triton_helpers.div_floor_integer((-1) + (triton_helpers.div_floor_integer((-1) + (ks1 // 2),  2)),  2))\")\n    tl .store (out_ptr0 +(tl .broadcast_to (2 *(((tmp6 //(2 +2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))))%(2 +2 *(ks0 //4 ))))+4 *(((tmp6 //(4 +4 *(ks0 //4 )+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))+4 *(ks0 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))))%(2 *(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 )))))+2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))*(((tmp6 //(2 +2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))))%(2 +2 *(ks0 //4 ))))+4 *(ks0 //4 )*(((tmp6 //(4 +4 *(ks0 //4 )+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))+4 *(ks0 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))))%(2 *(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 )))))+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))*(((tmp6 //(4 +4 *(ks0 //4 )+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))+4 *(ks0 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))))%(2 *(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 )))))+4 *(ks0 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))*(((tmp6 //(4 +4 *(ks0 //4 )+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))+4 *(ks0 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))))%(2 *(triton_helpers .div_floor_integer (ks2 *(triton_helpers .div_floor_integer (4 +ks0 ,2 +(ks0 //2 )))*(triton_helpers .div_floor_integer (4 +ks1 ,2 +(ks1 //2 ))),2 )))))+((tmp6 %(2 +2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks1 //2 ),2 )),2 ))))),[XBLOCK ])),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_relu_5 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +2 *((((x0 +2 *x1 +4 *x2 +2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )+4 *x2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 )))//(2 +2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))))%(2 +2 *(ks4 //4 ))))+4 *((((x0 +2 *x1 +4 *x2 +2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )+4 *x2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 )))//(4 +4 *(ks4 //4 )+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))))%(2 *(triton_helpers .div_floor_integer (ks3 *(triton_helpers .div_floor_integer (4 +ks4 ,2 +(ks4 //2 )))*(triton_helpers .div_floor_integer (4 +ks5 ,2 +(ks5 //2 ))),2 )))))+2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))*((((x0 +2 *x1 +4 *x2 +2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )+4 *x2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 )))//(2 +2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))))%(2 +2 *(ks4 //4 ))))+4 *(ks4 //4 )*((((x0 +2 *x1 +4 *x2 +2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )+4 *x2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 )))//(4 +4 *(ks4 //4 )+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))))%(2 *(triton_helpers .div_floor_integer (ks3 *(triton_helpers .div_floor_integer (4 +ks4 ,2 +(ks4 //2 )))*(triton_helpers .div_floor_integer (4 +ks5 ,2 +(ks5 //2 ))),2 )))))+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))*((((x0 +2 *x1 +4 *x2 +2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )+4 *x2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 )))//(4 +4 *(ks4 //4 )+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))))%(2 *(triton_helpers .div_floor_integer (ks3 *(triton_helpers .div_floor_integer (4 +ks4 ,2 +(ks4 //2 )))*(triton_helpers .div_floor_integer (4 +ks5 ,2 +(ks5 //2 ))),2 )))))+4 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))*((((x0 +2 *x1 +4 *x2 +2 *x1 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )+4 *x2 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *x2 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 )))//(4 +4 *(ks4 //4 )+4 *(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))+4 *(ks4 //4 )*(triton_helpers .div_floor_integer ((-1 )+(triton_helpers .div_floor_integer ((-1 )+(ks5 //2 ),2 )),2 ))))%(2 *(triton_helpers .div_floor_integer (ks3 *(triton_helpers .div_floor_integer (4 +ks4 ,2 +(ks4 //2 )))*(triton_helpers .div_floor_integer (4 +ks5 ,2 +(ks5 //2 ))),2 )))))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        (1 +(s2 //2 ))//2 \n        buf0 =empty_strided_cuda ((1 ,2 *s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 )))+s0 *(s1 //2 )*((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))),1 ,(1 +(s2 //2 ))//2 ),(2 *s0 *((1 +(s2 //2 ))//2 )*((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 )))+s0 *(s1 //2 )*((1 +(s2 //2 ))//2 )*((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))),(1 +(s2 //2 ))//2 ,(1 +(s2 //2 ))//2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_0_xnumel =2 *s0 *((1 +(s2 //2 ))//2 )*((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 )))+s0 *(s1 //2 )*((1 +(s2 //2 ))//2 )*((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 )))\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_0 [grid (triton_poi_fused_avg_pool2d_0_xnumel )](arg3_1 ,buf0 ,16 ,3 ,64 ,64 ,6528 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((1 ,1 ,2 +(s1 //2 ),(1 +(s2 //2 ))//2 ),(2 *((1 +(s2 //2 ))//2 )+(s1 //2 )*((1 +(s2 //2 ))//2 ),2 *((1 +(s2 //2 ))//2 )+(s1 //2 )*((1 +(s2 //2 ))//2 ),(1 +(s2 //2 ))//2 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,1 ,2 +(s1 //2 ),(1 +(s2 //2 ))//2 ),(2 *((1 +(s2 //2 ))//2 )+(s1 //2 )*((1 +(s2 //2 ))//2 ),2 *((1 +(s2 //2 ))//2 )+(s1 //2 )*((1 +(s2 //2 ))//2 ),(1 +(s2 //2 ))//2 ,1 ),torch .float32 )\n\n        triton_red_fused__softmax_neg_softplus_1_xnumel =2 *((1 +(s2 //2 ))//2 )+(s1 //2 )*((1 +(s2 //2 ))//2 )\n        s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 )))\n        get_raw_stream (0 )\n        triton_red_fused__softmax_neg_softplus_1 [grid (triton_red_fused__softmax_neg_softplus_1_xnumel )](buf0 ,buf1 ,buf2 ,16 ,64 ,544 ,12 ,XBLOCK =1 ,R0_BLOCK =16 ,num_warps =2 ,num_stages =1 )\n        2 *((1 +(s2 //2 ))//2 )+(s1 //2 )*((1 +(s2 //2 ))//2 )\n        buf3 =reinterpret_tensor (buf0 ,(1 ,s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))),2 +(s1 //2 ),(1 +(s2 //2 ))//2 ),(2 *s0 *((1 +(s2 //2 ))//2 )*((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 )))+s0 *(s1 //2 )*((1 +(s2 //2 ))//2 )*((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))),2 *((1 +(s2 //2 ))//2 )+(s1 //2 )*((1 +(s2 //2 ))//2 ),(1 +(s2 //2 ))//2 ,1 ),0 );del buf0 \n\n        triton_poi_fused__softmax_neg_softplus_2_xnumel =2 *s0 *((1 +(s2 //2 ))//2 )*((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 )))+s0 *(s1 //2 )*((1 +(s2 //2 ))//2 )*((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 )))\n        get_raw_stream (0 )\n        triton_poi_fused__softmax_neg_softplus_2 [grid (triton_poi_fused__softmax_neg_softplus_2_xnumel )](buf3 ,buf1 ,buf2 ,544 ,6528 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        del buf2 \n\n        buf4 =torch .ops .aten .max_pool3d_with_indices .default (reinterpret_tensor (buf3 ,(1 ,1 ,s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))),2 +(s1 //2 ),(1 +(s2 //2 ))//2 ),(0 ,0 ,2 *((1 +(s2 //2 ))//2 )+(s1 //2 )*((1 +(s2 //2 ))//2 ),(1 +(s2 //2 ))//2 ,1 ),0 ),[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del buf3 \n        buf5 =buf4 [0 ]\n        buf6 =buf4 [1 ]\n        del buf4 \n        buf7 =empty_strided_cuda ((1 ,1 ,2 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 ),2 +2 *(s1 //4 ),2 +2 *(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )),(8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 ),8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 ),4 +4 *(s1 //4 )+4 *(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )+4 *(s1 //4 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 ),2 +2 *(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool3d_3_xnumel =8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_3 [grid (triton_poi_fused_max_unpool3d_3_xnumel )](buf7 ,6528 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool3d_4_xnumel =((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*((1 +(s2 //2 ))//4 )+(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*((1 +(s2 //2 ))//4 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_4 [grid (triton_poi_fused_max_unpool3d_4_xnumel )](buf6 ,buf5 ,buf7 ,64 ,64 ,3 ,816 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf5 \n        del buf6 \n        2 +2 *(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )\n        2 +2 *(s1 //4 )\n        4 +4 *(s1 //4 )+4 *(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )+4 *(s1 //4 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )\n        buf9 =empty_strided_cuda ((1 ,2 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 ),2 +2 *(s1 //4 ),2 +2 *(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )),(8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 ),4 +4 *(s1 //4 )+4 *(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )+4 *(s1 //4 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 ),2 +2 *(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_relu_5_xnumel =8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )+8 *((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )+8 *(s1 //4 )*((s0 *((4 +s1 )//(2 +(s1 //2 )))*((4 +s2 )//(2 +(s2 //2 ))))//2 )*(((-1 )+(((-1 )+(s2 //2 ))//2 ))//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_relu_5 [grid (triton_poi_fused_relu_5_xnumel )](buf7 ,buf9 ,16 ,34 ,544 ,3 ,64 ,64 ,6528 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf7 \n    return (buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "35a9cc14-63c1-4934-84e0-d091c3f81680",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PixelShuffle', 'CrossMapLRN2d', 'LazyLinear', 'AvgPool3d', 'Hardsigmoid', 'SmoothL1Loss', 'MaxPool3d', 'PReLU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pixel_shuffle = nn.PixelShuffle(2)\n        self.cross_map_lrn2d = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.lazy_linear = nn.LazyLinear(128)\n        self.avg_pool3d = nn.AvgPool3d(kernel_size=2, stride=2)\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.smooth_l1_loss = nn.SmoothL1Loss()\n        self.max_pool3d = nn.MaxPool3d(kernel_size=2, stride=2)\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        # Apply PixelShuffle\n        x = self.pixel_shuffle(x)\n        \n        # Apply CrossMapLRN2d\n        x = self.cross_map_lrn2d(x)\n        \n        # Reshape for 3D operations\n        x = x.unsqueeze(2)  # Add a dummy dimension to make it 5D (batch, channels, depth, height, width)\n        \n        # Apply AvgPool3d\n        x = self.avg_pool3d(x)\n        \n        # Apply MaxPool3d\n        x = self.max_pool3d(x)\n        \n        # Remove the dummy dimension\n        x = x.squeeze(2)\n        \n        # Flatten the tensor for LazyLinear\n        x = x.view(x.size(0), -1)\n        \n        # Apply LazyLinear\n        x = self.lazy_linear(x)\n        \n        # Apply PReLU\n        x = self.prelu(x)\n        \n        # Apply Hardsigmoid\n        x = self.hardsigmoid(x)\n        \n        # Compute SmoothL1Loss (assuming a dummy target for demonstration)\n        dummy_target = torch.zeros_like(x)\n        loss = self.smooth_l1_loss(x, dummy_target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 16, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_pixel_shuffle_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    xnumel =2 \n    yoffset =(tl .program_id (1 )+tl .program_id (2 )*tl .num_programs (1 ))*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x4 =xindex \n    y0 =(yindex %ks0 )\n    y1 =((yindex //ks0 )%2 )\n    y2 =((yindex //ks1 )%ks2 )\n    y3 =yindex //ks3 \n    y5 =yindex \n    tmp0 =tl .load (in_ptr0 +(y0 +ks0 *y2 +ks0 *ks2 *x4 +2 *ks0 *ks2 *y1 +4 *ks0 *ks2 *y3 ),xmask &ymask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x4 +2 *y5 ),tmp0 ,xmask &ymask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 *s2 \n        2 *s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 //4 ,s1 ,2 ,s2 ,2 ),(4 *s1 *s2 *(s0 //4 ),4 *s1 *s2 ,4 *s2 ,2 *s2 ,2 ,1 ),torch .float32 )\n\n        triton_poi_fused_pixel_shuffle_0_ynumel =2 *s1 *s2 *(s0 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_pixel_shuffle_0 [grid (triton_poi_fused_pixel_shuffle_0_ynumel ,2 )](arg3_1 ,buf0 ,64 ,128 ,64 ,8192 ,32768 ,2 ,XBLOCK =2 ,YBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,s0 //4 ,2 *s1 ,2 *s2 ),(2 *s1 *s2 *(s0 //4 )*(s0 //(2 *(s0 //4 ))),4 *s1 *s2 ,2 *s2 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =16 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,16 ,64 ,64 ),(65536 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "37f5ac71-e336-4ddf-97d6-849ab80e5c26",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReLU', 'SELU', 'ReflectionPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ReflectionPad3d(1)\n        self.pad2 = nn.ReflectionPad3d(2)\n        self.pad3 = nn.ReflectionPad3d(3)\n        self.relu = nn.ReLU()\n        self.selu = nn.SELU()\n\n    def forward(self, x):\n        # Apply ReflectionPad3d with different padding sizes\n        x = self.pad1(x)\n        x = self.relu(x)\n        x = self.pad2(x)\n        x = self.selu(x)\n        x = self.pad3(x)\n        x = self.relu(x)\n        \n        # Reshape the output to match the input shape\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x = x.view(x.size(0), 1, -1)  # Reshape to 3D tensor\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 64, 64, 64).cuda()  # Assuming 3D input for ReflectionPad3d\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_reflection_pad3d_relu_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks5 *(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+(tl .where (1 +ks4 +((-1 )*tl_math .abs (1 +ks4 +((-1 )*tl_math .abs ((-2 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks4 +((-1 )*tl_math .abs ((-2 )+x1 ))))+2 *ks4 ,1 +ks4 +((-1 )*tl_math .abs (1 +ks4 +((-1 )*tl_math .abs ((-2 )+x1 )))))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+(tl .where (1 +ks4 +((-1 )*tl_math .abs (1 +ks4 +((-1 )*tl_math .abs ((-2 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks4 +((-1 )*tl_math .abs ((-2 )+x1 ))))+2 *ks4 ,1 +ks4 +((-1 )*tl_math .abs (1 +ks4 +((-1 )*tl_math .abs ((-2 )+x1 )))))))))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+(tl .where (1 +ks4 +((-1 )*tl_math .abs (1 +ks4 +((-1 )*tl_math .abs ((-2 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks4 +((-1 )*tl_math .abs ((-2 )+x1 ))))+2 *ks4 ,1 +ks4 +((-1 )*tl_math .abs (1 +ks4 +((-1 )*tl_math .abs ((-2 )+x1 )))))))))))+ks4 *ks5 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+(tl .where (1 +ks3 +((-1 )*tl_math .abs (1 +ks3 +((-1 )*tl_math .abs ((-2 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks3 +((-1 )*tl_math .abs ((-2 )+x2 ))))+2 *ks3 ,1 +ks3 +((-1 )*tl_math .abs (1 +ks3 +((-1 )*tl_math .abs ((-2 )+x2 )))))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+(tl .where (1 +ks3 +((-1 )*tl_math .abs (1 +ks3 +((-1 )*tl_math .abs ((-2 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks3 +((-1 )*tl_math .abs ((-2 )+x2 ))))+2 *ks3 ,1 +ks3 +((-1 )*tl_math .abs (1 +ks3 +((-1 )*tl_math .abs ((-2 )+x2 )))))))))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+(tl .where (1 +ks3 +((-1 )*tl_math .abs (1 +ks3 +((-1 )*tl_math .abs ((-2 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks3 +((-1 )*tl_math .abs ((-2 )+x2 ))))+2 *ks3 ,1 +ks3 +((-1 )*tl_math .abs (1 +ks3 +((-1 )*tl_math .abs ((-2 )+x2 )))))))))))+(tl .where ((-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+(tl .where (1 +ks5 +((-1 )*tl_math .abs (1 +ks5 +((-1 )*tl_math .abs ((-2 )+x0 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks5 +((-1 )*tl_math .abs ((-2 )+x0 ))))+2 *ks5 ,1 +ks5 +((-1 )*tl_math .abs (1 +ks5 +((-1 )*tl_math .abs ((-2 )+x0 )))))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+(tl .where (1 +ks5 +((-1 )*tl_math .abs (1 +ks5 +((-1 )*tl_math .abs ((-2 )+x0 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks5 +((-1 )*tl_math .abs ((-2 )+x0 ))))+2 *ks5 ,1 +ks5 +((-1 )*tl_math .abs (1 +ks5 +((-1 )*tl_math .abs ((-2 )+x0 )))))))))+2 *ks5 ,(-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+(tl .where (1 +ks5 +((-1 )*tl_math .abs (1 +ks5 +((-1 )*tl_math .abs ((-2 )+x0 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks5 +((-1 )*tl_math .abs ((-2 )+x0 ))))+2 *ks5 ,1 +ks5 +((-1 )*tl_math .abs (1 +ks5 +((-1 )*tl_math .abs ((-2 )+x0 )))))))))))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_elu_reflection_pad3d_relu_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(6 *(tl .where (5 +ks4 +((-1 )*tl_math .abs (5 +ks4 +((-1 )*tl_math .abs ((-3 )+x1 ))))<0 ,11 +((-1 )*tl_math .abs (5 +ks4 +((-1 )*tl_math .abs ((-3 )+x1 ))))+2 *ks4 ,5 +ks4 +((-1 )*tl_math .abs (5 +ks4 +((-1 )*tl_math .abs ((-3 )+x1 ))))))+36 *(tl .where (5 +ks3 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))<0 ,11 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))+2 *ks3 ,5 +ks3 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))))+ks5 *(tl .where (5 +ks4 +((-1 )*tl_math .abs (5 +ks4 +((-1 )*tl_math .abs ((-3 )+x1 ))))<0 ,11 +((-1 )*tl_math .abs (5 +ks4 +((-1 )*tl_math .abs ((-3 )+x1 ))))+2 *ks4 ,5 +ks4 +((-1 )*tl_math .abs (5 +ks4 +((-1 )*tl_math .abs ((-3 )+x1 ))))))+6 *ks4 *(tl .where (5 +ks3 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))<0 ,11 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))+2 *ks3 ,5 +ks3 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))))+6 *ks5 *(tl .where (5 +ks3 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))<0 ,11 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))+2 *ks3 ,5 +ks3 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))))+ks4 *ks5 *(tl .where (5 +ks3 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))<0 ,11 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))+2 *ks3 ,5 +ks3 +((-1 )*tl_math .abs (5 +ks3 +((-1 )*tl_math .abs ((-3 )+x2 ))))))+(tl .where (5 +ks5 +((-1 )*tl_math .abs (5 +ks5 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,11 +((-1 )*tl_math .abs (5 +ks5 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks5 ,5 +ks5 +((-1 )*tl_math .abs (5 +ks5 +((-1 )*tl_math .abs ((-3 )+x0 ))))))),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =1.0507009873554805 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =1.0 \n    tmp6 =tmp0 *tmp5 \n    tmp7 =libdevice .expm1 (tmp6 )\n    tmp8 =1.7580993408473766 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =tl .where (tmp2 ,tmp4 ,tmp9 )\n    tmp11 =tl .full ([1 ],0 ,tl .int32 )\n    tmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\n    tl .store (out_ptr0 +(x3 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(12 *(((x0 //ks0 )%ks1 ))+144 *(x0 //(144 +12 *ks2 +12 *ks3 +ks2 *ks3 ))+ks3 *(((x0 //ks0 )%ks1 ))+12 *ks2 *(x0 //(144 +12 *ks2 +12 *ks3 +ks2 *ks3 ))+12 *ks3 *(x0 //(144 +12 *ks2 +12 *ks3 +ks2 *ks3 ))+ks2 *ks3 *(x0 //(144 +12 *ks2 +12 *ks3 +ks2 *ks3 ))+((x0 %ks0 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        6 +s2 \n        6 +s1 \n        36 +6 *s1 +6 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,1 ,6 +s0 ,6 +s1 ,6 +s2 ),(216 +36 *s0 +36 *s1 +36 *s2 +6 *s0 *s1 +6 *s0 *s2 +6 *s1 *s2 +s0 *s1 *s2 ,216 +36 *s0 +36 *s1 +36 *s2 +6 *s0 *s1 +6 *s0 *s2 +6 *s1 *s2 +s0 *s1 *s2 ,36 +6 *s1 +6 *s2 +s1 *s2 ,6 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_reflection_pad3d_relu_0_xnumel =216 +36 *s0 +36 *s1 +36 *s2 +6 *s0 *s1 +6 *s0 *s2 +6 *s1 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_reflection_pad3d_relu_0 [grid (triton_poi_fused_reflection_pad3d_relu_0_xnumel )](arg3_1 ,buf0 ,70 ,70 ,4900 ,64 ,64 ,64 ,343000 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg3_1 \n        12 +s2 \n        12 +s1 \n        144 +12 *s1 +12 *s2 +s1 *s2 \n        buf1 =empty_strided_cuda ((1 ,1 ,12 +s0 ,12 +s1 ,12 +s2 ),(1728 +144 *s0 +144 *s1 +144 *s2 +12 *s0 *s1 +12 *s0 *s2 +12 *s1 *s2 +s0 *s1 *s2 ,1 ,144 +12 *s1 +12 *s2 +s1 *s2 ,12 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_elu_reflection_pad3d_relu_1_xnumel =1728 +144 *s0 +144 *s1 +144 *s2 +12 *s0 *s1 +12 *s0 *s2 +12 *s1 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_elu_reflection_pad3d_relu_1 [grid (triton_poi_fused_elu_reflection_pad3d_relu_1_xnumel )](buf0 ,buf1 ,76 ,76 ,5776 ,64 ,64 ,64 ,438976 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,1 ,1728 +144 *s0 +144 *s1 +144 *s2 +12 *s0 *s1 +12 *s0 *s2 +12 *s1 *s2 +s0 *s1 *s2 ),(1728 +144 *s0 +144 *s1 +144 *s2 +12 *s0 *s1 +12 *s0 *s2 +12 *s1 *s2 +s0 *s1 *s2 ,1728 +144 *s0 +144 *s1 +144 *s2 +12 *s0 *s1 +12 *s0 *s2 +12 *s1 *s2 +s0 *s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_view_2_xnumel =1728 +144 *s0 +144 *s1 +144 *s2 +12 *s0 *s1 +12 *s0 *s2 +12 *s1 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (triton_poi_fused_view_2_xnumel )](buf1 ,buf2 ,76 ,76 ,64 ,64 ,438976 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,1 ,64 ,64 ,64 ),(262144 ,262144 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "3835db47-c995-4442-b8c6-a6c795d7e080",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CrossMapLRN2d', 'MaxUnpool3d', 'Softmin', 'ModuleList', 'PixelShuffle', 'EmbeddingBag']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.max_unpool = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.softmin = nn.Softmin(dim=1)\n        self.pixel_shuffle = nn.PixelShuffle(2)\n        self.embedding_bag = nn.EmbeddingBag(num_embeddings=100, embedding_dim=10, mode='mean')\n        \n        # Using ModuleList to hold multiple layers\n        self.module_list = nn.ModuleList([\n            nn.Conv2d(1, 10, kernel_size=3),\n            nn.Conv2d(10, 20, kernel_size=3),\n            nn.Linear(320, 50),\n            nn.Linear(50, 10)\n        ])\n\n    def forward(self, x):\n        # Apply CrossMapLRN2d\n        x = self.lrn(x)\n        \n        # Apply Conv2d layers from ModuleList\n        x = self.module_list[0](x)\n        x = F.relu(x)\n        x = self.module_list[1](x)\n        x = F.relu(x)\n        \n        # Reshape for MaxUnpool3d\n        x = x.unsqueeze(1)  # Add a dummy dimension for 3D\n        x, indices = F.max_pool3d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool(x, indices)\n        x = x.squeeze(1)  # Remove the dummy dimension\n        \n        # Apply PixelShuffle\n        x = self.pixel_shuffle(x)\n        \n        # Reshape for EmbeddingBag\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.embedding_bag(x.long())  # Convert to long for embedding\n        \n        # Apply Softmin\n        x = self.softmin(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_relu_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =38440 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //3844 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x2 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_relu_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =72000 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //3600 \n    x0 =(xindex %3600 )\n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (out_ptr0 +(x0 +3616 *x1 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_2 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =72000 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =9000 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp8 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([XBLOCK ],72000 ,tl .int32 )\n    tmp4 =tmp2 +tmp3 \n    tmp5 =tmp2 <0 \n    tmp6 =tl .where (tmp5 ,tmp4 ,tmp2 )\n    tl .device_assert (((0 <=tmp6 )&(tmp6 <72000 ))|~(xmask ),\"index out of bounds: 0 <= tmp6 < 72000\")\n    tl .store (out_ptr0 +(tl .broadcast_to (tmp6 ,[XBLOCK ])),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_arange_4 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp0 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_5 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =72000 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(60 *((((x0 //120 )%120 ))//2 )+3600 *((((x0 %120 ))%2 ))+7200 *(((((x0 //120 )%120 ))%2 ))+14400 *(x0 //14400 )+(((x0 %120 ))//2 )),xmask ,eviction_policy ='evict_last')\n    tmp1 =tmp0 .to (tl .int64 )\n    tl .store (out_ptr0 +(x0 ),tmp1 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_neg_6 (in_out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =-tmp0 \n    tmp2 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .where (r0_mask ,tmp2 ,float (\"-inf\"))\n    tmp5 =triton_helpers .max2 (tmp4 ,1 )[:,None ]\n    tmp6 =tmp1 -tmp5 \n    tmp7 =tl_math .exp (tmp6 )\n    tmp8 =tl .broadcast_to (tmp7 ,[XBLOCK ,R0_BLOCK ])\n    tmp10 =tl .where (r0_mask ,tmp8 ,0 )\n    tmp11 =tl .sum (tmp10 ,1 )[:,None ]\n    tmp12 =tmp7 /tmp11 \n    tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp12 ,r0_mask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,1 ,64 ,64 ),(4096 ,4096 ,64 ,1 ))\n    assert_size_stride (primals_2 ,(10 ,1 ,3 ,3 ),(9 ,9 ,3 ,1 ))\n    assert_size_stride (primals_3 ,(10 ,),(1 ,))\n    assert_size_stride (primals_4 ,(20 ,10 ,3 ,3 ),(90 ,9 ,3 ,1 ))\n    assert_size_stride (primals_5 ,(20 ,),(1 ,))\n    assert_size_stride (primals_6 ,(100 ,10 ),(10 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_1 ,primals_2 ,stride =(1 ,1 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =False ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,10 ,62 ,62 ),(38440 ,3844 ,62 ,1 ))\n        del primals_1 \n        del primals_2 \n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_relu_0 [grid (38440 )](buf1 ,primals_3 ,38440 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del primals_3 \n\n        buf2 =extern_kernels .convolution (buf1 ,primals_4 ,stride =(1 ,1 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =False ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf2 ,(1 ,20 ,60 ,60 ),(72000 ,3600 ,60 ,1 ))\n        del buf1 \n        del primals_4 \n        buf3 =empty_strided_cuda ((1 ,20 ,60 ,60 ),(72320 ,3616 ,60 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_relu_1 [grid (72000 )](buf2 ,primals_5 ,buf3 ,72000 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_5 \n\n        buf4 =torch .ops .aten .max_pool3d_with_indices .default (reinterpret_tensor (buf3 ,(1 ,1 ,20 ,60 ,60 ),(0 ,0 ,3616 ,60 ,1 ),0 ),[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del buf3 \n        buf5 =buf4 [0 ]\n        buf6 =buf4 [1 ]\n        del buf4 \n        buf7 =reinterpret_tensor (buf2 ,(72000 ,),(1 ,),0 );del buf2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_2 [grid (72000 )](buf7 ,72000 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_3 [grid (9000 )](buf6 ,buf5 ,buf7 ,9000 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf5 \n        del buf6 \n        buf9 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_arange_4 [grid (1 )](buf9 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        buf10 =empty_strided_cuda ((1 ,72000 ),(72000 ,1 ),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_5 [grid (72000 )](buf7 ,buf10 ,72000 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf7 \n\n        buf11 =torch .ops .aten ._embedding_bag .default (primals_6 ,reinterpret_tensor (buf10 ,(72000 ,),(1 ,),0 ),buf9 ,False ,1 )\n        del primals_6 \n        buf12 =buf11 [0 ]\n        buf13 =buf11 [1 ]\n        buf14 =buf11 [2 ]\n        buf15 =buf11 [3 ]\n        del buf11 \n        buf18 =buf12 ;del buf12 \n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_neg_6 [grid (1 )](buf18 ,1 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n    return (buf18 ,buf9 ,reinterpret_tensor (buf10 ,(72000 ,),(1 ,),0 ),buf13 ,buf14 ,buf15 ,buf18 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,1 ,64 ,64 ),(4096 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((10 ,1 ,3 ,3 ),(9 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((20 ,10 ,3 ,3 ),(90 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((100 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "390e17e1-f770-49c7-b90f-c4fda888dde8",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MultiLabelSoftMarginLoss', 'AdaptiveMaxPool2d', 'AvgPool1d', 'InstanceNorm2d', 'FractionalMaxPool3d', 'SiLU', 'MaxPool1d', 'BCELoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool2d = nn.AdaptiveMaxPool2d((16, 16))\n        self.avg_pool1d = nn.AvgPool1d(kernel_size=2)\n        self.instance_norm2d = nn.InstanceNorm2d(16)\n        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))\n        self.silu = nn.SiLU()\n        self.max_pool1d = nn.MaxPool1d(kernel_size=2)\n        self.bce_loss = nn.BCELoss()\n        self.multi_label_soft_margin_loss = nn.MultiLabelSoftMarginLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        x = self.adaptive_max_pool2d(x)  # Shape: (batch_size, channels, 16, 16)\n        x = self.instance_norm2d(x)  # Shape: (batch_size, 16, 16, 16)\n        \n        # Reshape for 1D operations\n        x = x.view(x.size(0), x.size(1), -1)  # Shape: (batch_size, 16, 256)\n        x = self.avg_pool1d(x)  # Shape: (batch_size, 16, 128)\n        x = self.max_pool1d(x)  # Shape: (batch_size, 16, 64)\n        \n        # Reshape for 3D operations\n        x = x.view(x.size(0), x.size(1), 8, 8, -1)  # Shape: (batch_size, 16, 8, 8, 8)\n        x = self.fractional_max_pool3d(x)  # Shape: (batch_size, 16, 8, 8, 8)\n        \n        # Apply SiLU activation\n        x = self.silu(x)  # Shape: (batch_size, 16, 8, 8, 8)\n        \n        # Flatten for loss computation\n        x = x.view(x.size(0), -1)  # Shape: (batch_size, 16*8*8*8)\n        \n        # Dummy target for loss computation (assuming binary classification)\n        target = torch.randint(0, 2, (x.size(0), x.size(1)), dtype=torch.float32).to(x.device)\n        \n        # Compute BCE loss\n        bce_loss = self.bce_loss(torch.sigmoid(x), target)\n        \n        # Compute MultiLabelSoftMarginLoss\n        multi_label_loss = self.multi_label_soft_margin_loss(x, target)\n        \n        # Return both losses for demonstration purposes\n        return bce_loss, multi_label_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_adaptive_max_pool2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(3 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(1 +ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(2 +ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(3 +ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(2 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr0 +(1 +2 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp19 =tl .load (in_ptr0 +(2 +2 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr0 +(3 +2 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp23 =tl .load (in_ptr0 +(3 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp25 =tl .load (in_ptr0 +(1 +3 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp27 =tl .load (in_ptr0 +(2 +3 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr0 +(3 +3 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\n    tmp8 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\n    tmp14 =triton_helpers .maximum (tmp13 ,tmp12 )\n    tmp16 =triton_helpers .maximum (tmp15 ,tmp14 )\n    tmp18 =triton_helpers .maximum (tmp17 ,tmp16 )\n    tmp20 =triton_helpers .maximum (tmp19 ,tmp18 )\n    tmp22 =triton_helpers .maximum (tmp21 ,tmp20 )\n    tmp24 =triton_helpers .maximum (tmp23 ,tmp22 )\n    tmp26 =triton_helpers .maximum (tmp25 ,tmp24 )\n    tmp28 =triton_helpers .maximum (tmp27 ,tmp26 )\n    tmp30 =triton_helpers .maximum (tmp29 ,tmp28 )\n    tl .store (out_ptr0 +(x3 ),tmp30 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //4 \n        s1 //4 \n        (s1 //4 )*(s2 //4 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //4 ,s2 //4 ),(s0 *(s1 //4 )*(s2 //4 ),(s1 //4 )*(s2 //4 ),s2 //4 ,1 ),torch .float32 )\n\n        triton_poi_fused_adaptive_max_pool2d_0_xnumel =s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_adaptive_max_pool2d_0 [grid (triton_poi_fused_adaptive_max_pool2d_0_xnumel )](arg3_1 ,buf0 ,16 ,16 ,256 ,64 ,64 ,768 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "395e2dcc-79c8-405a-902c-3551acda78d7",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['TransformerEncoder', 'ELU', 'LazyLinear', 'ZeroPad3d', 'Upsample', 'MaxPool2d', 'InstanceNorm2d', 'Unflatten']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad = nn.ZeroPad3d(1)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.instance_norm = nn.InstanceNorm2d(64)\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=64, nhead=8),\n            num_layers=3\n        )\n        self.elu = nn.ELU()\n        self.lazy_linear = nn.LazyLinear(128)\n        self.unflatten = nn.Unflatten(1, (16, 8))\n\n    def forward(self, x):\n        # ZeroPad3d\n        x = self.zero_pad(x)\n        \n        # MaxPool2d\n        x = self.max_pool(x)\n        \n        # InstanceNorm2d\n        x = self.instance_norm(x)\n        \n        # Upsample\n        x = self.upsample(x)\n        \n        # Reshape for TransformerEncoder\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1).permute(2, 0, 1)  # (seq_len, batch, feature)\n        \n        # TransformerEncoder\n        x = self.transformer_encoder(x)\n        \n        # Reshape back to original dimensions\n        x = x.permute(1, 2, 0).view(batch_size, channels, height, width)\n        \n        # ELU\n        x = self.elu(x)\n        \n        # Flatten for LazyLinear\n        x = x.view(batch_size, -1)\n        \n        # LazyLinear\n        x = self.lazy_linear(x)\n        \n        # Unflatten\n        x = self.unflatten(x)\n        \n        return x\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x5 =xindex //ks0 \n    x1 =((xindex //ks2 )%ks3 )\n    x0 =(xindex %ks2 )\n    x2 =xindex //ks6 \n    x7 =xindex \n    tmp0 =(-1 )+x5 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+2 *x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks4 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-1 )+2 *x0 \n    tmp10 =tmp9 >=tmp1 \n    tmp11 =ks5 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =tl .load (in_ptr0 +((-1 )+((-1 )*ks5 )+2 *x0 +((-1 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =2 *x0 \n    tmp20 =tmp19 >=tmp1 \n    tmp21 =tmp19 <tmp11 \n    tmp22 =tmp15 &tmp20 \n    tmp23 =tmp22 &tmp21 \n    tmp24 =tl .load (in_ptr0 +(((-1 )*ks5 )+2 *x0 +((-1 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp23 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp25 =triton_helpers .maximum (tmp24 ,tmp18 )\n    tmp26 =2 *x1 \n    tmp27 =tmp26 >=tmp1 \n    tmp28 =tmp26 <tmp7 \n    tmp29 =tmp13 &tmp27 \n    tmp30 =tmp29 &tmp28 \n    tmp31 =tmp30 &tmp10 \n    tmp32 =tmp31 &tmp12 \n    tmp33 =tl .load (in_ptr0 +((-1 )+2 *x0 +((-1 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp32 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =triton_helpers .maximum (tmp33 ,tmp25 )\n    tmp35 =tmp30 &tmp20 \n    tmp36 =tmp35 &tmp21 \n    tmp37 =tl .load (in_ptr0 +(2 *x0 +((-1 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp36 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp38 =triton_helpers .maximum (tmp37 ,tmp34 )\n    tl .store (out_ptr0 +(x7 ),tmp38 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        1 +(s1 //2 )*(s2 //2 )+(s1 //2 )+(s2 //2 )\n        1 +(s2 //2 )\n        1 +(s1 //2 )\n        1 +(s1 //2 )*(s2 //2 )+(s1 //2 )+(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,2 +s0 ,1 +(s1 //2 ),1 +(s2 //2 )),(2 +s0 +2 *(s1 //2 )+2 *(s2 //2 )+s0 *(s1 //2 )+s0 *(s2 //2 )+2 *(s1 //2 )*(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),1 +(s1 //2 )*(s2 //2 )+(s1 //2 )+(s2 //2 ),1 +(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_pool2d_with_indices_0_xnumel =2 +s0 +2 *(s1 //2 )+2 *(s2 //2 )+s0 *(s1 //2 )+s0 *(s2 //2 )+2 *(s1 //2 )*(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_0 [grid (triton_poi_fused_max_pool2d_with_indices_0_xnumel )](arg3_1 ,buf0 ,289 ,3 ,17 ,17 ,32 ,32 ,289 ,1445 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "3a042a8a-7e7b-4ba5-987e-b8ddacf686c5",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PairwiseDistance', 'UpsamplingBilinear2d', 'LazyLinear', 'AdaptiveMaxPool3d', 'PReLU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pairwise_distance = nn.PairwiseDistance()\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.lazy_linear1 = nn.LazyLinear(128)\n        self.lazy_linear2 = nn.LazyLinear(64)\n        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((8, 8, 8))\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        # Assuming x is a batch of 3D tensors, we first apply PairwiseDistance\n        # PairwiseDistance requires two inputs, so we split the input tensor into two halves\n        x1, x2 = torch.chunk(x, 2, dim=1)\n        x = self.pairwise_distance(x1, x2)\n        \n        # Reshape to 4D tensor for UpsamplingBilinear2d\n        x = x.unsqueeze(1).unsqueeze(1)\n        x = self.upsample(x)\n        \n        # Reshape to 2D tensor for LazyLinear\n        x = x.view(x.size(0), -1)\n        x = self.prelu(self.lazy_linear1(x))\n        x = self.prelu(self.lazy_linear2(x))\n        \n        # Reshape to 5D tensor for AdaptiveMaxPool3d\n        x = x.view(x.size(0), 1, 8, 8, 8)\n        x = self.adaptive_max_pool3d(x)\n        \n        # Final reshape to 2D tensor for output\n        x = x.view(x.size(0), -1)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_norm_sub_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp7 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +ks0 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +(r0_1 +ks0 *x0 +ks0 *ks2 *((1 +ks1 )//2 )),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp3 =1e-06 \n        tmp4 =tmp2 +tmp3 \n        tmp5 =tmp4 *tmp4 \n        tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n        tmp8 =_tmp7 +tmp6 \n        _tmp7 =tl .where (r0_mask &xmask ,tmp8 ,_tmp7 )\n    tmp7 =tl .sum (_tmp7 ,1 )[:,None ]\n    tmp9 =libdevice .sqrt (tmp7 )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp9 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,(1 +s0 )//2 ,s1 ),(s1 *((1 +s0 )//2 ),s1 ,1 ),torch .float32 )\n        buf1 =buf0 ;del buf0 \n\n        triton_red_fused_add_norm_sub_0_xnumel =s1 *((1 +s0 )//2 )\n        get_raw_stream (0 )\n        triton_red_fused_add_norm_sub_0 [grid (triton_red_fused_add_norm_sub_0_xnumel )](buf1 ,arg3_1 ,32 ,64 ,32 ,1024 ,32 ,XBLOCK =8 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf1 ,(1 ,1 ,1 ,(1 +s0 )//2 ,s1 ),(s1 *((1 +s0 )//2 ),s1 *((1 +s0 )//2 ),s1 *((1 +s0 )//2 ),s1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,64 ,32 ,32 ),(65536 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "3aa9a543-4055-4158-90c0-a7f995b94851",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool1d', 'UpsamplingNearest2d', 'AdaptiveAvgPool3d', 'Mish']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool1d = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.upsample_nearest2d = nn.UpsamplingNearest2d(scale_factor=2)\n        self.adaptive_avgpool3d = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.mish = nn.Mish()\n\n    def forward(self, x):\n        # Assuming input is 4D (batch, channels, height, width)\n        # Reshape to 3D for MaxPool1d\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size * channels, height, width)  # Reshape to (batch*channels, height, width)\n        x = self.maxpool1d(x)  # Apply MaxPool1d\n        x = x.view(batch_size, channels, x.shape[1], x.shape[2])  # Reshape back to 4D\n\n        # Apply Mish activation\n        x = self.mish(x)\n\n        # Apply UpsamplingNearest2d\n        x = self.upsample_nearest2d(x)\n\n        # Reshape to 5D for AdaptiveAvgPool3d\n        x = x.unsqueeze(2)  # Add a depth dimension\n        x = self.adaptive_avgpool3d(x)  # Apply AdaptiveAvgPool3d\n\n        # Reshape back to 4D\n        x = x.squeeze(2)\n\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input: (batch_size, channels, height, width)\n    return [x]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__unsafe_index_mean_mish_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp29 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index //64 \n        r0_1 =(r0_index %64 )\n        tmp0 =tl .full ([1 ,1 ],2.0 ,tl .float64 )\n        tmp1 =ks0 \n        tmp2 =tmp1 .to (tl .float64 )\n        tmp3 =tmp0 *tmp2 \n        tmp4 =tmp2 /tmp3 \n        tmp5 =tmp4 .to (tl .float32 )\n        tmp6 =r0_2 \n        tmp7 =tmp6 .to (tl .float32 )\n        tmp8 =tmp7 *tmp5 \n        tmp9 =tmp8 .to (tl .int64 )\n        tmp10 =tmp9 +tmp1 \n        tmp11 =tmp9 <0 \n        tmp12 =tl .where (tmp11 ,tmp10 ,tmp9 )\n        tmp13 =r0_1 \n        tmp14 =tmp13 .to (tl .float32 )\n        tmp15 =0.5 \n        tmp16 =tmp14 *tmp15 \n        tmp17 =tmp16 .to (tl .int64 )\n        tmp18 =tl .load (in_ptr0 +(2 *tmp17 +64 *tmp12 +64 *ks0 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp19 =tl .load (in_ptr0 +(1 +2 *tmp17 +64 *tmp12 +64 *ks0 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp20 =triton_helpers .maximum (tmp19 ,tmp18 )\n        tmp21 =20.0 \n        tmp22 =tmp20 >tmp21 \n        tmp23 =tl_math .exp (tmp20 )\n        tmp24 =libdevice .log1p (tmp23 )\n        tmp25 =tl .where (tmp22 ,tmp20 ,tmp24 )\n        tmp26 =libdevice .tanh (tmp25 )\n        tmp27 =tmp20 *tmp26 \n        tmp28 =tl .broadcast_to (tmp27 ,[XBLOCK ,R0_BLOCK ])\n        tmp30 =_tmp29 +tmp28 \n        _tmp29 =tl .where (r0_mask &xmask ,tmp30 ,_tmp29 )\n    tmp29 =tl .sum (_tmp29 ,1 )[:,None ]\n    tmp31 =128 *ks0 \n    tmp32 =tmp31 .to (tl .float32 )\n    tmp33 =tmp29 /tmp32 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp33 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ,64 ),(64 *s0 *s1 ,64 *s1 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ,s0 ),torch .float32 )\n        buf2 =buf1 ;del buf1 \n\n        128 *s1 \n        get_raw_stream (0 )\n        triton_red_fused__unsafe_index_mean_mish_0 [grid (s0 )](buf2 ,arg2_1 ,64 ,3 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg2_1 \n    return (reinterpret_tensor (buf2 ,(1 ,s0 ,1 ,1 ),(s0 ,1 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "3c834972-9e84-4fd7-88c0-72a65b7f81d3",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['InstanceNorm1d', 'GRU', 'MultiLabelMarginLoss', 'LPPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.instance_norm = nn.InstanceNorm1d(128)\n        self.gru1 = nn.GRU(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n        self.gru2 = nn.GRU(input_size=64, hidden_size=32, num_layers=2, batch_first=True)\n        self.lp_pool = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)\n        self.multi_label_margin_loss = nn.MultiLabelMarginLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        \n        # Reshape and normalize using InstanceNorm1d\n        x = x.view(batch_size, channels, -1)  # Flatten height and width\n        x = self.instance_norm(x)\n        \n        # Reshape for GRU\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, seq_len, input_size)\n        \n        # Pass through first GRU\n        x, _ = self.gru1(x)\n        \n        # Pass through second GRU\n        x, _ = self.gru2(x)\n        \n        # Reshape for LPPool2d\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, channels, seq_len)\n        x = x.view(batch_size, -1, height // 2, width // 2)  # Reshape to 2D\n        \n        # Apply LPPool2d\n        x = self.lp_pool(x)\n        \n        # Flatten for loss calculation\n        x = x.view(batch_size, -1)\n        \n        # Dummy target for MultiLabelMarginLoss (assuming binary classification)\n        target = torch.randint(0, 2, (batch_size, x.size(1))).float()\n        \n        # Calculate loss\n        loss = self.multi_label_margin_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 128, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr2 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp8 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp9 =tmp8 -tmp2 \n        tmp10 =ks0 *ks1 \n        tmp11 =tmp10 .to (tl .float32 )\n        tmp12 =tmp3 /tmp11 \n        tmp13 =1e-05 \n        tmp14 =tmp12 +tmp13 \n        tmp15 =libdevice .rsqrt (tmp14 )\n        tmp16 =tmp9 *tmp15 \n        tl .store (out_ptr2 +(r0_1 +ks0 *ks1 *x0 ),tmp16 ,r0_mask &xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    s2 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,128 ,s1 ,s2 ),(128 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf3 =empty_strided_cuda ((1 ,128 ,s1 *s2 ),(128 *s1 *s2 ,s1 *s2 ,1 ),torch .float32 )\n\n        s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_0 [grid (128 )](arg2_1 ,buf3 ,64 ,64 ,128 ,4096 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg2_1 \n    return (reinterpret_tensor (buf3 ,(1 ,s1 *s2 ,128 ),(128 *s1 *s2 ,1 ,s1 *s2 ),0 ),s1 ,s2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,128 ,64 ,64 ),(524288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "3cb83beb-3156-4696-a437-0c87da3c2ee1",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReplicationPad2d', 'HingeEmbeddingLoss', 'FeatureAlphaDropout', 'SmoothL1Loss', 'InstanceNorm3d', 'UpsamplingNearest2d', 'TransformerEncoder', 'SyncBatchNorm', 'AvgPool3d', 'Hardshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.replication_pad = nn.ReplicationPad2d(2)\n        self.instance_norm = nn.InstanceNorm3d(10)\n        self.upsampling = nn.UpsamplingNearest2d(scale_factor=2)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=64, nhead=8), num_layers=3\n        )\n        self.sync_batch_norm = nn.SyncBatchNorm(64)\n        self.avg_pool = nn.AvgPool3d(kernel_size=2)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.hardshrink = nn.Hardshrink(lambd=0.5)\n        self.hinge_embedding_loss = nn.HingeEmbeddingLoss()\n        self.smooth_l1_loss = nn.SmoothL1Loss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        x = self.replication_pad(x)  # Apply ReplicationPad2d\n        x = x.unsqueeze(2)  # Add a dimension to make it 5D for InstanceNorm3d\n        x = self.instance_norm(x)  # Apply InstanceNorm3d\n        x = x.squeeze(2)  # Remove the added dimension\n        x = self.upsampling(x)  # Apply UpsamplingNearest2d\n        x = x.view(x.size(0), -1, 64)  # Reshape for TransformerEncoder\n        x = self.transformer_encoder(x)  # Apply TransformerEncoder\n        x = x.view(x.size(0), 64, -1)  # Reshape back\n        x = self.sync_batch_norm(x)  # Apply SyncBatchNorm\n        x = x.unsqueeze(2).unsqueeze(3)  # Add dimensions for AvgPool3d\n        x = self.avg_pool(x)  # Apply AvgPool3d\n        x = x.squeeze(3).squeeze(2)  # Remove added dimensions\n        x = self.feature_alpha_dropout(x)  # Apply FeatureAlphaDropout\n        x = self.hardshrink(x)  # Apply Hardshrink\n        \n        # For loss functions, we need targets and possibly other inputs\n        # Here we just return the processed tensor, but in practice, you would compute the loss\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_replication_pad2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks4 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 )))))+(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))*((((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *x2 +(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks4 )))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x3 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_replication_pad2d_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_replication_pad2d_0 [grid (triton_poi_fused_replication_pad2d_0_xnumel )](arg3_1 ,buf0 ,68 ,68 ,4624 ,64 ,64 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,s0 ,1 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "3cc5acce-6b0b-4fa6-90bd-36cbc11cdbe9",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GaussianNLLLoss', 'NLLLoss2d', 'AdaptiveMaxPool2d', 'RNN', 'LazyBatchNorm1d', 'MaxPool1d', 'CircularPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool = nn.AdaptiveMaxPool2d((16, 16))\n        self.rnn = nn.RNN(input_size=256, hidden_size=128, num_layers=2, batch_first=True)\n        self.lazy_batch_norm = nn.LazyBatchNorm1d()\n        self.max_pool = nn.MaxPool1d(kernel_size=2)\n        self.circular_pad = nn.CircularPad1d(padding=1)\n        self.gaussian_nll_loss = nn.GaussianNLLLoss()\n        self.nll_loss_2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Assume input x is of shape (batch_size, channels, height, width)\n        x = self.adaptive_max_pool(x)  # Shape: (batch_size, channels, 16, 16)\n        x = x.view(x.size(0), x.size(1), -1)  # Shape: (batch_size, channels, 256)\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, 256, channels)\n        x, _ = self.rnn(x)  # Shape: (batch_size, 256, 128)\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, 128, 256)\n        x = self.lazy_batch_norm(x)  # Shape: (batch_size, 128, 256)\n        x = self.max_pool(x)  # Shape: (batch_size, 128, 128)\n        x = self.circular_pad(x)  # Shape: (batch_size, 128, 130)\n        \n        # Dummy target for loss functions (not used in actual forward pass)\n        target = torch.randint(0, 10, (x.size(0), 128, 130))\n        var = torch.ones_like(x)\n        \n        # Apply loss functions (not typical in forward pass, but included as per the module list)\n        gaussian_loss = self.gaussian_nll_loss(x, target, var)\n        nll_loss = self.nll_loss_2d(x, target)\n        \n        # Return the losses as part of the output (not typical, but included to use all modules)\n        return x, gaussian_loss, nll_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape: (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_adaptive_max_pool2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(3 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(1 +ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(2 +ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(3 +ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(2 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr0 +(1 +2 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp19 =tl .load (in_ptr0 +(2 +2 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr0 +(3 +2 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp23 =tl .load (in_ptr0 +(3 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp25 =tl .load (in_ptr0 +(1 +3 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp27 =tl .load (in_ptr0 +(2 +3 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr0 +(3 +3 *ks4 +4 *x0 +4 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\n    tmp8 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\n    tmp14 =triton_helpers .maximum (tmp13 ,tmp12 )\n    tmp16 =triton_helpers .maximum (tmp15 ,tmp14 )\n    tmp18 =triton_helpers .maximum (tmp17 ,tmp16 )\n    tmp20 =triton_helpers .maximum (tmp19 ,tmp18 )\n    tmp22 =triton_helpers .maximum (tmp21 ,tmp20 )\n    tmp24 =triton_helpers .maximum (tmp23 ,tmp22 )\n    tmp26 =triton_helpers .maximum (tmp25 ,tmp24 )\n    tmp28 =triton_helpers .maximum (tmp27 ,tmp26 )\n    tmp30 =triton_helpers .maximum (tmp29 ,tmp28 )\n    tl .store (out_ptr0 +(x3 ),tmp30 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //4 \n        s1 //4 \n        (s1 //4 )*(s2 //4 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //4 ,s2 //4 ),(s0 *(s1 //4 )*(s2 //4 ),(s1 //4 )*(s2 //4 ),s2 //4 ,1 ),torch .float32 )\n\n        triton_poi_fused_adaptive_max_pool2d_0_xnumel =s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_adaptive_max_pool2d_0 [grid (triton_poi_fused_adaptive_max_pool2d_0_xnumel )](arg3_1 ,buf0 ,16 ,16 ,256 ,64 ,64 ,768 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,(s1 //4 )*(s2 //4 ),s0 ),(s0 *(s1 //4 )*(s2 //4 ),1 ,(s1 //4 )*(s2 //4 )),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "3db34eda-b669-4119-b82a-cd74ba76a583",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Dropout1d', 'AdaptiveMaxPool1d', 'Identity', 'ModuleDict', 'FeatureAlphaDropout', 'MaxPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout1d = nn.Dropout1d(p=0.5)\n        self.adaptive_max_pool1d = nn.AdaptiveMaxPool1d(output_size=10)\n        self.identity = nn.Identity()\n        self.module_dict = nn.ModuleDict({\n            'feature_alpha_dropout': nn.FeatureAlphaDropout(p=0.5),\n            'max_pool1d': nn.MaxPool1d(kernel_size=2, stride=2)\n        })\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, sequence_length)\n        x = self.dropout1d(x)\n        x = self.adaptive_max_pool1d(x)\n        x = self.identity(x)\n        x = self.module_dict['feature_alpha_dropout'](x)\n        x = self.module_dict['max_pool1d'](x)\n        x = self.feature_alpha_dropout(x)\n        x = self.max_pool1d(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 32, 100).cuda()  # Example input shape (batch_size=1, channels=32, sequence_length=100)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =0.5 \n    tmp3 =tmp1 <tmp2 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =2.0 \n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp0 *tmp6 \n    tl .store (out_ptr0 +(x2 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_adaptive_max_pool2d_add_bernoulli_mul_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(3 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(4 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(5 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(6 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(7 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(8 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr0 +(9 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp19 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\n    tmp8 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\n    tmp14 =triton_helpers .maximum (tmp13 ,tmp12 )\n    tmp16 =triton_helpers .maximum (tmp15 ,tmp14 )\n    tmp18 =triton_helpers .maximum (tmp17 ,tmp16 )\n    tmp20 =0.5 \n    tmp21 =tmp19 <tmp20 \n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =0.8864048946659319 \n    tmp24 =tmp22 *tmp23 \n    tmp25 =tmp18 *tmp24 \n    tmp26 =-1.0 \n    tmp27 =tmp22 +tmp26 \n    tmp28 =1.558387861036063 \n    tmp29 =tmp27 *tmp28 \n    tmp30 =0.7791939305180315 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =tmp25 +tmp31 \n    tl .store (in_out_ptr0 +(x2 ),tmp32 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_mul_4 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =0.5 \n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =0.8864048946659319 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =tmp2 *tmp8 \n    tmp10 =-1.0 \n    tmp11 =tmp6 +tmp10 \n    tmp12 =1.558387861036063 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.7791939305180315 \n    tmp15 =tmp13 +tmp14 \n    tmp16 =tmp9 +tmp15 \n    tl .store (out_ptr0 +(x2 ),tmp16 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_5 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tl .store (out_ptr0 +(x2 ),tmp2 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((3 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[3 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ),(s0 ,1 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,2 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_bernoulli_div_mul_1_xnumel =s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_1 [grid (triton_poi_fused__to_copy_bernoulli_div_mul_1_xnumel )](arg2_1 ,buf1 ,buf2 ,100 ,3200 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        buf4 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_2 [grid (s0 )](buf0 ,buf4 ,1 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        s1 //10 \n        buf3 =empty_strided_cuda ((1 ,s0 ,1 ,s1 //10 ),(s0 *(s1 //10 ),s1 //10 ,s1 //10 ,1 ),torch .float32 )\n        buf5 =reinterpret_tensor (buf3 ,(1 ,s0 ,s1 //10 ),(s0 *(s1 //10 ),s1 //10 ,1 ),0 );del buf3 \n\n        triton_poi_fused__to_copy_adaptive_max_pool2d_add_bernoulli_mul_3_xnumel =s0 *(s1 //10 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_adaptive_max_pool2d_add_bernoulli_mul_3 [grid (triton_poi_fused__to_copy_adaptive_max_pool2d_add_bernoulli_mul_3_xnumel )](buf5 ,buf2 ,buf4 ,10 ,100 ,320 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        buf6 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf6 ,2 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        s1 //20 \n        buf7 =empty_strided_cuda ((1 ,s0 ,s1 //20 ),(s0 *(s1 //20 ),s1 //20 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_add_bernoulli_mul_4_xnumel =s0 *(s1 //20 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_mul_4 [grid (triton_poi_fused__to_copy_add_bernoulli_mul_4_xnumel )](buf5 ,buf6 ,buf7 ,5 ,10 ,160 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf5 \n        del buf6 \n        s1 //40 \n        buf8 =empty_strided_cuda ((1 ,s0 ,1 ,s1 //40 ),(s0 *(s1 //40 ),s1 //40 ,s1 //40 ,1 ),torch .float32 )\n\n        triton_poi_fused_max_pool2d_with_indices_5_xnumel =s0 *(s1 //40 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_5 [grid (triton_poi_fused_max_pool2d_with_indices_5_xnumel )](buf7 ,buf8 ,2 ,5 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del buf7 \n    return (reinterpret_tensor (buf8 ,(1 ,s0 ,s1 //40 ),(s0 *(s1 //40 ),s1 //40 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =32 \n    arg1_1 =100 \n    arg2_1 =rand_strided ((1 ,32 ,100 ),(3200 ,100 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "4023bfed-b2f2-4dd5-ab6b-5c81fd8e9c6b",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReLU', 'Flatten', 'FractionalMaxPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.fractional_max_pool = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.flatten = nn.Flatten()\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # Apply FractionalMaxPool2d\n        x = self.fractional_max_pool(x)\n        \n        # Apply ReLU\n        x = self.relu(x)\n        \n        # Flatten the tensor\n        x = self.flatten(x)\n        \n        # Apply ReLU again\n        x = self.relu(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 28, 28).cuda()  # Example input shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_fractional_max_pool2d_relu_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex //196 \n    x1 =((xindex //14 )%14 )\n    x0 =(xindex %14 )\n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr0 +(1 +2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =((-2 )+ks0 )/13 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =x1 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =tmp4 +tmp0 \n    tmp6 =tmp5 *tmp2 \n    tmp7 =libdevice .floor (tmp6 )\n    tmp8 =tmp0 *tmp2 \n    tmp9 =libdevice .floor (tmp8 )\n    tmp10 =tmp7 -tmp9 \n    tmp11 =tmp10 .to (tl .int64 )\n    tmp12 =tl .full ([1 ],13 ,tl .int64 )\n    tmp13 =tmp4 <tmp12 \n    tmp14 =(-2 )+ks0 \n    tmp15 =tl .where (tmp13 ,tmp11 ,tmp14 )\n    tmp16 =ks0 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =tmp15 <0 \n    tmp19 =tl .where (tmp18 ,tmp17 ,tmp15 )\n    tl .device_assert (((0 <=tmp19 )&(tmp19 <ks0 ))|~(xmask ),\"index out of bounds: 0 <= tmp19 < ks0\")\n    tmp22 =((-2 )+ks1 )/13 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =x0 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 +tmp21 \n    tmp27 =tmp26 *tmp23 \n    tmp28 =libdevice .floor (tmp27 )\n    tmp29 =tmp21 *tmp23 \n    tmp30 =libdevice .floor (tmp29 )\n    tmp31 =tmp28 -tmp30 \n    tmp32 =tmp31 .to (tl .int64 )\n    tmp33 =tmp25 <tmp12 \n    tmp34 =(-2 )+ks1 \n    tmp35 =tl .where (tmp33 ,tmp32 ,tmp34 )\n    tmp36 =ks1 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =tmp35 <0 \n    tmp39 =tl .where (tmp38 ,tmp37 ,tmp35 )\n    tl .device_assert (((0 <=tmp39 )&(tmp39 <ks1 ))|~(xmask ),\"index out of bounds: 0 <= tmp39 < ks1\")\n    tmp41 =tl .load (in_ptr1 +(tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp42 =tl .load (in_ptr1 +(1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =triton_helpers .maximum (tmp42 ,tmp41 )\n    tmp44 =tl .load (in_ptr1 +(ks1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp45 =triton_helpers .maximum (tmp44 ,tmp43 )\n    tmp46 =tl .load (in_ptr1 +(1 +ks1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp47 =triton_helpers .maximum (tmp46 ,tmp45 )\n    tmp48 =tl .full ([1 ],0 ,tl .int32 )\n    tmp49 =triton_helpers .maximum (tmp48 ,tmp47 )\n    tmp50 =triton_helpers .maximum (tmp48 ,tmp49 )\n    tl .store (in_out_ptr0 +(x3 ),tmp50 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,2 ),(2 *s0 ,2 ,1 ),torch .float32 )\n\n        triton_poi_fused_rand_0_xnumel =2 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (triton_poi_fused_rand_0_xnumel )](buf0 ,buf1 ,0 ,6 ,XBLOCK =8 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,s0 ,14 ,14 ),(196 *s0 ,196 ,14 ,1 ),torch .float32 )\n        buf3 =reinterpret_tensor (buf2 ,(1 ,196 *s0 ),(196 *s0 ,1 ),0 );del buf2 \n\n        triton_poi_fused_fractional_max_pool2d_relu_1_xnumel =196 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_fractional_max_pool2d_relu_1 [grid (triton_poi_fused_fractional_max_pool2d_relu_1_xnumel )](buf3 ,buf1 ,arg3_1 ,28 ,28 ,588 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =28 \n    arg2_1 =28 \n    arg3_1 =rand_strided ((1 ,3 ,28 ,28 ),(2352 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "405cba21-3410-4d68-9279-469f7f672cbf",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ModuleList', 'ParameterList', 'Softshrink', 'Container', 'CELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        # Using ModuleList to store multiple layers\n        self.module_list = nn.ModuleList([\n            nn.Softshrink(lambd=0.5),\n            nn.CELU(alpha=1.0),\n            nn.Softshrink(lambd=0.5),\n            nn.CELU(alpha=1.0),\n            nn.Softshrink(lambd=0.5)\n        ])\n        \n        # Using ParameterList to store parameters\n        self.parameter_list = nn.ParameterList([\n            nn.Parameter(torch.randn(10)),\n            nn.Parameter(torch.randn(10)),\n            nn.Parameter(torch.randn(10))\n        ])\n        \n        # Using Container to group layers\n        self.container = nn.Sequential(\n            nn.Softshrink(lambd=0.5),\n            nn.CELU(alpha=1.0)\n        )\n\n    def forward(self, x):\n        # Apply ModuleList layers\n        for layer in self.module_list:\n            x = layer(x)\n        \n        # Apply ParameterList parameters\n        for param in self.parameter_list:\n            x = x + param.view(1, -1)\n        \n        # Apply Container layers\n        x = self.container(x)\n        \n        # Reshape to ensure output is a single vector\n        x = x.view(x.size(0), -1)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_celu_gt_mul_sign_sub_view_where_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp46 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp48 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp50 =tl .load (in_ptr3 +(x0 ),xmask )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =0.5 \n    tmp3 =tmp1 >tmp2 \n    tmp4 =tl .full ([1 ],0 ,tl .int32 )\n    tmp5 =tmp4 <tmp0 \n    tmp6 =tmp5 .to (tl .int8 )\n    tmp7 =tmp0 <tmp4 \n    tmp8 =tmp7 .to (tl .int8 )\n    tmp9 =tmp6 -tmp8 \n    tmp10 =tmp9 .to (tmp0 .dtype )\n    tmp11 =tmp10 *tmp2 \n    tmp12 =tmp0 -tmp11 \n    tmp13 =0.0 \n    tmp14 =tmp0 *tmp13 \n    tmp15 =tl .where (tmp3 ,tmp12 ,tmp14 )\n    tmp16 =tmp15 >tmp13 \n    tmp17 =libdevice .expm1 (tmp15 )\n    tmp18 =tl .where (tmp16 ,tmp15 ,tmp17 )\n    tmp19 =tl_math .abs (tmp18 )\n    tmp20 =tmp19 >tmp2 \n    tmp21 =tmp4 <tmp18 \n    tmp22 =tmp21 .to (tl .int8 )\n    tmp23 =tmp18 <tmp4 \n    tmp24 =tmp23 .to (tl .int8 )\n    tmp25 =tmp22 -tmp24 \n    tmp26 =tmp25 .to (tmp18 .dtype )\n    tmp27 =tmp26 *tmp2 \n    tmp28 =tmp18 -tmp27 \n    tmp29 =tmp18 *tmp13 \n    tmp30 =tl .where (tmp20 ,tmp28 ,tmp29 )\n    tmp31 =tmp30 >tmp13 \n    tmp32 =libdevice .expm1 (tmp30 )\n    tmp33 =tl .where (tmp31 ,tmp30 ,tmp32 )\n    tmp34 =tl_math .abs (tmp33 )\n    tmp35 =tmp34 >tmp2 \n    tmp36 =tmp4 <tmp33 \n    tmp37 =tmp36 .to (tl .int8 )\n    tmp38 =tmp33 <tmp4 \n    tmp39 =tmp38 .to (tl .int8 )\n    tmp40 =tmp37 -tmp39 \n    tmp41 =tmp40 .to (tmp33 .dtype )\n    tmp42 =tmp41 *tmp2 \n    tmp43 =tmp33 -tmp42 \n    tmp44 =tmp33 *tmp13 \n    tmp45 =tl .where (tmp35 ,tmp43 ,tmp44 )\n    tmp47 =tmp45 +tmp46 \n    tmp49 =tmp47 +tmp48 \n    tmp51 =tmp49 +tmp50 \n    tmp52 =tl_math .abs (tmp51 )\n    tmp53 =tmp52 >tmp2 \n    tmp54 =tmp4 <tmp51 \n    tmp55 =tmp54 .to (tl .int8 )\n    tmp56 =tmp51 <tmp4 \n    tmp57 =tmp56 .to (tl .int8 )\n    tmp58 =tmp55 -tmp57 \n    tmp59 =tmp58 .to (tmp51 .dtype )\n    tmp60 =tmp59 *tmp2 \n    tmp61 =tmp51 -tmp60 \n    tmp62 =tmp51 *tmp13 \n    tmp63 =tl .where (tmp53 ,tmp61 ,tmp62 )\n    tmp64 =tmp63 >tmp13 \n    tmp65 =libdevice .expm1 (tmp63 )\n    tmp66 =tl .where (tmp64 ,tmp63 ,tmp65 )\n    tl .store (in_out_ptr0 +(x0 ),tmp51 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp66 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ),(10 ,1 ))\n    assert_size_stride (primals_2 ,(10 ,),(1 ,))\n    assert_size_stride (primals_3 ,(10 ,),(1 ,))\n    assert_size_stride (primals_4 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n        buf1 =buf0 ;del buf0 \n        buf2 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_celu_gt_mul_sign_sub_view_where_0 [grid (10 )](buf1 ,primals_1 ,primals_2 ,primals_3 ,primals_4 ,buf2 ,10 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del primals_1 \n        del primals_2 \n        del primals_3 \n        del primals_4 \n    return (buf2 ,buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "411dcd23-4a0e-40fa-b6bb-31f908562699",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Sigmoid', 'ReplicationPad2d', 'Upsample', 'BCELoss', 'AdaptiveAvgPool2d', 'ConstantPad2d', 'Identity', 'TripletMarginLoss', 'CELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.replication_pad = nn.ReplicationPad2d(2)\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((5, 5))\n        self.constant_pad = nn.ConstantPad2d(1, 0.5)\n        self.identity = nn.Identity()\n        self.celu = nn.CELU()\n        self.sigmoid = nn.Sigmoid()\n        self.bce_loss = nn.BCELoss()\n        self.triplet_margin_loss = nn.TripletMarginLoss()\n\n    def forward(self, x):\n        # Apply ReplicationPad2d\n        x = self.replication_pad(x)\n        \n        # Apply Upsample\n        x = self.upsample(x)\n        \n        # Apply AdaptiveAvgPool2d\n        x = self.adaptive_avg_pool(x)\n        \n        # Apply ConstantPad2d\n        x = self.constant_pad(x)\n        \n        # Apply Identity\n        x = self.identity(x)\n        \n        # Apply CELU\n        x = self.celu(x)\n        \n        # Apply Sigmoid\n        x = self.sigmoid(x)\n        \n        # Calculate BCELoss (requires a target, so we create a dummy target)\n        target = torch.ones_like(x)\n        bce_loss = self.bce_loss(x, target)\n        \n        # Calculate TripletMarginLoss (requires anchor, positive, and negative, so we create dummy tensors)\n        anchor = x\n        positive = torch.ones_like(x)\n        negative = torch.zeros_like(x)\n        triplet_loss = self.triplet_margin_loss(anchor, positive, negative)\n        \n        # Return the final output and the losses\n        return x, bce_loss, triplet_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_replication_pad2d_sub_view_0 (in_out_ptr1 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //72 )%72 )\n    x0 =(xindex %72 )\n    x2 =xindex //5184 \n    x4 =xindex \n    tmp0 =4.0 \n    tmp1 =32.0 \n    tmp2 =tmp0 +tmp1 \n    tmp3 =tmp2 .to (tl .float64 )\n    tmp4 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp5 =tmp4 +tmp3 \n    tmp6 =2.0 \n    tmp7 =tmp6 *tmp1 \n    tmp8 =8.0 \n    tmp9 =tmp8 +tmp7 \n    tmp10 =tmp9 .to (tl .float64 )\n    tmp11 =tmp4 +tmp10 \n    tmp12 =tmp5 /tmp11 \n    tmp13 =tmp12 .to (tl .float32 )\n    tmp14 =x1 \n    tmp15 =tmp14 .to (tl .float32 )\n    tmp16 =tmp15 *tmp13 \n    tmp17 =0.0 \n    tmp18 =triton_helpers .maximum (tmp16 ,tmp17 )\n    tmp19 =tmp18 .to (tl .int32 )\n    tmp20 =tl .full ([1 ],1 ,tl .int64 )\n    tmp21 =tmp19 +tmp20 \n    tmp22 =tl .full ([1 ],35 ,tl .int64 )\n    tmp23 =triton_helpers .minimum (tmp21 ,tmp22 )\n    tmp24 =x0 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 *tmp13 \n    tmp27 =triton_helpers .maximum (tmp26 ,tmp17 )\n    tmp28 =tmp27 .to (tl .int32 )\n    tmp29 =tl .load (in_ptr0 +(32 *((31 )*((31 )<=(((0 )*((0 )>=((-2 )+tmp23 ))+((-2 )+tmp23 )*(((-2 )+tmp23 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp23 ))+((-2 )+tmp23 )*(((-2 )+tmp23 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp23 ))+((-2 )+tmp23 )*(((-2 )+tmp23 )>(0 ))))<(31 )))+1024 *x2 +((31 )*((31 )<=(((0 )*((0 )>=((-2 )+tmp28 ))+((-2 )+tmp28 )*(((-2 )+tmp28 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp28 ))+((-2 )+tmp28 )*(((-2 )+tmp28 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp28 ))+((-2 )+tmp28 )*(((-2 )+tmp28 )>(0 ))))<(31 )))),xmask ,eviction_policy ='evict_last')\n    tmp30 =tmp28 +tmp20 \n    tmp31 =triton_helpers .minimum (tmp30 ,tmp22 )\n    tmp32 =tl .load (in_ptr0 +(32 *((31 )*((31 )<=(((0 )*((0 )>=((-2 )+tmp23 ))+((-2 )+tmp23 )*(((-2 )+tmp23 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp23 ))+((-2 )+tmp23 )*(((-2 )+tmp23 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp23 ))+((-2 )+tmp23 )*(((-2 )+tmp23 )>(0 ))))<(31 )))+1024 *x2 +((31 )*((31 )<=(((0 )*((0 )>=((-2 )+tmp31 ))+((-2 )+tmp31 )*(((-2 )+tmp31 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp31 ))+((-2 )+tmp31 )*(((-2 )+tmp31 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp31 ))+((-2 )+tmp31 )*(((-2 )+tmp31 )>(0 ))))<(31 )))),xmask ,eviction_policy ='evict_last')\n    tmp33 =tmp32 -tmp29 \n    tmp34 =tmp28 .to (tl .float32 )\n    tmp35 =tmp27 -tmp34 \n    tmp36 =triton_helpers .maximum (tmp35 ,tmp17 )\n    tmp37 =1.0 \n    tmp38 =triton_helpers .minimum (tmp36 ,tmp37 )\n    tmp39 =tmp33 *tmp38 \n    tmp40 =tmp29 +tmp39 \n    tmp41 =tl .load (in_ptr0 +(32 *((31 )*((31 )<=(((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 ))))<(31 )))+1024 *x2 +((31 )*((31 )<=(((0 )*((0 )>=((-2 )+tmp28 ))+((-2 )+tmp28 )*(((-2 )+tmp28 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp28 ))+((-2 )+tmp28 )*(((-2 )+tmp28 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp28 ))+((-2 )+tmp28 )*(((-2 )+tmp28 )>(0 ))))<(31 )))),xmask ,eviction_policy ='evict_last')\n    tmp42 =tl .load (in_ptr0 +(32 *((31 )*((31 )<=(((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 ))))<(31 )))+1024 *x2 +((31 )*((31 )<=(((0 )*((0 )>=((-2 )+tmp31 ))+((-2 )+tmp31 )*(((-2 )+tmp31 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp31 ))+((-2 )+tmp31 )*(((-2 )+tmp31 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp31 ))+((-2 )+tmp31 )*(((-2 )+tmp31 )>(0 ))))<(31 )))),xmask ,eviction_policy ='evict_last')\n    tmp43 =tmp42 -tmp41 \n    tmp44 =tmp43 *tmp38 \n    tmp45 =tmp41 +tmp44 \n    tmp46 =tmp40 -tmp45 \n    tmp47 =tmp19 .to (tl .float32 )\n    tmp48 =tmp18 -tmp47 \n    tmp49 =triton_helpers .maximum (tmp48 ,tmp17 )\n    tmp50 =triton_helpers .minimum (tmp49 ,tmp37 )\n    tmp51 =tmp46 *tmp50 \n    tmp52 =tmp45 +tmp51 \n    tl .store (in_out_ptr1 +(x4 ),tmp52 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_binary_cross_entropy_celu_constant_pad_nd_sub_1 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =((r0_index //7 )%7 )\n    r0_0 =(r0_index %7 )\n    r0_2 =r0_index //49 \n    r0_3 =r0_index \n    tmp0 =(-1 )+r0_1 \n    tmp1 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ,1 ],5 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+r0_0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =tmp2 &tmp4 \n    tmp9 =tmp8 &tmp6 \n    tmp10 =tmp9 &tmp7 \n    tmp11 =tl .load (in_ptr0 +(tl .broadcast_to ((-6 )+r0_0 +5 *r0_1 +25 *r0_2 ,[XBLOCK ,R0_BLOCK ])),r0_mask &tmp10 ,other =0.5 )\n    tmp12 =0.0 \n    tmp13 =tmp11 >tmp12 \n    tmp14 =libdevice .expm1 (tmp11 )\n    tmp15 =tl .where (tmp13 ,tmp11 ,tmp14 )\n    tmp16 =tl .sigmoid (tmp15 )\n    tmp17 =-tmp16 \n    tmp18 =libdevice .log1p (tmp17 )\n    tmp19 =-100.0 \n    tmp20 =triton_helpers .maximum (tmp18 ,tmp19 )\n    tmp21 =tmp12 *tmp20 \n    tmp22 =tl_math .log (tmp16 )\n    tmp23 =triton_helpers .maximum (tmp22 ,tmp19 )\n    tmp24 =tmp21 -tmp23 \n    tmp25 =tl .broadcast_to (tmp24 ,[XBLOCK ,R0_BLOCK ])\n    tmp27 =tl .where (r0_mask ,tmp25 ,0 )\n    tmp28 =tl .sum (tmp27 ,1 )[:,None ]\n    tmp29 =49 *ks0 \n    tmp30 =tmp29 .to (tl .float32 )\n    tmp31 =tmp28 /tmp30 \n    tl .store (out_ptr0 +(tl .broadcast_to (r0_3 ,[XBLOCK ,R0_BLOCK ])),tmp16 ,r0_mask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp31 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_mean_norm_ones_like_sub_2 (in_out_ptr1 ,in_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp63 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(7 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp6 =tl .load (in_ptr0 +(1 +7 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp11 =tl .load (in_ptr0 +(2 +7 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp16 =tl .load (in_ptr0 +(3 +7 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp21 =tl .load (in_ptr0 +(4 +7 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp26 =tl .load (in_ptr0 +(5 +7 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp31 =tl .load (in_ptr0 +(6 +7 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =1.0 \n        tmp2 =tmp0 -tmp1 \n        tmp3 =1e-06 \n        tmp4 =tmp2 +tmp3 \n        tmp5 =tmp4 *tmp4 \n        tmp7 =tmp6 -tmp1 \n        tmp8 =tmp7 +tmp3 \n        tmp9 =tmp8 *tmp8 \n        tmp10 =tmp5 +tmp9 \n        tmp12 =tmp11 -tmp1 \n        tmp13 =tmp12 +tmp3 \n        tmp14 =tmp13 *tmp13 \n        tmp15 =tmp10 +tmp14 \n        tmp17 =tmp16 -tmp1 \n        tmp18 =tmp17 +tmp3 \n        tmp19 =tmp18 *tmp18 \n        tmp20 =tmp15 +tmp19 \n        tmp22 =tmp21 -tmp1 \n        tmp23 =tmp22 +tmp3 \n        tmp24 =tmp23 *tmp23 \n        tmp25 =tmp20 +tmp24 \n        tmp27 =tmp26 -tmp1 \n        tmp28 =tmp27 +tmp3 \n        tmp29 =tmp28 *tmp28 \n        tmp30 =tmp25 +tmp29 \n        tmp32 =tmp31 -tmp1 \n        tmp33 =tmp32 +tmp3 \n        tmp34 =tmp33 *tmp33 \n        tmp35 =tmp30 +tmp34 \n        tmp36 =libdevice .sqrt (tmp35 )\n        tmp37 =tmp36 +tmp1 \n        tmp38 =tmp0 +tmp3 \n        tmp39 =tmp38 *tmp38 \n        tmp40 =tmp6 +tmp3 \n        tmp41 =tmp40 *tmp40 \n        tmp42 =tmp39 +tmp41 \n        tmp43 =tmp11 +tmp3 \n        tmp44 =tmp43 *tmp43 \n        tmp45 =tmp42 +tmp44 \n        tmp46 =tmp16 +tmp3 \n        tmp47 =tmp46 *tmp46 \n        tmp48 =tmp45 +tmp47 \n        tmp49 =tmp21 +tmp3 \n        tmp50 =tmp49 *tmp49 \n        tmp51 =tmp48 +tmp50 \n        tmp52 =tmp26 +tmp3 \n        tmp53 =tmp52 *tmp52 \n        tmp54 =tmp51 +tmp53 \n        tmp55 =tmp31 +tmp3 \n        tmp56 =tmp55 *tmp55 \n        tmp57 =tmp54 +tmp56 \n        tmp58 =libdevice .sqrt (tmp57 )\n        tmp59 =tmp37 -tmp58 \n        tmp60 =0.0 \n        tmp61 =triton_helpers .maximum (tmp59 ,tmp60 )\n        tmp62 =tl .broadcast_to (tmp61 ,[XBLOCK ,R0_BLOCK ])\n        tmp64 =_tmp63 +tmp62 \n        _tmp63 =tl .where (r0_mask ,tmp64 ,_tmp63 )\n    tmp63 =tl .sum (_tmp63 ,1 )[:,None ]\n    tmp65 =7 *ks0 \n    tmp66 =tmp65 .to (tl .float32 )\n    tmp67 =tmp63 /tmp66 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp67 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,32 ,32 ),(1024 *s0 ,1024 ,32 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf3 =empty_strided_cuda ((1 ,s0 ,72 ,72 ),(5184 *s0 ,5184 ,72 ,1 ),torch .float32 )\n        buf5 =buf3 ;del buf3 \n        buf7 =buf5 ;del buf5 \n\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_replication_pad2d_sub_view_0_xnumel =5184 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_replication_pad2d_sub_view_0 [grid (triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_replication_pad2d_sub_view_0_xnumel )](buf7 ,arg3_1 ,15552 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n\n        buf8 =torch .ops .aten ._adaptive_avg_pool2d .default (buf7 ,[5 ,5 ])\n        del buf7 \n        buf9 =buf8 \n        del buf8 \n        buf10 =empty_strided_cuda ((1 ,s0 ,7 ,7 ),(49 *s0 ,49 ,7 ,1 ),torch .float32 )\n        buf11 =empty_strided_cuda ((),(),torch .float32 )\n        buf15 =buf11 ;del buf11 \n\n        49 *s0 \n        get_raw_stream (0 )\n        triton_per_fused_binary_cross_entropy_celu_constant_pad_nd_sub_1 [grid (1 )](buf15 ,buf9 ,buf10 ,3 ,1 ,147 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf9 \n        buf14 =empty_strided_cuda ((),(),torch .float32 )\n        buf16 =buf14 ;del buf14 \n\n        7 *s0 \n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_mean_norm_ones_like_sub_2 [grid (1 )](buf16 ,buf10 ,3 ,1 ,21 ,XBLOCK =1 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n    return (buf10 ,buf15 ,buf16 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "430938c3-845b-470f-bc2d-635f1d159dae",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GLU', 'Container', 'LazyInstanceNorm2d', 'Hardshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.glu1 = nn.GLU(dim=1)\n        self.container1 = nn.Sequential(\n            nn.LazyInstanceNorm2d(),\n            nn.Hardshrink(),\n            nn.GLU(dim=1)\n        )\n        self.container2 = nn.Sequential(\n            nn.LazyInstanceNorm2d(),\n            nn.Hardshrink(),\n            nn.GLU(dim=1)\n        )\n        self.container3 = nn.Sequential(\n            nn.LazyInstanceNorm2d(),\n            nn.Hardshrink(),\n            nn.GLU(dim=1)\n        )\n        self.container4 = nn.Sequential(\n            nn.LazyInstanceNorm2d(),\n            nn.Hardshrink(),\n            nn.GLU(dim=1)\n        )\n        self.container5 = nn.Sequential(\n            nn.LazyInstanceNorm2d(),\n            nn.Hardshrink(),\n            nn.GLU(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.glu1(x)\n        x = self.container1(x)\n        x = self.container2(x)\n        x = self.container3(x)\n        x = self.container4(x)\n        x = self.container5(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_glu_mean_0 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr1 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,out_ptr8 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +1024 *x0 ),None )\n    tmp1 =tl .load (in_ptr0 +(32768 +r0_1 +1024 *x0 ),None )\n    tmp26 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp33 =tl .load (in_ptr2 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr3 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp41 =tl .load (in_ptr4 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp2 =tl .sigmoid (tmp1 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tl .broadcast_to (tmp3 ,[R0_BLOCK ])\n    tmp6 =tl .broadcast_to (tmp4 ,[R0_BLOCK ])\n    tmp8 =triton_helpers .promote_to_tensor (tl .sum (tmp6 ,0 ))\n    tmp9 =tl .full ([1 ],1024 ,tl .int32 )\n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp8 /tmp10 \n    tmp12 =tmp4 -tmp11 \n    tmp13 =tmp12 *tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[R0_BLOCK ])\n    tmp16 =triton_helpers .promote_to_tensor (tl .sum (tmp14 ,0 ))\n    tmp17 =1024.0 \n    tmp18 =tmp16 /tmp17 \n    tmp19 =1e-05 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =libdevice .rsqrt (tmp20 )\n    tmp22 =1.0009775171065494 \n    tmp23 =tmp18 *tmp22 \n    tmp24 =0.1 \n    tmp25 =tmp23 *tmp24 \n    tmp27 =0.9 \n    tmp28 =tmp26 *tmp27 \n    tmp29 =tmp25 +tmp28 \n    tmp30 =1.0 \n    tmp31 =tmp29 /tmp30 \n    tmp32 =tmp11 *tmp24 \n    tmp34 =tmp33 *tmp27 \n    tmp35 =tmp32 +tmp34 \n    tmp36 =tmp35 /tmp30 \n    tmp37 =tmp3 -tmp11 \n    tmp38 =tmp37 *tmp21 \n    tmp40 =tmp38 *tmp39 \n    tmp42 =tmp40 +tmp41 \n    tl .store (out_ptr0 +(r0_1 +1024 *x0 ),tmp3 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp21 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp31 ,None )\n    tl .store (out_ptr7 +(x0 ),tmp36 ,None )\n    tl .store (out_ptr8 +(r0_1 +1024 *x0 ),tmp42 ,None )\n    tl .store (out_ptr1 +(x0 ),tmp11 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_abs_glu_le_mean_scalar_tensor_where_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr1 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +1024 *x0 ),None )\n    tmp6 =tl .load (in_ptr0 +(16384 +r0_1 +1024 *x0 ),None )\n    tmp34 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp41 =tl .load (in_ptr2 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp47 =tl .load (in_ptr3 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp49 =tl .load (in_ptr4 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =0.5 \n    tmp3 =tmp1 <=tmp2 \n    tmp4 =0.0 \n    tmp5 =tl .where (tmp3 ,tmp4 ,tmp0 )\n    tmp7 =tl_math .abs (tmp6 )\n    tmp8 =tmp7 <=tmp2 \n    tmp9 =tl .where (tmp8 ,tmp4 ,tmp6 )\n    tmp10 =tl .sigmoid (tmp9 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =tl .broadcast_to (tmp11 ,[R0_BLOCK ])\n    tmp14 =tl .broadcast_to (tmp12 ,[R0_BLOCK ])\n    tmp16 =triton_helpers .promote_to_tensor (tl .sum (tmp14 ,0 ))\n    tmp17 =tl .full ([1 ],1024 ,tl .int32 )\n    tmp18 =tmp17 .to (tl .float32 )\n    tmp19 =tmp16 /tmp18 \n    tmp20 =tmp12 -tmp19 \n    tmp21 =tmp20 *tmp20 \n    tmp22 =tl .broadcast_to (tmp21 ,[R0_BLOCK ])\n    tmp24 =triton_helpers .promote_to_tensor (tl .sum (tmp22 ,0 ))\n    tmp25 =1024.0 \n    tmp26 =tmp24 /tmp25 \n    tmp27 =1e-05 \n    tmp28 =tmp26 +tmp27 \n    tmp29 =libdevice .rsqrt (tmp28 )\n    tmp30 =1.0009775171065494 \n    tmp31 =tmp26 *tmp30 \n    tmp32 =0.1 \n    tmp33 =tmp31 *tmp32 \n    tmp35 =0.9 \n    tmp36 =tmp34 *tmp35 \n    tmp37 =tmp33 +tmp36 \n    tmp38 =1.0 \n    tmp39 =tmp37 /tmp38 \n    tmp40 =tmp19 *tmp32 \n    tmp42 =tmp41 *tmp35 \n    tmp43 =tmp40 +tmp42 \n    tmp44 =tmp43 /tmp38 \n    tmp45 =tmp11 -tmp19 \n    tmp46 =tmp45 *tmp29 \n    tmp48 =tmp46 *tmp47 \n    tmp50 =tmp48 +tmp49 \n    tmp51 =tl_math .abs (tmp50 )\n    tmp52 =tmp51 <=tmp2 \n    tmp53 =tl .where (tmp52 ,tmp4 ,tmp50 )\n    tl .store (out_ptr0 +(r0_1 +1024 *x0 ),tmp11 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp29 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp39 ,None )\n    tl .store (out_ptr7 +(x0 ),tmp44 ,None )\n    tl .store (in_out_ptr0 +(r0_1 +1024 *x0 ),tmp53 ,None )\n    tl .store (out_ptr1 +(x0 ),tmp19 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_abs_glu_le_mean_scalar_tensor_where_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr1 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +1024 *x0 ),None )\n    tmp1 =tl .load (in_ptr0 +(8192 +r0_1 +1024 *x0 ),None )\n    tmp26 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp33 =tl .load (in_ptr2 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr3 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp41 =tl .load (in_ptr4 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp2 =tl .sigmoid (tmp1 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tl .broadcast_to (tmp3 ,[R0_BLOCK ])\n    tmp6 =tl .broadcast_to (tmp4 ,[R0_BLOCK ])\n    tmp8 =triton_helpers .promote_to_tensor (tl .sum (tmp6 ,0 ))\n    tmp9 =tl .full ([1 ],1024 ,tl .int32 )\n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp8 /tmp10 \n    tmp12 =tmp4 -tmp11 \n    tmp13 =tmp12 *tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[R0_BLOCK ])\n    tmp16 =triton_helpers .promote_to_tensor (tl .sum (tmp14 ,0 ))\n    tmp17 =1024.0 \n    tmp18 =tmp16 /tmp17 \n    tmp19 =1e-05 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =libdevice .rsqrt (tmp20 )\n    tmp22 =1.0009775171065494 \n    tmp23 =tmp18 *tmp22 \n    tmp24 =0.1 \n    tmp25 =tmp23 *tmp24 \n    tmp27 =0.9 \n    tmp28 =tmp26 *tmp27 \n    tmp29 =tmp25 +tmp28 \n    tmp30 =1.0 \n    tmp31 =tmp29 /tmp30 \n    tmp32 =tmp11 *tmp24 \n    tmp34 =tmp33 *tmp27 \n    tmp35 =tmp32 +tmp34 \n    tmp36 =tmp35 /tmp30 \n    tmp37 =tmp3 -tmp11 \n    tmp38 =tmp37 *tmp21 \n    tmp40 =tmp38 *tmp39 \n    tmp42 =tmp40 +tmp41 \n    tmp43 =tl_math .abs (tmp42 )\n    tmp44 =0.5 \n    tmp45 =tmp43 <=tmp44 \n    tmp46 =0.0 \n    tmp47 =tl .where (tmp45 ,tmp46 ,tmp42 )\n    tl .store (out_ptr0 +(r0_1 +1024 *x0 ),tmp3 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp21 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp31 ,None )\n    tl .store (out_ptr7 +(x0 ),tmp36 ,None )\n    tl .store (in_out_ptr0 +(r0_1 +1024 *x0 ),tmp47 ,None )\n    tl .store (out_ptr1 +(x0 ),tmp11 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_abs_glu_le_mean_scalar_tensor_where_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr1 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +1024 *x0 ),None )\n    tmp1 =tl .load (in_ptr0 +(4096 +r0_1 +1024 *x0 ),None )\n    tmp26 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp33 =tl .load (in_ptr2 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr3 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp41 =tl .load (in_ptr4 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp2 =tl .sigmoid (tmp1 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tl .broadcast_to (tmp3 ,[R0_BLOCK ])\n    tmp6 =tl .broadcast_to (tmp4 ,[R0_BLOCK ])\n    tmp8 =triton_helpers .promote_to_tensor (tl .sum (tmp6 ,0 ))\n    tmp9 =tl .full ([1 ],1024 ,tl .int32 )\n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp8 /tmp10 \n    tmp12 =tmp4 -tmp11 \n    tmp13 =tmp12 *tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[R0_BLOCK ])\n    tmp16 =triton_helpers .promote_to_tensor (tl .sum (tmp14 ,0 ))\n    tmp17 =1024.0 \n    tmp18 =tmp16 /tmp17 \n    tmp19 =1e-05 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =libdevice .rsqrt (tmp20 )\n    tmp22 =1.0009775171065494 \n    tmp23 =tmp18 *tmp22 \n    tmp24 =0.1 \n    tmp25 =tmp23 *tmp24 \n    tmp27 =0.9 \n    tmp28 =tmp26 *tmp27 \n    tmp29 =tmp25 +tmp28 \n    tmp30 =1.0 \n    tmp31 =tmp29 /tmp30 \n    tmp32 =tmp11 *tmp24 \n    tmp34 =tmp33 *tmp27 \n    tmp35 =tmp32 +tmp34 \n    tmp36 =tmp35 /tmp30 \n    tmp37 =tmp3 -tmp11 \n    tmp38 =tmp37 *tmp21 \n    tmp40 =tmp38 *tmp39 \n    tmp42 =tmp40 +tmp41 \n    tmp43 =tl_math .abs (tmp42 )\n    tmp44 =0.5 \n    tmp45 =tmp43 <=tmp44 \n    tmp46 =0.0 \n    tmp47 =tl .where (tmp45 ,tmp46 ,tmp42 )\n    tl .store (out_ptr0 +(r0_1 +1024 *x0 ),tmp3 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp21 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp31 ,None )\n    tl .store (out_ptr7 +(x0 ),tmp36 ,None )\n    tl .store (in_out_ptr0 +(r0_1 +1024 *x0 ),tmp47 ,None )\n    tl .store (out_ptr1 +(x0 ),tmp11 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_abs_glu_le_mean_scalar_tensor_where_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr1 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +1024 *x0 ),None )\n    tmp1 =tl .load (in_ptr0 +(2048 +r0_1 +1024 *x0 ),None )\n    tmp24 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp33 =tl .load (in_ptr2 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr3 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp41 =tl .load (in_ptr4 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp2 =tl .sigmoid (tmp1 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tl .broadcast_to (tmp3 ,[R0_BLOCK ])\n    tmp6 =tl .broadcast_to (tmp4 ,[R0_BLOCK ])\n    tmp8 =triton_helpers .promote_to_tensor (tl .sum (tmp6 ,0 ))\n    tmp9 =tl .full ([1 ],1024 ,tl .int32 )\n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp8 /tmp10 \n    tmp12 =tmp4 -tmp11 \n    tmp13 =tmp12 *tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[R0_BLOCK ])\n    tmp16 =triton_helpers .promote_to_tensor (tl .sum (tmp14 ,0 ))\n    tmp17 =1024.0 \n    tmp18 =tmp16 /tmp17 \n    tmp19 =1e-05 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =libdevice .rsqrt (tmp20 )\n    tmp22 =0.1 \n    tmp23 =tmp11 *tmp22 \n    tmp25 =0.9 \n    tmp26 =tmp24 *tmp25 \n    tmp27 =tmp23 +tmp26 \n    tmp28 =1.0 \n    tmp29 =tmp27 /tmp28 \n    tmp30 =1.0009775171065494 \n    tmp31 =tmp18 *tmp30 \n    tmp32 =tmp31 *tmp22 \n    tmp34 =tmp33 *tmp25 \n    tmp35 =tmp32 +tmp34 \n    tmp36 =tmp35 /tmp28 \n    tmp37 =tmp3 -tmp11 \n    tmp38 =tmp37 *tmp21 \n    tmp40 =tmp38 *tmp39 \n    tmp42 =tmp40 +tmp41 \n    tmp43 =tl_math .abs (tmp42 )\n    tmp44 =0.5 \n    tmp45 =tmp43 <=tmp44 \n    tmp46 =0.0 \n    tmp47 =tl .where (tmp45 ,tmp46 ,tmp42 )\n    tl .store (out_ptr0 +(r0_1 +1024 *x0 ),tmp3 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp21 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp29 ,None )\n    tl .store (out_ptr7 +(x0 ),tmp36 ,None )\n    tl .store (in_out_ptr0 +(r0_1 +1024 *x0 ),tmp47 ,None )\n    tl .store (out_ptr1 +(x0 ),tmp11 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_glu_5 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1024 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(1024 +x0 ),xmask )\n    tmp2 =tl .sigmoid (tmp1 )\n    tmp3 =tmp0 *tmp2 \n    tl .store (out_ptr0 +(x0 ),tmp3 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,64 ,32 ,32 ),(65536 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_2 ,(32 ,),(1 ,))\n    assert_size_stride (primals_3 ,(32 ,),(1 ,))\n    assert_size_stride (primals_4 ,(32 ,),(1 ,))\n    assert_size_stride (primals_5 ,(32 ,),(1 ,))\n    assert_size_stride (primals_6 ,(16 ,),(1 ,))\n    assert_size_stride (primals_7 ,(16 ,),(1 ,))\n    assert_size_stride (primals_8 ,(16 ,),(1 ,))\n    assert_size_stride (primals_9 ,(16 ,),(1 ,))\n    assert_size_stride (primals_10 ,(8 ,),(1 ,))\n    assert_size_stride (primals_11 ,(8 ,),(1 ,))\n    assert_size_stride (primals_12 ,(8 ,),(1 ,))\n    assert_size_stride (primals_13 ,(8 ,),(1 ,))\n    assert_size_stride (primals_14 ,(4 ,),(1 ,))\n    assert_size_stride (primals_15 ,(4 ,),(1 ,))\n    assert_size_stride (primals_16 ,(4 ,),(1 ,))\n    assert_size_stride (primals_17 ,(4 ,),(1 ,))\n    assert_size_stride (primals_18 ,(2 ,),(1 ,))\n    assert_size_stride (primals_19 ,(2 ,),(1 ,))\n    assert_size_stride (primals_20 ,(2 ,),(1 ,))\n    assert_size_stride (primals_21 ,(2 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,32 ,32 ,32 ),(32768 ,1024 ,32 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,32 ,1 ,1 ),(32 ,1 ,1 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,32 ,1 ,1 ),(32 ,1 ,1 ,1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,32 ,32 ,32 ),(32768 ,1024 ,32 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_glu_mean_0 [grid (32 )](primals_1 ,primals_3 ,primals_2 ,primals_4 ,primals_5 ,buf0 ,buf1 ,buf4 ,primals_3 ,primals_2 ,buf5 ,32 ,1024 ,num_warps =8 ,num_stages =1 )\n        del primals_1 \n        del primals_2 \n        del primals_3 \n        buf6 =empty_strided_cuda ((1 ,16 ,32 ,32 ),(16384 ,1024 ,32 ,1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,16 ,1 ,1 ),(16 ,1 ,1 ,1 ),torch .float32 )\n        buf10 =empty_strided_cuda ((1 ,16 ,1 ,1 ),(16 ,1 ,1 ,1 ),torch .float32 )\n        buf11 =empty_strided_cuda ((1 ,16 ,32 ,32 ),(16384 ,1024 ,32 ,1 ),torch .float32 )\n        buf12 =buf11 ;del buf11 \n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_abs_glu_le_mean_scalar_tensor_where_1 [grid (16 )](buf12 ,buf5 ,primals_7 ,primals_6 ,primals_8 ,primals_9 ,buf6 ,buf7 ,buf10 ,primals_7 ,primals_6 ,16 ,1024 ,num_warps =8 ,num_stages =1 )\n        del buf5 \n        del primals_6 \n        del primals_7 \n        buf13 =empty_strided_cuda ((1 ,8 ,32 ,32 ),(8192 ,1024 ,32 ,1 ),torch .float32 )\n        buf14 =empty_strided_cuda ((1 ,8 ,1 ,1 ),(8 ,1 ,1 ,1 ),torch .float32 )\n        buf17 =empty_strided_cuda ((1 ,8 ,1 ,1 ),(8 ,1 ,1 ,1 ),torch .float32 )\n        buf18 =empty_strided_cuda ((1 ,8 ,32 ,32 ),(8192 ,1024 ,32 ,1 ),torch .float32 )\n        buf19 =buf18 ;del buf18 \n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_abs_glu_le_mean_scalar_tensor_where_2 [grid (8 )](buf19 ,buf12 ,primals_11 ,primals_10 ,primals_12 ,primals_13 ,buf13 ,buf14 ,buf17 ,primals_11 ,primals_10 ,8 ,1024 ,num_warps =8 ,num_stages =1 )\n        del primals_10 \n        del primals_11 \n        buf20 =empty_strided_cuda ((1 ,4 ,32 ,32 ),(4096 ,1024 ,32 ,1 ),torch .float32 )\n        buf21 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,1 ,1 ),torch .float32 )\n        buf24 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,1 ,1 ),torch .float32 )\n        buf25 =empty_strided_cuda ((1 ,4 ,32 ,32 ),(4096 ,1024 ,32 ,1 ),torch .float32 )\n        buf26 =buf25 ;del buf25 \n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_abs_glu_le_mean_scalar_tensor_where_3 [grid (4 )](buf26 ,buf19 ,primals_15 ,primals_14 ,primals_16 ,primals_17 ,buf20 ,buf21 ,buf24 ,primals_15 ,primals_14 ,4 ,1024 ,num_warps =8 ,num_stages =1 )\n        del primals_14 \n        del primals_15 \n        buf27 =empty_strided_cuda ((1 ,2 ,32 ,32 ),(2048 ,1024 ,32 ,1 ),torch .float32 )\n        buf28 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,1 ,1 ),torch .float32 )\n        buf31 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,1 ,1 ),torch .float32 )\n        buf32 =empty_strided_cuda ((1 ,2 ,32 ,32 ),(2048 ,1024 ,32 ,1 ),torch .float32 )\n        buf33 =buf32 ;del buf32 \n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_abs_glu_le_mean_scalar_tensor_where_4 [grid (2 )](buf33 ,buf26 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,buf27 ,buf28 ,buf31 ,primals_18 ,primals_19 ,2 ,1024 ,num_warps =8 ,num_stages =1 )\n        del primals_18 \n        del primals_19 \n        buf34 =empty_strided_cuda ((1 ,1 ,32 ,32 ),(1024 ,1024 ,32 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_glu_5 [grid (1024 )](buf33 ,buf34 ,1024 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n    return (buf34 ,primals_4 ,primals_5 ,primals_8 ,primals_9 ,primals_12 ,primals_13 ,primals_16 ,primals_17 ,primals_20 ,primals_21 ,buf0 ,buf1 ,buf4 ,buf6 ,buf7 ,buf10 ,buf12 ,buf13 ,buf14 ,buf17 ,buf19 ,buf20 ,buf21 ,buf24 ,buf26 ,buf27 ,buf28 ,buf31 ,buf33 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,64 ,32 ,32 ),(65536 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((8 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((8 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((8 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((8 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((4 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((4 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((4 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((4 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_18 =rand_strided ((2 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((2 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((2 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((2 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "44b20f66-f411-4e1e-93f9-2196c96041f8",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Container', 'PairwiseDistance', 'MaxPool2d', 'TripletMarginWithDistanceLoss', 'PoissonNLLLoss', 'HuberLoss', 'SoftMarginLoss', 'ConstantPad3d', 'Hardtanh', 'Tanh']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.container = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.ConstantPad3d(padding=(1, 1, 1, 1, 1, 1), value=0),\n            nn.Hardtanh(min_val=-1, max_val=1),\n            nn.Tanh(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.ConstantPad3d(padding=(1, 1, 1, 1, 1, 1), value=0),\n            nn.Hardtanh(min_val=-1, max_val=1),\n            nn.Tanh(),\n        )\n        self.pairwise_distance = nn.PairwiseDistance(p=2)\n        self.triplet_loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: self.pairwise_distance(x, y))\n        self.poisson_loss = nn.PoissonNLLLoss(log_input=True)\n        self.huber_loss = nn.HuberLoss()\n        self.softmargin_loss = nn.SoftMarginLoss()\n\n    def forward(self, x):\n        # Apply the container module\n        x = self.container(x)\n        \n        # Reshape the output to fit the PairwiseDistance input\n        x = x.view(x.size(0), -1)\n        \n        # Create anchor, positive, and negative samples for TripletMarginWithDistanceLoss\n        anchor = x[:x.size(0)//2]\n        positive = x[x.size(0)//2:]\n        negative = torch.flip(anchor, dims=[0])\n        \n        # Compute the triplet loss\n        triplet_loss = self.triplet_loss(anchor, positive, negative)\n        \n        # Compute the PoissonNLLLoss\n        poisson_loss = self.poisson_loss(x, torch.ones_like(x))\n        \n        # Compute the HuberLoss\n        huber_loss = self.huber_loss(x, torch.zeros_like(x))\n        \n        # Compute the SoftMarginLoss\n        softmargin_loss = self.softmargin_loss(x, torch.ones_like(x))\n        \n        # Return the average of all losses\n        return (triplet_loss + poisson_loss + huber_loss + softmargin_loss) / 4\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_hardtanh_max_pool2d_with_indices_tanh_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x5 =xindex //ks0 \n    x1 =((xindex //ks2 )%ks3 )\n    x0 =(xindex %ks2 )\n    x2 =xindex //ks6 \n    x6 =xindex \n    tmp0 =(-1 )+x5 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks4 //2 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-1 )+x0 \n    tmp10 =tmp9 >=tmp1 \n    tmp11 =ks5 //2 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =tl .load (in_ptr0 +((-2 )+((-2 )*ks5 )+2 *x0 +((-1 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =tl .load (in_ptr0 +((-1 )+((-2 )*ks5 )+2 *x0 +((-1 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =triton_helpers .maximum (tmp19 ,tmp18 )\n    tmp21 =tl .load (in_ptr0 +((-2 )+((-1 )*ks5 )+2 *x0 +((-1 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =triton_helpers .maximum (tmp21 ,tmp20 )\n    tmp23 =tl .load (in_ptr0 +((-1 )+((-1 )*ks5 )+2 *x0 +((-1 )*ks4 *ks5 )+2 *ks5 *x1 +ks4 *ks5 *x2 ),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp24 =triton_helpers .maximum (tmp23 ,tmp22 )\n    tmp25 =tl .full (tmp24 .shape ,0.0 ,tmp24 .dtype )\n    tmp26 =tl .where (tmp17 ,tmp24 ,tmp25 )\n    tmp27 =-1.0 \n    tmp28 =triton_helpers .maximum (tmp26 ,tmp27 )\n    tmp29 =1.0 \n    tmp30 =triton_helpers .minimum (tmp28 ,tmp29 )\n    tmp31 =libdevice .tanh (tmp30 )\n    tl .store (out_ptr0 +(x6 ),tmp31 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_hardtanh_max_pool2d_with_indices_tanh_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x5 =xindex //ks0 \n    x1 =((xindex //ks2 )%ks3 )\n    x0 =(xindex %ks2 )\n    x2 =xindex //ks6 \n    x6 =xindex \n    tmp0 =(-1 )+x5 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =2 +ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =1 +(ks4 //4 )\n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-1 )+x0 \n    tmp10 =tmp9 >=tmp1 \n    tmp11 =1 +(ks5 //4 )\n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =tl .load (in_ptr0 +((-10 )+((-4 )*(ks5 //2 ))+((-2 )*(ks4 //2 ))+2 *x0 +4 *x1 +4 *x2 +((-1 )*(ks4 //2 )*(ks5 //2 ))+2 *x1 *(ks5 //2 )+2 *x2 *(ks4 //2 )+2 *x2 *(ks5 //2 )+x2 *(ks4 //2 )*(ks5 //2 )),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =tl .load (in_ptr0 +((-9 )+((-4 )*(ks5 //2 ))+((-2 )*(ks4 //2 ))+2 *x0 +4 *x1 +4 *x2 +((-1 )*(ks4 //2 )*(ks5 //2 ))+2 *x1 *(ks5 //2 )+2 *x2 *(ks4 //2 )+2 *x2 *(ks5 //2 )+x2 *(ks4 //2 )*(ks5 //2 )),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =triton_helpers .maximum (tmp19 ,tmp18 )\n    tmp21 =tl .load (in_ptr0 +((-8 )+((-3 )*(ks5 //2 ))+((-2 )*(ks4 //2 ))+2 *x0 +4 *x1 +4 *x2 +((-1 )*(ks4 //2 )*(ks5 //2 ))+2 *x1 *(ks5 //2 )+2 *x2 *(ks4 //2 )+2 *x2 *(ks5 //2 )+x2 *(ks4 //2 )*(ks5 //2 )),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =triton_helpers .maximum (tmp21 ,tmp20 )\n    tmp23 =tl .load (in_ptr0 +((-7 )+((-3 )*(ks5 //2 ))+((-2 )*(ks4 //2 ))+2 *x0 +4 *x1 +4 *x2 +((-1 )*(ks4 //2 )*(ks5 //2 ))+2 *x1 *(ks5 //2 )+2 *x2 *(ks4 //2 )+2 *x2 *(ks5 //2 )+x2 *(ks4 //2 )*(ks5 //2 )),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp24 =triton_helpers .maximum (tmp23 ,tmp22 )\n    tmp25 =tl .full (tmp24 .shape ,0.0 ,tmp24 .dtype )\n    tmp26 =tl .where (tmp17 ,tmp24 ,tmp25 )\n    tmp27 =-1.0 \n    tmp28 =triton_helpers .maximum (tmp26 ,tmp27 )\n    tmp29 =1.0 \n    tmp30 =triton_helpers .minimum (tmp28 ,tmp29 )\n    tmp31 =libdevice .tanh (tmp30 )\n    tl .store (out_ptr0 +(x6 ),tmp31 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_div_exp_huber_loss_mean_mul_norm_ones_like_soft_margin_loss_sub_zeros_like_2 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp6 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp19 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp26 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl_math .exp (tmp0 )\n        tmp2 =1.0 \n        tmp3 =tmp2 *tmp0 \n        tmp4 =tmp1 -tmp3 \n        tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n        tmp7 =_tmp6 +tmp5 \n        _tmp6 =tl .where (r0_mask ,tmp7 ,_tmp6 )\n        tmp8 =0.0 \n        tmp9 =tmp0 -tmp8 \n        tmp10 =tl_math .abs (tmp9 )\n        tmp11 =tmp10 <tmp2 \n        tmp12 =0.5 \n        tmp13 =tmp10 *tmp12 \n        tmp14 =tmp13 *tmp10 \n        tmp15 =tmp10 -tmp12 \n        tmp16 =tmp15 *tmp2 \n        tmp17 =tl .where (tmp11 ,tmp14 ,tmp16 )\n        tmp18 =tl .broadcast_to (tmp17 ,[XBLOCK ,R0_BLOCK ])\n        tmp20 =_tmp19 +tmp18 \n        _tmp19 =tl .where (r0_mask ,tmp20 ,_tmp19 )\n        tmp21 =-tmp0 \n        tmp22 =tmp21 *tmp2 \n        tmp23 =tl_math .exp (tmp22 )\n        tmp24 =libdevice .log1p (tmp23 )\n        tmp25 =tl .broadcast_to (tmp24 ,[XBLOCK ,R0_BLOCK ])\n        tmp27 =_tmp26 +tmp25 \n        _tmp26 =tl .where (r0_mask ,tmp27 ,_tmp26 )\n    tmp6 =tl .sum (_tmp6 ,1 )[:,None ]\n    tmp19 =tl .sum (_tmp19 ,1 )[:,None ]\n    tmp26 =tl .sum (_tmp26 ,1 )[:,None ]\n    tmp28 =0.0 \n    tmp29 =tmp28 /tmp28 \n    tmp30 =36 +9 *ks0 +12 *(ks1 //4 )+12 *(ks2 //4 )+3 *ks0 *(ks1 //4 )+3 *ks0 *(ks2 //4 )+4 *(ks1 //4 )*(ks2 //4 )+ks0 *(ks1 //4 )*(ks2 //4 )\n    tmp31 =tmp30 .to (tl .float32 )\n    tmp32 =tmp6 /tmp31 \n    tmp33 =tmp29 +tmp32 \n    tmp34 =tmp19 /tmp31 \n    tmp35 =tmp33 +tmp34 \n    tmp36 =tmp26 /tmp31 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =0.25 \n    tmp39 =tmp37 *tmp38 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp39 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        2 +(s2 //2 )\n        2 +(s1 //2 )\n        4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,2 +s0 ,2 +(s1 //2 ),2 +(s2 //2 )),(8 +4 *s0 +4 *(s1 //2 )+4 *(s2 //2 )+2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+2 *(s1 //2 )*(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 ),2 +(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_hardtanh_max_pool2d_with_indices_tanh_0_xnumel =8 +4 *s0 +4 *(s1 //2 )+4 *(s2 //2 )+2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+2 *(s1 //2 )*(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_hardtanh_max_pool2d_with_indices_tanh_0 [grid (triton_poi_fused_constant_pad_nd_hardtanh_max_pool2d_with_indices_tanh_0_xnumel )](arg3_1 ,buf0 ,1156 ,3 ,34 ,34 ,64 ,64 ,1156 ,5780 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        9 +3 *(s1 //4 )+3 *(s2 //4 )+(s1 //4 )*(s2 //4 )\n        3 +(s2 //4 )\n        3 +(s1 //4 )\n        9 +3 *(s1 //4 )+3 *(s2 //4 )+(s1 //4 )*(s2 //4 )\n        buf1 =empty_strided_cuda ((1 ,4 +s0 ,3 +(s1 //4 ),3 +(s2 //4 )),(36 +9 *s0 +12 *(s1 //4 )+12 *(s2 //4 )+3 *s0 *(s1 //4 )+3 *s0 *(s2 //4 )+4 *(s1 //4 )*(s2 //4 )+s0 *(s1 //4 )*(s2 //4 ),9 +3 *(s1 //4 )+3 *(s2 //4 )+(s1 //4 )*(s2 //4 ),3 +(s2 //4 ),1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_hardtanh_max_pool2d_with_indices_tanh_1_xnumel =36 +9 *s0 +12 *(s1 //4 )+12 *(s2 //4 )+3 *s0 *(s1 //4 )+3 *s0 *(s2 //4 )+4 *(s1 //4 )*(s2 //4 )+s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_hardtanh_max_pool2d_with_indices_tanh_1 [grid (triton_poi_fused_constant_pad_nd_hardtanh_max_pool2d_with_indices_tanh_1_xnumel )](buf0 ,buf1 ,361 ,3 ,19 ,19 ,64 ,64 ,361 ,2527 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf7 =buf4 ;del buf4 \n\n        36 +9 *s0 +12 *(s1 //4 )+12 *(s2 //4 )+3 *s0 *(s1 //4 )+3 *s0 *(s2 //4 )+4 *(s1 //4 )*(s2 //4 )+s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_div_exp_huber_loss_mean_mul_norm_ones_like_soft_margin_loss_sub_zeros_like_2 [grid (1 )](buf7 ,buf1 ,3 ,64 ,64 ,1 ,2527 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf1 \n    return (buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "45ce70e1-24b3-4e3a-acf8-cbc098813b39",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PairwiseDistance', 'Module', 'ReplicationPad3d', 'LPPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.replication_pad3d = nn.ReplicationPad3d(padding=1)\n        self.lp_pool2d = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)\n        self.pairwise_distance = nn.PairwiseDistance(p=2)\n        self.module = nn.Module()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, depth, height, width)\n        # Apply ReplicationPad3d\n        x = self.replication_pad3d(x)\n        \n        # Reshape to apply LPPool2d\n        # Flatten depth dimension into channels\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size, channels * depth, height, width)\n        \n        # Apply LPPool2d\n        x = self.lp_pool2d(x)\n        \n        # Reshape back to original dimensions (excluding pooled height and width)\n        pooled_height, pooled_width = x.shape[-2:]\n        x = x.view(batch_size, channels, depth, pooled_height, pooled_width)\n        \n        # Apply PairwiseDistance\n        # Split the tensor into two along the channel dimension\n        x1, x2 = torch.split(x, channels // 2, dim=1)\n        x = self.pairwise_distance(x1, x2)\n        \n        # Apply Module (identity operation in this case)\n        x = self.module(x)\n        \n        return x\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 4, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_pow_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks5 *(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 ))))<((-1 )+ks4 )))+ks4 *ks5 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 )))))+(((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 ))))*((((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *ks5 *(x2 //(2 +ks3 ))+(((-1 )+ks5 )*(((-1 )+ks5 )<=(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))<((-1 )+ks5 )))),xmask ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(ks5 *(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 ))))<((-1 )+ks4 )))+ks4 *ks5 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 )))))+(((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 ))))*((((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *ks5 *(x2 //(2 +ks3 ))+((2 *x0 )*((2 *x0 )<=((-1 )+ks5 ))+((-1 )+ks5 )*(((-1 )+ks5 )<(2 *x0 )))),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(ks5 *((2 *x1 )*((2 *x1 )<=((-1 )+ks4 ))+((-1 )+ks4 )*(((-1 )+ks4 )<(2 *x1 )))+ks4 *ks5 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 )))))+(((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 ))))*((((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *ks5 *(x2 //(2 +ks3 ))+(((-1 )+ks5 )*(((-1 )+ks5 )<=(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))<((-1 )+ks5 )))),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr0 +(ks5 *((2 *x1 )*((2 *x1 )<=((-1 )+ks4 ))+((-1 )+ks4 )*(((-1 )+ks4 )<(2 *x1 )))+ks4 *ks5 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 )))))+(((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 ))))*((((0 )*((0 )>=((-1 )+((x2 %(2 +ks3 )))))+((-1 )+((x2 %(2 +ks3 ))))*(((-1 )+((x2 %(2 +ks3 ))))>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *ks5 *(x2 //(2 +ks3 ))+((2 *x0 )*((2 *x0 )<=((-1 )+ks5 ))+((-1 )+ks5 )*(((-1 )+ks5 )<(2 *x0 )))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tmp0 *tmp0 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tmp3 +tmp1 \n    tmp6 =tmp5 *tmp5 \n    tmp7 =tmp6 +tmp4 \n    tmp9 =tmp8 *tmp8 \n    tmp10 =tmp9 +tmp7 \n    tmp11 =0.25 \n    tmp12 =tmp10 *tmp11 \n    tl .store (out_ptr0 +(x3 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_mul_pow_relu_sign_1 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tmp1 <tmp0 \n    tmp3 =tmp2 .to (tl .int8 )\n    tmp4 =tmp0 <tmp1 \n    tmp5 =tmp4 .to (tl .int8 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =tmp6 .to (tmp0 .dtype )\n    tmp8 =tl_math .abs (tmp0 )\n    tmp9 =triton_helpers .maximum (tmp1 ,tmp8 )\n    tmp10 =tmp7 *tmp9 \n    tmp11 =4.0 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =libdevice .sqrt (tmp12 )\n    tl .store (in_out_ptr0 +(x0 ),tmp13 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        1 +(s3 //2 )\n        1 +(s2 //2 )\n        1 +(s2 //2 )*(s3 //2 )+(s2 //2 )+(s3 //2 )\n        buf0 =empty_strided_cuda ((1 ,2 *s0 +s0 *s1 ,1 +(s2 //2 ),1 +(s3 //2 )),(2 *s0 +s0 *s1 +2 *s0 *(s2 //2 )+2 *s0 *(s3 //2 )+s0 *s1 *(s2 //2 )+s0 *s1 *(s3 //2 )+2 *s0 *(s2 //2 )*(s3 //2 )+s0 *s1 *(s2 //2 )*(s3 //2 ),1 +(s2 //2 )*(s3 //2 )+(s2 //2 )+(s3 //2 ),1 +(s3 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_pow_0_xnumel =2 *s0 +s0 *s1 +2 *s0 *(s2 //2 )+2 *s0 *(s3 //2 )+s0 *s1 *(s2 //2 )+s0 *s1 *(s3 //2 )+2 *s0 *(s2 //2 )*(s3 //2 )+s0 *s1 *(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_pow_0 [grid (triton_poi_fused_avg_pool2d_pow_0_xnumel )](arg4_1 ,buf0 ,17 ,17 ,289 ,4 ,32 ,32 ,5202 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg4_1 \n        buf1 =buf0 ;del buf0 \n\n        triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel =2 *s0 +s0 *s1 +2 *s0 *(s2 //2 )+2 *s0 *(s3 //2 )+s0 *s1 *(s2 //2 )+s0 *s1 *(s3 //2 )+2 *s0 *(s2 //2 )*(s3 //2 )+s0 *s1 *(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_mul_pow_relu_sign_1 [grid (triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel )](buf1 ,5202 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n    return (reinterpret_tensor (buf1 ,(1 ,s0 ,2 +s1 ,1 +(s2 //2 ),1 +(s3 //2 )),(2 *s0 +s0 *s1 +2 *s0 *(s2 //2 )+2 *s0 *(s3 //2 )+s0 *s1 *(s2 //2 )+s0 *s1 *(s3 //2 )+2 *s0 *(s2 //2 )*(s3 //2 )+s0 *s1 *(s2 //2 )*(s3 //2 ),2 +s1 +2 *(s2 //2 )+2 *(s3 //2 )+s1 *(s2 //2 )+s1 *(s3 //2 )+2 *(s2 //2 )*(s3 //2 )+s1 *(s2 //2 )*(s3 //2 ),1 +(s2 //2 )*(s3 //2 )+(s2 //2 )+(s3 //2 ),1 +(s3 //2 ),1 ),0 ),s0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =4 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,4 ,32 ,32 ),(12288 ,4096 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "471f4017-481d-4835-a6f3-9072526ae0ca",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Tanh', 'FractionalMaxPool3d', 'Hardswish', 'RNN', 'Softshrink', 'SiLU', 'LayerNorm', 'LazyBatchNorm1d', 'PoissonNLLLoss', 'LazyLinear']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.tanh = nn.Tanh()\n        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))\n        self.hardswish = nn.Hardswish()\n        self.rnn = nn.RNN(input_size=8, hidden_size=16, num_layers=2, batch_first=True)\n        self.softshrink = nn.Softshrink()\n        self.silu = nn.SiLU()\n        self.layer_norm = nn.LayerNorm(16)\n        self.lazy_batch_norm1d = nn.LazyBatchNorm1d()\n        self.poisson_nll_loss = nn.PoissonNLLLoss()\n        self.lazy_linear = nn.LazyLinear(10)\n\n    def forward(self, x):\n        # Apply Tanh activation\n        x = self.tanh(x)\n        \n        # Reshape for FractionalMaxPool3d\n        x = x.view(-1, 1, 16, 16, 16)\n        x = self.fractional_max_pool3d(x)\n        \n        # Reshape for RNN\n        x = x.view(-1, 8, 8)\n        x, _ = self.rnn(x)\n        \n        # Apply Hardswish activation\n        x = self.hardswish(x)\n        \n        # Apply Softshrink\n        x = self.softshrink(x)\n        \n        # Apply SiLU activation\n        x = self.silu(x)\n        \n        # Apply LayerNorm\n        x = self.layer_norm(x)\n        \n        # Reshape for LazyBatchNorm1d\n        x = x.view(-1, 16)\n        x = self.lazy_batch_norm1d(x)\n        \n        # Reshape for LazyLinear\n        x = x.view(-1, 16)\n        x = self.lazy_linear(x)\n        \n        # Apply PoissonNLLLoss (assuming target is provided externally)\n        # Note: PoissonNLLLoss is typically used in the loss function, not in the forward pass\n        # For demonstration purposes, we will skip applying it here\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 16, 16, 16).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_tanh_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),None )\n    tmp1 =libdevice .tanh (tmp0 )\n    tl .store (out_ptr0 +(x0 ),tmp1 ,None )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,16 ,16 ,16 ),(4096 ,256 ,16 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,1 ,3 ),(3 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (3 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,16 ,16 ,16 ),(4096 ,256 ,16 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_tanh_1 [grid (4096 )](arg0_1 ,buf2 ,4096 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n\n        buf3 =torch .ops .aten .fractional_max_pool3d .default (reinterpret_tensor (buf2 ,(1 ,1 ,16 ,16 ,16 ),(0 ,0 ,256 ,16 ,1 ),0 ),[2 ,2 ,2 ],[8 ,8 ,8 ],buf1 )\n        del buf1 \n        del buf2 \n        buf4 =buf3 [0 ]\n        del buf3 \n    return (reinterpret_tensor (buf4 ,(8 ,8 ,8 ),(64 ,8 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,16 ,16 ,16 ),(4096 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "47b5a127-1e92-475b-85c1-bd77c0fd6b53",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['SmoothL1Loss', 'Unfold', 'LPPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.unfold = nn.Unfold(kernel_size=(3, 3), stride=(1, 1))\n        self.lp_pool1d = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.smooth_l1_loss = nn.SmoothL1Loss()\n\n    def forward(self, x):\n        # Unfold the input to extract patches\n        x = self.unfold(x)\n        \n        # Reshape to fit LPPool1d input requirements\n        batch_size, channels, _ = x.shape\n        x = x.view(batch_size, channels, -1)\n        \n        # Apply LPPool1d\n        x = self.lp_pool1d(x)\n        \n        # Reshape back to a 2D tensor for SmoothL1Loss\n        x = x.view(batch_size, -1)\n        \n        # Create a dummy target tensor for SmoothL1Loss\n        target = torch.zeros_like(x)\n        \n        # Compute SmoothL1Loss\n        loss = self.smooth_l1_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels and 64x64 spatial dimensions\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_mul_pow_relu_sign_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    yoffset =(tl .program_id (1 )+tl .program_id (2 )*tl .num_programs (1 ))*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x1 =xindex \n    y0 =yindex \n    tl .device_assert (((((x1 //3 )%3 ))+((((2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))<ks0 )|~(xmask &ymask ),\"index out of bounds: (((x1 // 3) % 3)) + ((((2*y0) // ((-2) + ks1)) % ((-2) + ks0))) < ks0\")\n    tl .device_assert ((((x1 %3 ))+(((2 *y0 )%((-2 )+ks1 )))<ks1 )|~(xmask &ymask ),\"index out of bounds: ((x1 % 3)) + (((2*y0) % ((-2) + ks1))) < ks1\")\n    tmp2 =tl .load (in_ptr0 +(ks1 *(((x1 //3 )%3 ))+ks1 *((((2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))+ks0 *ks1 *(x1 //9 )+((x1 %3 ))+(((2 *y0 )%((-2 )+ks1 )))),xmask &ymask ,eviction_policy ='evict_last')\n    tl .device_assert (((((x1 //3 )%3 ))+((((1 +2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))<ks0 )|~(xmask &ymask ),\"index out of bounds: (((x1 // 3) % 3)) + ((((1 + 2*y0) // ((-2) + ks1)) % ((-2) + ks0))) < ks0\")\n    tl .device_assert ((((x1 %3 ))+(((1 +2 *y0 )%((-2 )+ks1 )))<ks1 )|~(xmask &ymask ),\"index out of bounds: ((x1 % 3)) + (((1 + 2*y0) % ((-2) + ks1))) < ks1\")\n    tmp6 =tl .load (in_ptr0 +(ks1 *(((x1 //3 )%3 ))+ks1 *((((1 +2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))+ks0 *ks1 *(x1 //9 )+((x1 %3 ))+(((1 +2 *y0 )%((-2 )+ks1 )))),xmask &ymask ,eviction_policy ='evict_last')\n    tl .device_assert (((((x1 //3 )%3 ))+((((2 +2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))<ks0 )|~(xmask &ymask ),\"index out of bounds: (((x1 // 3) % 3)) + ((((2 + 2*y0) // ((-2) + ks1)) % ((-2) + ks0))) < ks0\")\n    tl .device_assert ((((x1 %3 ))+(((2 +2 *y0 )%((-2 )+ks1 )))<ks1 )|~(xmask &ymask ),\"index out of bounds: ((x1 % 3)) + (((2 + 2*y0) % ((-2) + ks1))) < ks1\")\n    tmp11 =tl .load (in_ptr0 +(ks1 *(((x1 //3 )%3 ))+ks1 *((((2 +2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))+ks0 *ks1 *(x1 //9 )+((x1 %3 ))+(((2 +2 *y0 )%((-2 )+ks1 )))),xmask &ymask ,eviction_policy ='evict_last')\n    tmp3 =tmp2 *tmp2 \n    tmp7 =tmp6 *tmp6 \n    tmp8 =tmp7 +tmp3 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tmp12 +tmp8 \n    tmp14 =0.3333333333333333 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp17 =tmp16 <tmp15 \n    tmp18 =tmp17 .to (tl .int8 )\n    tmp19 =tmp15 <tmp16 \n    tmp20 =tmp19 .to (tl .int8 )\n    tmp21 =tmp18 -tmp20 \n    tmp22 =tmp21 .to (tmp15 .dtype )\n    tmp23 =tl_math .abs (tmp15 )\n    tmp24 =triton_helpers .maximum (tmp16 ,tmp23 )\n    tmp25 =tmp22 *tmp24 \n    tmp26 =3.0 \n    tmp27 =tmp25 *tmp26 \n    tmp28 =libdevice .sqrt (tmp27 )\n    tl .store (out_ptr0 +(y0 +x1 *((3 +ks0 *ks1 )//2 )+((-1 )*ks0 *x1 )+((-1 )*ks1 *x1 )),tmp28 ,xmask &ymask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_smooth_l1_loss_zeros_like_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =7 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp18 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *(triton_helpers .div_floor_integer (6 +((-9 )*ks0 *ks1 )+((-9 )*ks0 *ks2 )+9 *ks0 *((3 +ks1 *ks2 )//2 ),7 ))\n        tmp1 =((-9 )*ks0 *ks1 )+((-9 )*ks0 *ks2 )+9 *ks0 *((3 +ks1 *ks2 )//2 )\n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(r0_1 +x0 *(triton_helpers .div_floor_integer (6 +((-9 )*ks0 *ks1 )+((-9 )*ks0 *ks2 )+9 *ks0 *((3 +ks1 *ks2 )//2 ),7 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =0.0 \n        tmp5 =tmp3 -tmp4 \n        tmp6 =tl_math .abs (tmp5 )\n        tmp7 =1.0 \n        tmp8 =tmp6 <tmp7 \n        tmp9 =tmp6 *tmp6 \n        tmp10 =0.5 \n        tmp11 =tmp9 *tmp10 \n        tmp12 =tmp11 *tmp7 \n        tmp13 =tmp6 -tmp10 \n        tmp14 =tl .where (tmp8 ,tmp12 ,tmp13 )\n        tmp15 =tl .full (tmp14 .shape ,0 ,tmp14 .dtype )\n        tmp16 =tl .where (tmp2 ,tmp14 ,tmp15 )\n        tmp17 =tl .broadcast_to (tmp16 ,[XBLOCK ,R0_BLOCK ])\n        tmp19 =_tmp18 +tmp17 \n        _tmp18 =tl .where (r0_mask &xmask ,tmp19 ,_tmp18 )\n    tmp18 =tl .sum (_tmp18 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp18 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_smooth_l1_loss_zeros_like_2 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =7 \n    R0_BLOCK :tl .constexpr =8 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =((-9 )*ks0 *ks1 )+((-9 )*ks0 *ks2 )+9 *ks0 *((3 +ks1 *ks2 )//2 )\n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tmp4 /tmp6 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp7 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,9 *s0 ,((-1 )*s1 )+((-1 )*s2 )+((3 +s1 *s2 )//2 )),(((-9 )*s0 *s1 )+((-9 )*s0 *s2 )+9 *s0 *((3 +s1 *s2 )//2 ),((-1 )*s1 )+((-1 )*s2 )+((3 +s1 *s2 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_abs_mul_pow_relu_sign_0_ynumel =((-1 )*s1 )+((-1 )*s2 )+((3 +s1 *s2 )//2 )\n        triton_poi_fused_abs_mul_pow_relu_sign_0_xnumel =9 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_mul_pow_relu_sign_0 [grid (triton_poi_fused_abs_mul_pow_relu_sign_0_ynumel ,triton_poi_fused_abs_mul_pow_relu_sign_0_xnumel )](arg3_1 ,buf0 ,64 ,64 ,1921 ,27 ,XBLOCK =1 ,YBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((7 ,),(1 ,),torch .float32 )\n\n        (6 +((-9 )*s0 *s1 )+((-9 )*s0 *s2 )+9 *s0 *((3 +s1 *s2 )//2 ))//7 \n        get_raw_stream (0 )\n        triton_red_fused_smooth_l1_loss_zeros_like_1 [grid (7 )](buf0 ,buf1 ,3 ,64 ,64 ,7 ,7410 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((),(),torch .float32 )\n        buf3 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_per_fused_smooth_l1_loss_zeros_like_2 [grid (1 )](buf3 ,buf1 ,3 ,64 ,64 ,1 ,7 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "489a4754-73d0-4c47-b7f2-806189c7905b",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Dropout', 'AdaptiveAvgPool3d', 'Flatten']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout1 = nn.Dropout(p=0.5)\n        self.adaptive_avg_pool3d = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.flatten = nn.Flatten()\n        self.dropout2 = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        # Apply dropout to the input\n        x = self.dropout1(x)\n        \n        # Apply adaptive average pooling to reduce the spatial dimensions to 1x1x1\n        x = self.adaptive_avg_pool3d(x)\n        \n        # Flatten the output to a 1D tensor\n        x = self.flatten(x)\n        \n        # Apply dropout again\n        x = self.dropout2(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64, 64).cuda()  # Example input with shape (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_mean_native_dropout_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %32 )\n    x1 =xindex //32 \n    _tmp14 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =r0_2 +x0 *((31 +ks0 *ks1 *ks2 )//32 )\n        tmp1 =ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(ks0 *ks1 *ks2 *x1 +(((r0_2 +x0 *((31 +ks0 *ks1 *ks2 )//32 ))%(ks0 *ks1 *ks2 )))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =0.5 \n        tmp5 =tmp3 >tmp4 \n        tmp6 =tmp5 .to (tl .float32 )\n        tmp7 =tl .load (in_ptr1 +(ks0 *ks1 *ks2 *x1 +(((r0_2 +x0 *((31 +ks0 *ks1 *ks2 )//32 ))%(ks0 *ks1 *ks2 )))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp8 =tmp6 *tmp7 \n        tmp9 =2.0 \n        tmp10 =tmp8 *tmp9 \n        tmp11 =tl .full (tmp10 .shape ,0 ,tmp10 .dtype )\n        tmp12 =tl .where (tmp2 ,tmp10 ,tmp11 )\n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =_tmp14 +tmp13 \n        _tmp14 =tl .where (r0_mask &xmask ,tmp15 ,_tmp14 )\n    tmp14 =tl .sum (_tmp14 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_mean_native_dropout_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,load_seed_offset ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =32 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +32 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (xmask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =tl .load (in_ptr1 +load_seed_offset )\n    tmp6 =x0 \n    tmp7 =tl .rand (tmp5 ,(tmp6 ).to (tl .uint32 ))\n    tmp8 =0.5 \n    tmp9 =tmp7 >tmp8 \n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =ks1 *ks2 *ks3 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =tmp4 /tmp12 \n    tmp14 =tmp10 *tmp13 \n    tmp15 =2.0 \n    tmp16 =tmp14 *tmp15 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp16 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_native_dropout_0_xnumel =s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_0 [grid (triton_poi_fused_native_dropout_0_xnumel )](buf0 ,buf2 ,0 ,786432 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ,32 ),(32 *s0 ,32 ,32 *s0 ,32 *s0 ,32 *s0 ,1 ),torch .float32 )\n\n        triton_red_fused_mean_native_dropout_1_xnumel =32 *s0 \n        (31 +s1 *s2 *s3 )//32 \n        get_raw_stream (0 )\n        triton_red_fused_mean_native_dropout_1 [grid (triton_red_fused_mean_native_dropout_1_xnumel )](buf2 ,arg4_1 ,buf3 ,64 ,64 ,64 ,96 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg4_1 \n        del buf2 \n        buf1 =empty_strided_cuda ((1 ,s0 ),(s0 ,1 ),torch .float32 )\n        buf5 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_per_fused_mean_native_dropout_2 [grid (s0 )](buf5 ,buf3 ,buf0 ,1 ,64 ,64 ,64 ,3 ,32 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n        del buf3 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =64 \n    arg4_1 =rand_strided ((1 ,3 ,64 ,64 ,64 ),(786432 ,262144 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "4a22f183-6ba5-44ee-aff8-3463b910fc11",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GRUCell', 'CrossMapLRN2d', 'TransformerDecoderLayer', 'Fold', 'ConstantPad2d', 'NLLLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.gru_cell = nn.GRUCell(input_size=128, hidden_size=256)\n        self.cross_map_lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=256, nhead=8)\n        self.fold = nn.Fold(output_size=(64, 64), kernel_size=(3, 3), stride=(1, 1))\n        self.constant_pad = nn.ConstantPad2d(padding=(1, 1, 1, 1), value=0)\n        self.nll_loss = nn.NLLLoss()\n\n    def forward(self, x):\n        # Assuming x is a 4D tensor (batch, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        \n        # Apply ConstantPad2d\n        x = self.constant_pad(x)\n        \n        # Apply CrossMapLRN2d\n        x = self.cross_map_lrn(x)\n        \n        # Reshape for GRUCell\n        x = x.view(batch_size, -1, 128)  # Reshape to (batch_size, seq_len, input_size)\n        hx = torch.zeros(batch_size, 256).to(x.device)  # Initialize hidden state\n        for t in range(x.size(1)):\n            hx = self.gru_cell(x[:, t, :], hx)\n        \n        # Reshape for TransformerDecoderLayer\n        hx = hx.unsqueeze(0)  # Add sequence dimension\n        memory = torch.zeros_like(hx)  # Dummy memory for TransformerDecoderLayer\n        x = self.transformer_decoder_layer(hx, memory)\n        \n        # Reshape for Fold\n        x = x.view(batch_size, -1, 64, 64)  # Reshape to (batch_size, channels, height, width)\n        x = self.fold(x)\n        \n        # Compute NLLLoss (assuming x is log probabilities and target is provided)\n        target = torch.randint(0, 10, (batch_size, 64, 64)).to(x.device)  # Dummy target\n        loss = self.nll_loss(x, target)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-1 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-1 )+x0 +((-1 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tl .store (out_ptr0 +(x4 ),tmp12 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +s2 \n        2 +s1 \n        4 +2 *s1 +2 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ),(4 *s0 +2 *s0 *s1 +2 *s0 *s2 +s0 *s1 *s2 ,4 +2 *s1 +2 *s2 +s1 *s2 ,2 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =4 *s0 +2 *s0 *s1 +2 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg3_1 ,buf0 ,66 ,66 ,64 ,64 ,4356 ,13068 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "4a39e84c-7379-49ce-b80f-3d239614028d",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReplicationPad2d', 'Hardswish', 'RNN', 'SiLU', 'GRU', 'Mish', 'MSELoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ReplicationPad2d(2)\n        self.hardswish = nn.Hardswish()\n        self.rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.silu = nn.SiLU()\n        self.gru = nn.GRU(input_size=20, hidden_size=30, num_layers=2, batch_first=True)\n        self.mish = nn.Mish()\n        self.mseloss = nn.MSELoss()\n\n    def forward(self, x):\n        # Apply ReplicationPad2d\n        x = self.pad(x)\n        \n        # Flatten the spatial dimensions to fit RNN input\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1).transpose(1, 2)  # (batch_size, seq_len, channels)\n        \n        # Apply Hardswish\n        x = self.hardswish(x)\n        \n        # Apply RNN\n        x, _ = self.rnn(x)\n        \n        # Apply SiLU\n        x = self.silu(x)\n        \n        # Apply GRU\n        x, _ = self.gru(x)\n        \n        # Apply Mish\n        x = self.mish(x)\n        \n        # Reshape back to original spatial dimensions (or any desired shape)\n        x = x.transpose(1, 2).view(batch_size, -1, height, width)\n        \n        # Compute MSELoss (assuming we have a target tensor)\n        target = torch.zeros_like(x)  # Example target\n        loss = self.mseloss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_hardswish_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks2 *(((-1 )+ks1 )*(((-1 )+ks1 )<=(((0 )*((0 )>=((-2 )+(x0 //(4 +ks2 ))))+((-2 )+(x0 //(4 +ks2 )))*(((-2 )+(x0 //(4 +ks2 )))>(0 )))))+(((0 )*((0 )>=((-2 )+(x0 //(4 +ks2 ))))+((-2 )+(x0 //(4 +ks2 )))*(((-2 )+(x0 //(4 +ks2 )))>(0 ))))*((((0 )*((0 )>=((-2 )+(x0 //(4 +ks2 ))))+((-2 )+(x0 //(4 +ks2 )))*(((-2 )+(x0 //(4 +ks2 )))>(0 ))))<((-1 )+ks1 )))+ks1 *ks2 *x1 +(((-1 )+ks2 )*(((-1 )+ks2 )<=(((0 )*((0 )>=((-2 )+((x0 %(4 +ks2 )))))+((-2 )+((x0 %(4 +ks2 ))))*(((-2 )+((x0 %(4 +ks2 ))))>(0 )))))+(((0 )*((0 )>=((-2 )+((x0 %(4 +ks2 )))))+((-2 )+((x0 %(4 +ks2 ))))*(((-2 )+((x0 %(4 +ks2 ))))>(0 ))))*((((0 )*((0 )>=((-2 )+((x0 %(4 +ks2 )))))+((-2 )+((x0 %(4 +ks2 ))))*(((-2 )+((x0 %(4 +ks2 ))))>(0 ))))<((-1 )+ks2 )))),xmask ,eviction_policy ='evict_last')\n    tmp1 =3.0 \n    tmp2 =tmp0 +tmp1 \n    tmp3 =0.0 \n    tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n    tmp5 =6.0 \n    tmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\n    tmp7 =tmp0 *tmp6 \n    tmp8 =0.16666666666666666 \n    tmp9 =tmp7 *tmp8 \n    tl .store (out_ptr0 +(x2 ),tmp9 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,16 +4 *s1 +4 *s2 +s1 *s2 ,s0 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,1 ,16 +4 *s1 +4 *s2 +s1 *s2 ),torch .float32 )\n\n        triton_poi_fused_hardswish_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_hardswish_0 [grid (triton_poi_fused_hardswish_0_xnumel )](arg3_1 ,buf0 ,1296 ,32 ,32 ,3888 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,4 +s1 ,4 +s2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "4ce45054-8d41-4568-992b-52cc4a0742d4",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['TransformerDecoderLayer', 'ParameterList', 'AdaptiveAvgPool3d']\nimport torch\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n        self.parameter_list = nn.ParameterList([nn.Parameter(torch.randn(512, 512)) for _ in range(3)])\n        self.adaptive_avg_pool3d = nn.AdaptiveAvgPool3d((1, 1, 1))\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, sequence_length, feature_dim)\n        batch_size, sequence_length, feature_dim = x.shape\n        \n        # Reshape x to fit the TransformerDecoderLayer input shape\n        x = x.permute(1, 0, 2)  # (sequence_length, batch_size, feature_dim)\n        \n        # Pass through TransformerDecoderLayer\n        x = self.transformer_decoder_layer(x, x)\n        \n        # Reshape back to (batch_size, sequence_length, feature_dim)\n        x = x.permute(1, 0, 2)\n        \n        # Apply ParameterList\n        for param in self.parameter_list:\n            x = torch.matmul(x, param)\n        \n        # Reshape to fit AdaptiveAvgPool3d input shape\n        x = x.unsqueeze(1)  # (batch_size, 1, sequence_length, feature_dim)\n        x = x.unsqueeze(1)  # (batch_size, 1, 1, sequence_length, feature_dim)\n        \n        # Apply AdaptiveAvgPool3d\n        x = self.adaptive_avg_pool3d(x)\n        \n        # Flatten the output\n        x = x.view(batch_size, -1)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 512).cuda()  # (batch_size, sequence_length, feature_dim)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =15360 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %512 )\n    x1 =((xindex //512 )%10 )\n    x2 =xindex //5120 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +512 *x2 +1536 *x1 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 +512 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =512 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(r0_1 +512 *x0 ),None )\n    tmp7 =tl .load (in_out_ptr0 +(r0_1 +512 *x0 ),None )\n    tmp8 =tl .load (in_ptr2 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp34 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp36 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +512 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[R0_BLOCK ])\n    tmp16 =tl .broadcast_to (tmp14 ,[R0_BLOCK ])\n    tmp18 =triton_helpers .promote_to_tensor (tl .sum (tmp16 ,0 ))\n    tmp19 =tl .full ([1 ],512 ,tl .int32 )\n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =tmp18 /tmp20 \n    tmp22 =tmp14 -tmp21 \n    tmp23 =tmp22 *tmp22 \n    tmp24 =tl .broadcast_to (tmp23 ,[R0_BLOCK ])\n    tmp26 =triton_helpers .promote_to_tensor (tl .sum (tmp24 ,0 ))\n    tmp27 =tmp13 -tmp21 \n    tmp28 =512.0 \n    tmp29 =tmp26 /tmp28 \n    tmp30 =1e-05 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =libdevice .rsqrt (tmp31 )\n    tmp33 =tmp27 *tmp32 \n    tmp35 =tmp33 *tmp34 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =0.001953125 \n    tmp39 =tmp32 *tmp38 \n    tl .store (out_ptr1 +(r0_1 +512 *x0 ),tmp4 ,None )\n    tl .store (in_out_ptr0 +(r0_1 +512 *x0 ),tmp33 ,None )\n    tl .store (out_ptr4 +(r0_1 +512 *x0 ),tmp37 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp39 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10240 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %512 )\n    x1 =((xindex //512 )%10 )\n    x2 =xindex //5120 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +512 *x2 +1024 *x1 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(512 +x0 +512 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =512 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(r0_1 +512 *x0 ),None )\n    tmp7 =tl .load (in_out_ptr0 +(r0_1 +512 *x0 ),None )\n    tmp8 =tl .load (in_ptr2 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp34 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp36 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +512 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[R0_BLOCK ])\n    tmp16 =tl .broadcast_to (tmp14 ,[R0_BLOCK ])\n    tmp18 =triton_helpers .promote_to_tensor (tl .sum (tmp16 ,0 ))\n    tmp19 =tl .full ([1 ],512 ,tl .int32 )\n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =tmp18 /tmp20 \n    tmp22 =tmp14 -tmp21 \n    tmp23 =tmp22 *tmp22 \n    tmp24 =tl .broadcast_to (tmp23 ,[R0_BLOCK ])\n    tmp26 =triton_helpers .promote_to_tensor (tl .sum (tmp24 ,0 ))\n    tmp27 =tmp13 -tmp21 \n    tmp28 =512.0 \n    tmp29 =tmp26 /tmp28 \n    tmp30 =1e-05 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =libdevice .rsqrt (tmp31 )\n    tmp33 =tmp27 *tmp32 \n    tmp35 =tmp33 *tmp34 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =0.001953125 \n    tmp39 =tmp32 *tmp38 \n    tl .store (out_ptr1 +(r0_1 +512 *x0 ),tmp4 ,None )\n    tl .store (in_out_ptr0 +(r0_1 +512 *x0 ),tmp33 ,None )\n    tl .store (out_ptr4 +(r0_1 +512 *x0 ),tmp37 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp39 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_threshold_backward_4 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    x1 =(xindex %2048 )\n    tmp6 =tl .load (in_ptr1 +(x0 ),None )\n    tmp7 =tl .load (in_ptr2 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.0 \n    tmp15 =tmp10 <=tmp14 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,None )\n    tl .store (out_ptr2 +(x0 ),tmp13 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp15 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_mean_5 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    r0_numel =5120 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =_tmp2 +tmp1 \n        _tmp2 =tl .where (r0_mask ,tmp3 ,_tmp2 )\n    tmp2 =tl .sum (_tmp2 ,1 )[:,None ]\n    tmp4 =5120.0 \n    tmp5 =tmp2 /tmp4 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp5 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ,512 ),(5120 ,512 ,1 ))\n    assert_size_stride (primals_2 ,(1536 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1536 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_4 ,(512 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_5 ,(512 ,),(1 ,))\n    assert_size_stride (primals_6 ,(512 ,),(1 ,))\n    assert_size_stride (primals_7 ,(512 ,),(1 ,))\n    assert_size_stride (primals_8 ,(1536 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_9 ,(1536 ,),(1 ,))\n    assert_size_stride (primals_10 ,(512 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_11 ,(512 ,),(1 ,))\n    assert_size_stride (primals_12 ,(512 ,),(1 ,))\n    assert_size_stride (primals_13 ,(512 ,),(1 ,))\n    assert_size_stride (primals_14 ,(2048 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_15 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_16 ,(512 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_17 ,(512 ,),(1 ,))\n    assert_size_stride (primals_18 ,(512 ,),(1 ,))\n    assert_size_stride (primals_19 ,(512 ,),(1 ,))\n    assert_size_stride (primals_20 ,(512 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_21 ,(512 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_22 ,(512 ,512 ),(512 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((10 ,1536 ),(1536 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(10 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_3 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf0 )\n        del primals_3 \n        buf1 =empty_strided_cuda ((3 ,10 ,1 ,512 ),(5120 ,512 ,512 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (15360 )](buf0 ,primals_2 ,buf1 ,15360 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        del primals_2 \n\n        buf2 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf1 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),0 ),reinterpret_tensor (buf1 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),5120 ),reinterpret_tensor (buf1 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),10240 ),None ,True ,0.1 )\n        buf3 =buf2 [0 ]\n        buf4 =buf2 [1 ]\n        buf5 =buf2 [2 ]\n        buf6 =buf2 [3 ]\n        del buf2 \n        buf7 =empty_strided_cuda ((10 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf3 ,(10 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_4 ,(512 ,512 ),(1 ,512 ),0 ),out =buf7 )\n        buf8 =empty_strided_cuda ((4 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[4 ],out =buf8 )\n        buf10 =empty_strided_cuda ((10 ,1 ,512 ),(512 ,512 ,1 ),torch .bool )\n        buf14 =reinterpret_tensor (buf7 ,(10 ,1 ,512 ),(512 ,512 ,1 ),0 );del buf7 \n        buf15 =empty_strided_cuda ((10 ,1 ,512 ),(512 ,512 ,1 ),torch .float32 )\n        buf52 =empty_strided_cuda ((10 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_1 [grid (10 )](buf14 ,buf8 ,primals_1 ,primals_5 ,primals_6 ,primals_7 ,buf10 ,buf15 ,buf52 ,3 ,10 ,512 ,num_warps =4 ,num_stages =1 )\n        del primals_5 \n        del primals_7 \n        buf16 =empty_strided_cuda ((10 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (reinterpret_tensor (primals_9 ,(512 ,),(1 ,),0 ),reinterpret_tensor (buf15 ,(10 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_8 ,(512 ,512 ),(1 ,512 ),0 ),alpha =1 ,beta =1 ,out =buf16 )\n        buf17 =empty_strided_cuda ((10 ,1024 ),(1024 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(10 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_8 ,(512 ,1024 ),(1 ,512 ),262144 ),out =buf17 )\n        buf18 =empty_strided_cuda ((2 ,10 ,1 ,512 ),(5120 ,512 ,512 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_2 [grid (10240 )](buf17 ,primals_9 ,buf18 ,10240 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf17 \n        del primals_9 \n\n        buf19 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf16 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),0 ),reinterpret_tensor (buf18 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),0 ),reinterpret_tensor (buf18 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),5120 ),None ,True ,0.1 )\n        buf20 =buf19 [0 ]\n        buf21 =buf19 [1 ]\n        buf22 =buf19 [2 ]\n        buf23 =buf19 [3 ]\n        del buf19 \n        buf24 =empty_strided_cuda ((10 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf20 ,(10 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_10 ,(512 ,512 ),(1 ,512 ),0 ),out =buf24 )\n        buf26 =empty_strided_cuda ((10 ,1 ,512 ),(512 ,512 ,1 ),torch .bool )\n        buf30 =reinterpret_tensor (buf24 ,(10 ,1 ,512 ),(512 ,512 ,1 ),0 );del buf24 \n        buf31 =empty_strided_cuda ((10 ,1 ,512 ),(512 ,512 ,1 ),torch .float32 )\n        buf51 =empty_strided_cuda ((10 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_3 [grid (10 )](buf30 ,buf8 ,buf15 ,primals_11 ,primals_12 ,primals_13 ,buf26 ,buf31 ,buf51 ,1 ,10 ,512 ,num_warps =4 ,num_stages =1 )\n        del primals_11 \n        del primals_13 \n        buf32 =empty_strided_cuda ((10 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf31 ,(10 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_14 ,(512 ,2048 ),(1 ,512 ),0 ),out =buf32 )\n        buf34 =empty_strided_cuda ((10 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n        buf35 =empty_strided_cuda ((10 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .float32 )\n        buf50 =empty_strided_cuda ((10 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_4 [grid (20480 )](buf8 ,buf32 ,primals_15 ,buf34 ,buf35 ,buf50 ,2 ,20480 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf32 \n        del primals_15 \n        buf36 =empty_strided_cuda ((10 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf35 ,(10 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_16 ,(2048 ,512 ),(1 ,2048 ),0 ),out =buf36 )\n        buf38 =empty_strided_cuda ((10 ,1 ,512 ),(512 ,512 ,1 ),torch .bool )\n        buf42 =reinterpret_tensor (buf36 ,(10 ,1 ,512 ),(512 ,512 ,1 ),0 );del buf36 \n        buf43 =empty_strided_cuda ((10 ,1 ,512 ),(512 ,512 ,1 ),torch .float32 )\n        buf49 =empty_strided_cuda ((10 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_1 [grid (10 )](buf42 ,buf8 ,buf31 ,primals_17 ,primals_18 ,primals_19 ,buf38 ,buf43 ,buf49 ,3 ,10 ,512 ,num_warps =4 ,num_stages =1 )\n        del buf8 \n        del primals_17 \n        del primals_19 \n        buf44 =empty_strided_cuda ((10 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf43 ,(10 ,512 ),(512 ,1 ),0 ),primals_20 ,out =buf44 )\n        buf45 =empty_strided_cuda ((10 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf44 ,primals_21 ,out =buf45 )\n        buf46 =empty_strided_cuda ((10 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf45 ,primals_22 ,out =buf46 )\n        buf47 =empty_strided_cuda ((1 ,1 ,1 ,1 ,1 ),(1 ,1 ,1 ,1 ,1 ),torch .float32 )\n        buf48 =buf47 ;del buf47 \n\n        get_raw_stream (0 )\n        triton_red_fused_mean_5 [grid (1 )](buf48 ,buf46 ,1 ,5120 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf46 \n    return (reinterpret_tensor (buf48 ,(1 ,1 ),(1 ,1 ),0 ),primals_6 ,primals_12 ,primals_18 ,reinterpret_tensor (primals_1 ,(10 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (buf1 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),0 ),reinterpret_tensor (buf1 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),5120 ),reinterpret_tensor (buf1 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),10240 ),buf3 ,buf4 ,buf5 ,buf6 ,buf10 ,buf14 ,reinterpret_tensor (buf15 ,(10 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (buf16 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),0 ),reinterpret_tensor (buf18 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),0 ),reinterpret_tensor (buf18 ,(1 ,8 ,10 ,64 ),(512 ,64 ,512 ,1 ),5120 ),buf20 ,buf21 ,buf22 ,buf23 ,buf26 ,buf30 ,reinterpret_tensor (buf31 ,(10 ,512 ),(512 ,1 ),0 ),buf34 ,reinterpret_tensor (buf35 ,(10 ,2048 ),(2048 ,1 ),0 ),buf38 ,buf42 ,reinterpret_tensor (buf45 ,(512 ,10 ),(1 ,512 ),0 ),reinterpret_tensor (primals_22 ,(512 ,512 ),(1 ,512 ),0 ),reinterpret_tensor (buf44 ,(512 ,10 ),(1 ,512 ),0 ),reinterpret_tensor (primals_21 ,(512 ,512 ),(1 ,512 ),0 ),reinterpret_tensor (buf43 ,(512 ,10 ),(1 ,512 ),0 ),reinterpret_tensor (primals_20 ,(512 ,512 ),(1 ,512 ),0 ),buf49 ,primals_16 ,buf50 ,primals_14 ,buf51 ,primals_10 ,reinterpret_tensor (primals_8 ,(512 ,512 ),(512 ,1 ),0 ),buf52 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ,512 ),(5120 ,512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((1536 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1536 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((512 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((1536 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((1536 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((512 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((2048 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((512 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_18 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((512 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((512 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_22 =rand_strided ((512 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "4d71ee2d-7a42-42d2-8f24-7fc08d306598",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GRUCell', 'RMSNorm', 'AdaptiveLogSoftmaxWithLoss', 'AdaptiveMaxPool3d', 'CrossEntropyLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.gru_cell1 = nn.GRUCell(input_size=128, hidden_size=256)\n        self.gru_cell2 = nn.GRUCell(input_size=256, hidden_size=512)\n        self.rms_norm = nn.RMSNorm(512)\n        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d(output_size=(1, 1, 1))\n        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(in_features=512, n_classes=10, cutoffs=[2, 4])\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, seq_len, input_size)\n        batch_size, seq_len, input_size = x.size()\n        \n        # Initialize hidden states for GRUCell\n        h1 = torch.zeros(batch_size, 256).to(x.device)\n        h2 = torch.zeros(batch_size, 512).to(x.device)\n        \n        # Process sequence with GRUCell\n        for t in range(seq_len):\n            h1 = self.gru_cell1(x[:, t, :], h1)\n            h2 = self.gru_cell2(h1, h2)\n        \n        # Apply RMSNorm\n        x = self.rms_norm(h2)\n        \n        # Reshape for AdaptiveMaxPool3d\n        x = x.view(batch_size, 512, 1, 1, 1)\n        x = self.adaptive_max_pool3d(x)\n        \n        # Flatten the output\n        x = x.view(batch_size, -1)\n        \n        # Apply AdaptiveLogSoftmaxWithLoss\n        log_probs = self.adaptive_log_softmax.log_prob(x)\n        \n        # Return log probabilities\n        return log_probs\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 128).cuda()  # (batch_size, seq_len, input_size)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =256 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =512 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_mean_mul_pow_rsqrt_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =512 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp11 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp1 =tmp0 *tmp0 \n    tmp2 =tl .broadcast_to (tmp1 ,[R0_BLOCK ])\n    tmp4 =triton_helpers .promote_to_tensor (tl .sum (tmp2 ,0 ))\n    tmp5 =512.0 \n    tmp6 =tmp4 /tmp5 \n    tmp7 =1.1920928955078125e-07 \n    tmp8 =tmp6 +tmp7 \n    tmp9 =libdevice .rsqrt (tmp8 )\n    tmp10 =tmp0 *tmp9 \n    tmp12 =tmp10 *tmp11 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp9 ,None )\n    tl .store (out_ptr0 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp12 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__log_softmax_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =4 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(0 ))\n    tmp2 =tl .broadcast_to (tmp1 ,[XBLOCK ])\n    tmp3 =tl .load (in_ptr0 +(1 ))\n    tmp4 =tl .broadcast_to (tmp3 ,[XBLOCK ])\n    tmp6 =tl .load (in_ptr0 +(2 ))\n    tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ])\n    tmp9 =tl .load (in_ptr0 +(3 ))\n    tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ])\n    tmp5 =triton_helpers .maximum (tmp2 ,tmp4 )\n    tmp8 =triton_helpers .maximum (tmp5 ,tmp7 )\n    tmp11 =triton_helpers .maximum (tmp8 ,tmp10 )\n    tmp12 =tmp0 -tmp11 \n    tl .store (out_ptr0 +(x0 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__log_softmax_4 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =4 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(0 ))\n    tmp2 =tl .broadcast_to (tmp1 ,[XBLOCK ])\n    tmp4 =tl .load (in_ptr0 +(1 ))\n    tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ])\n    tmp8 =tl .load (in_ptr0 +(2 ))\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ])\n    tmp12 =tl .load (in_ptr0 +(3 ))\n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ])\n    tmp3 =tl_math .exp (tmp2 )\n    tmp6 =tl_math .exp (tmp5 )\n    tmp7 =tmp3 +tmp6 \n    tmp10 =tl_math .exp (tmp9 )\n    tmp11 =tmp7 +tmp10 \n    tmp14 =tl_math .exp (tmp13 )\n    tmp15 =tmp11 +tmp14 \n    tmp16 =tl_math .log (tmp15 )\n    tmp17 =tmp0 -tmp16 \n    tl .store (out_ptr0 +(x0 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__log_softmax_5 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .load (in_ptr0 +(1 ))\n    tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ])\n    tmp5 =tl .load (in_ptr0 +(2 ))\n    tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ])\n    tmp8 =tl .load (in_ptr0 +(3 ))\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ])\n    tmp11 =tl .load (in_ptr0 +(4 ))\n    tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ])\n    tmp14 =tl .load (in_ptr0 +(5 ))\n    tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ])\n    tmp4 =triton_helpers .maximum (tmp1 ,tmp3 )\n    tmp7 =triton_helpers .maximum (tmp4 ,tmp6 )\n    tmp10 =triton_helpers .maximum (tmp7 ,tmp9 )\n    tmp13 =triton_helpers .maximum (tmp10 ,tmp12 )\n    tmp16 =triton_helpers .maximum (tmp13 ,tmp15 )\n    tmp17 =tmp1 -tmp16 \n    tmp18 =tl_math .exp (tmp17 )\n    tmp19 =tmp3 -tmp16 \n    tmp20 =tl_math .exp (tmp19 )\n    tmp21 =tmp18 +tmp20 \n    tmp22 =tmp6 -tmp16 \n    tmp23 =tl_math .exp (tmp22 )\n    tmp24 =tmp21 +tmp23 \n    tmp25 =tmp9 -tmp16 \n    tmp26 =tl_math .exp (tmp25 )\n    tmp27 =tmp24 +tmp26 \n    tmp28 =tmp12 -tmp16 \n    tmp29 =tl_math .exp (tmp28 )\n    tmp30 =tmp27 +tmp29 \n    tmp31 =tmp15 -tmp16 \n    tmp32 =tl_math .exp (tmp31 )\n    tmp33 =tmp30 +tmp32 \n    tmp34 =tl_math .log (tmp33 )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp16 ,None )\n    tl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp34 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__log_softmax_add_copy_6 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],2 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],4 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-2 )+x0 ),tmp5 &xmask ,other =0.0 )\n    tmp7 =tl .load (in_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp5 ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =tl .load (in_ptr0 +(tl .full ([XBLOCK ],1 ,tl .int32 )),tmp5 ,eviction_policy ='evict_last',other =0.0 )\n    tmp9 =triton_helpers .maximum (tmp7 ,tmp8 )\n    tmp10 =tmp6 -tmp9 \n    tmp11 =tmp7 -tmp9 \n    tmp12 =tl_math .exp (tmp11 )\n    tmp13 =tmp8 -tmp9 \n    tmp14 =tl_math .exp (tmp13 )\n    tmp15 =tmp12 +tmp14 \n    tmp16 =tl_math .log (tmp15 )\n    tmp17 =tmp10 -tmp16 \n    tmp18 =tl .load (in_ptr1 +(tl .full ([XBLOCK ],2 ,tl .int32 )),tmp5 ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =tmp17 +tmp18 \n    tmp20 =tl .full (tmp19 .shape ,0.0 ,tmp19 .dtype )\n    tmp21 =tl .where (tmp5 ,tmp19 ,tmp20 )\n    tmp22 =tmp0 <tmp1 \n    tmp23 =tl .load (in_ptr1 +(x0 ),tmp22 &xmask ,other =0.0 )\n    tmp24 =float (\"nan\")\n    tmp25 =tl .where (tmp22 ,tmp23 ,tmp24 )\n    tmp26 =tl .where (tmp5 ,tmp21 ,tmp25 )\n    tmp27 =tmp0 >=tmp3 \n    tmp28 =tl .load (in_ptr2 +((-4 )+x0 ),tmp27 &xmask ,other =0.0 )\n    tmp29 =tl .load (in_ptr3 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp27 ,eviction_policy ='evict_last',other =0.0 )\n    tmp30 =tmp28 -tmp29 \n    tmp31 =tl .load (in_ptr4 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp27 ,eviction_policy ='evict_last',other =0.0 )\n    tmp32 =tmp30 -tmp31 \n    tmp33 =tl .load (in_ptr1 +(tl .full ([XBLOCK ],3 ,tl .int32 )),tmp27 ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tmp32 +tmp33 \n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =tl .where (tmp27 ,tmp36 ,tmp26 )\n    tl .store (in_out_ptr0 +(x0 ),tmp37 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ,128 ),(1280 ,128 ,1 ))\n    assert_size_stride (primals_2 ,(768 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_3 ,(768 ,256 ),(256 ,1 ))\n    assert_size_stride (primals_4 ,(768 ,),(1 ,))\n    assert_size_stride (primals_5 ,(768 ,),(1 ,))\n    assert_size_stride (primals_6 ,(1536 ,256 ),(256 ,1 ))\n    assert_size_stride (primals_7 ,(1536 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_8 ,(1536 ,),(1 ,))\n    assert_size_stride (primals_9 ,(1536 ,),(1 ,))\n    assert_size_stride (primals_10 ,(512 ,),(1 ,))\n    assert_size_stride (primals_11 ,(4 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_12 ,(128 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_13 ,(2 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_14 ,(32 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_15 ,(6 ,32 ),(32 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_0 [grid (256 )](buf0 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_1 [grid (512 )](buf1 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,768 ),(768 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),0 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,768 ),(768 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf0 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf3 )\n\n        buf4 =torch .ops .aten ._thnn_fused_gru_cell .default (buf2 ,buf3 ,buf0 ,primals_4 ,primals_5 )\n        buf5 =buf4 [0 ]\n        buf6 =buf4 [1 ]\n        del buf4 \n        buf7 =empty_strided_cuda ((1 ,1536 ),(1536 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf5 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf7 )\n        buf8 =empty_strided_cuda ((1 ,1536 ),(1536 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf1 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf8 )\n\n        buf9 =torch .ops .aten ._thnn_fused_gru_cell .default (buf7 ,buf8 ,buf1 ,primals_8 ,primals_9 )\n        buf10 =buf9 [0 ]\n        buf11 =buf9 [1 ]\n        del buf9 \n        buf12 =buf3 ;del buf3 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),128 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf12 )\n        buf13 =buf2 ;del buf2 \n\n        extern_kernels .mm (buf5 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf13 )\n\n        buf14 =torch .ops .aten ._thnn_fused_gru_cell .default (buf12 ,buf13 ,buf5 ,primals_4 ,primals_5 )\n        buf15 =buf14 [0 ]\n        buf16 =buf14 [1 ]\n        del buf14 \n        buf17 =buf8 ;del buf8 \n\n        extern_kernels .mm (buf15 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf17 )\n        buf18 =buf7 ;del buf7 \n\n        extern_kernels .mm (buf10 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf18 )\n\n        buf19 =torch .ops .aten ._thnn_fused_gru_cell .default (buf17 ,buf18 ,buf10 ,primals_8 ,primals_9 )\n        buf20 =buf19 [0 ]\n        buf21 =buf19 [1 ]\n        del buf19 \n        buf22 =buf13 ;del buf13 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),256 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf22 )\n        buf23 =buf12 ;del buf12 \n\n        extern_kernels .mm (buf15 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf23 )\n\n        buf24 =torch .ops .aten ._thnn_fused_gru_cell .default (buf22 ,buf23 ,buf15 ,primals_4 ,primals_5 )\n        buf25 =buf24 [0 ]\n        buf26 =buf24 [1 ]\n        del buf24 \n        buf27 =buf18 ;del buf18 \n\n        extern_kernels .mm (buf25 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf27 )\n        buf28 =buf17 ;del buf17 \n\n        extern_kernels .mm (buf20 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf28 )\n\n        buf29 =torch .ops .aten ._thnn_fused_gru_cell .default (buf27 ,buf28 ,buf20 ,primals_8 ,primals_9 )\n        buf30 =buf29 [0 ]\n        buf31 =buf29 [1 ]\n        del buf29 \n        buf32 =buf23 ;del buf23 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),384 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf32 )\n        buf33 =buf22 ;del buf22 \n\n        extern_kernels .mm (buf25 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf33 )\n\n        buf34 =torch .ops .aten ._thnn_fused_gru_cell .default (buf32 ,buf33 ,buf25 ,primals_4 ,primals_5 )\n        buf35 =buf34 [0 ]\n        buf36 =buf34 [1 ]\n        del buf34 \n        buf37 =buf28 ;del buf28 \n\n        extern_kernels .mm (buf35 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf37 )\n        buf38 =buf27 ;del buf27 \n\n        extern_kernels .mm (buf30 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf38 )\n\n        buf39 =torch .ops .aten ._thnn_fused_gru_cell .default (buf37 ,buf38 ,buf30 ,primals_8 ,primals_9 )\n        buf40 =buf39 [0 ]\n        buf41 =buf39 [1 ]\n        del buf39 \n        buf42 =buf33 ;del buf33 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),512 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf42 )\n        buf43 =buf32 ;del buf32 \n\n        extern_kernels .mm (buf35 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf43 )\n\n        buf44 =torch .ops .aten ._thnn_fused_gru_cell .default (buf42 ,buf43 ,buf35 ,primals_4 ,primals_5 )\n        buf45 =buf44 [0 ]\n        buf46 =buf44 [1 ]\n        del buf44 \n        buf47 =buf38 ;del buf38 \n\n        extern_kernels .mm (buf45 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf47 )\n        buf48 =buf37 ;del buf37 \n\n        extern_kernels .mm (buf40 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf48 )\n\n        buf49 =torch .ops .aten ._thnn_fused_gru_cell .default (buf47 ,buf48 ,buf40 ,primals_8 ,primals_9 )\n        buf50 =buf49 [0 ]\n        buf51 =buf49 [1 ]\n        del buf49 \n        buf52 =buf43 ;del buf43 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),640 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf52 )\n        buf53 =buf42 ;del buf42 \n\n        extern_kernels .mm (buf45 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf53 )\n\n        buf54 =torch .ops .aten ._thnn_fused_gru_cell .default (buf52 ,buf53 ,buf45 ,primals_4 ,primals_5 )\n        buf55 =buf54 [0 ]\n        buf56 =buf54 [1 ]\n        del buf54 \n        buf57 =buf48 ;del buf48 \n\n        extern_kernels .mm (buf55 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf57 )\n        buf58 =buf47 ;del buf47 \n\n        extern_kernels .mm (buf50 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf58 )\n\n        buf59 =torch .ops .aten ._thnn_fused_gru_cell .default (buf57 ,buf58 ,buf50 ,primals_8 ,primals_9 )\n        buf60 =buf59 [0 ]\n        buf61 =buf59 [1 ]\n        del buf59 \n        buf62 =buf53 ;del buf53 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),768 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf62 )\n        buf63 =buf52 ;del buf52 \n\n        extern_kernels .mm (buf55 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf63 )\n\n        buf64 =torch .ops .aten ._thnn_fused_gru_cell .default (buf62 ,buf63 ,buf55 ,primals_4 ,primals_5 )\n        buf65 =buf64 [0 ]\n        buf66 =buf64 [1 ]\n        del buf64 \n        buf67 =buf58 ;del buf58 \n\n        extern_kernels .mm (buf65 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf67 )\n        buf68 =buf57 ;del buf57 \n\n        extern_kernels .mm (buf60 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf68 )\n\n        buf69 =torch .ops .aten ._thnn_fused_gru_cell .default (buf67 ,buf68 ,buf60 ,primals_8 ,primals_9 )\n        buf70 =buf69 [0 ]\n        buf71 =buf69 [1 ]\n        del buf69 \n        buf72 =buf63 ;del buf63 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),896 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf72 )\n        buf73 =buf62 ;del buf62 \n\n        extern_kernels .mm (buf65 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf73 )\n\n        buf74 =torch .ops .aten ._thnn_fused_gru_cell .default (buf72 ,buf73 ,buf65 ,primals_4 ,primals_5 )\n        buf75 =buf74 [0 ]\n        buf76 =buf74 [1 ]\n        del buf74 \n        buf77 =buf68 ;del buf68 \n\n        extern_kernels .mm (buf75 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf77 )\n        buf78 =buf67 ;del buf67 \n\n        extern_kernels .mm (buf70 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf78 )\n\n        buf79 =torch .ops .aten ._thnn_fused_gru_cell .default (buf77 ,buf78 ,buf70 ,primals_8 ,primals_9 )\n        buf80 =buf79 [0 ]\n        buf81 =buf79 [1 ]\n        del buf79 \n        buf82 =buf73 ;del buf73 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),1024 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf82 )\n        buf83 =buf72 ;del buf72 \n\n        extern_kernels .mm (buf75 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf83 )\n\n        buf84 =torch .ops .aten ._thnn_fused_gru_cell .default (buf82 ,buf83 ,buf75 ,primals_4 ,primals_5 )\n        buf85 =buf84 [0 ]\n        buf86 =buf84 [1 ]\n        del buf84 \n        buf87 =buf78 ;del buf78 \n\n        extern_kernels .mm (buf85 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf87 )\n        buf88 =buf77 ;del buf77 \n\n        extern_kernels .mm (buf80 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf88 )\n\n        buf89 =torch .ops .aten ._thnn_fused_gru_cell .default (buf87 ,buf88 ,buf80 ,primals_8 ,primals_9 )\n        buf90 =buf89 [0 ]\n        buf91 =buf89 [1 ]\n        del buf89 \n        buf92 =buf83 ;del buf83 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),1152 ),reinterpret_tensor (primals_2 ,(128 ,768 ),(1 ,128 ),0 ),out =buf92 )\n        del primals_2 \n        buf93 =buf82 ;del buf82 \n\n        extern_kernels .mm (buf85 ,reinterpret_tensor (primals_3 ,(256 ,768 ),(1 ,256 ),0 ),out =buf93 )\n\n        buf94 =torch .ops .aten ._thnn_fused_gru_cell .default (buf92 ,buf93 ,buf85 ,primals_4 ,primals_5 )\n        del buf92 \n        del buf93 \n        del primals_4 \n        del primals_5 \n        buf95 =buf94 [0 ]\n        buf96 =buf94 [1 ]\n        del buf94 \n        buf97 =buf88 ;del buf88 \n\n        extern_kernels .mm (buf95 ,reinterpret_tensor (primals_6 ,(256 ,1536 ),(1 ,256 ),0 ),out =buf97 )\n        buf98 =buf87 ;del buf87 \n\n        extern_kernels .mm (buf90 ,reinterpret_tensor (primals_7 ,(512 ,1536 ),(1 ,512 ),0 ),out =buf98 )\n\n        buf99 =torch .ops .aten ._thnn_fused_gru_cell .default (buf97 ,buf98 ,buf90 ,primals_8 ,primals_9 )\n        del buf97 \n        del buf98 \n        del primals_8 \n        del primals_9 \n        buf100 =buf99 [0 ]\n        buf101 =buf99 [1 ]\n        del buf99 \n        buf102 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf103 =buf102 ;del buf102 \n        buf104 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_mean_mul_pow_rsqrt_2 [grid (1 )](buf103 ,buf100 ,primals_10 ,buf104 ,1 ,512 ,num_warps =4 ,num_stages =1 )\n\n        buf105 =torch .ops .aten .adaptive_max_pool3d .default (reinterpret_tensor (buf104 ,(1 ,512 ,1 ,1 ,1 ),(0 ,1 ,0 ,0 ,0 ),0 ),[1 ,1 ,1 ])\n        buf106 =buf105 [0 ]\n        buf107 =buf105 [1 ]\n        del buf105 \n        buf108 =empty_strided_cuda ((1 ,4 ),(4 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf106 ,(1 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_11 ,(512 ,4 ),(1 ,512 ),0 ),out =buf108 )\n        buf110 =empty_strided_cuda ((1 ,4 ),(4 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__log_softmax_3 [grid (4 )](buf108 ,buf110 ,4 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf111 =empty_strided_cuda ((1 ,4 ),(4 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__log_softmax_4 [grid (4 )](buf110 ,buf111 ,4 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf110 \n        buf112 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf106 ,(1 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_12 ,(512 ,128 ),(1 ,512 ),0 ),out =buf112 )\n        buf113 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf112 ,reinterpret_tensor (primals_13 ,(128 ,2 ),(1 ,128 ),0 ),out =buf113 )\n        buf115 =empty_strided_cuda ((1 ,32 ),(32 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf106 ,(1 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_14 ,(512 ,32 ),(1 ,512 ),0 ),out =buf115 )\n        buf116 =empty_strided_cuda ((1 ,6 ),(6 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf115 ,reinterpret_tensor (primals_15 ,(32 ,6 ),(1 ,32 ),0 ),out =buf116 )\n        buf117 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf118 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__log_softmax_5 [grid (1 )](buf116 ,buf117 ,buf118 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        buf114 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n        buf119 =buf114 ;del buf114 \n\n        get_raw_stream (0 )\n        triton_poi_fused__log_softmax_add_copy_6 [grid (10 )](buf119 ,buf113 ,buf111 ,buf116 ,buf117 ,buf118 ,10 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf111 \n    return (buf119 ,primals_10 ,buf0 ,buf1 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),0 ),buf5 ,buf6 ,buf10 ,buf11 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),128 ),buf15 ,buf16 ,buf20 ,buf21 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),256 ),buf25 ,buf26 ,buf30 ,buf31 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),384 ),buf35 ,buf36 ,buf40 ,buf41 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),512 ),buf45 ,buf46 ,buf50 ,buf51 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),640 ),buf55 ,buf56 ,buf60 ,buf61 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),768 ),buf65 ,buf66 ,buf70 ,buf71 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),896 ),buf75 ,buf76 ,buf80 ,buf81 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),1024 ),buf85 ,buf86 ,buf90 ,buf91 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),1152 ),buf95 ,buf96 ,buf100 ,buf101 ,buf103 ,reinterpret_tensor (buf104 ,(1 ,512 ,1 ,1 ,1 ),(512 ,1 ,1 ,1 ,1 ),0 ),buf107 ,reinterpret_tensor (buf106 ,(1 ,512 ),(512 ,1 ),0 ),buf108 ,buf112 ,buf113 ,buf115 ,buf116 ,buf117 ,buf118 ,primals_15 ,primals_14 ,primals_13 ,primals_12 ,primals_11 ,primals_7 ,primals_6 ,primals_3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ,128 ),(1280 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((768 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((768 ,256 ),(256 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((768 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((768 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((1536 ,256 ),(256 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((1536 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((1536 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((1536 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((4 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((128 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((2 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((32 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((6 ,32 ),(32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "4f68516b-e3dd-4a66-99ed-8746b5a6619f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LPPool1d', 'TripletMarginWithDistanceLoss', 'Softplus', 'LogSigmoid']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lp_pool1d = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.softplus = nn.Softplus()\n        self.log_sigmoid = nn.LogSigmoid()\n        self.triplet_loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n\n    def forward(self, x):\n        # Ensure the input is at least 3D (batch, channels, length)\n        if x.dim() == 2:\n            x = x.unsqueeze(1)  # Add channel dimension if missing\n        \n        # Apply LPPool1d\n        x = self.lp_pool1d(x)\n        \n        # Apply Softplus\n        x = self.softplus(x)\n        \n        # Apply LogSigmoid\n        x = self.log_sigmoid(x)\n        \n        # For TripletMarginWithDistanceLoss, we need three inputs: anchor, positive, and negative\n        # Here, we use the same input for anchor and positive, and a shifted version for negative\n        anchor = x\n        positive = x\n        negative = torch.roll(x, shifts=1, dims=2)  # Shift the input to create a negative example\n        \n        # Compute the triplet loss\n        loss = self.triplet_loss(anchor, positive, negative)\n        \n        # Return the loss and the processed tensor\n        return loss, x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 64).cuda()  # Example input: (batch_size, channels, length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_log_sigmoid_forward_mul_pow_relu_sign_softplus_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(1 +2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(2 +2 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tmp0 *tmp0 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tmp3 +tmp1 \n    tmp6 =tmp5 *tmp5 \n    tmp7 =tmp6 +tmp4 \n    tmp8 =0.3333333333333333 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =tl .full ([1 ],0 ,tl .int32 )\n    tmp11 =tmp10 <tmp9 \n    tmp12 =tmp11 .to (tl .int8 )\n    tmp13 =tmp9 <tmp10 \n    tmp14 =tmp13 .to (tl .int8 )\n    tmp15 =tmp12 -tmp14 \n    tmp16 =tmp15 .to (tmp9 .dtype )\n    tmp17 =tl_math .abs (tmp9 )\n    tmp18 =triton_helpers .maximum (tmp10 ,tmp17 )\n    tmp19 =tmp16 *tmp18 \n    tmp20 =3.0 \n    tmp21 =tmp19 *tmp20 \n    tmp22 =libdevice .sqrt (tmp21 )\n    tmp23 =1.0 \n    tmp24 =tmp22 *tmp23 \n    tmp25 =20.0 \n    tmp26 =tmp24 >tmp25 \n    tmp27 =tl_math .exp (tmp24 )\n    tmp28 =libdevice .log1p (tmp27 )\n    tmp29 =tmp28 *tmp23 \n    tmp30 =tl .where (tmp26 ,tmp22 ,tmp29 )\n    tmp31 =0.0 \n    tmp32 =triton_helpers .minimum (tmp31 ,tmp30 )\n    tmp33 =tl_math .abs (tmp30 )\n    tmp34 =-tmp33 \n    tmp35 =tl_math .exp (tmp34 )\n    tmp36 =libdevice .log1p (tmp35 )\n    tmp37 =tmp32 -tmp36 \n    tl .store (out_ptr0 +(x0 +x1 +x1 *(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))),tmp37 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_norm_roll_sub_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp6 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp14 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +x0 +x0 *(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tl .device_assert ((((r0_1 +((triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))%(1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 )))))%(1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))))<triton_helpers .div_floor_integer ((-1 )+ks0 ,2 ))|~(r0_mask ),\"index out of bounds: ((r0_1 + ((triton_helpers.div_floor_integer((-3) + ks0,  2)) % (1 + (triton_helpers.div_floor_integer((-3) + ks0,  2))))) % (1 + (triton_helpers.div_floor_integer((-3) + ks0,  2)))) < triton_helpers.div_floor_integer((-1) + ks0,  2)\")\n        tmp9 =tl .load (in_ptr0 +(x0 +x0 *(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))+(((r0_1 +((triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))%(1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 )))))%(1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 )))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tmp0 -tmp0 \n        tmp2 =1e-06 \n        tmp3 =tmp1 +tmp2 \n        tmp4 =tmp3 *tmp3 \n        tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n        tmp7 =_tmp6 +tmp5 \n        _tmp6 =tl .where (r0_mask &xmask ,tmp7 ,_tmp6 )\n        tmp10 =tmp0 -tmp9 \n        tmp11 =tmp10 +tmp2 \n        tmp12 =tmp11 *tmp11 \n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =_tmp14 +tmp13 \n        _tmp14 =tl .where (r0_mask &xmask ,tmp15 ,_tmp14 )\n    tmp6 =tl .sum (_tmp6 ,1 )[:,None ]\n    tmp14 =tl .sum (_tmp14 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp6 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_mean_norm_sub_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp10 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =libdevice .sqrt (tmp0 )\n        tmp2 =1.0 \n        tmp3 =tmp1 +tmp2 \n        tmp5 =libdevice .sqrt (tmp4 )\n        tmp6 =tmp3 -tmp5 \n        tmp7 =0.0 \n        tmp8 =triton_helpers .maximum (tmp6 ,tmp7 )\n        tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n        tmp11 =_tmp10 +tmp9 \n        _tmp10 =tl .where (r0_mask ,tmp11 ,_tmp10 )\n    tmp10 =tl .sum (_tmp10 ,1 )[:,None ]\n    tmp12 =ks0 \n    tmp13 =tmp12 .to (tl .float32 )\n    tmp14 =tmp10 /tmp13 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp14 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        ((-1 )+s1 )//2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,((-1 )+s1 )//2 ),(s0 +s0 *(((-3 )+s1 )//2 ),1 +(((-3 )+s1 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_abs_log_sigmoid_forward_mul_pow_relu_sign_softplus_0_xnumel =s0 *(((-1 )+s1 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_log_sigmoid_forward_mul_pow_relu_sign_softplus_0 [grid (triton_poi_fused_abs_log_sigmoid_forward_mul_pow_relu_sign_softplus_0_xnumel )](arg2_1 ,buf0 ,31 ,64 ,310 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        buf1 =empty_strided_cuda ((1 ,s0 ),(s0 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,s0 ),(s0 ,1 ),torch .float32 )\n\n        ((-1 )+s1 )//2 \n        get_raw_stream (0 )\n        triton_red_fused_add_norm_roll_sub_1 [grid (s0 )](buf0 ,buf1 ,buf2 ,64 ,10 ,31 ,XBLOCK =1 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((),(),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_mean_norm_sub_2 [grid (1 )](buf4 ,buf1 ,buf2 ,10 ,1 ,10 ,XBLOCK =1 ,R0_BLOCK =16 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n        del buf2 \n    return (buf4 ,buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,10 ,64 ),(640 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "4fab54a9-7daf-4b68-8e89-91387651e90c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LogSigmoid', 'MaxUnpool2d', 'GELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.max_unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.max_unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.gelu1 = nn.GELU()\n        self.gelu2 = nn.GELU()\n        self.log_sigmoid = nn.LogSigmoid()\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        \n        # Apply GELU activation\n        x = self.gelu1(x)\n        \n        # Perform max pooling to get indices for unpooling\n        x, indices1 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n        \n        # Apply GELU activation again\n        x = self.gelu2(x)\n        \n        # Perform max pooling to get indices for unpooling\n        x, indices2 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n        \n        # Apply MaxUnpool2d to unpool the tensor\n        x = self.max_unpool1(x, indices2)\n        x = self.max_unpool2(x, indices1)\n        \n        # Apply LogSigmoid activation\n        x = self.log_sigmoid(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_gelu_max_pool2d_with_indices_0 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp20 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp30 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.5 \n    tmp2 =tmp0 *tmp1 \n    tmp3 =0.7071067811865476 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .erf (tmp4 )\n    tmp6 =1.0 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =tmp2 *tmp7 \n    tmp10 =tmp9 *tmp1 \n    tmp11 =tmp9 *tmp3 \n    tmp12 =libdevice .erf (tmp11 )\n    tmp13 =tmp12 +tmp6 \n    tmp14 =tmp10 *tmp13 \n    tmp15 =tmp14 >tmp8 \n    tmp16 =tl .full ([1 ],1 ,tl .int8 )\n    tmp17 =tl .full ([1 ],0 ,tl .int8 )\n    tmp18 =tl .where (tmp15 ,tmp16 ,tmp17 )\n    tmp19 =triton_helpers .maximum (tmp14 ,tmp8 )\n    tmp21 =tmp20 *tmp1 \n    tmp22 =tmp20 *tmp3 \n    tmp23 =libdevice .erf (tmp22 )\n    tmp24 =tmp23 +tmp6 \n    tmp25 =tmp21 *tmp24 \n    tmp26 =tmp25 >tmp19 \n    tmp27 =tl .full ([1 ],2 ,tl .int8 )\n    tmp28 =tl .where (tmp26 ,tmp27 ,tmp18 )\n    tmp29 =triton_helpers .maximum (tmp25 ,tmp19 )\n    tmp31 =tmp30 *tmp1 \n    tmp32 =tmp30 *tmp3 \n    tmp33 =libdevice .erf (tmp32 )\n    tmp34 =tmp33 +tmp6 \n    tmp35 =tmp31 *tmp34 \n    tmp36 =tmp35 >tmp29 \n    tmp37 =tl .full ([1 ],3 ,tl .int8 )\n    tmp38 =tl .where (tmp36 ,tmp37 ,tmp28 )\n    tmp39 =triton_helpers .maximum (tmp35 ,tmp29 )\n    tmp40 =tmp39 *tmp1 \n    tmp41 =tmp39 *tmp3 \n    tmp42 =libdevice .erf (tmp41 )\n    tmp43 =tmp42 +tmp6 \n    tmp44 =tmp40 *tmp43 \n    tl .store (out_ptr0 +(x3 ),tmp38 ,xmask )\n    tl .store (in_out_ptr0 +(x3 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_gelu_max_pool2d_with_indices_max_unpool2d_2 (in_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks3 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks3 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(ks3 +2 *x0 +2 *ks3 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr0 +(1 +ks3 +2 *x0 +2 *ks3 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp35 =tl .load (in_ptr0 +(2 *((x3 %ks0 ))+2 *ks3 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp36 =tl .load (in_ptr0 +(1 +2 *((x3 %ks0 ))+2 *ks3 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp38 =tl .load (in_ptr0 +(ks3 +2 *((x3 %ks0 ))+2 *ks3 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp40 =tl .load (in_ptr0 +(1 +ks3 +2 *((x3 %ks0 ))+2 *ks3 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp8 =tmp7 >tmp6 \n    tmp9 =tl .full ([1 ],2 ,tl .int8 )\n    tmp10 =tl .where (tmp8 ,tmp9 ,tmp5 )\n    tmp11 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp13 =tmp12 >tmp11 \n    tmp14 =tl .full ([1 ],3 ,tl .int8 )\n    tmp15 =tl .where (tmp13 ,tmp14 ,tmp10 )\n    triton_helpers .maximum (tmp12 ,tmp11 )\n    tmp17 =tl .full ([1 ],2 ,tl .int32 )\n    tmp18 =tl .where ((tmp15 <0 )!=(tmp17 <0 ),tl .where (tmp15 %tmp17 !=0 ,tmp15 //tmp17 -1 ,tmp15 //tmp17 ),tmp15 //tmp17 )\n    tmp19 =tmp18 *tmp17 \n    tmp20 =tmp15 -tmp19 \n    tmp21 =2 *x1 \n    tmp22 =tmp21 +tmp18 \n    tmp23 =2 *x0 \n    tmp24 =tmp23 +tmp20 \n    tmp25 =ks3 \n    tmp26 =tmp22 *tmp25 \n    tmp27 =tmp26 +tmp24 \n    tmp28 =4 *ks0 *ks1 *x2 \n    tmp29 =tmp27 +tmp28 \n    tmp30 =4 *ks0 *ks1 *ks5 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =tmp29 <0 \n    tmp33 =tl .where (tmp32 ,tmp31 ,tmp29 )\n    tl .device_assert (((0 <=tmp33 )&(tmp33 <4 *ks5 *(ks6 //4 )*(ks7 //4 )))|~(xmask ),\"index out of bounds: 0 <= tmp33 < 4*ks5*(ks6 // 4)*(ks7 // 4)\")\n    tmp37 =triton_helpers .maximum (tmp36 ,tmp35 )\n    tmp39 =triton_helpers .maximum (tmp38 ,tmp37 )\n    tmp41 =triton_helpers .maximum (tmp40 ,tmp39 )\n    tl .store (out_ptr1 +(tl .broadcast_to ((tmp33 %(4 *ks0 *ks1 *ks5 )),[XBLOCK ])),tmp41 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_3 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_4 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp19 =tl .load (in_ptr1 +(2 *ks4 *((((2 *ks4 *(((x0 //(2 *ks4 ))%(2 *ks5 )))+((x0 %(2 *ks4 ))))//(2 *ks4 ))%(2 *ks5 )))+4 *ks4 *ks5 *((((2 *ks4 *(((x0 //(2 *ks4 ))%(2 *ks5 )))+4 *ks4 *ks5 *(((x0 //(4 *ks4 *ks5 ))%ks6 ))+((x0 %(2 *ks4 ))))//(4 *ks4 *ks5 ))%ks6 ))+((((x0 %(2 *ks4 )))%(2 *ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([1 ],2 ,tl .int32 )\n    tmp2 =tl .where ((tmp0 <0 )!=(tmp1 <0 ),tl .where (tmp0 %tmp1 !=0 ,tmp0 //tmp1 -1 ,tmp0 //tmp1 ),tmp0 //tmp1 )\n    tmp3 =tmp2 *tmp1 \n    tmp4 =tmp0 -tmp3 \n    tmp5 =2 *(((x0 //ks0 )%ks1 ))\n    tmp6 =tmp5 +tmp2 \n    tmp7 =2 *((x0 %ks0 ))\n    tmp8 =tmp7 +tmp4 \n    tmp9 =ks2 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =tmp10 +tmp8 \n    tmp12 =16 *ks4 *ks5 *(x0 //ks3 )\n    tmp13 =tmp11 +tmp12 \n    tmp14 =16 *ks4 *ks5 *ks6 \n    tmp15 =tmp13 +tmp14 \n    tmp16 =tmp13 <0 \n    tmp17 =tl .where (tmp16 ,tmp15 ,tmp13 )\n    tl .device_assert (((0 <=tmp17 )&(tmp17 <16 *ks6 *(ks2 //4 )*(ks7 //4 )))|~(xmask ),\"index out of bounds: 0 <= tmp17 < 16*ks6*(ks2 // 4)*(ks7 // 4)\")\n    tl .store (out_ptr0 +(tl .broadcast_to ((tmp17 %(16 *ks4 *ks5 *ks6 )),[XBLOCK ])),tmp19 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_log_sigmoid_forward_5 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +4 *ks3 *((((x0 +4 *ks3 *x1 )//(4 *ks3 ))%(4 *ks4 )))+16 *ks3 *ks4 *((((x0 +4 *ks3 *x1 +16 *ks3 *ks4 *x2 )//(16 *ks3 *ks4 ))%ks5 ))),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(x0 +4 *ks3 *((((x0 +4 *ks3 *x1 )//ks0 )%ks1 ))+16 *ks3 *ks4 *((((x0 +4 *ks3 *x1 +16 *ks3 *ks4 *x2 )//ks2 )%ks5 ))),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.0 \n    tmp2 =triton_helpers .minimum (tmp1 ,tmp0 )\n    tmp4 =tl_math .abs (tmp3 )\n    tmp5 =-tmp4 \n    tmp6 =tl_math .exp (tmp5 )\n    tmp7 =libdevice .log1p (tmp6 )\n    tmp8 =tmp2 -tmp7 \n    tl .store (out_ptr0 +(x3 ),tmp8 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //2 \n        s1 //2 \n        (s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .int8 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n        buf3 =buf1 ;del buf1 \n\n        triton_poi_fused_gelu_max_pool2d_with_indices_0_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_gelu_max_pool2d_with_indices_0 [grid (triton_poi_fused_gelu_max_pool2d_with_indices_0_xnumel )](buf3 ,arg3_1 ,buf0 ,32 ,32 ,1024 ,64 ,64 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf5 =empty_strided_cuda ((1 ,s0 ,2 *(s1 //4 ),2 *(s2 //4 )),(4 *s0 *(s1 //4 )*(s2 //4 ),4 *(s1 //4 )*(s2 //4 ),2 *(s2 //4 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_1_xnumel =4 *s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_1 [grid (triton_poi_fused_max_unpool2d_1_xnumel )](buf5 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        s2 //4 \n        s1 //4 \n        (s1 //4 )*(s2 //4 )\n\n        triton_poi_fused_gelu_max_pool2d_with_indices_max_unpool2d_2_xnumel =s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_gelu_max_pool2d_with_indices_max_unpool2d_2 [grid (triton_poi_fused_gelu_max_pool2d_with_indices_max_unpool2d_2_xnumel )](buf3 ,buf5 ,16 ,16 ,256 ,32 ,32 ,3 ,64 ,64 ,768 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        buf7 =empty_strided_cuda ((1 ,s0 ,4 *(s1 //4 ),4 *(s2 //4 )),(16 *s0 *(s1 //4 )*(s2 //4 ),16 *(s1 //4 )*(s2 //4 ),4 *(s2 //4 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_3_xnumel =16 *s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_3 [grid (triton_poi_fused_max_unpool2d_3_xnumel )](buf7 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool2d_4_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_4 [grid (triton_poi_fused_max_unpool2d_4_xnumel )](buf0 ,buf5 ,buf7 ,32 ,32 ,64 ,1024 ,16 ,16 ,3 ,64 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        del buf5 \n        4 *(s2 //4 )\n        4 *(s1 //4 )\n        16 *(s1 //4 )*(s2 //4 )\n        buf9 =empty_strided_cuda ((1 ,s0 ,4 *(s1 //4 ),4 *(s2 //4 )),(16 *s0 *(s1 //4 )*(s2 //4 ),16 *(s1 //4 )*(s2 //4 ),4 *(s2 //4 ),1 ),torch .float32 )\n\n        triton_poi_fused_log_sigmoid_forward_5_xnumel =16 *s0 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_log_sigmoid_forward_5 [grid (triton_poi_fused_log_sigmoid_forward_5_xnumel )](buf7 ,buf9 ,64 ,64 ,4096 ,16 ,16 ,3 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf7 \n    return (buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "5010a5ee-2afc-4008-89c1-c71b41a7d3f9",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AvgPool2d', 'Softplus', 'Fold']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.softplus = nn.Softplus()\n        self.fold = nn.Fold(output_size=(8, 8), kernel_size=(2, 2), stride=(2, 2))\n\n    def forward(self, x):\n        # Apply the first AvgPool2d\n        x = self.avgpool1(x)\n        \n        # Apply the Softplus activation\n        x = self.softplus(x)\n        \n        # Apply the second AvgPool2d\n        x = self.avgpool2(x)\n        \n        # Reshape the tensor to fit the Fold module\n        batch_size = x.size(0)\n        channels = x.size(1)\n        x = x.view(batch_size, channels * 4, -1)  # Reshape to (batch_size, channels * 4, height * width)\n        \n        # Apply the Fold operation\n        x = self.fold(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input with shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_softplus_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =1.0 \n    tmp10 =tmp8 *tmp9 \n    tmp11 =20.0 \n    tmp12 =tmp10 >tmp11 \n    tmp13 =tl_math .exp (tmp10 )\n    tmp14 =libdevice .log1p (tmp13 )\n    tmp15 =tmp14 *tmp9 \n    tmp16 =tl .where (tmp12 ,tmp8 ,tmp15 )\n    tl .store (out_ptr0 +(x3 ),tmp16 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_col2im_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_col2im_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    yoffset =(tl .program_id (1 )+tl .program_id (2 )*tl .num_programs (1 ))*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x3 =(xindex %4 )\n    x4 =xindex //4 \n    y0 =(yindex %2 )\n    y1 =((yindex //2 )%2 )\n    y2 =yindex //4 \n    tmp0 =tl .load (in_ptr0 +(2 *(((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))%(ks4 //4 )))+2 *ks0 *((((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))//(ks4 //4 ))%(ks3 //4 )))+ks0 *ks1 *((((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))//((ks3 //4 )*(ks4 //4 )))%ks2 ))),xmask &ymask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *(((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))%(ks4 //4 )))+2 *ks0 *((((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))//(ks4 //4 ))%(ks3 //4 )))+ks0 *ks1 *((((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))//((ks3 //4 )*(ks4 //4 )))%ks2 ))),xmask &ymask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks0 +2 *(((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))%(ks4 //4 )))+2 *ks0 *((((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))//(ks4 //4 ))%(ks3 //4 )))+ks0 *ks1 *((((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))//((ks3 //4 )*(ks4 //4 )))%ks2 ))),xmask &ymask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +ks0 +2 *(((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))%(ks4 //4 )))+2 *ks0 *((((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))//(ks4 //4 ))%(ks3 //4 )))+ks0 *ks1 *((((x3 +4 *x4 +y0 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+2 *y1 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 ))+4 *y2 *(triton_helpers .div_floor_integer ((ks3 //4 )*(ks4 //4 ),4 )))//((ks3 //4 )*(ks4 //4 )))%ks2 ))),xmask &ymask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tl .atomic_add (out_ptr0 +(y0 +2 *x3 +8 *y1 +16 *x4 +64 *y2 ),tmp8 ,xmask &ymask ,sem ='relaxed')\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //2 \n        s1 //2 \n        (s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_softplus_0_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_softplus_0 [grid (triton_poi_fused_avg_pool2d_softplus_0_xnumel )](arg3_1 ,buf0 ,16 ,16 ,256 ,32 ,32 ,768 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((1 ,s0 ,8 ,8 ),(64 *s0 ,64 ,8 ,1 ),torch .float32 )\n\n        triton_poi_fused_col2im_1_xnumel =64 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_col2im_1 [grid (triton_poi_fused_col2im_1_xnumel )](buf1 ,192 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_col2im_2_ynumel =4 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_col2im_2 [grid (triton_poi_fused_col2im_2_ynumel ,16 )](buf0 ,buf1 ,16 ,16 ,3 ,32 ,32 ,12 ,16 ,XBLOCK =16 ,YBLOCK =16 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "505491dc-4e01-49c3-b293-ccc8d628214f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['TripletMarginWithDistanceLoss', 'KLDivLoss', 'MaxPool3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool3d = nn.MaxPool3d(kernel_size=2, stride=2)\n        self.triplet_loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n        self.kldiv_loss = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, x):\n        # Apply MaxPool3d to the input\n        x = self.maxpool3d(x)\n        \n        # Generate anchor, positive, and negative samples for TripletMarginWithDistanceLoss\n        anchor = x[:, :, 0, 0, 0].unsqueeze(2).unsqueeze(3).unsqueeze(4)  # Extract a single point as anchor\n        positive = x[:, :, 1, 1, 1].unsqueeze(2).unsqueeze(3).unsqueeze(4)  # Extract another point as positive\n        negative = x[:, :, 2, 2, 2].unsqueeze(2).unsqueeze(3).unsqueeze(4)  # Extract another point as negative\n        \n        # Compute TripletMarginWithDistanceLoss\n        triplet_loss = self.triplet_loss(anchor, positive, negative)\n        \n        # Reshape x for KLDivLoss\n        x_reshaped = x.view(x.size(0), -1)  # Flatten the tensor\n        target = torch.softmax(torch.randn_like(x_reshaped), dim=1)  # Generate a random target distribution\n        \n        # Compute KLDivLoss\n        kldiv_loss = self.kldiv_loss(F.log_softmax(x_reshaped, dim=1), target)\n        \n        # Return both losses as a tuple\n        return triplet_loss, kldiv_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 16, 16, 16).cuda()  # Example input with shape (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax__softmax_div_mul_randn_like_sub_sum_xlogy_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr0 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp4 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =r0_0 \n        tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n        tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n        tmp5 =triton_helpers .maximum (_tmp4 ,tmp3 )\n        _tmp4 =tl .where (r0_mask ,tmp5 ,_tmp4 )\n        tl .store (out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp2 ,r0_mask )\n    tmp4 =triton_helpers .max2 (_tmp4 ,1 )[:,None ]\n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp6 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =triton_helpers .maximum (_tmp8 ,tmp7 )\n        _tmp8 =tl .where (r0_mask ,tmp9 ,_tmp8 )\n    tmp8 =triton_helpers .max2 (_tmp8 ,1 )[:,None ]\n    _tmp14 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp10 =tl .load (out_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp11 =tmp10 -tmp4 \n        tmp12 =tl_math .exp (tmp11 )\n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =_tmp14 +tmp13 \n        _tmp14 =tl .where (r0_mask ,tmp15 ,_tmp14 )\n    tmp14 =tl .sum (_tmp14 ,1 )[:,None ]\n    _tmp20 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp16 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp17 =tmp16 -tmp8 \n        tmp18 =tl_math .exp (tmp17 )\n        tmp19 =tl .broadcast_to (tmp18 ,[XBLOCK ,R0_BLOCK ])\n        tmp21 =_tmp20 +tmp19 \n        _tmp20 =tl .where (r0_mask ,tmp21 ,_tmp20 )\n    tmp20 =tl .sum (_tmp20 ,1 )[:,None ]\n    _tmp41 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp22 =tl .load (out_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp34 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp23 =tmp22 -tmp4 \n        tmp24 =tl_math .exp (tmp23 )\n        tmp25 =tmp24 /tmp14 \n        tmp26 =libdevice .isnan (tmp25 ).to (tl .int1 )\n        tmp27 =0.0 \n        tmp28 =tmp25 ==tmp27 \n        tmp29 =tl_math .log (tmp25 )\n        tmp30 =tmp25 *tmp29 \n        tmp31 =tl .where (tmp28 ,tmp27 ,tmp30 )\n        tmp32 =float (\"nan\")\n        tmp33 =tl .where (tmp26 ,tmp32 ,tmp31 )\n        tmp35 =tmp34 -tmp8 \n        tmp36 =tl_math .log (tmp20 )\n        tmp37 =tmp35 -tmp36 \n        tmp38 =tmp25 *tmp37 \n        tmp39 =tmp33 -tmp38 \n        tmp40 =tl .broadcast_to (tmp39 ,[XBLOCK ,R0_BLOCK ])\n        tmp42 =_tmp41 +tmp40 \n        _tmp41 =tl .where (r0_mask ,tmp42 ,_tmp41 )\n    tmp41 =tl .sum (_tmp41 ,1 )[:,None ]\n    tmp43 =1.0 \n    tmp44 =tmp41 *tmp43 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp44 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_mean_norm_sub_1 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp18 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +(1 +(ks1 //2 )*(ks2 //2 )+r0_0 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )+(ks2 //2 )),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp9 =tl .load (in_ptr0 +(2 +2 *(ks2 //2 )+2 *(ks1 //2 )*(ks2 //2 )+r0_0 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp3 =1e-06 \n        tmp4 =tmp2 +tmp3 \n        tmp5 =tmp4 *tmp4 \n        tmp6 =libdevice .sqrt (tmp5 )\n        tmp7 =1.0 \n        tmp8 =tmp6 +tmp7 \n        tmp10 =tmp0 -tmp9 \n        tmp11 =tmp10 +tmp3 \n        tmp12 =tmp11 *tmp11 \n        tmp13 =libdevice .sqrt (tmp12 )\n        tmp14 =tmp8 -tmp13 \n        tmp15 =0.0 \n        tmp16 =triton_helpers .maximum (tmp14 ,tmp15 )\n        tmp17 =tl .broadcast_to (tmp16 ,[XBLOCK ,R0_BLOCK ])\n        tmp19 =_tmp18 +tmp17 \n        _tmp18 =tl .where (r0_mask ,tmp19 ,_tmp18 )\n    tmp18 =tl .sum (_tmp18 ,1 )[:,None ]\n    tmp20 =ks3 \n    tmp21 =tmp20 .to (tl .float32 )\n    tmp22 =tmp18 /tmp21 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp22 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .max_pool3d_with_indices .default (arg4_1 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del arg4_1 \n        buf4 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf4 )\n        buf1 =buf0 [0 ]\n        del buf0 \n        buf5 =empty_strided_cuda ((1 ,s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )),(s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),1 ),torch .float32 )\n        buf6 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf10 =reinterpret_tensor (buf6 ,(),(),0 );del buf6 \n        buf12 =buf10 ;del buf10 \n\n        s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_red_fused__log_softmax__softmax_div_mul_randn_like_sub_sum_xlogy_0 [grid (1 )](buf12 ,buf4 ,buf1 ,buf5 ,0 ,1 ,1536 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf4 \n        del buf5 \n        buf3 =empty_strided_cuda ((),(),torch .float32 )\n        buf11 =buf3 ;del buf3 \n\n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_mean_norm_sub_1 [grid (1 )](buf11 ,buf1 ,16 ,16 ,16 ,3 ,1 ,3 ,XBLOCK =1 ,R0_BLOCK =4 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n    return (buf11 ,buf12 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =16 \n    arg2_1 =16 \n    arg3_1 =16 \n    arg4_1 =rand_strided ((1 ,3 ,16 ,16 ,16 ),(12288 ,4096 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "50650ab4-90df-47ad-9458-5e5b063bb2b3",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Tanhshrink', 'ConstantPad3d', 'SmoothL1Loss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ConstantPad3d(padding=1, value=0.5)\n        self.pad2 = nn.ConstantPad3d(padding=2, value=0.25)\n        self.tanhshrink1 = nn.Tanhshrink()\n        self.tanhshrink2 = nn.Tanhshrink()\n        self.loss = nn.SmoothL1Loss()\n\n    def forward(self, x):\n        # Apply padding to the input\n        x = self.pad1(x)\n        x = self.pad2(x)\n        \n        # Apply Tanhshrink activation\n        x = self.tanhshrink1(x)\n        x = self.tanhshrink2(x)\n        \n        # Compute the SmoothL1Loss with respect to a target tensor of zeros\n        target = torch.zeros_like(x)\n        loss = self.loss(x, target)\n        \n        # Return the loss as the output\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape: (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x6 =((xindex //ks0 )%ks1 )\n    x1 =((xindex //ks3 )%ks4 )\n    x0 =(xindex %ks3 )\n    x2 =((xindex //ks7 )%ks1 )\n    x3 =xindex //ks8 \n    x8 =xindex \n    tmp0 =(-2 )+x6 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =2 +ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =2 +ks5 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-2 )+x0 \n    tmp10 =tmp9 >=tmp1 \n    tmp11 =2 +ks6 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =(-3 )+x6 \n    tmp19 =tl .full ([1 ],0 ,tl .int64 )\n    tmp20 =tmp18 >=tmp19 \n    tmp21 =tl .broadcast_to (ks2 ,[XBLOCK ])\n    tmp22 =tmp18 <tmp21 \n    tmp23 =(-3 )+x1 \n    tmp24 =tmp23 >=tmp19 \n    tmp25 =tl .broadcast_to (ks5 ,[XBLOCK ])\n    tmp26 =tmp23 <tmp25 \n    tmp27 =(-3 )+x0 \n    tmp28 =tmp27 >=tmp19 \n    tmp29 =tl .broadcast_to (ks6 ,[XBLOCK ])\n    tmp30 =tmp27 <tmp29 \n    tmp31 =tmp20 &tmp22 \n    tmp32 =tmp31 &tmp24 \n    tmp33 =tmp32 &tmp26 \n    tmp34 =tmp33 &tmp28 \n    tmp35 =tmp34 &tmp30 \n    tmp36 =tmp35 &tmp17 \n    tmp37 =tl .load (in_ptr0 +((-3 )+x0 +((-3 )*ks6 )+ks6 *x1 +((-3 )*ks5 *ks6 )+ks5 *ks6 *x2 +ks2 *ks5 *ks6 *x3 ),tmp36 &xmask ,eviction_policy ='evict_last',other =0.5 )\n    tmp38 =tl .full (tmp37 .shape ,0.25 ,tmp37 .dtype )\n    tmp39 =tl .where (tmp17 ,tmp37 ,tmp38 )\n    tl .store (out_ptr0 +(x8 ),tmp39 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_smooth_l1_loss_sub_tanh_zeros_like_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =21 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp22 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 )\n        tmp1 =216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(6 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//ks6 )%ks7 ))+36 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//ks4 )%ks5 ))+216 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//(216 +36 *ks1 +36 *ks2 +36 *ks3 +6 *ks1 *ks2 +6 *ks1 *ks3 +6 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+ks3 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//ks6 )%ks7 ))+6 *ks2 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//ks4 )%ks5 ))+6 *ks3 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//ks4 )%ks5 ))+36 *ks1 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//(216 +36 *ks1 +36 *ks2 +36 *ks3 +6 *ks1 *ks2 +6 *ks1 *ks3 +6 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+36 *ks2 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//(216 +36 *ks1 +36 *ks2 +36 *ks3 +6 *ks1 *ks2 +6 *ks1 *ks3 +6 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+36 *ks3 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//(216 +36 *ks1 +36 *ks2 +36 *ks3 +6 *ks1 *ks2 +6 *ks1 *ks3 +6 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+ks2 *ks3 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//ks4 )%ks5 ))+6 *ks1 *ks2 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//(216 +36 *ks1 +36 *ks2 +36 *ks3 +6 *ks1 *ks2 +6 *ks1 *ks3 +6 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+6 *ks1 *ks3 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//(216 +36 *ks1 +36 *ks2 +36 *ks3 +6 *ks1 *ks2 +6 *ks1 *ks3 +6 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+6 *ks2 *ks3 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//(216 +36 *ks1 +36 *ks2 +36 *ks3 +6 *ks1 *ks2 +6 *ks1 *ks3 +6 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+ks1 *ks2 *ks3 *((((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))//(216 +36 *ks1 +36 *ks2 +36 *ks3 +6 *ks1 *ks2 +6 *ks1 *ks3 +6 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+(((r0_1 +x0 *((20 +216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//21 ))%ks6 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =libdevice .tanh (tmp3 )\n        tmp5 =tmp3 -tmp4 \n        tmp6 =libdevice .tanh (tmp5 )\n        tmp7 =tmp5 -tmp6 \n        tmp8 =0.0 \n        tmp9 =tmp7 -tmp8 \n        tmp10 =tl_math .abs (tmp9 )\n        tmp11 =1.0 \n        tmp12 =tmp10 <tmp11 \n        tmp13 =tmp10 *tmp10 \n        tmp14 =0.5 \n        tmp15 =tmp13 *tmp14 \n        tmp16 =tmp15 *tmp11 \n        tmp17 =tmp10 -tmp14 \n        tmp18 =tl .where (tmp12 ,tmp16 ,tmp17 )\n        tmp19 =tl .full (tmp18 .shape ,0 ,tmp18 .dtype )\n        tmp20 =tl .where (tmp2 ,tmp18 ,tmp19 )\n        tmp21 =tl .broadcast_to (tmp20 ,[XBLOCK ,R0_BLOCK ])\n        tmp23 =_tmp22 +tmp21 \n        _tmp22 =tl .where (r0_mask &xmask ,tmp23 ,_tmp22 )\n    tmp22 =tl .sum (_tmp22 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp22 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_smooth_l1_loss_sub_tanh_zeros_like_2 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =21 \n    R0_BLOCK :tl .constexpr =32 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =216 *ks0 +36 *ks0 *ks1 +36 *ks0 *ks2 +36 *ks0 *ks3 +6 *ks0 *ks1 *ks2 +6 *ks0 *ks1 *ks3 +6 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tmp4 /tmp6 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp7 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        36 +6 *s2 +6 *s3 +s2 *s3 \n        6 +s1 \n        6 +s3 \n        6 +s2 \n        36 +6 *s2 +6 *s3 +s2 *s3 \n        216 +36 *s1 +36 *s2 +36 *s3 +6 *s1 *s2 +6 *s1 *s3 +6 *s2 *s3 +s1 *s2 *s3 \n        buf0 =empty_strided_cuda ((1 ,s0 ,6 +s1 ,6 +s2 ,6 +s3 ),(216 *s0 +36 *s0 *s1 +36 *s0 *s2 +36 *s0 *s3 +6 *s0 *s1 *s2 +6 *s0 *s1 *s3 +6 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,216 +36 *s1 +36 *s2 +36 *s3 +6 *s1 *s2 +6 *s1 *s3 +6 *s2 *s3 +s1 *s2 *s3 ,36 +6 *s2 +6 *s3 +s2 *s3 ,6 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =216 *s0 +36 *s0 *s1 +36 *s0 *s2 +36 *s0 *s3 +6 *s0 *s1 *s2 +6 *s0 *s1 *s3 +6 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg4_1 ,buf0 ,1444 ,38 ,32 ,38 ,38 ,32 ,32 ,1444 ,54872 ,164616 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg4_1 \n        buf1 =empty_strided_cuda ((21 ,),(1 ,),torch .float32 )\n\n        (20 +216 *s0 +36 *s0 *s1 +36 *s0 *s2 +36 *s0 *s3 +6 *s0 *s1 *s2 +6 *s0 *s1 *s3 +6 *s0 *s2 *s3 +s0 *s1 *s2 *s3 )//21 \n        get_raw_stream (0 )\n        triton_red_fused_smooth_l1_loss_sub_tanh_zeros_like_1 [grid (21 )](buf0 ,buf1 ,3 ,32 ,32 ,32 ,1444 ,38 ,38 ,38 ,21 ,7839 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((),(),torch .float32 )\n        buf3 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_per_fused_smooth_l1_loss_sub_tanh_zeros_like_2 [grid (1 )](buf3 ,buf1 ,3 ,32 ,32 ,32 ,1 ,21 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "50b94851-7dd5-4d30-8bca-b6094ba6cfa0",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FeatureAlphaDropout', 'KLDivLoss', 'ZeroPad2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad = nn.ZeroPad2d(2)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, x):\n        # Apply ZeroPad2d to the input\n        x = self.zero_pad(x)\n        \n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Reshape the tensor to match the expected input shape for KLDivLoss\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x = F.log_softmax(x, dim=1)  # Apply log_softmax to get log probabilities\n        \n        # Create a target tensor with the same shape as x\n        target = torch.softmax(torch.randn_like(x), dim=1)\n        \n        # Compute KLDivLoss\n        loss = self.kl_div_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax__softmax_div_mul_randn_like_sub_sum_xlogy_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr3 ,out_ptr4 ,ks0 ,ks1 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp28 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp13 =tl .load (in_ptr1 +(r0_0 //(16 +4 *ks0 +4 *ks1 +ks0 *ks1 )),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp0 =(-2 )+(((r0_0 //(4 +ks1 ))%(4 +ks0 )))\n        tmp1 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp2 =tmp0 >=tmp1 \n        tmp3 =ks0 \n        tmp4 =tmp0 <tmp3 \n        tmp5 =(-2 )+((r0_0 %(4 +ks1 )))\n        tmp6 =tmp5 >=tmp1 \n        tmp7 =ks1 \n        tmp8 =tmp5 <tmp7 \n        tmp9 =tmp2 &tmp4 \n        tmp10 =tmp9 &tmp6 \n        tmp11 =tmp10 &tmp8 \n        tmp12 =tl .load (in_ptr0 +(tl .broadcast_to ((-2 )+((-2 )*ks1 )+ks1 *(((r0_0 //(4 +ks1 ))%(4 +ks0 )))+ks0 *ks1 *(r0_0 //(16 +4 *ks0 +4 *ks1 +ks0 *ks1 ))+((r0_0 %(4 +ks1 ))),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp11 ,eviction_policy ='evict_last',other =0.0 )\n        tmp14 =0.5 \n        tmp15 =tmp13 <tmp14 \n        tmp16 =tmp15 .to (tl .float32 )\n        tmp17 =0.8864048946659319 \n        tmp18 =tmp16 *tmp17 \n        tmp19 =tmp12 *tmp18 \n        tmp20 =-1.0 \n        tmp21 =tmp16 +tmp20 \n        tmp22 =1.558387861036063 \n        tmp23 =tmp21 *tmp22 \n        tmp24 =0.7791939305180315 \n        tmp25 =tmp23 +tmp24 \n        tmp26 =tmp19 +tmp25 \n        tmp27 =tl .broadcast_to (tmp26 ,[XBLOCK ,R0_BLOCK ])\n        tmp29 =triton_helpers .maximum (_tmp28 ,tmp27 )\n        _tmp28 =tl .where (r0_mask ,tmp29 ,_tmp28 )\n    tmp28 =triton_helpers .max2 (_tmp28 ,1 )[:,None ]\n    _tmp60 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp43 =tl .load (in_ptr1 +(r0_0 //(16 +4 *ks0 +4 *ks1 +ks0 *ks1 )),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp30 =(-2 )+(((r0_0 //(4 +ks1 ))%(4 +ks0 )))\n        tmp31 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp32 =tmp30 >=tmp31 \n        tmp33 =ks0 \n        tmp34 =tmp30 <tmp33 \n        tmp35 =(-2 )+((r0_0 %(4 +ks1 )))\n        tmp36 =tmp35 >=tmp31 \n        tmp37 =ks1 \n        tmp38 =tmp35 <tmp37 \n        tmp39 =tmp32 &tmp34 \n        tmp40 =tmp39 &tmp36 \n        tmp41 =tmp40 &tmp38 \n        tmp42 =tl .load (in_ptr0 +(tl .broadcast_to ((-2 )+((-2 )*ks1 )+ks1 *(((r0_0 //(4 +ks1 ))%(4 +ks0 )))+ks0 *ks1 *(r0_0 //(16 +4 *ks0 +4 *ks1 +ks0 *ks1 ))+((r0_0 %(4 +ks1 ))),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp41 ,eviction_policy ='evict_last',other =0.0 )\n        tmp44 =0.5 \n        tmp45 =tmp43 <tmp44 \n        tmp46 =tmp45 .to (tl .float32 )\n        tmp47 =0.8864048946659319 \n        tmp48 =tmp46 *tmp47 \n        tmp49 =tmp42 *tmp48 \n        tmp50 =-1.0 \n        tmp51 =tmp46 +tmp50 \n        tmp52 =1.558387861036063 \n        tmp53 =tmp51 *tmp52 \n        tmp54 =0.7791939305180315 \n        tmp55 =tmp53 +tmp54 \n        tmp56 =tmp49 +tmp55 \n        tmp57 =tmp56 -tmp28 \n        tmp58 =tl_math .exp (tmp57 )\n        tmp59 =tl .broadcast_to (tmp58 ,[XBLOCK ,R0_BLOCK ])\n        tmp61 =_tmp60 +tmp59 \n        _tmp60 =tl .where (r0_mask ,tmp61 ,_tmp60 )\n    tmp60 =tl .sum (_tmp60 ,1 )[:,None ]\n    _tmp96 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp75 =tl .load (in_ptr1 +(r0_0 //(16 +4 *ks0 +4 *ks1 +ks0 *ks1 )),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp62 =(-2 )+(((r0_0 //(4 +ks1 ))%(4 +ks0 )))\n        tmp63 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp64 =tmp62 >=tmp63 \n        tmp65 =ks0 \n        tmp66 =tmp62 <tmp65 \n        tmp67 =(-2 )+((r0_0 %(4 +ks1 )))\n        tmp68 =tmp67 >=tmp63 \n        tmp69 =ks1 \n        tmp70 =tmp67 <tmp69 \n        tmp71 =tmp64 &tmp66 \n        tmp72 =tmp71 &tmp68 \n        tmp73 =tmp72 &tmp70 \n        tmp74 =tl .load (in_ptr0 +(tl .broadcast_to ((-2 )+((-2 )*ks1 )+ks1 *(((r0_0 //(4 +ks1 ))%(4 +ks0 )))+ks0 *ks1 *(r0_0 //(16 +4 *ks0 +4 *ks1 +ks0 *ks1 ))+((r0_0 %(4 +ks1 ))),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp73 ,eviction_policy ='evict_last',other =0.0 )\n        tmp76 =0.5 \n        tmp77 =tmp75 <tmp76 \n        tmp78 =tmp77 .to (tl .float32 )\n        tmp79 =0.8864048946659319 \n        tmp80 =tmp78 *tmp79 \n        tmp81 =tmp74 *tmp80 \n        tmp82 =-1.0 \n        tmp83 =tmp78 +tmp82 \n        tmp84 =1.558387861036063 \n        tmp85 =tmp83 *tmp84 \n        tmp86 =0.7791939305180315 \n        tmp87 =tmp85 +tmp86 \n        tmp88 =tmp81 +tmp87 \n        tmp89 =tmp88 -tmp28 \n        tmp90 =tl_math .log (tmp60 )\n        tmp91 =tmp89 -tmp90 \n        tmp92 =tl .load (in_ptr2 +load_seed_offset )\n        tmp93 =r0_0 \n        tmp94 =tl .randn (tmp92 ,(tmp93 ).to (tl .uint32 ))\n        tmp95 =tl .broadcast_to (tmp94 ,[XBLOCK ,R0_BLOCK ])\n        tmp97 =triton_helpers .maximum (_tmp96 ,tmp95 )\n        _tmp96 =tl .where (r0_mask ,tmp97 ,_tmp96 )\n        tl .store (out_ptr3 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp91 ,r0_mask )\n        tl .store (out_ptr4 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp94 ,r0_mask )\n    tmp96 =triton_helpers .max2 (_tmp96 ,1 )[:,None ]\n    _tmp102 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp98 =tl .load (out_ptr4 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp99 =tmp98 -tmp96 \n        tmp100 =tl_math .exp (tmp99 )\n        tmp101 =tl .broadcast_to (tmp100 ,[XBLOCK ,R0_BLOCK ])\n        tmp103 =_tmp102 +tmp101 \n        _tmp102 =tl .where (r0_mask ,tmp103 ,_tmp102 )\n    tmp102 =tl .sum (_tmp102 ,1 )[:,None ]\n    _tmp120 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp104 =tl .load (out_ptr4 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp116 =tl .load (out_ptr3 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp105 =tmp104 -tmp96 \n        tmp106 =tl_math .exp (tmp105 )\n        tmp107 =tmp106 /tmp102 \n        tmp108 =libdevice .isnan (tmp107 ).to (tl .int1 )\n        tmp109 =0.0 \n        tmp110 =tmp107 ==tmp109 \n        tmp111 =tl_math .log (tmp107 )\n        tmp112 =tmp107 *tmp111 \n        tmp113 =tl .where (tmp110 ,tmp109 ,tmp112 )\n        tmp114 =float (\"nan\")\n        tmp115 =tl .where (tmp108 ,tmp114 ,tmp113 )\n        tmp117 =tmp107 *tmp116 \n        tmp118 =tmp115 -tmp117 \n        tmp119 =tl .broadcast_to (tmp118 ,[XBLOCK ,R0_BLOCK ])\n        tmp121 =_tmp120 +tmp119 \n        _tmp120 =tl .where (r0_mask ,tmp121 ,_tmp120 )\n    tmp120 =tl .sum (_tmp120 ,1 )[:,None ]\n    tmp122 =1.0 \n    tmp123 =tmp120 *tmp122 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp123 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf8 =empty_strided_cuda ((1 ,16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,1 ),torch .float32 )\n        buf3 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf9 =reinterpret_tensor (buf3 ,(),(),0 );del buf3 \n        buf10 =buf9 ;del buf9 \n\n        16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__log_softmax__softmax_div_mul_randn_like_sub_sum_xlogy_1 [grid (1 )](buf10 ,arg3_1 ,buf1 ,buf0 ,buf8 ,buf2 ,32 ,32 ,1 ,1 ,3888 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg3_1 \n        del buf0 \n        del buf1 \n        del buf2 \n        del buf8 \n    return (buf10 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "529a6f20-084f-45d3-84cf-636e10e8a530",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LocalResponseNorm', 'CircularPad3d', 'RNN', 'ZeroPad2d', 'PReLU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lrn = nn.LocalResponseNorm(size=5)\n        self.circular_pad = nn.CircularPad3d(padding=(1, 1, 1, 1, 1, 1))\n        self.rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.zero_pad = nn.ZeroPad2d(padding=(1, 1, 1, 1))\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        # Apply LocalResponseNorm\n        x = self.lrn(x)\n        \n        # Reshape for CircularPad3d\n        x = x.view(x.size(0), x.size(1), x.size(2), x.size(3), -1)\n        x = self.circular_pad(x)\n        \n        # Reshape back for RNN\n        x = x.view(x.size(0), x.size(1), -1)\n        \n        # Apply RNN\n        x, _ = self.rnn(x)\n        \n        # Reshape for ZeroPad2d\n        x = x.view(x.size(0), x.size(1), x.size(2), -1)\n        x = self.zero_pad(x)\n        \n        # Apply PReLU\n        x = self.prelu(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool3d_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +(x2 +((-2 )*ks2 *ks3 )),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tmp6 *tmp6 \n    tmp8 =tl .full (tmp7 .shape ,0.0 ,tmp7 .dtype )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tmp10 =(-1 )+x1 \n    tmp11 =tmp10 >=tmp1 \n    tmp12 =tmp10 <tmp3 \n    tmp13 =tmp11 &tmp12 \n    tmp14 =tl .load (in_ptr0 +(x2 +((-1 )*ks2 *ks3 )),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =tmp14 *tmp14 \n    tmp16 =tl .full (tmp15 .shape ,0.0 ,tmp15 .dtype )\n    tmp17 =tl .where (tmp13 ,tmp15 ,tmp16 )\n    tmp18 =tmp17 +tmp9 \n    tmp19 =x1 \n    tmp20 =tmp19 >=tmp1 \n    tmp21 =tmp19 <tmp3 \n    tmp22 =tmp20 &tmp21 \n    tmp23 =tl .load (in_ptr0 +(x2 ),tmp22 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp24 =tmp23 *tmp23 \n    tmp25 =tl .full (tmp24 .shape ,0.0 ,tmp24 .dtype )\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tmp26 +tmp18 \n    tmp28 =1 +x1 \n    tmp29 =tmp28 >=tmp1 \n    tmp30 =tmp28 <tmp3 \n    tmp31 =tmp29 &tmp30 \n    tmp32 =tl .load (in_ptr0 +(ks0 +x2 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tmp32 *tmp32 \n    tmp34 =tl .full (tmp33 .shape ,0.0 ,tmp33 .dtype )\n    tmp35 =tl .where (tmp31 ,tmp33 ,tmp34 )\n    tmp36 =tmp35 +tmp27 \n    tmp37 =2 +x1 \n    tmp38 =tmp37 >=tmp1 \n    tmp39 =tmp37 <tmp3 \n    tmp40 =tmp38 &tmp39 \n    tmp41 =tl .load (in_ptr0 +(x2 +2 *ks2 *ks3 ),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tmp41 *tmp41 \n    tmp43 =tl .full (tmp42 .shape ,0.0 ,tmp42 .dtype )\n    tmp44 =tl .where (tmp40 ,tmp42 ,tmp43 )\n    tmp45 =tmp44 +tmp36 \n    tmp46 =0.2 \n    tmp47 =tmp45 *tmp46 \n    tl .store (out_ptr0 +(x2 ),tmp47 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %3 )\n    x1 =((xindex //3 )%ks0 )\n    x2 =((xindex //ks2 )%ks3 )\n    x3 =xindex //ks5 \n    x5 =xindex //3 \n    x4 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],2 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =x1 \n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 >=tmp7 \n    tmp9 =tl .broadcast_to (1 +ks1 ,[XBLOCK ])\n    tmp10 =tmp6 <tmp9 \n    tmp11 =tmp8 &tmp10 \n    tmp12 =tmp11 &tmp5 \n    tmp13 =x2 \n    tmp14 =tl .full ([1 ],1 ,tl .int64 )\n    tmp15 =tmp13 >=tmp14 \n    tmp16 =tl .broadcast_to (1 +ks4 ,[XBLOCK ])\n    tmp17 =tmp13 <tmp16 \n    tmp18 =tmp15 &tmp17 \n    tmp19 =tmp18 &tmp12 \n    tmp20 =tl .load (in_ptr0 +((-1 )+x1 +((-1 )*ks1 )+ks1 *x2 +ks1 *ks4 *x3 ),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr1 +((-1 )+x1 +((-1 )*ks1 )+ks1 *x2 +ks1 *ks4 *x3 ),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =0.0001 \n    tmp23 =tmp21 *tmp22 \n    tmp24 =1.0 \n    tmp25 =tmp23 +tmp24 \n    tmp26 =0.75 \n    tmp27 =libdevice .pow (tmp25 ,tmp26 )\n    tmp28 =tmp20 /tmp27 \n    tmp29 =tl .full (tmp28 .shape ,0.0 ,tmp28 .dtype )\n    tmp30 =tl .where (tmp19 ,tmp28 ,tmp29 )\n    tmp31 =tl .load (in_ptr2 +(1 +3 *x5 ),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp32 =tl .where (tmp18 ,tmp30 ,tmp31 )\n    tmp33 =tl .full (tmp32 .shape ,0.0 ,tmp32 .dtype )\n    tmp34 =tl .where (tmp12 ,tmp32 ,tmp33 )\n    tmp35 =tl .load (in_ptr2 +(1 +3 *x5 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp36 =tl .where (tmp11 ,tmp34 ,tmp35 )\n    tmp37 =tl .full (tmp36 .shape ,0.0 ,tmp36 .dtype )\n    tmp38 =tl .where (tmp5 ,tmp36 ,tmp37 )\n    tmp39 =float (\"nan\")\n    tmp40 =tl .where (tmp5 ,tmp38 ,tmp39 )\n    tl .store (out_ptr0 +(x4 ),tmp40 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //3 )%ks0 )\n    x0 =(xindex %3 )\n    x3 =xindex //3 \n    x4 =xindex \n    tmp39 =tl .load (in_ptr0 +(x4 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =x0 \n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =(-1 )+x0 \n    tmp8 =tl .full ([1 ],1 ,tl .int64 )\n    tmp9 =tmp7 <tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(1 +3 *ks1 +3 *x3 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +((-1 )+x4 +3 *ks1 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =tl .full ([1 ],1 ,tl .int64 )\n    tmp17 =tmp3 <tmp16 \n    tmp18 =tmp17 &tmp2 \n    tmp19 =tl .load (in_ptr0 +(1 +3 *ks1 +3 *x3 ),tmp18 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =tl .load (in_ptr0 +(x4 +3 *ks1 ),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .where (tmp17 ,tmp19 ,tmp20 )\n    tmp22 =tl .where (tmp5 ,tmp15 ,tmp21 )\n    tmp23 =tl .full (tmp22 .shape ,0.0 ,tmp22 .dtype )\n    tmp24 =tl .where (tmp2 ,tmp22 ,tmp23 )\n    tmp25 =x0 \n    tmp26 =tl .full ([1 ],2 ,tl .int64 )\n    tmp27 =tmp25 >=tmp26 \n    tmp28 =(-1 )+x0 \n    tmp29 =tl .full ([1 ],1 ,tl .int64 )\n    tmp30 =tmp28 <tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(1 +3 *x3 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +((-1 )+x4 ),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =tmp25 <tmp1 \n    tmp38 =tl .load (in_ptr0 +(1 +3 *x3 ),tmp37 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp40 =tl .where (tmp37 ,tmp38 ,tmp39 )\n    tmp41 =tl .where (tmp27 ,tmp36 ,tmp40 )\n    tmp42 =tl .where (tmp2 ,tmp24 ,tmp41 )\n    tl .store (out_ptr0 +(x4 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =((xindex //ks0 )%ks1 )\n    x1 =((xindex //3 )%ks3 )\n    x0 =(xindex %3 )\n    x6 =xindex //ks0 \n    x4 =xindex \n    tmp41 =tl .load (in_ptr0 +(x4 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x2 \n    tmp1 =1 +ks2 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =x2 +((-1 )*ks2 )\n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x1 \n    tmp8 =tl .broadcast_to (1 +ks4 ,[XBLOCK ])\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(3 +x0 +6 *x6 +3 *ks4 *x6 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x4 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =x1 \n    tmp17 =tl .broadcast_to (1 +ks4 ,[XBLOCK ])\n    tmp18 =tmp16 >=tmp17 \n    tmp19 =tmp18 &tmp2 \n    tmp20 =tl .load (in_ptr0 +(3 +x0 +((-6 )*ks2 )+6 *x6 +((-3 )*ks2 *ks4 )+3 *ks4 *x6 ),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr0 +(x4 +((-6 )*ks2 )+((-3 )*ks2 *ks4 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\n    tmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],1 ,tl .int64 )\n    tmp27 =tmp0 <tmp26 \n    tmp28 =x1 \n    tmp29 =tl .broadcast_to (1 +ks4 ,[XBLOCK ])\n    tmp30 =tmp28 >=tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(3 +x0 +6 *ks2 +6 *x6 +3 *ks2 *ks4 +3 *ks4 *x6 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(x4 +6 *ks2 +3 *ks2 *ks4 ),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =x1 \n    tmp38 =1 +ks4 \n    tmp39 =tmp37 >=tmp38 \n    tmp40 =tl .load (in_ptr0 +(3 +x0 +6 *x6 +3 *ks4 *x6 ),tmp39 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tl .where (tmp39 ,tmp40 ,tmp41 )\n    tmp43 =tl .where (tmp27 ,tmp36 ,tmp42 )\n    tmp44 =tl .where (tmp2 ,tmp25 ,tmp43 )\n    tl .store (out_ptr0 +(x4 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_4 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(6 *(x0 //ks1 )+12 *x1 +3 *ks3 *(x0 //ks1 )+6 *ks2 *x1 +6 *ks3 *x1 +3 *ks2 *ks3 *x1 +((x0 %ks1 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,3 ),(12 *s0 +6 *s0 *s1 +6 *s0 *s2 +3 *s0 *s1 *s2 ,12 +6 *s1 +6 *s2 +3 *s1 *s2 ,6 +3 *s2 ,3 ,1 ),torch .float32 )\n        s1 *s2 \n        buf1 =empty_strided_cuda ((1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool3d_constant_pad_nd_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool3d_constant_pad_nd_0 [grid (triton_poi_fused_avg_pool3d_constant_pad_nd_0_xnumel )](arg3_1 ,buf1 ,1024 ,3 ,32 ,32 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        2 +s2 \n        6 +3 *s2 \n        2 +s1 \n        12 +6 *s1 +6 *s2 +3 *s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,3 ),(12 *s0 +6 *s0 *s1 +6 *s0 *s2 +3 *s0 *s1 *s2 ,12 +6 *s1 +6 *s2 +3 *s1 *s2 ,6 +3 *s2 ,3 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_1_xnumel =12 *s0 +6 *s0 *s1 +6 *s0 *s2 +3 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_1 [grid (triton_poi_fused_copy_1_xnumel )](arg3_1 ,buf1 ,buf0 ,buf2 ,34 ,32 ,102 ,34 ,32 ,3468 ,10404 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        buf3 =buf0 ;del buf0 \n\n        triton_poi_fused_2_xnumel =12 *s0 +6 *s0 *s1 +6 *s0 *s2 +3 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_2 [grid (triton_poi_fused_2_xnumel )](buf2 ,buf3 ,34 ,32 ,10404 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf4 =buf2 ;del buf2 \n\n        triton_poi_fused_3_xnumel =12 *s0 +6 *s0 *s1 +6 *s0 *s2 +3 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_3 [grid (triton_poi_fused_3_xnumel )](buf3 ,buf4 ,102 ,34 ,32 ,34 ,32 ,10404 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        12 +6 *s1 +6 *s2 +3 *s1 *s2 \n        buf5 =reinterpret_tensor (buf3 ,(1 ,s0 ,12 +6 *s1 +6 *s2 +3 *s1 *s2 ),(12 *s0 +6 *s0 *s1 +6 *s0 *s2 +3 *s0 *s1 *s2 ,12 +6 *s1 +6 *s2 +3 *s1 *s2 ,1 ),0 );del buf3 \n\n        triton_poi_fused_4_xnumel =12 *s0 +6 *s0 *s1 +6 *s0 *s2 +3 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_4 [grid (triton_poi_fused_4_xnumel )](buf4 ,buf5 ,3468 ,102 ,32 ,32 ,10404 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "52f00fc7-2234-4eaf-a406-96fd278f8883",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LocalResponseNorm', 'ConstantPad2d', 'KLDivLoss', 'CrossEntropyLoss', 'CosineEmbeddingLoss', 'CrossMapLRN2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.local_response_norm = nn.LocalResponseNorm(size=5)\n        self.constant_pad = nn.ConstantPad2d(padding=2, value=1.0)\n        self.cross_map_lrn = nn.CrossMapLRN2d(size=5)\n        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()\n\n    def forward(self, x, target=None):\n        # Apply LocalResponseNorm\n        x = self.local_response_norm(x)\n        \n        # Apply ConstantPad2d\n        x = self.constant_pad(x)\n        \n        # Apply CrossMapLRN2d\n        x = self.cross_map_lrn(x)\n        \n        # Reshape for KLDivLoss\n        x_reshaped = x.view(x.size(0), -1)\n        log_softmax_x = F.log_softmax(x_reshaped, dim=1)\n        \n        # Compute KLDivLoss if target is provided\n        if target is not None:\n            kl_loss = self.kl_div_loss(log_softmax_x, target)\n        else:\n            kl_loss = torch.tensor(0.0)\n        \n        # Compute CrossEntropyLoss if target is provided\n        if target is not None:\n            cross_entropy_loss = self.cross_entropy_loss(log_softmax_x, target.argmax(dim=1))\n        else:\n            cross_entropy_loss = torch.tensor(0.0)\n        \n        # Compute CosineEmbeddingLoss if target is provided\n        if target is not None:\n            cosine_embedding_loss = self.cosine_embedding_loss(log_softmax_x, target, torch.ones(x.size(0)))\n        else:\n            cosine_embedding_loss = torch.tensor(0.0)\n        \n        # Return the losses as a dictionary\n        return {\n            'kl_div_loss': kl_loss,\n            'cross_entropy_loss': cross_entropy_loss,\n            'cosine_embedding_loss': cosine_embedding_loss\n        }\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    target = torch.randn(1, 3 * 32 * 32).cuda()\n    return [x, target]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool3d_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +(x2 +((-2 )*ks2 *ks3 )),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tmp6 *tmp6 \n    tmp8 =tl .full (tmp7 .shape ,0.0 ,tmp7 .dtype )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tmp10 =(-1 )+x1 \n    tmp11 =tmp10 >=tmp1 \n    tmp12 =tmp10 <tmp3 \n    tmp13 =tmp11 &tmp12 \n    tmp14 =tl .load (in_ptr0 +(x2 +((-1 )*ks2 *ks3 )),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =tmp14 *tmp14 \n    tmp16 =tl .full (tmp15 .shape ,0.0 ,tmp15 .dtype )\n    tmp17 =tl .where (tmp13 ,tmp15 ,tmp16 )\n    tmp18 =tmp17 +tmp9 \n    tmp19 =x1 \n    tmp20 =tmp19 >=tmp1 \n    tmp21 =tmp19 <tmp3 \n    tmp22 =tmp20 &tmp21 \n    tmp23 =tl .load (in_ptr0 +(x2 ),tmp22 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp24 =tmp23 *tmp23 \n    tmp25 =tl .full (tmp24 .shape ,0.0 ,tmp24 .dtype )\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tmp26 +tmp18 \n    tmp28 =1 +x1 \n    tmp29 =tmp28 >=tmp1 \n    tmp30 =tmp28 <tmp3 \n    tmp31 =tmp29 &tmp30 \n    tmp32 =tl .load (in_ptr0 +(ks0 +x2 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tmp32 *tmp32 \n    tmp34 =tl .full (tmp33 .shape ,0.0 ,tmp33 .dtype )\n    tmp35 =tl .where (tmp31 ,tmp33 ,tmp34 )\n    tmp36 =tmp35 +tmp27 \n    tmp37 =2 +x1 \n    tmp38 =tmp37 >=tmp1 \n    tmp39 =tmp37 <tmp3 \n    tmp40 =tmp38 &tmp39 \n    tmp41 =tl .load (in_ptr0 +(x2 +2 *ks2 *ks3 ),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tmp41 *tmp41 \n    tmp43 =tl .full (tmp42 .shape ,0.0 ,tmp42 .dtype )\n    tmp44 =tl .where (tmp40 ,tmp42 ,tmp43 )\n    tmp45 =tmp44 +tmp36 \n    tmp46 =0.2 \n    tmp47 =tmp45 *tmp46 \n    tl .store (out_ptr0 +(x2 ),tmp47 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_constant_pad_nd_div_mul_pow_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .load (in_ptr1 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp14 =0.0001 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =1.0 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =0.75 \n    tmp19 =libdevice .pow (tmp17 ,tmp18 )\n    tmp20 =tmp12 /tmp19 \n    tmp21 =tl .full (tmp20 .shape ,1.0 ,tmp20 .dtype )\n    tmp22 =tl .where (tmp11 ,tmp20 ,tmp21 )\n    tl .store (out_ptr0 +(x4 ),tmp22 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool3d_constant_pad_nd_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool3d_constant_pad_nd_0 [grid (triton_poi_fused_avg_pool3d_constant_pad_nd_0_xnumel )](arg3_1 ,buf0 ,1024 ,3 ,32 ,32 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf1 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_add_constant_pad_nd_div_mul_pow_1_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_add_constant_pad_nd_div_mul_pow_1 [grid (triton_poi_fused_add_constant_pad_nd_div_mul_pow_1_xnumel )](arg3_1 ,buf0 ,buf1 ,36 ,36 ,32 ,32 ,1296 ,3888 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf0 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "537f418f-7448-456f-881b-c51872e4e379",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GRUCell', 'LogSigmoid', 'RMSNorm']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, input_size=128, hidden_size=64) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.gru_cell1 = nn.GRUCell(input_size, hidden_size)\n        self.gru_cell2 = nn.GRUCell(hidden_size, hidden_size)\n        self.rms_norm = nn.LayerNorm(hidden_size)  # RMSNorm is not directly available in PyTorch, using LayerNorm as a substitute\n        self.log_sigmoid = nn.LogSigmoid()\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, sequence_length, input_size)\n        batch_size, seq_len, _ = x.size()\n        \n        # Initialize hidden state\n        hx = torch.zeros(batch_size, self.hidden_size).to(x.device)\n        \n        # Process through GRUCells\n        for t in range(seq_len):\n            hx = self.gru_cell1(x[:, t, :], hx)\n            hx = self.gru_cell2(hx, hx)\n        \n        # Apply RMSNorm (LayerNorm as substitute)\n        hx = self.rms_norm(hx)\n        \n        # Apply LogSigmoid\n        output = self.log_sigmoid(hx)\n        \n        return output\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 128).cuda()  # batch_size=1, sequence_length=10, input_size=128\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_log_sigmoid_forward_native_layer_norm_native_layer_norm_backward_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr2 ,out_ptr3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_out_ptr0 +(r0_0 ),None )\n    tmp21 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp23 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp6 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp5 /tmp7 \n    tmp9 =tmp1 -tmp8 \n    tmp10 =tmp9 *tmp9 \n    tmp11 =tl .broadcast_to (tmp10 ,[XBLOCK ,R0_BLOCK ])\n    tmp13 =tl .sum (tmp11 ,1 )[:,None ]\n    tmp14 =tmp0 -tmp8 \n    tmp15 =64.0 \n    tmp16 =tmp13 /tmp15 \n    tmp17 =1e-05 \n    tmp18 =tmp16 +tmp17 \n    tmp19 =libdevice .rsqrt (tmp18 )\n    tmp20 =tmp14 *tmp19 \n    tmp22 =tmp20 *tmp21 \n    tmp24 =tmp22 +tmp23 \n    tmp25 =0.0 \n    tmp26 =triton_helpers .minimum (tmp25 ,tmp24 )\n    tmp27 =tl_math .abs (tmp24 )\n    tmp28 =-tmp27 \n    tmp29 =tl_math .exp (tmp28 )\n    tmp30 =libdevice .log1p (tmp29 )\n    tmp31 =tmp26 -tmp30 \n    tmp32 =0.015625 \n    tmp33 =tmp19 *tmp32 \n    tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp20 ,None )\n    tl .store (out_ptr2 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp31 ,None )\n    tl .store (out_ptr3 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp33 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ,128 ),(1280 ,128 ,1 ))\n    assert_size_stride (primals_2 ,(192 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_3 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_4 ,(192 ,),(1 ,))\n    assert_size_stride (primals_5 ,(192 ,),(1 ,))\n    assert_size_stride (primals_6 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_7 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_8 ,(192 ,),(1 ,))\n    assert_size_stride (primals_9 ,(192 ,),(1 ,))\n    assert_size_stride (primals_10 ,(64 ,),(1 ,))\n    assert_size_stride (primals_11 ,(64 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_0 [grid (64 )](buf0 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((1 ,192 ),(192 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),0 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf1 )\n        buf2 =empty_strided_cuda ((1 ,192 ),(192 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf0 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf2 )\n\n        buf3 =torch .ops .aten ._thnn_fused_gru_cell .default (buf1 ,buf2 ,buf0 ,primals_4 ,primals_5 )\n        buf4 =buf3 [0 ]\n        buf5 =buf3 [1 ]\n        del buf3 \n        buf6 =buf2 ;del buf2 \n\n        extern_kernels .mm (buf4 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf6 )\n        buf7 =buf1 ;del buf1 \n\n        extern_kernels .mm (buf4 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf7 )\n\n        buf8 =torch .ops .aten ._thnn_fused_gru_cell .default (buf6 ,buf7 ,buf4 ,primals_8 ,primals_9 )\n        buf9 =buf8 [0 ]\n        buf10 =buf8 [1 ]\n        del buf8 \n        buf11 =buf7 ;del buf7 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),128 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf11 )\n        buf12 =buf6 ;del buf6 \n\n        extern_kernels .mm (buf9 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf12 )\n\n        buf13 =torch .ops .aten ._thnn_fused_gru_cell .default (buf11 ,buf12 ,buf9 ,primals_4 ,primals_5 )\n        buf14 =buf13 [0 ]\n        buf15 =buf13 [1 ]\n        del buf13 \n        buf16 =buf12 ;del buf12 \n\n        extern_kernels .mm (buf14 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf16 )\n        buf17 =buf11 ;del buf11 \n\n        extern_kernels .mm (buf14 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf17 )\n\n        buf18 =torch .ops .aten ._thnn_fused_gru_cell .default (buf16 ,buf17 ,buf14 ,primals_8 ,primals_9 )\n        buf19 =buf18 [0 ]\n        buf20 =buf18 [1 ]\n        del buf18 \n        buf21 =buf17 ;del buf17 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),256 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf21 )\n        buf22 =buf16 ;del buf16 \n\n        extern_kernels .mm (buf19 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf22 )\n\n        buf23 =torch .ops .aten ._thnn_fused_gru_cell .default (buf21 ,buf22 ,buf19 ,primals_4 ,primals_5 )\n        buf24 =buf23 [0 ]\n        buf25 =buf23 [1 ]\n        del buf23 \n        buf26 =buf22 ;del buf22 \n\n        extern_kernels .mm (buf24 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf26 )\n        buf27 =buf21 ;del buf21 \n\n        extern_kernels .mm (buf24 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf27 )\n\n        buf28 =torch .ops .aten ._thnn_fused_gru_cell .default (buf26 ,buf27 ,buf24 ,primals_8 ,primals_9 )\n        buf29 =buf28 [0 ]\n        buf30 =buf28 [1 ]\n        del buf28 \n        buf31 =buf27 ;del buf27 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),384 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf31 )\n        buf32 =buf26 ;del buf26 \n\n        extern_kernels .mm (buf29 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf32 )\n\n        buf33 =torch .ops .aten ._thnn_fused_gru_cell .default (buf31 ,buf32 ,buf29 ,primals_4 ,primals_5 )\n        buf34 =buf33 [0 ]\n        buf35 =buf33 [1 ]\n        del buf33 \n        buf36 =buf32 ;del buf32 \n\n        extern_kernels .mm (buf34 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf36 )\n        buf37 =buf31 ;del buf31 \n\n        extern_kernels .mm (buf34 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf37 )\n\n        buf38 =torch .ops .aten ._thnn_fused_gru_cell .default (buf36 ,buf37 ,buf34 ,primals_8 ,primals_9 )\n        buf39 =buf38 [0 ]\n        buf40 =buf38 [1 ]\n        del buf38 \n        buf41 =buf37 ;del buf37 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),512 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf41 )\n        buf42 =buf36 ;del buf36 \n\n        extern_kernels .mm (buf39 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf42 )\n\n        buf43 =torch .ops .aten ._thnn_fused_gru_cell .default (buf41 ,buf42 ,buf39 ,primals_4 ,primals_5 )\n        buf44 =buf43 [0 ]\n        buf45 =buf43 [1 ]\n        del buf43 \n        buf46 =buf42 ;del buf42 \n\n        extern_kernels .mm (buf44 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf46 )\n        buf47 =buf41 ;del buf41 \n\n        extern_kernels .mm (buf44 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf47 )\n\n        buf48 =torch .ops .aten ._thnn_fused_gru_cell .default (buf46 ,buf47 ,buf44 ,primals_8 ,primals_9 )\n        buf49 =buf48 [0 ]\n        buf50 =buf48 [1 ]\n        del buf48 \n        buf51 =buf47 ;del buf47 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),640 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf51 )\n        buf52 =buf46 ;del buf46 \n\n        extern_kernels .mm (buf49 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf52 )\n\n        buf53 =torch .ops .aten ._thnn_fused_gru_cell .default (buf51 ,buf52 ,buf49 ,primals_4 ,primals_5 )\n        buf54 =buf53 [0 ]\n        buf55 =buf53 [1 ]\n        del buf53 \n        buf56 =buf52 ;del buf52 \n\n        extern_kernels .mm (buf54 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf56 )\n        buf57 =buf51 ;del buf51 \n\n        extern_kernels .mm (buf54 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf57 )\n\n        buf58 =torch .ops .aten ._thnn_fused_gru_cell .default (buf56 ,buf57 ,buf54 ,primals_8 ,primals_9 )\n        buf59 =buf58 [0 ]\n        buf60 =buf58 [1 ]\n        del buf58 \n        buf61 =buf57 ;del buf57 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),768 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf61 )\n        buf62 =buf56 ;del buf56 \n\n        extern_kernels .mm (buf59 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf62 )\n\n        buf63 =torch .ops .aten ._thnn_fused_gru_cell .default (buf61 ,buf62 ,buf59 ,primals_4 ,primals_5 )\n        buf64 =buf63 [0 ]\n        buf65 =buf63 [1 ]\n        del buf63 \n        buf66 =buf62 ;del buf62 \n\n        extern_kernels .mm (buf64 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf66 )\n        buf67 =buf61 ;del buf61 \n\n        extern_kernels .mm (buf64 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf67 )\n\n        buf68 =torch .ops .aten ._thnn_fused_gru_cell .default (buf66 ,buf67 ,buf64 ,primals_8 ,primals_9 )\n        buf69 =buf68 [0 ]\n        buf70 =buf68 [1 ]\n        del buf68 \n        buf71 =buf67 ;del buf67 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),896 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf71 )\n        buf72 =buf66 ;del buf66 \n\n        extern_kernels .mm (buf69 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf72 )\n\n        buf73 =torch .ops .aten ._thnn_fused_gru_cell .default (buf71 ,buf72 ,buf69 ,primals_4 ,primals_5 )\n        buf74 =buf73 [0 ]\n        buf75 =buf73 [1 ]\n        del buf73 \n        buf76 =buf72 ;del buf72 \n\n        extern_kernels .mm (buf74 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf76 )\n        buf77 =buf71 ;del buf71 \n\n        extern_kernels .mm (buf74 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf77 )\n\n        buf78 =torch .ops .aten ._thnn_fused_gru_cell .default (buf76 ,buf77 ,buf74 ,primals_8 ,primals_9 )\n        buf79 =buf78 [0 ]\n        buf80 =buf78 [1 ]\n        del buf78 \n        buf81 =buf77 ;del buf77 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),1024 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf81 )\n        buf82 =buf76 ;del buf76 \n\n        extern_kernels .mm (buf79 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf82 )\n\n        buf83 =torch .ops .aten ._thnn_fused_gru_cell .default (buf81 ,buf82 ,buf79 ,primals_4 ,primals_5 )\n        buf84 =buf83 [0 ]\n        buf85 =buf83 [1 ]\n        del buf83 \n        buf86 =buf82 ;del buf82 \n\n        extern_kernels .mm (buf84 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf86 )\n        buf87 =buf81 ;del buf81 \n\n        extern_kernels .mm (buf84 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf87 )\n\n        buf88 =torch .ops .aten ._thnn_fused_gru_cell .default (buf86 ,buf87 ,buf84 ,primals_8 ,primals_9 )\n        buf89 =buf88 [0 ]\n        buf90 =buf88 [1 ]\n        del buf88 \n        buf91 =buf87 ;del buf87 \n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),1152 ),reinterpret_tensor (primals_2 ,(128 ,192 ),(1 ,128 ),0 ),out =buf91 )\n        del primals_2 \n        buf92 =buf86 ;del buf86 \n\n        extern_kernels .mm (buf89 ,reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf92 )\n\n        buf93 =torch .ops .aten ._thnn_fused_gru_cell .default (buf91 ,buf92 ,buf89 ,primals_4 ,primals_5 )\n        del primals_4 \n        del primals_5 \n        buf94 =buf93 [0 ]\n        buf95 =buf93 [1 ]\n        del buf93 \n        buf96 =buf92 ;del buf92 \n\n        extern_kernels .mm (buf94 ,reinterpret_tensor (primals_6 ,(64 ,192 ),(1 ,64 ),0 ),out =buf96 )\n        buf97 =buf91 ;del buf91 \n\n        extern_kernels .mm (buf94 ,reinterpret_tensor (primals_7 ,(64 ,192 ),(1 ,64 ),0 ),out =buf97 )\n\n        buf98 =torch .ops .aten ._thnn_fused_gru_cell .default (buf96 ,buf97 ,buf94 ,primals_8 ,primals_9 )\n        del buf96 \n        del buf97 \n        del primals_8 \n        del primals_9 \n        buf99 =buf98 [0 ]\n        buf100 =buf98 [1 ]\n        del buf98 \n        buf104 =buf99 ;del buf99 \n        buf105 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n        buf106 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_log_sigmoid_forward_native_layer_norm_native_layer_norm_backward_1 [grid (1 )](buf104 ,primals_10 ,primals_11 ,buf105 ,buf106 ,1 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n    return (buf105 ,primals_10 ,primals_11 ,buf0 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),0 ),buf4 ,buf5 ,buf9 ,buf10 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),128 ),buf14 ,buf15 ,buf19 ,buf20 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),256 ),buf24 ,buf25 ,buf29 ,buf30 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),384 ),buf34 ,buf35 ,buf39 ,buf40 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),512 ),buf44 ,buf45 ,buf49 ,buf50 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),640 ),buf54 ,buf55 ,buf59 ,buf60 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),768 ),buf64 ,buf65 ,buf69 ,buf70 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),896 ),buf74 ,buf75 ,buf79 ,buf80 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),1024 ),buf84 ,buf85 ,buf89 ,buf90 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),1152 ),buf94 ,buf95 ,buf100 ,buf104 ,buf106 ,primals_7 ,primals_6 ,primals_3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ,128 ),(1280 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((192 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "53fb565b-0653-44c6-818d-215bb38c43e2",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['InstanceNorm1d', 'MaxPool1d', 'ModuleDict', 'UpsamplingBilinear2d', 'ReplicationPad1d', 'Dropout2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.instance_norm = nn.InstanceNorm1d(64)\n        self.max_pool = nn.MaxPool1d(kernel_size=2)\n        self.module_dict = nn.ModuleDict({\n            'upsample': nn.UpsamplingBilinear2d(scale_factor=2),\n            'replication_pad': nn.ReplicationPad1d(2),\n            'dropout': nn.Dropout2d(p=0.5)\n        })\n        self.instance_norm2 = nn.InstanceNorm1d(32)\n        self.max_pool2 = nn.MaxPool1d(kernel_size=2)\n        self.dropout2 = nn.Dropout2d(p=0.3)\n\n    def forward(self, x):\n        # Assuming input is of shape (batch_size, channels, length)\n        x = self.instance_norm(x)\n        x = self.max_pool(x)\n        \n        # Reshape to 2D for UpsamplingBilinear2d\n        x = x.unsqueeze(-1)  # Add height dimension\n        x = self.module_dict['upsample'](x)\n        \n        # Reshape back to 1D for ReplicationPad1d\n        x = x.squeeze(-1)\n        x = self.module_dict['replication_pad'](x)\n        \n        # Apply dropout\n        x = x.unsqueeze(-1).unsqueeze(-1)  # Add height and width dimensions for Dropout2d\n        x = self.module_dict['dropout'](x)\n        x = x.squeeze(-1).squeeze(-1)  # Remove height and width dimensions\n        \n        # Apply second instance norm and max pool\n        x = self.instance_norm2(x)\n        x = self.max_pool2(x)\n        \n        # Apply second dropout\n        x = x.unsqueeze(-1).unsqueeze(-1)  # Add height and width dimensions for Dropout2d\n        x = self.dropout2(x)\n        x = x.squeeze(-1).squeeze(-1)  # Remove height and width dimensions\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 128).cuda()  # Example input shape (batch_size, channels, length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +128 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp1 ,0 )\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp6 =tl .where (xmask ,tmp4 ,0 )\n    tmp7 =tl .sum (tmp6 ,1 )[:,None ]\n    tmp8 =tl .full ([XBLOCK ,1 ],128 ,tl .int32 )\n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp7 /tmp9 \n    tmp11 =tmp1 -tmp10 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n    tmp15 =tl .where (xmask ,tmp13 ,0 )\n    tmp16 =tl .sum (tmp15 ,1 )[:,None ]\n    tmp17 =tmp0 -tmp10 \n    tmp18 =128.0 \n    tmp19 =tmp16 /tmp18 \n    tmp20 =1e-05 \n    tmp21 =tmp19 +tmp20 \n    tmp22 =libdevice .rsqrt (tmp21 )\n    tmp23 =tmp17 *tmp22 \n    tl .store (out_ptr2 +(r0_1 +128 *x0 ),tmp23 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_1 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x1 =((xindex //2 )%128 )\n    x2 =xindex //256 \n    x4 =xindex \n    tmp0 =x1 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.49606299212598426 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],63 ,tl .int64 )\n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tmp11 =tl .load (in_ptr0 +(2 *tmp10 +128 *x2 ),None ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr0 +(1 +2 *tmp10 +128 *x2 ),None ,eviction_policy ='evict_last')\n    tmp13 =triton_helpers .maximum (tmp12 ,tmp11 )\n    tmp14 =tmp13 -tmp13 \n    tmp15 =tmp14 *tmp4 \n    tmp16 =tmp13 +tmp15 \n    tmp17 =tl .load (in_ptr0 +(2 *tmp6 +128 *x2 ),None ,eviction_policy ='evict_last')\n    tmp18 =tl .load (in_ptr0 +(1 +2 *tmp6 +128 *x2 ),None ,eviction_policy ='evict_last')\n    tmp19 =triton_helpers .maximum (tmp18 ,tmp17 )\n    tmp20 =tmp19 -tmp19 \n    tmp21 =tmp20 *tmp4 \n    tmp22 =tmp19 +tmp21 \n    tmp23 =tmp16 -tmp22 \n    tmp24 =tmp6 .to (tl .float32 )\n    tmp25 =tmp5 -tmp24 \n    tmp26 =triton_helpers .maximum (tmp25 ,tmp4 )\n    tmp27 =1.0 \n    tmp28 =triton_helpers .minimum (tmp26 ,tmp27 )\n    tmp29 =tmp23 *tmp28 \n    tmp30 =tmp22 +tmp29 \n    tl .store (in_out_ptr0 +(x4 ),tmp30 ,None )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,64 ,128 ),(8192 ,128 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf3 =empty_strided_cuda ((1 ,64 ,128 ),(8192 ,128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_0 [grid (64 )](arg0_1 ,buf3 ,64 ,128 ,XBLOCK =8 ,num_warps =8 ,num_stages =1 )\n        del arg0_1 \n        buf4 =empty_strided_cuda ((1 ,64 ,128 ,2 ),(16384 ,256 ,2 ,1 ),torch .float32 )\n        buf5 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_1 [grid (16384 )](buf5 ,buf3 ,16384 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,64 ,128 ),(8192 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "57aec599-33b8-4c13-ac04-51a2e35b83ee",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Unflatten', 'Upsample', 'InstanceNorm1d', 'Fold', 'MaxPool2d', 'LogSoftmax', 'CELU', 'TransformerEncoderLayer', 'MultiLabelMarginLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.unflatten = nn.Unflatten(1, (1, 28, 28))  # Assuming input is flattened\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.instance_norm = nn.InstanceNorm1d(28 * 28)\n        self.fold = nn.Fold(output_size=(14, 14), kernel_size=(2, 2), stride=(2, 2))\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.celu = nn.CELU()\n        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=14 * 14, nhead=7)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n        self.loss = nn.MultiLabelMarginLoss()\n\n    def forward(self, x):\n        # Assuming input is flattened, unflatten it\n        x = self.unflatten(x)\n        \n        # Upsample the input\n        x = self.upsample(x)\n        \n        # Flatten for InstanceNorm1d\n        x = x.view(x.size(0), -1, x.size(2) * x.size(3))\n        x = self.instance_norm(x)\n        \n        # Reshape back to 2D\n        x = x.view(x.size(0), -1, 28, 28)\n        \n        # Fold the input\n        x = self.fold(x)\n        \n        # Apply MaxPool2d\n        x = self.max_pool(x)\n        \n        # Apply CELU activation\n        x = self.celu(x)\n        \n        # Flatten for TransformerEncoderLayer\n        x = x.view(x.size(0), -1, 14 * 14)\n        x = self.transformer_encoder_layer(x)\n        \n        # Reshape back to 2D\n        x = x.view(x.size(0), -1, 14, 14)\n        \n        # Apply LogSoftmax\n        x = x.view(x.size(0), -1)\n        x = self.log_softmax(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1 * 28 * 28).cuda()  # Assuming input is flattened\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3136 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //56 \n    x0 =(xindex %56 )\n    x2 =xindex \n    tmp0 =x1 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.4909090909090909 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],27 ,tl .int64 )\n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tmp11 =x0 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =tmp12 *tmp2 \n    tmp14 =triton_helpers .maximum (tmp13 ,tmp4 )\n    tmp15 =tmp14 .to (tl .int32 )\n    tmp16 =tl .load (in_ptr0 +(tmp15 +28 *tmp10 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tmp15 +tmp7 \n    tmp18 =triton_helpers .minimum (tmp17 ,tmp9 )\n    tmp19 =tl .load (in_ptr0 +(tmp18 +28 *tmp10 ),xmask ,eviction_policy ='evict_last')\n    tmp20 =tmp19 -tmp16 \n    tmp21 =tmp15 .to (tl .float32 )\n    tmp22 =tmp14 -tmp21 \n    tmp23 =triton_helpers .maximum (tmp22 ,tmp4 )\n    tmp24 =1.0 \n    tmp25 =triton_helpers .minimum (tmp23 ,tmp24 )\n    tmp26 =tmp20 *tmp25 \n    tmp27 =tmp16 +tmp26 \n    tmp28 =tl .load (in_ptr0 +(tmp15 +28 *tmp6 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr0 +(tmp18 +28 *tmp6 ),xmask ,eviction_policy ='evict_last')\n    tmp30 =tmp29 -tmp28 \n    tmp31 =tmp30 *tmp25 \n    tmp32 =tmp28 +tmp31 \n    tmp33 =tmp27 -tmp32 \n    tmp34 =tmp6 .to (tl .float32 )\n    tmp35 =tmp5 -tmp34 \n    tmp36 =triton_helpers .maximum (tmp35 ,tmp4 )\n    tmp37 =triton_helpers .minimum (tmp36 ,tmp24 )\n    tmp38 =tmp33 *tmp37 \n    tmp39 =tmp32 +tmp38 \n    tl .store (in_out_ptr0 +(x2 ),tmp39 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,784 ),(784 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,56 ,56 ),(3136 ,3136 ,56 ,1 ),torch .float32 )\n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_0 [grid (3136 )](buf1 ,arg0_1 ,3136 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n    return (reinterpret_tensor (buf1 ,(1 ,1 ,3136 ),(3136 ,3136 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,784 ),(784 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "5a3fc60f-9b8b-4344-a5d4-45bf3e376813",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ZeroPad2d', 'CircularPad2d', 'AvgPool2d', 'LSTM', 'Hardtanh', 'Fold']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad = nn.ZeroPad2d(2)\n        self.circular_pad = nn.CircularPad2d(2)\n        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.hardtanh = nn.Hardtanh(min_val=-1.0, max_val=1.0)\n        self.fold = nn.Fold(output_size=(8, 8), kernel_size=(2, 2), stride=(2, 2))\n\n    def forward(self, x):\n        # Apply ZeroPad2d\n        x = self.zero_pad(x)\n        \n        # Apply CircularPad2d\n        x = self.circular_pad(x)\n        \n        # Apply AvgPool2d\n        x = self.avg_pool(x)\n        \n        # Reshape for LSTM\n        batch_size, channels, height, width = x.size()\n        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # Reshape to (batch_size, seq_len, input_size)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Reshape back to 4D tensor\n        x = x.permute(0, 2, 1).view(batch_size, -1, height, width)\n        \n        # Apply Hardtanh\n        x = self.hardtanh(x)\n        \n        # Apply Fold\n        x = self.fold(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks2 )\n    x2 =xindex //ks4 \n    x3 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],2 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =6 +ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =x1 \n    tmp7 =tl .full ([1 ],2 ,tl .int64 )\n    tmp8 =tmp6 >=tmp7 \n    tmp9 =tl .broadcast_to (6 +ks3 ,[XBLOCK ])\n    tmp10 =tmp6 <tmp9 \n    tmp11 =tmp8 &tmp10 \n    tmp12 =tmp11 &tmp5 \n    tmp13 =(-4 )+x1 \n    tmp14 =tl .full ([1 ],0 ,tl .int64 )\n    tmp15 =tmp13 >=tmp14 \n    tmp16 =tl .broadcast_to (ks3 ,[XBLOCK ])\n    tmp17 =tmp13 <tmp16 \n    tmp18 =(-4 )+x0 \n    tmp19 =tmp18 >=tmp14 \n    tmp20 =tl .broadcast_to (ks1 ,[XBLOCK ])\n    tmp21 =tmp18 <tmp20 \n    tmp22 =tmp15 &tmp17 \n    tmp23 =tmp22 &tmp19 \n    tmp24 =tmp23 &tmp21 \n    tmp25 =tmp24 &tmp12 \n    tmp26 =tl .load (in_ptr0 +((-4 )+x0 +((-4 )*ks1 )+ks1 *x1 +ks1 *ks3 *x2 ),tmp25 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp27 =tl .full (tmp26 .shape ,0.0 ,tmp26 .dtype )\n    tmp28 =tl .where (tmp12 ,tmp26 ,tmp27 )\n    tmp29 =tl .load (in_ptr1 +(x3 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp30 =tl .where (tmp11 ,tmp28 ,tmp29 )\n    tmp31 =tl .full (tmp30 .shape ,0.0 ,tmp30 .dtype )\n    tmp32 =tl .where (tmp5 ,tmp30 ,tmp31 )\n    tmp33 =float (\"nan\")\n    tmp34 =tl .where (tmp5 ,tmp32 ,tmp33 )\n    tl .store (out_ptr0 +(x3 ),tmp34 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x3 =xindex \n    tmp39 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],2 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =x0 \n    tmp4 =tl .broadcast_to (6 +ks2 ,[XBLOCK ])\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =(-4 )+x0 +((-1 )*ks2 )\n    tmp8 =tl .full ([1 ],2 ,tl .int64 )\n    tmp9 =tmp7 <tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(32 +x3 +4 *ks2 +8 *ks3 +ks2 *ks3 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(28 +x3 +3 *ks2 +8 *ks3 +ks2 *ks3 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =tl .full ([1 ],2 ,tl .int64 )\n    tmp17 =tmp3 <tmp16 \n    tmp18 =tmp17 &tmp2 \n    tmp19 =tl .load (in_ptr0 +(36 +x3 +5 *ks2 +8 *ks3 +ks2 *ks3 ),tmp18 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =tl .load (in_ptr0 +(32 +x3 +4 *ks2 +8 *ks3 +ks2 *ks3 ),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .where (tmp17 ,tmp19 ,tmp20 )\n    tmp22 =tl .where (tmp5 ,tmp15 ,tmp21 )\n    tmp23 =tl .full (tmp22 .shape ,0.0 ,tmp22 .dtype )\n    tmp24 =tl .where (tmp2 ,tmp22 ,tmp23 )\n    tmp25 =x0 \n    tmp26 =6 +ks2 \n    tmp27 =tmp25 >=tmp26 \n    tmp28 =(-4 )+x0 +((-1 )*ks2 )\n    tmp29 =tl .full ([1 ],2 ,tl .int64 )\n    tmp30 =tmp28 <tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(x3 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +((-4 )+x3 +((-1 )*ks2 )),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =tmp25 <tmp1 \n    tmp38 =tl .load (in_ptr0 +(4 +ks2 +x3 ),tmp37 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp40 =tl .where (tmp37 ,tmp38 ,tmp39 )\n    tmp41 =tl .where (tmp27 ,tmp36 ,tmp40 )\n    tmp42 =tl .where (tmp2 ,tmp24 ,tmp41 )\n    tl .store (out_ptr0 +(x3 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x3 =xindex \n    tmp4 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =6 +ks2 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .load (in_ptr0 +((-32 )+x3 +((-8 )*ks2 )+((-4 )*ks3 )+((-1 )*ks2 *ks3 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tl .store (out_ptr0 +(x3 ),tmp5 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +16 *x1 +64 *x2 +2 *ks4 *x1 +8 *ks3 *x2 +8 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +16 *x1 +64 *x2 +2 *ks4 *x1 +8 *ks3 *x2 +8 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(8 +ks4 +2 *x0 +16 *x1 +64 *x2 +2 *ks4 *x1 +8 *ks3 *x2 +8 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(9 +ks4 +2 *x0 +16 *x1 +64 *x2 +2 *ks4 *x1 +8 *ks3 *x2 +8 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tl .store (out_ptr0 +(x3 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_permute_4 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *(x0 //ks1 )+16 *x1 +(ks3 //2 )*(x0 //ks1 )+4 *x1 *(ks2 //2 )+4 *x1 *(ks3 //2 )+x1 *(ks2 //2 )*(ks3 //2 )+((x0 %ks1 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,8 +s1 ,8 +s2 ),(64 *s0 +8 *s0 *s1 +8 *s0 *s2 +s0 *s1 *s2 ,64 +8 *s1 +8 *s2 +s1 *s2 ,8 +s2 ,1 ),torch .float32 )\n        8 +s2 \n        8 +s1 \n        64 +8 *s1 +8 *s2 +s1 *s2 \n        buf1 =empty_strided_cuda ((1 ,s0 ,8 +s1 ,8 +s2 ),(64 *s0 +8 *s0 *s1 +8 *s0 *s2 +s0 *s1 *s2 ,64 +8 *s1 +8 *s2 +s1 *s2 ,8 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_0_xnumel =64 *s0 +8 *s0 *s1 +8 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (triton_poi_fused_copy_0_xnumel )](arg3_1 ,buf0 ,buf1 ,40 ,32 ,40 ,32 ,1600 ,4800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf2 =buf0 ;del buf0 \n\n        triton_poi_fused_1_xnumel =64 *s0 +8 *s0 *s1 +8 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_1 [grid (triton_poi_fused_1_xnumel )](buf1 ,buf2 ,40 ,40 ,32 ,32 ,4800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf3 =buf1 ;del buf1 \n\n        triton_poi_fused_2_xnumel =64 *s0 +8 *s0 *s1 +8 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_2 [grid (triton_poi_fused_2_xnumel )](buf2 ,buf3 ,40 ,40 ,32 ,32 ,4800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        4 +(s2 //2 )\n        4 +(s1 //2 )\n        16 +4 *(s1 //2 )+4 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        buf4 =empty_strided_cuda ((1 ,s0 ,4 +(s1 //2 ),4 +(s2 //2 )),(16 *s0 +4 *s0 *(s1 //2 )+4 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),16 +4 *(s1 //2 )+4 *(s2 //2 )+(s1 //2 )*(s2 //2 ),4 +(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_3_xnumel =16 *s0 +4 *s0 *(s1 //2 )+4 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_3 [grid (triton_poi_fused_avg_pool2d_3_xnumel )](buf3 ,buf4 ,20 ,20 ,400 ,32 ,32 ,1200 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        16 +4 *(s1 //2 )+4 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        buf5 =empty_strided_cuda ((1 ,16 +4 *(s1 //2 )+4 *(s2 //2 )+(s1 //2 )*(s2 //2 ),s0 ),(16 *s0 +4 *s0 *(s1 //2 )+4 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),1 ,16 +4 *(s1 //2 )+4 *(s2 //2 )+(s1 //2 )*(s2 //2 )),torch .float32 )\n\n        triton_poi_fused_permute_4_xnumel =16 *s0 +4 *s0 *(s1 //2 )+4 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_permute_4 [grid (triton_poi_fused_permute_4_xnumel )](buf4 ,buf5 ,400 ,20 ,32 ,32 ,1200 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n    return (buf5 ,4 +(s1 //2 ),4 +(s2 //2 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "5a9158ce-36b0-464a-9aa0-e084b50f7a4e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Sigmoid', 'PixelUnshuffle', 'GELU', 'Flatten', 'MultiMarginLoss', 'MaxUnpool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pixel_unshuffle = nn.PixelUnshuffle(downscale_factor=2)\n        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)\n        self.gelu = nn.GELU()\n        self.sigmoid = nn.Sigmoid()\n        self.flatten = nn.Flatten()\n        self.multi_margin_loss = nn.MultiMarginLoss()\n\n    def forward(self, x):\n        # Apply PixelUnshuffle\n        x = self.pixel_unshuffle(x)\n        \n        # Reshape for MaxUnpool1d\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels * height, width)\n        \n        # Apply MaxUnpool1d (requires indices from a previous MaxPool1d)\n        # For simplicity, we assume the input has been pooled before\n        pool_output, indices = F.max_pool1d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool1d(pool_output, indices)\n        \n        # Reshape back to original dimensions\n        x = x.view(batch_size, channels, height, width)\n        \n        # Apply GELU\n        x = self.gelu(x)\n        \n        # Apply Sigmoid\n        x = self.sigmoid(x)\n        \n        # Flatten the output\n        x = self.flatten(x)\n        \n        # Compute MultiMarginLoss (requires target labels)\n        # For simplicity, we assume the target labels are randomly generated\n        target = torch.randint(0, 10, (batch_size,), device=x.device)\n        loss = self.multi_margin_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *((x0 %(ks2 //4 )))+2 *(ks2 //2 )*((((((x0 //((ks1 //2 )*(ks2 //4 )))%(4 *ks0 )))//2 )%2 ))+4 *(ks2 //2 )*(((x0 //(ks2 //4 ))%(ks1 //2 )))+4 *(ks1 //2 )*(ks2 //2 )*((((((x0 //((ks1 //2 )*(ks2 //4 )))%(4 *ks0 )))//4 )%ks0 ))+(((((x0 //((ks1 //2 )*(ks2 //4 )))%(4 *ks0 )))%2 ))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(2 +4 *((x0 %(ks2 //4 )))+2 *(ks2 //2 )*((((((x0 //((ks1 //2 )*(ks2 //4 )))%(4 *ks0 )))//2 )%2 ))+4 *(ks2 //2 )*(((x0 //(ks2 //4 ))%(ks1 //2 )))+4 *(ks1 //2 )*(ks2 //2 )*((((((x0 //((ks1 //2 )*(ks2 //4 )))%(4 *ks0 )))//4 )%ks0 ))+(((((x0 //((ks1 //2 )*(ks2 //4 )))%(4 *ks0 )))%2 ))),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp7 =tl .full ([1 ],2 ,tl .int32 )\n    tmp8 =tl .where ((tmp5 <0 )!=(tmp7 <0 ),tl .where (tmp5 %tmp7 !=0 ,tmp5 //tmp7 -1 ,tmp5 //tmp7 ),tmp5 //tmp7 )\n    tmp9 =tmp8 *tmp7 \n    tmp10 =tmp5 -tmp9 \n    tmp11 =tl .full ([1 ],0 ,tl .int64 )\n    tmp12 =tmp11 +tmp8 \n    tmp13 =2 *((x0 %(ks2 //4 )))\n    tmp14 =tmp13 +tmp10 \n    tmp15 =ks2 //2 \n    tmp16 =tmp12 *tmp15 \n    tmp17 =tmp16 +tmp14 \n    tmp18 =2 *(ks2 //4 )*(triton_helpers .div_floor_integer (x0 ,ks2 //4 ))\n    tmp19 =tmp17 +tmp18 \n    tmp20 =2 *ks0 *(ks1 //2 )*(triton_helpers .div_floor_integer (ks1 ,ks1 //2 ))*(ks2 //4 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))\n    tmp21 =tmp19 +tmp20 \n    tmp22 =tmp19 <0 \n    tmp23 =tl .where (tmp22 ,tmp21 ,tmp19 )\n    tl .device_assert (((0 <=tmp23 )&(tmp23 <2 *ks0 *(ks1 //2 )*(triton_helpers .div_floor_integer (ks1 ,ks1 //2 ))*(ks2 //4 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))))|~(xmask ),\"index out of bounds: 0 <= tmp23 < 2*ks0*(ks1 // 2)*(triton_helpers.div_floor_integer(ks1,  ks1 // 2))*(ks2 // 4)*(triton_helpers.div_floor_integer(ks2,  ks2 // 2))\")\n    tl .store (out_ptr0 +(tl .broadcast_to ((tmp23 %(2 *ks0 *(ks1 //2 )*(triton_helpers .div_floor_integer (ks1 ,ks1 //2 ))*(ks2 //4 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 )))),[XBLOCK ])),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_arange_clamp_min_gather_mean_ne_rsub_scalar_tensor_where_2 (in_ptr0 ,in_ptr1 ,out_ptr1 ,load_seed_offset ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp35 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp23 =tl .load (in_ptr1 +(2 *(ks3 //4 )*((((2 *(ks3 //4 )*((((r0_1 +ks1 *x0 *(ks2 //2 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(ks3 //4 )*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 )))//(ks3 //2 ))%(ks2 //2 )))+2 *(ks2 //2 )*(ks3 //4 )*((((r0_1 +ks1 *x0 *(ks2 //2 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(ks3 //4 )*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 )))//((ks2 //2 )*(ks3 //2 )))%(ks1 *(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 )))))+(((r0_1 +ks1 *x0 *(ks2 //2 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(ks3 //4 )*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 )))%(ks3 //2 ))))//(2 *(ks3 //4 )))%(ks1 *(ks2 //2 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 )))))+(((((r0_1 +ks1 *x0 *(ks2 //2 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(ks3 //4 )*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 )))%(ks3 //2 )))%(2 *(ks3 //4 ))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp2 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp3 =tl .full ([1 ,1 ],10 ,tl .int64 )\n        tmp4 =triton_helpers .randint64 (tmp0 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp3 )\n        tmp5 =r0_1 +ks1 *x0 *(ks2 //2 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(ks3 //4 )*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 ))\n        tmp6 =tmp5 !=tmp4 \n        tmp7 =2 *ks1 *(ks2 //2 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(ks3 //4 )*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 ))\n        tmp8 =tmp4 +tmp7 \n        tmp9 =tmp4 <0 \n        tmp10 =tl .where (tmp9 ,tmp8 ,tmp4 )\n        tl .device_assert ((0 <=tmp10 )&(tmp10 <2 *ks1 *(ks2 //2 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(ks3 //4 )*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 ))),\"index out of bounds: 0 <= tmp10 < 2*ks1*(ks2 // 2)*(triton_helpers.div_floor_integer(ks2,  ks2 // 2))*(ks3 // 4)*(triton_helpers.div_floor_integer(ks3,  ks3 // 2))\")\n        tmp12 =tl .load (in_ptr1 +(2 *(ks3 //4 )*((((2 *(ks3 //4 )*(((tmp10 //(ks3 //2 ))%(ks2 //2 )))+2 *(ks2 //2 )*(ks3 //4 )*(((tmp10 //((ks2 //2 )*(ks3 //2 )))%(ks1 *(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 )))))+((tmp10 %(ks3 //2 ))))//(2 *(ks3 //4 )))%(ks1 *(ks2 //2 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))*(triton_helpers .div_floor_integer (ks3 ,ks3 //2 )))))+((((tmp10 %(ks3 //2 )))%(2 *(ks3 //4 ))))),None ,eviction_policy ='evict_last')\n        tmp13 =0.5 \n        tmp14 =tmp12 *tmp13 \n        tmp15 =0.7071067811865476 \n        tmp16 =tmp12 *tmp15 \n        tmp17 =libdevice .erf (tmp16 )\n        tmp18 =1.0 \n        tmp19 =tmp17 +tmp18 \n        tmp20 =tmp14 *tmp19 \n        tmp21 =tl .sigmoid (tmp20 )\n        tmp22 =tmp18 -tmp21 \n        tmp24 =tmp23 *tmp13 \n        tmp25 =tmp23 *tmp15 \n        tmp26 =libdevice .erf (tmp25 )\n        tmp27 =tmp26 +tmp18 \n        tmp28 =tmp24 *tmp27 \n        tmp29 =tl .sigmoid (tmp28 )\n        tmp30 =tmp22 +tmp29 \n        tmp31 =0.0 \n        tmp32 =triton_helpers .maximum (tmp30 ,tmp31 )\n        tmp33 =tl .where (tmp6 ,tmp32 ,tmp31 )\n        tmp34 =tl .broadcast_to (tmp33 ,[XBLOCK ,R0_BLOCK ])\n        tmp36 =_tmp35 +tmp34 \n        _tmp35 =tl .where (r0_mask &xmask ,tmp36 ,_tmp35 )\n    tmp35 =tl .sum (_tmp35 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp35 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_mean_3 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =2 *ks0 *(ks1 //2 )*(triton_helpers .div_floor_integer (ks1 ,ks1 //2 ))*(ks2 //4 )*(triton_helpers .div_floor_integer (ks2 ,ks2 //2 ))\n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp6 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 *(s1 //2 )*(s1 //(s1 //2 ))*(s2 //(s2 //2 )),2 *(s2 //4 ),1 ),(2 *s0 *(s1 //2 )*(s1 //(s1 //2 ))*(s2 //4 )*(s2 //(s2 //2 )),2 *(s2 //4 ),1 ,1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_0_xnumel =2 *s0 *(s1 //2 )*(s1 //(s1 //2 ))*(s2 //4 )*(s2 //(s2 //2 ))\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_0 [grid (triton_poi_fused_max_unpool2d_0_xnumel )](buf0 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool2d_1_xnumel =s0 *(s1 //2 )*(s1 //(s1 //2 ))*(s2 //4 )*(s2 //(s2 //2 ))\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_1 [grid (triton_poi_fused_max_unpool2d_1_xnumel )](arg3_1 ,buf0 ,3 ,64 ,64 ,6144 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf2 )\n        buf4 =empty_strided_cuda ((2 ,),(1 ,),torch .float32 )\n\n        s0 *(s1 //2 )*(s1 //(s1 //2 ))*(s2 //4 )*(s2 //(s2 //2 ))\n        get_raw_stream (0 )\n        triton_red_fused_add_arange_clamp_min_gather_mean_ne_rsub_scalar_tensor_where_2 [grid (2 )](buf2 ,buf0 ,buf4 ,0 ,3 ,64 ,64 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf0 \n        del buf2 \n        buf5 =empty_strided_cuda ((),(),torch .float32 )\n        buf6 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_per_fused_mean_3 [grid (1 )](buf6 ,buf4 ,3 ,64 ,64 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf4 \n    return (buf6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "5ab0f0d3-6167-49f9-9ebc-a014833bffa0",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReflectionPad1d', 'BatchNorm1d', 'CrossMapLRN2d', 'AdaptiveAvgPool1d', 'Module', 'MaxUnpool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reflection_pad = nn.ReflectionPad1d(2)\n        self.batch_norm = nn.BatchNorm1d(10)\n        self.cross_map_lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool1d(10)\n        self.max_unpool = nn.MaxUnpool1d(kernel_size=2, stride=2, padding=0)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, length)\n        x = self.reflection_pad(x)  # Shape: (batch_size, channels, length + 4)\n        x = x.unsqueeze(2)  # Shape: (batch_size, channels, 1, length + 4)\n        x = self.cross_map_lrn(x)  # Shape: (batch_size, channels, 1, length + 4)\n        x = x.squeeze(2)  # Shape: (batch_size, channels, length + 4)\n        x = self.batch_norm(x)  # Shape: (batch_size, channels, length + 4)\n        x = self.adaptive_avg_pool(x)  # Shape: (batch_size, channels, 10)\n        x = x.unsqueeze(2)  # Shape: (batch_size, channels, 1, 10)\n        x = self.max_unpool(x, indices=torch.zeros_like(x).long())  # Shape: (batch_size, channels, 1, 20)\n        x = x.squeeze(2)  # Shape: (batch_size, channels, 20)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 16).cuda()  # Example input shape: (batch_size, channels, length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_reflection_pad1d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks1 *x1 +(tl .where ((-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-2 )+x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-2 )+x0 )))+2 *ks1 ,(-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-2 )+x0 )))))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s1 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ),(4 *s0 +s0 *s1 ,4 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_reflection_pad1d_0_xnumel =4 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_reflection_pad1d_0 [grid (triton_poi_fused_reflection_pad1d_0_xnumel )](arg2_1 ,buf0 ,20 ,16 ,200 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n    return (reinterpret_tensor (buf0 ,(1 ,s0 ,1 ,4 +s1 ),(4 *s0 +s0 *s1 ,4 +s1 ,4 +s1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =16 \n    arg2_1 =rand_strided ((1 ,10 ,16 ),(160 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "5afb88b1-e1d5-4e6c-92a0-02425cfbc062",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Mish', 'PixelShuffle', 'NLLLoss2d', 'Dropout2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.mish1 = nn.Mish()\n        self.pixel_shuffle1 = nn.PixelShuffle(2)\n        self.dropout2d1 = nn.Dropout2d(0.5)\n        self.mish2 = nn.Mish()\n        self.pixel_shuffle2 = nn.PixelShuffle(2)\n        self.dropout2d2 = nn.Dropout2d(0.5)\n        self.nll_loss2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Apply Mish activation\n        x = self.mish1(x)\n        \n        # Apply PixelShuffle to rearrange the tensor\n        x = self.pixel_shuffle1(x)\n        \n        # Apply Dropout2d for regularization\n        x = self.dropout2d1(x)\n        \n        # Apply Mish activation again\n        x = self.mish2(x)\n        \n        # Apply PixelShuffle again\n        x = self.pixel_shuffle2(x)\n        \n        # Apply Dropout2d again\n        x = self.dropout2d2(x)\n        \n        # Apply NLLLoss2d (assuming x is the log probabilities and target is provided)\n        # Note: NLLLoss2d requires a target, which is not part of the input. \n        # This is just a placeholder to use the module.\n        # In practice, you would need to provide a target tensor.\n        # For the sake of this example, we will return x instead of applying NLLLoss2d.\n        # If you want to apply NLLLoss2d, you would need to modify the forward method to accept a target.\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 16, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks4 \n    x3 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(ks3 *(x1 //4 )+ks2 *ks3 *(((x0 //2 )%2 ))+2 *ks2 *ks3 *(((x1 //2 )%2 ))+4 *ks2 *ks3 *((x0 %2 ))+8 *ks2 *ks3 *((x1 %2 ))+(((x0 //4 )%ks3 ))),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr1 +(2 *((x1 %2 ))+((x0 %2 ))),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr2 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =20.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =tl_math .exp (tmp0 )\n    tmp4 =libdevice .log1p (tmp3 )\n    tmp5 =tl .where (tmp2 ,tmp0 ,tmp4 )\n    tmp6 =libdevice .tanh (tmp5 )\n    tmp7 =tmp0 *tmp6 \n    tmp9 =0.5 \n    tmp10 =tmp8 <tmp9 \n    tmp11 =tmp10 .to (tl .float32 )\n    tmp12 =2.0 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =tmp7 *tmp13 \n    tmp15 =tmp14 >tmp1 \n    tmp16 =tl_math .exp (tmp14 )\n    tmp17 =libdevice .log1p (tmp16 )\n    tmp18 =tl .where (tmp15 ,tmp14 ,tmp17 )\n    tmp19 =libdevice .tanh (tmp18 )\n    tmp20 =tmp14 *tmp19 \n    tmp22 =tmp21 <tmp9 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =tmp23 *tmp12 \n    tmp25 =tmp20 *tmp24 \n    tl .store (out_ptr0 +(x0 +ks3 *x3 *(triton_helpers .div_floor_integer (ks5 ,2 *(ks5 //4 )))*(triton_helpers .div_floor_integer (ks5 ,8 *(ks5 //16 )))),tmp25 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 //4 ,1 ,1 ),(s0 //4 ,1 ,s0 //4 ,s0 //4 ),torch .float32 )\n\n        triton_poi_fused_bernoulli_0_xnumel =s0 //4 \n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (triton_poi_fused_bernoulli_0_xnumel )](buf0 ,buf1 ,0 ,4 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,s0 //16 ,1 ,1 ),(s0 //16 ,1 ,s0 //16 ,s0 //16 ),torch .float32 )\n\n        triton_poi_fused_bernoulli_1_xnumel =s0 //16 \n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_1 [grid (triton_poi_fused_bernoulli_1_xnumel )](buf0 ,buf2 ,1 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        2 *s2 *(s0 //(2 *(s0 //4 )))\n        4 *s1 \n        8 *s1 *s2 *(s0 //(2 *(s0 //4 )))\n        buf3 =empty_strided_cuda ((1 ,s0 //16 ,4 *s1 ,2 *s2 *(s0 //(2 *(s0 //4 )))),(4 *s1 *s2 *(s0 //16 )*(s0 //(2 *(s0 //4 )))*(s0 //(8 *(s0 //16 ))),4 *s1 *s2 *(s0 //(2 *(s0 //4 )))*(s0 //(8 *(s0 //16 ))),s2 *(s0 //(2 *(s0 //4 )))*(s0 //(8 *(s0 //16 ))),1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_bernoulli_div_mul_2_xnumel =8 *s1 *s2 *(s0 //16 )*(s0 //(2 *(s0 //4 )))\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_2 [grid (triton_poi_fused__to_copy_bernoulli_div_mul_2_xnumel )](arg3_1 ,buf1 ,buf2 ,buf3 ,256 ,256 ,64 ,64 ,65536 ,16 ,65536 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        del buf2 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =16 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,16 ,64 ,64 ),(65536 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "5cbc565a-8a54-41e3-8d62-a54c6cf42a04",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool2d', 'InstanceNorm2d', 'ConstantPad2d', 'FractionalMaxPool2d', 'ZeroPad2d', 'ReplicationPad1d', 'TransformerDecoder']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.instance_norm2d = nn.InstanceNorm2d(num_features=10)\n        self.constant_pad2d = nn.ConstantPad2d(padding=1, value=0)\n        self.fractional_maxpool2d = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.zero_pad2d = nn.ZeroPad2d(padding=1)\n        self.replication_pad1d = nn.ReplicationPad1d(padding=1)\n        self.transformer_decoder = nn.TransformerDecoder(\n            nn.TransformerDecoderLayer(d_model=512, nhead=8), num_layers=3\n        )\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        x = self.constant_pad2d(x)  # Apply ConstantPad2d\n        x = self.maxpool2d(x)  # Apply MaxPool2d\n        x = self.instance_norm2d(x)  # Apply InstanceNorm2d\n        x = self.fractional_maxpool2d(x)  # Apply FractionalMaxPool2d\n        x = self.zero_pad2d(x)  # Apply ZeroPad2d\n        \n        # Reshape for ReplicationPad1d\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels * height, width)  # Reshape to (batch_size, channels*height, width)\n        x = self.replication_pad1d(x)  # Apply ReplicationPad1d\n        \n        # Reshape for TransformerDecoder\n        x = x.view(batch_size, -1, 512)  # Reshape to (batch_size, seq_len, d_model=512)\n        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, d_model)\n        \n        # Create a dummy memory tensor for the TransformerDecoder\n        memory = torch.zeros_like(x)\n        x = self.transformer_decoder(x, memory)  # Apply TransformerDecoder\n        \n        # Reshape back to original shape (batch_size, channels, height, width)\n        x = x.permute(1, 0, 2)  # Reshape to (batch_size, seq_len, d_model)\n        x = x.view(batch_size, channels, height, width)  # Reshape back to (batch_size, channels, height, width)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape (batch_size=1, channels=3, height=64, width=64)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-1 )+2 *x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+2 *x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-1 )+((-1 )*ks3 )+2 *x0 +2 *ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =2 *x0 \n    tmp14 =tmp13 >=tmp1 \n    tmp15 =tmp13 <tmp7 \n    tmp16 =tmp9 &tmp14 \n    tmp17 =tmp16 &tmp15 \n    tmp18 =tl .load (in_ptr0 +(((-1 )*ks3 )+2 *x0 +2 *ks3 *x1 +ks2 *ks3 *x2 ),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =triton_helpers .maximum (tmp18 ,tmp12 )\n    tmp20 =2 *x1 \n    tmp21 =tmp20 >=tmp1 \n    tmp22 =tmp20 <tmp3 \n    tmp23 =tmp21 &tmp22 \n    tmp24 =tmp23 &tmp6 \n    tmp25 =tmp24 &tmp8 \n    tmp26 =tl .load (in_ptr0 +((-1 )+2 *x0 +2 *ks3 *x1 +ks2 *ks3 *x2 ),tmp25 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp27 =triton_helpers .maximum (tmp26 ,tmp19 )\n    tmp28 =tmp23 &tmp14 \n    tmp29 =tmp28 &tmp15 \n    tmp30 =tl .load (in_ptr0 +(2 *x0 +2 *ks3 *x1 +ks2 *ks3 *x2 ),tmp29 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp31 =triton_helpers .maximum (tmp30 ,tmp27 )\n    tl .store (out_ptr0 +(x4 ),tmp31 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        1 +(s2 //2 )\n        1 +(s1 //2 )\n        1 +(s1 //2 )*(s2 //2 )+(s1 //2 )+(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,1 +(s1 //2 ),1 +(s2 //2 )),(s0 +s0 *(s1 //2 )+s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),1 +(s1 //2 )*(s2 //2 )+(s1 //2 )+(s2 //2 ),1 +(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_pool2d_with_indices_0_xnumel =s0 +s0 *(s1 //2 )+s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_0 [grid (triton_poi_fused_max_pool2d_with_indices_0_xnumel )](arg3_1 ,buf0 ,33 ,33 ,64 ,64 ,1089 ,3267 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "5d5b6a36-fcb3-43ce-b247-a98c1ce30f39",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ELU', 'Softsign', 'RNN']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.elu1 = nn.ELU()\n        self.softsign1 = nn.Softsign()\n        self.rnn1 = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.elu2 = nn.ELU()\n        self.softsign2 = nn.Softsign()\n        self.rnn2 = nn.RNN(input_size=20, hidden_size=30, num_layers=2, batch_first=True)\n        self.elu3 = nn.ELU()\n        self.softsign3 = nn.Softsign()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, sequence_length, input_size)\n        # If not, reshape it accordingly\n        if len(x.shape) == 2:\n            x = x.unsqueeze(0)  # Add batch dimension if missing\n        elif len(x.shape) == 1:\n            x = x.unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions if missing\n\n        x = self.elu1(x)\n        x = self.softsign1(x)\n        \n        # Reshape for RNN input\n        batch_size, seq_len, input_size = x.size()\n        x, _ = self.rnn1(x)\n        \n        x = self.elu2(x)\n        x = self.softsign2(x)\n        \n        x, _ = self.rnn2(x)\n        \n        x = self.elu3(x)\n        x = self.softsign3(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 10).cuda()  # (batch_size, sequence_length, input_size)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_div_elu_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =1.0 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .expm1 (tmp4 )\n    tmp6 =tmp5 *tmp3 \n    tmp7 =tl .where (tmp2 ,tmp4 ,tmp6 )\n    tmp8 =tl_math .abs (tmp7 )\n    tmp9 =tmp8 +tmp3 \n    tmp10 =tmp7 /tmp9 \n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_add_div_elu_0_xnumel =s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_div_elu_0 [grid (triton_poi_fused_abs_add_div_elu_0_xnumel )](arg2_1 ,buf0 ,100 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =10 \n    arg2_1 =rand_strided ((1 ,10 ,10 ),(100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "624a7772-1701-4373-a9d8-a549593a238f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softmax', 'KLDivLoss', 'AdaptiveMaxPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool = nn.AdaptiveMaxPool1d(output_size=10)\n        self.softmax = nn.Softmax(dim=1)\n        self.kldivloss = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, x):\n        # Ensure the input is at least 2D\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n        \n        # Apply AdaptiveMaxPool1d\n        x = self.adaptive_max_pool(x)\n        \n        # Apply Softmax\n        x = self.softmax(x)\n        \n        # Generate a target distribution for KLDivLoss\n        target = torch.ones_like(x) / x.size(1)  # Uniform distribution\n        \n        # Compute KLDivLoss\n        loss = self.kldivloss(x.log(), target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 20, 100).cuda()  # Example input shape: (batch_size, channels, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_adaptive_max_pool2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(3 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(4 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(5 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(6 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(7 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(8 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr0 +(9 +10 *x0 +ks1 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\n    tmp8 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\n    tmp14 =triton_helpers .maximum (tmp13 ,tmp12 )\n    tmp16 =triton_helpers .maximum (tmp15 ,tmp14 )\n    tmp18 =triton_helpers .maximum (tmp17 ,tmp16 )\n    tl .store (out_ptr0 +(x2 ),tmp18 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(x0 +ks0 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =triton_helpers .maximum (_tmp2 ,tmp1 )\n        _tmp2 =tl .where (r0_mask &xmask ,tmp3 ,_tmp2 )\n    tmp2 =triton_helpers .max2 (_tmp2 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp4 =tl .load (in_ptr0 +(x0 +ks0 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp5 =tmp4 -tmp2 \n        tmp6 =tl_math .exp (tmp5 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =_tmp8 +tmp7 \n        _tmp8 =tl .where (r0_mask &xmask ,tmp9 ,_tmp8 )\n    tmp8 =tl .sum (_tmp8 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_div_log_mul_ones_like_sub_sum_xlogy_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp22 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        r0_0 =(r0_index %ks1 )\n        tmp12 =tl .load (in_ptr0 +(r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp13 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp16 =tl .load (in_ptr2 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp0 =1.0 \n        tmp1 =ks0 \n        tmp2 =tmp1 .to (tl .float32 )\n        tmp3 =tmp0 /tmp2 \n        tmp4 =libdevice .isnan (tmp3 ).to (tl .int1 )\n        tmp5 =0.0 \n        tmp6 =tmp3 ==tmp5 \n        tmp7 =tl_math .log (tmp3 )\n        tmp8 =tmp3 *tmp7 \n        tmp9 =tl .where (tmp6 ,tmp5 ,tmp8 )\n        tmp10 =float (\"nan\")\n        tmp11 =tl .where (tmp4 ,tmp10 ,tmp9 )\n        tmp14 =tmp12 -tmp13 \n        tmp15 =tl_math .exp (tmp14 )\n        tmp17 =tmp15 /tmp16 \n        tmp18 =tl_math .log (tmp17 )\n        tmp19 =tmp3 *tmp18 \n        tmp20 =tmp11 -tmp19 \n        tmp21 =tl .broadcast_to (tmp20 ,[XBLOCK ,R0_BLOCK ])\n        tmp23 =_tmp22 +tmp21 \n        _tmp22 =tl .where (r0_mask ,tmp23 ,_tmp22 )\n    tmp22 =tl .sum (_tmp22 ,1 )[:,None ]\n    tmp24 =1.0 \n    tmp25 =tmp22 *tmp24 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp25 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s1 //10 \n        buf0 =empty_strided_cuda ((1 ,s0 ,1 ,s1 //10 ),(s0 *(s1 //10 ),s1 //10 ,s1 //10 ,1 ),torch .float32 )\n\n        triton_poi_fused_adaptive_max_pool2d_0_xnumel =s0 *(s1 //10 )\n        get_raw_stream (0 )\n        triton_poi_fused_adaptive_max_pool2d_0 [grid (triton_poi_fused_adaptive_max_pool2d_0_xnumel )](arg2_1 ,buf0 ,10 ,100 ,200 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        buf1 =empty_strided_cuda ((1 ,1 ,s1 //10 ),(s1 //10 ,s1 //10 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,1 ,s1 //10 ),(s1 //10 ,s1 //10 ,1 ),torch .float32 )\n\n        triton_red_fused__softmax_1_xnumel =s1 //10 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_1 [grid (triton_red_fused__softmax_1_xnumel )](buf0 ,buf1 ,buf2 ,10 ,10 ,20 ,XBLOCK =16 ,R0_BLOCK =32 ,num_warps =4 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((),(),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        s0 *(s1 //10 )\n        get_raw_stream (0 )\n        triton_red_fused__softmax_div_log_mul_ones_like_sub_sum_xlogy_2 [grid (1 )](buf4 ,buf0 ,buf1 ,buf2 ,20 ,10 ,1 ,200 ,XBLOCK =1 ,R0_BLOCK =256 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n        del buf1 \n        del buf2 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =20 \n    arg1_1 =100 \n    arg2_1 =rand_strided ((1 ,20 ,100 ),(2000 ,100 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "64118b22-3d0b-43e1-8124-c69bb275a782",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FeatureAlphaDropout', 'Dropout3d', 'SoftMarginLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.feature_alpha_dropout1 = nn.FeatureAlphaDropout(p=0.5)\n        self.feature_alpha_dropout2 = nn.FeatureAlphaDropout(p=0.5)\n        self.dropout3d1 = nn.Dropout3d(p=0.5)\n        self.dropout3d2 = nn.Dropout3d(p=0.5)\n        self.soft_margin_loss = nn.SoftMarginLoss()\n\n    def forward(self, x):\n        # Apply FeatureAlphaDropout twice\n        x = self.feature_alpha_dropout1(x)\n        x = self.feature_alpha_dropout2(x)\n        \n        # Reshape the input to fit Dropout3d (assuming input is 4D: [batch, channels, depth, height, width])\n        # If the input is not 5D, we reshape it to make it 5D\n        if len(x.shape) == 4:\n            x = x.unsqueeze(2)  # Add a depth dimension\n        elif len(x.shape) == 3:\n            x = x.unsqueeze(1).unsqueeze(2)  # Add channels and depth dimensions\n        elif len(x.shape) == 2:\n            x = x.unsqueeze(1).unsqueeze(2).unsqueeze(3)  # Add channels, depth, height, and width dimensions\n        \n        # Apply Dropout3d twice\n        x = self.dropout3d1(x)\n        x = self.dropout3d2(x)\n        \n        # Reshape back to the original shape (or a shape that can be used with SoftMarginLoss)\n        # SoftMarginLoss expects the input to be of shape (N, *) where * means any number of additional dimensions\n        # and the target to be of the same shape as the input.\n        # For simplicity, we assume the target is the same as the input (which is not practical but fits the requirement)\n        target = x.detach()  # Detach to avoid backpropagation through the target\n        loss = self.soft_margin_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr2 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp24 =tl .load (in_ptr3 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp30 =tl .load (in_ptr4 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =0.5 \n    tmp3 =tmp1 <tmp2 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =0.8864048946659319 \n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp0 *tmp6 \n    tmp8 =-1.0 \n    tmp9 =tmp4 +tmp8 \n    tmp10 =1.558387861036063 \n    tmp11 =tmp9 *tmp10 \n    tmp12 =0.7791939305180315 \n    tmp13 =tmp11 +tmp12 \n    tmp14 =tmp7 +tmp13 \n    tmp16 =tmp15 <tmp2 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =tmp17 *tmp5 \n    tmp19 =tmp14 *tmp18 \n    tmp20 =tmp17 +tmp8 \n    tmp21 =tmp20 *tmp10 \n    tmp22 =tmp21 +tmp12 \n    tmp23 =tmp19 +tmp22 \n    tmp25 =tmp24 <tmp2 \n    tmp26 =tmp25 .to (tl .float32 )\n    tmp27 =2.0 \n    tmp28 =tmp26 *tmp27 \n    tmp29 =tmp23 *tmp28 \n    tmp31 =tmp30 <tmp2 \n    tmp32 =tmp31 .to (tl .float32 )\n    tmp33 =tmp32 *tmp27 \n    tmp34 =tmp29 *tmp33 \n    tl .store (out_ptr0 +(x2 ),tmp34 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_soft_margin_loss_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )\n        tmp1 =ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 ))%(ks0 *ks1 *ks2 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =-tmp3 \n        tmp5 =tmp4 *tmp3 \n        tmp6 =tl_math .exp (tmp5 )\n        tmp7 =libdevice .log1p (tmp6 )\n        tmp8 =tl .full (tmp7 .shape ,0 ,tmp7 .dtype )\n        tmp9 =tl .where (tmp2 ,tmp7 ,tmp8 )\n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =_tmp11 +tmp10 \n        _tmp11 =tl .where (r0_mask &xmask ,tmp12 ,_tmp11 )\n    tmp11 =tl .sum (_tmp11 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_soft_margin_loss_4 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =ks0 *ks1 *ks2 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp6 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((4 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[4 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,3 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_1 [grid (s0 )](buf0 ,buf2 ,1 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf3 ,3 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf4 ,3 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        s1 *s2 \n        buf5 =empty_strided_cuda ((1 ,s0 ,1 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s0 *s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_bernoulli_div_mul_2_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_2 [grid (triton_poi_fused__to_copy_bernoulli_div_mul_2_xnumel )](arg3_1 ,buf1 ,buf2 ,buf3 ,buf4 ,buf5 ,4096 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        del buf2 \n        del buf3 \n        del buf4 \n        buf6 =empty_strided_cuda ((2 ,),(1 ,),torch .float32 )\n\n        (1 +s0 *s1 *s2 )//2 \n        get_raw_stream (0 )\n        triton_red_fused_soft_margin_loss_3 [grid (2 )](buf5 ,buf6 ,3 ,64 ,64 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf5 \n        buf7 =empty_strided_cuda ((),(),torch .float32 )\n        buf8 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_per_fused_soft_margin_loss_4 [grid (1 )](buf8 ,buf6 ,3 ,64 ,64 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf6 \n    return (buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "648b159e-9c28-48fd-a715-f95667e56084",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FractionalMaxPool3d', 'Softsign', 'RReLU', 'Tanhshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))\n        self.softsign = nn.Softsign()\n        self.rrelu1 = nn.RReLU()\n        self.rrelu2 = nn.RReLU()\n        self.tanhshrink = nn.Tanhshrink()\n\n    def forward(self, x):\n        # Ensure the input is 5D (batch, channels, depth, height, width)\n        if x.dim() == 4:\n            x = x.unsqueeze(1)  # Add a channel dimension if necessary\n        elif x.dim() == 3:\n            x = x.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions if necessary\n        \n        x = self.fractional_max_pool3d(x)\n        x = self.softsign(x)\n        x = self.rrelu1(x)\n        x = self.rrelu2(x)\n        x = self.tanhshrink(x)\n        \n        # Flatten the output for compatibility with any downstream layers\n        x = x.view(x.size(0), -1)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 16, 16, 16).cuda()  # Example input with arbitrary shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_div_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =512 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =1.0 \n    tmp3 =tmp1 +tmp2 \n    tmp4 =tmp0 /tmp3 \n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_div_rrelu_with_noise_functional_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =512 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp7 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp8 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =1.0 \n    tmp3 =tmp1 +tmp2 \n    tmp4 =tmp0 /tmp3 \n    tmp5 =0.0 \n    tmp6 =tmp4 <=tmp5 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =tl .where (tmp6 ,tmp9 ,tmp7 )\n    tl .store (in_out_ptr0 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rrelu_with_noise_functional_sub_tanh_3 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =512 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =tmp0 <=tmp1 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =tl .where (tmp2 ,tmp4 ,tmp0 )\n    tmp6 =libdevice .tanh (tmp5 )\n    tmp7 =tmp5 -tmp6 \n    tl .store (in_out_ptr0 +(x0 ),tmp7 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,1 ,16 ,16 ,16 ),(4096 ,4096 ,256 ,16 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,1 ,3 ),(3 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (3 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n\n        buf2 =torch .ops .aten .fractional_max_pool3d .default (arg0_1 ,[2 ,2 ,2 ],[8 ,8 ,8 ],buf1 )\n        del arg0_1 \n        del buf1 \n        buf3 =buf2 [0 ]\n        del buf2 \n        buf5 =empty_strided_cuda ((1 ,1 ,8 ,8 ,8 ),(512 ,512 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_div_1 [grid (512 )](buf3 ,buf5 ,512 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n\n        buf6 =torch .ops .aten .uniform .default (buf5 ,0.125 ,0.3333333333333333 )\n        buf7 =buf6 \n        del buf6 \n        buf8 =buf3 ;del buf3 \n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_div_rrelu_with_noise_functional_2 [grid (512 )](buf8 ,buf5 ,buf7 ,512 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf5 \n        del buf7 \n\n        buf9 =torch .ops .aten .uniform .default (buf8 ,0.125 ,0.3333333333333333 )\n        buf10 =buf9 \n        del buf9 \n        buf11 =reinterpret_tensor (buf8 ,(1 ,1 ,8 ,8 ,8 ),(512 ,1 ,64 ,8 ,1 ),0 );del buf8 \n\n        get_raw_stream (0 )\n        triton_poi_fused_rrelu_with_noise_functional_sub_tanh_3 [grid (512 )](buf11 ,buf10 ,512 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf10 \n    return (reinterpret_tensor (buf11 ,(1 ,512 ),(512 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,1 ,16 ,16 ,16 ),(4096 ,4096 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "64ca8579-1665-47b2-90a9-33cc4e5d52df",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad2d', 'TripletMarginWithDistanceLoss', 'ModuleDict', 'SELU', 'AdaptiveAvgPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad2d(2, 3.0)\n        self.module_dict = nn.ModuleDict({\n            'selu1': nn.SELU(),\n            'selu2': nn.SELU(),\n            'selu3': nn.SELU(),\n            'selu4': nn.SELU(),\n            'selu5': nn.SELU()\n        })\n        self.avg_pool = nn.AdaptiveAvgPool1d(10)\n        self.loss = nn.TripletMarginWithDistanceLoss()\n\n    def forward(self, x):\n        # Apply padding\n        x = self.pad(x)\n        \n        # Flatten the input to 1D for AdaptiveAvgPool1d\n        x = x.view(x.size(0), -1)\n        \n        # Apply AdaptiveAvgPool1d\n        x = self.avg_pool(x)\n        \n        # Apply SELU activations from ModuleDict\n        for i in range(1, 6):\n            x = self.module_dict[f'selu{i}'](x)\n        \n        # Generate anchor, positive, and negative samples for TripletMarginWithDistanceLoss\n        anchor = x\n        positive = x + torch.randn_like(x) * 0.1\n        negative = x + torch.randn_like(x) * 0.2\n        \n        # Compute the triplet loss\n        loss = self.loss(anchor, positive, negative)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =3.0 )\n    tl .store (out_ptr0 +(x4 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *(((x0 //ks0 )%ks1 ))+16 *(x0 //(16 +4 *ks2 +4 *ks3 +ks2 *ks3 ))+ks3 *(((x0 //ks0 )%ks1 ))+4 *ks2 *(x0 //(16 +4 *ks2 +4 *ks3 +ks2 *ks3 ))+4 *ks3 *(x0 //(16 +4 *ks2 +4 *ks3 +ks2 *ks3 ))+ks2 *ks3 *(x0 //(16 +4 *ks2 +4 *ks3 +ks2 *ks3 ))+((x0 %ks0 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_clamp_min_elu_mean_mul_norm_randn_like_sub_2 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,load_seed_offset ,load_seed_offset1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =1.0507009873554805 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =1.0 \n    tmp6 =tmp0 *tmp5 \n    tmp7 =libdevice .expm1 (tmp6 )\n    tmp8 =1.7580993408473766 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =tl .where (tmp2 ,tmp4 ,tmp9 )\n    tmp11 =tmp10 >tmp1 \n    tmp12 =tmp10 *tmp3 \n    tmp13 =tmp10 *tmp5 \n    tmp14 =libdevice .expm1 (tmp13 )\n    tmp15 =tmp14 *tmp8 \n    tmp16 =tl .where (tmp11 ,tmp12 ,tmp15 )\n    tmp17 =tmp16 >tmp1 \n    tmp18 =tmp16 *tmp3 \n    tmp19 =tmp16 *tmp5 \n    tmp20 =libdevice .expm1 (tmp19 )\n    tmp21 =tmp20 *tmp8 \n    tmp22 =tl .where (tmp17 ,tmp18 ,tmp21 )\n    tmp23 =tmp22 >tmp1 \n    tmp24 =tmp22 *tmp3 \n    tmp25 =tmp22 *tmp5 \n    tmp26 =libdevice .expm1 (tmp25 )\n    tmp27 =tmp26 *tmp8 \n    tmp28 =tl .where (tmp23 ,tmp24 ,tmp27 )\n    tmp29 =tmp28 >tmp1 \n    tmp30 =tmp28 *tmp3 \n    tmp31 =tmp28 *tmp5 \n    tmp32 =libdevice .expm1 (tmp31 )\n    tmp33 =tmp32 *tmp8 \n    tmp34 =tl .where (tmp29 ,tmp30 ,tmp33 )\n    tmp35 =tl .load (in_ptr0 +load_seed_offset )\n    tmp36 =r0_0 \n    tmp37 =tl .randn (tmp35 ,(tmp36 ).to (tl .uint32 ))\n    tmp38 =tl .load (in_ptr0 +load_seed_offset1 )\n    tmp39 =tl .randn (tmp38 ,(tmp36 ).to (tl .uint32 ))\n    tmp40 =0.1 \n    tmp41 =tmp39 *tmp40 \n    tmp42 =tmp34 +tmp41 \n    tmp43 =tmp34 -tmp42 \n    tmp44 =1e-06 \n    tmp45 =tmp43 +tmp44 \n    tmp46 =tmp45 *tmp45 \n    tmp47 =tl .broadcast_to (tmp46 ,[XBLOCK ,R0_BLOCK ])\n    tmp49 =tl .where (r0_mask ,tmp47 ,0 )\n    tmp50 =tl .sum (tmp49 ,1 )[:,None ]\n    tmp51 =0.2 \n    tmp52 =tmp37 *tmp51 \n    tmp53 =tmp34 +tmp52 \n    tmp54 =tmp34 -tmp53 \n    tmp55 =tmp54 +tmp44 \n    tmp56 =tmp55 *tmp55 \n    tmp57 =tl .broadcast_to (tmp56 ,[XBLOCK ,R0_BLOCK ])\n    tmp59 =tl .where (r0_mask ,tmp57 ,0 )\n    tmp60 =tl .sum (tmp59 ,1 )[:,None ]\n    tmp61 =libdevice .sqrt (tmp50 )\n    tmp62 =tmp61 +tmp5 \n    tmp63 =libdevice .sqrt (tmp60 )\n    tmp64 =tmp62 -tmp63 \n    tmp65 =triton_helpers .maximum (tmp64 ,tmp1 )\n    tmp66 =tmp65 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp66 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg3_1 ,buf0 ,36 ,36 ,32 ,32 ,1296 ,3888 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((1 ,1 ,16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__adaptive_avg_pool2d_1_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_1 [grid (triton_poi_fused__adaptive_avg_pool2d_1_xnumel )](buf0 ,buf1 ,36 ,36 ,32 ,32 ,3888 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n\n        buf2 =torch .ops .aten ._adaptive_avg_pool2d .default (buf1 ,[1 ,10 ])\n        del buf1 \n        buf3 =buf2 \n        del buf2 \n        buf5 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf5 )\n        buf4 =reinterpret_tensor (buf3 ,(1 ,10 ),(10 ,1 ),0 );del buf3 \n        buf7 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf10 =reinterpret_tensor (buf7 ,(),(),0 );del buf7 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_clamp_min_elu_mean_mul_norm_randn_like_sub_2 [grid (1 )](buf4 ,buf10 ,buf5 ,1 ,0 ,1 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf4 \n        del buf5 \n    return (buf10 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "6556ec52-f537-4cbd-be42-09e4180b4c18",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardswish', 'AlphaDropout', 'LogSoftmax', 'MSELoss', 'ModuleList', 'AdaptiveAvgPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((7, 7))\n        self.module_list = nn.ModuleList([\n            nn.Sequential(\n                nn.Hardswish(),\n                nn.AlphaDropout(p=0.5)\n            ) for _ in range(3)\n        ])\n        self.log_softmax = nn.LogSoftmax(dim=1)\n        self.mse_loss = nn.MSELoss()\n\n    def forward(self, x):\n        x = self.adaptive_avg_pool(x)\n        for module in self.module_list:\n            x = module(x)\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x = self.log_softmax(x)\n        # Assuming we have a target tensor for MSELoss, we generate a dummy one here\n        target = torch.zeros_like(x)\n        loss = self.mse_loss(x, target)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax__to_copy_add_bernoulli_hardswish_mse_loss_mul_zeros_like_0 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,load_seed_offset ,load_seed_offset1 ,load_seed_offset2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp57 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp7 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =r0_0 \n        tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n        tmp3 =tl .load (in_ptr0 +load_seed_offset1 )\n        tmp4 =tl .rand (tmp3 ,(tmp1 ).to (tl .uint32 ))\n        tmp5 =tl .load (in_ptr0 +load_seed_offset2 )\n        tmp6 =tl .rand (tmp5 ,(tmp1 ).to (tl .uint32 ))\n        tmp8 =3.0 \n        tmp9 =tmp7 +tmp8 \n        tmp10 =0.0 \n        tmp11 =triton_helpers .maximum (tmp9 ,tmp10 )\n        tmp12 =6.0 \n        tmp13 =triton_helpers .minimum (tmp11 ,tmp12 )\n        tmp14 =tmp7 *tmp13 \n        tmp15 =0.16666666666666666 \n        tmp16 =tmp14 *tmp15 \n        tmp17 =0.5 \n        tmp18 =tmp2 <tmp17 \n        tmp19 =tmp18 .to (tl .float32 )\n        tmp20 =0.8864048946659319 \n        tmp21 =tmp19 *tmp20 \n        tmp22 =tmp16 *tmp21 \n        tmp23 =-1.0 \n        tmp24 =tmp19 +tmp23 \n        tmp25 =1.558387861036063 \n        tmp26 =tmp24 *tmp25 \n        tmp27 =0.7791939305180315 \n        tmp28 =tmp26 +tmp27 \n        tmp29 =tmp22 +tmp28 \n        tmp30 =tmp29 +tmp8 \n        tmp31 =triton_helpers .maximum (tmp30 ,tmp10 )\n        tmp32 =triton_helpers .minimum (tmp31 ,tmp12 )\n        tmp33 =tmp29 *tmp32 \n        tmp34 =tmp33 *tmp15 \n        tmp35 =tmp6 <tmp17 \n        tmp36 =tmp35 .to (tl .float32 )\n        tmp37 =tmp36 *tmp20 \n        tmp38 =tmp34 *tmp37 \n        tmp39 =tmp36 +tmp23 \n        tmp40 =tmp39 *tmp25 \n        tmp41 =tmp40 +tmp27 \n        tmp42 =tmp38 +tmp41 \n        tmp43 =tmp42 +tmp8 \n        tmp44 =triton_helpers .maximum (tmp43 ,tmp10 )\n        tmp45 =triton_helpers .minimum (tmp44 ,tmp12 )\n        tmp46 =tmp42 *tmp45 \n        tmp47 =tmp46 *tmp15 \n        tmp48 =tmp4 <tmp17 \n        tmp49 =tmp48 .to (tl .float32 )\n        tmp50 =tmp49 *tmp20 \n        tmp51 =tmp47 *tmp50 \n        tmp52 =tmp49 +tmp23 \n        tmp53 =tmp52 *tmp25 \n        tmp54 =tmp53 +tmp27 \n        tmp55 =tmp51 +tmp54 \n        tmp56 =tl .broadcast_to (tmp55 ,[XBLOCK ,R0_BLOCK ])\n        tmp58 =triton_helpers .maximum (_tmp57 ,tmp56 )\n        _tmp57 =tl .where (r0_mask ,tmp58 ,_tmp57 )\n        tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp55 ,r0_mask )\n    tmp57 =triton_helpers .max2 (_tmp57 ,1 )[:,None ]\n    _tmp63 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp59 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp60 =tmp59 -tmp57 \n        tmp61 =tl_math .exp (tmp60 )\n        tmp62 =tl .broadcast_to (tmp61 ,[XBLOCK ,R0_BLOCK ])\n        tmp64 =_tmp63 +tmp62 \n        _tmp63 =tl .where (r0_mask ,tmp64 ,_tmp63 )\n    tmp63 =tl .sum (_tmp63 ,1 )[:,None ]\n    _tmp73 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp65 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp66 =tmp65 -tmp57 \n        tmp67 =tl_math .log (tmp63 )\n        tmp68 =tmp66 -tmp67 \n        tmp69 =0.0 \n        tmp70 =tmp68 -tmp69 \n        tmp71 =tmp70 *tmp70 \n        tmp72 =tl .broadcast_to (tmp71 ,[XBLOCK ,R0_BLOCK ])\n        tmp74 =_tmp73 +tmp72 \n        _tmp73 =tl .where (r0_mask ,tmp74 ,_tmp73 )\n    tmp73 =tl .sum (_tmp73 ,1 )[:,None ]\n    tmp75 =49 *ks3 \n    tmp76 =tmp75 .to (tl .float32 )\n    tmp77 =tmp73 /tmp76 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp77 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,64 ,64 ),(4096 *s0 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten ._adaptive_avg_pool2d .default (arg3_1 ,[7 ,7 ])\n        del arg3_1 \n        buf1 =buf0 \n        del buf0 \n        buf2 =empty_strided_cuda ((3 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[3 ],out =buf2 )\n        buf5 =buf1 ;del buf1 \n        buf7 =buf5 ;del buf5 \n        buf8 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf10 =reinterpret_tensor (buf8 ,(),(),0 );del buf8 \n        buf11 =buf10 ;del buf10 \n\n        49 *s0 \n        get_raw_stream (0 )\n        triton_red_fused__log_softmax__to_copy_add_bernoulli_hardswish_mse_loss_mul_zeros_like_0 [grid (1 )](buf7 ,buf11 ,buf2 ,0 ,2 ,1 ,3 ,1 ,147 ,XBLOCK =1 ,R0_BLOCK =256 ,num_warps =2 ,num_stages =1 )\n        del buf2 \n        del buf7 \n    return (buf11 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "65b35773-4208-45d2-b54a-b21a1c631a35",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['HingeEmbeddingLoss', 'ConstantPad1d', 'Hardswish']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ConstantPad1d(padding=2, value=0)\n        self.pad2 = nn.ConstantPad1d(padding=3, value=1)\n        self.hardswish = nn.Hardswish()\n        self.loss = nn.HingeEmbeddingLoss()\n\n    def forward(self, x):\n        # Ensure the input is 1D for ConstantPad1d\n        if x.dim() > 2:\n            x = x.view(x.size(0), -1)  # Flatten to 1D\n        \n        x = self.pad1(x)\n        x = self.hardswish(x)\n        x = self.pad2(x)\n        x = self.hardswish(x)\n        \n        # Dummy target for HingeEmbeddingLoss\n        target = torch.ones_like(x)\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10).cuda()  # Arbitrary shape, flattened to 1D\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_constant_pad_nd_fill_hardswish_mean_ne_ones_like_sub_where_zeros_like_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp44 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =1.0 \n        tmp1 =tmp0 !=tmp0 \n        tmp2 =(-3 )+r0_0 \n        tmp3 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp4 =tmp2 >=tmp3 \n        tmp5 =4 +ks0 \n        tmp6 =tmp2 <tmp5 \n        tmp7 =tmp4 &tmp6 \n        tmp8 =tl .broadcast_to ((-5 )+r0_0 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp10 =tmp8 >=tmp9 \n        tmp11 =tl .broadcast_to (ks0 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =tmp8 <tmp11 \n        tmp13 =tmp10 &tmp12 \n        tmp14 =tmp13 &tmp7 \n        tmp15 =tl .load (in_ptr0 +(tl .broadcast_to ((-5 )+r0_0 ,[XBLOCK ,R0_BLOCK ])),r0_mask &tmp14 ,eviction_policy ='evict_first',other =0.0 )\n        tmp16 =3.0 \n        tmp17 =tmp15 +tmp16 \n        tmp18 =0.0 \n        tmp19 =triton_helpers .maximum (tmp17 ,tmp18 )\n        tmp20 =6.0 \n        tmp21 =triton_helpers .minimum (tmp19 ,tmp20 )\n        tmp22 =tmp15 *tmp21 \n        tmp23 =0.16666666666666666 \n        tmp24 =tmp22 *tmp23 \n        tmp25 =tl .full (tmp24 .shape ,1.0 ,tmp24 .dtype )\n        tmp26 =tl .where (tmp7 ,tmp24 ,tmp25 )\n        tmp27 =3.0 \n        tmp28 =tmp26 +tmp27 \n        tmp29 =0.0 \n        tmp30 =triton_helpers .maximum (tmp28 ,tmp29 )\n        tmp31 =6.0 \n        tmp32 =triton_helpers .minimum (tmp30 ,tmp31 )\n        tmp33 =tmp26 *tmp32 \n        tmp34 =0.16666666666666666 \n        tmp35 =tmp33 *tmp34 \n        tmp36 =tmp0 -tmp35 \n        tmp37 =triton_helpers .maximum (tmp36 ,tmp29 )\n        tmp38 =tl .where (tmp1 ,tmp37 ,tmp29 )\n        tmp39 =-1.0 \n        tmp40 =tmp0 !=tmp39 \n        tmp41 =tl .where (tmp40 ,tmp35 ,tmp29 )\n        tmp42 =tmp38 +tmp41 \n        tmp43 =tl .broadcast_to (tmp42 ,[XBLOCK ,R0_BLOCK ])\n        tmp45 =_tmp44 +tmp43 \n        _tmp44 =tl .where (r0_mask ,tmp45 ,_tmp44 )\n    tmp44 =tl .sum (_tmp44 ,1 )[:,None ]\n    tmp46 =10 +ks0 \n    tmp47 =tmp46 .to (tl .float32 )\n    tmp48 =tmp44 /tmp47 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp48 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg1_1 ,(1 ,s0 ),(s0 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((),(),torch .float32 )\n        buf1 =buf0 ;del buf0 \n\n        10 +s0 \n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_constant_pad_nd_fill_hardswish_mean_ne_ones_like_sub_where_zeros_like_0 [grid (1 )](buf1 ,arg1_1 ,10 ,1 ,20 ,XBLOCK =1 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n        del arg1_1 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =rand_strided ((1 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "65d693dd-814e-4295-81ea-2ef5f31c245d",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['HuberLoss', 'Dropout1d', 'AdaptiveAvgPool3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout1d = nn.Dropout1d(p=0.5)\n        self.adaptive_avg_pool3d = nn.AdaptiveAvgPool3d((5, 5, 5))\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, depth, height, width)\n        x = self.adaptive_avg_pool3d(x)  # Shape: (batch_size, channels, 5, 5, 5)\n        \n        # Reshape to apply Dropout1d\n        x = x.view(x.size(0), x.size(1), -1)  # Shape: (batch_size, channels, 125)\n        x = self.dropout1d(x)  # Shape: (batch_size, channels, 125)\n        \n        # Reshape back to 3D\n        x = x.view(x.size(0), x.size(1), 5, 5, 5)  # Shape: (batch_size, channels, 5, 5, 5)\n        \n        # Dummy target for HuberLoss (same shape as x)\n        target = torch.zeros_like(x)\n        \n        # Apply HuberLoss\n        loss = self.huber_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 10, 10, 10).cuda()  # Example input shape: (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_huber_loss_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp17 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        r0_1 =r0_index //125 \n        tmp0 =tl .load (in_ptr0 +(r0_2 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .load (in_ptr1 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =0.5 \n        tmp3 =tmp1 <tmp2 \n        tmp4 =tmp3 .to (tl .float32 )\n        tmp5 =2.0 \n        tmp6 =tmp4 *tmp5 \n        tmp7 =tmp0 *tmp6 \n        tmp8 =tl_math .abs (tmp7 )\n        tmp9 =1.0 \n        tmp10 =tmp8 <tmp9 \n        tmp11 =tmp8 *tmp2 \n        tmp12 =tmp11 *tmp8 \n        tmp13 =tmp8 -tmp2 \n        tmp14 =tmp13 *tmp9 \n        tmp15 =tl .where (tmp10 ,tmp12 ,tmp14 )\n        tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ,R0_BLOCK ])\n        tmp18 =_tmp17 +tmp16 \n        _tmp17 =tl .where (r0_mask ,tmp18 ,_tmp17 )\n    tmp17 =tl .sum (_tmp17 ,1 )[:,None ]\n    tmp19 =125 *ks0 \n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =tmp17 /tmp20 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp21 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten ._adaptive_avg_pool3d .default (arg4_1 ,[5 ,5 ,5 ])\n        del arg4_1 \n        buf1 =buf0 \n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,s0 ,1 ),(s0 ,1 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf2 ,buf3 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf2 \n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf5 =buf4 ;del buf4 \n\n        125 *s0 \n        get_raw_stream (0 )\n        triton_red_fused_huber_loss_1 [grid (1 )](buf5 ,buf1 ,buf3 ,3 ,1 ,375 ,XBLOCK =1 ,R0_BLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        del buf3 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =10 \n    arg2_1 =10 \n    arg3_1 =10 \n    arg4_1 =rand_strided ((1 ,3 ,10 ,10 ,10 ),(3000 ,1000 ,100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "673638de-882a-4a01-870e-ef66b8e61ea9",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardsigmoid', 'LeakyReLU', 'AdaptiveAvgPool1d', 'RNNCell', 'Dropout1d', 'L1Loss', 'Softmax']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.leaky_relu = nn.LeakyReLU()\n        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=10)\n        self.rnn_cell = nn.RNNCell(input_size=10, hidden_size=20)\n        self.dropout1d = nn.Dropout1d(p=0.5)\n        self.l1_loss = nn.L1Loss()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, sequence_length, feature_dim)\n        x = self.hardsigmoid(x)\n        x = self.leaky_relu(x)\n        \n        # Reshape for AdaptiveAvgPool1d\n        x = x.permute(0, 2, 1)  # (batch_size, feature_dim, sequence_length)\n        x = self.adaptive_avg_pool1d(x)\n        x = x.permute(0, 2, 1)  # (batch_size, sequence_length, feature_dim)\n        \n        # Process through RNNCell\n        batch_size, seq_len, feature_dim = x.size()\n        hx = torch.zeros(batch_size, 20).to(x.device)  # Initialize hidden state\n        outputs = []\n        for i in range(seq_len):\n            hx = self.rnn_cell(x[:, i, :], hx)\n            outputs.append(hx)\n        x = torch.stack(outputs, dim=1)  # (batch_size, sequence_length, hidden_size)\n        \n        # Apply Dropout1d\n        x = x.permute(0, 2, 1)  # (batch_size, hidden_size, sequence_length)\n        x = self.dropout1d(x)\n        x = x.permute(0, 2, 1)  # (batch_size, sequence_length, hidden_size)\n        \n        # Apply Softmax\n        x = self.softmax(x)\n        \n        # Compute L1 Loss (dummy target for demonstration)\n        target = torch.zeros_like(x)\n        loss = self.l1_loss(x, target)\n        \n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 20, 10).cuda()  # (batch_size, sequence_length, feature_dim)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =20 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =100 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %10 )\n    x1 =xindex //10 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +20 *x1 ),xmask )\n    tmp13 =tl .load (in_ptr0 +(10 +x0 +20 *x1 ),xmask )\n    tmp1 =3.0 \n    tmp2 =tmp0 +tmp1 \n    tmp3 =0.0 \n    tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n    tmp5 =6.0 \n    tmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\n    tmp7 =0.16666666666666666 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =tmp8 >tmp3 \n    tmp10 =0.01 \n    tmp11 =tmp8 *tmp10 \n    tmp12 =tl .where (tmp9 ,tmp8 ,tmp11 )\n    tmp14 =tmp13 +tmp1 \n    tmp15 =triton_helpers .maximum (tmp14 ,tmp3 )\n    tmp16 =triton_helpers .minimum (tmp15 ,tmp5 )\n    tmp17 =tmp16 *tmp7 \n    tmp18 =tmp17 >tmp3 \n    tmp19 =tmp17 *tmp10 \n    tmp20 =tl .where (tmp18 ,tmp17 ,tmp19 )\n    tmp21 =tmp20 +tmp12 \n    tmp22 =0.5 \n    tmp23 =tmp21 *tmp22 \n    tl .store (out_ptr0 +(x2 ),tmp23 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_stack_tanh_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =20 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tl .store (in_out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_stack_tanh_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =20 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tl .store (in_out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_tanh_tanh_backward_4 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =20 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr3 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tmp8 =tmp7 *tmp7 \n    tmp9 =1.0 \n    tmp10 =tmp9 -tmp8 \n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_bernoulli_5 (in_ptr0 ,in_ptr1 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =20 \n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    x0 =xindex \n    r0_1 =r0_index \n    tmp5 =tl .load (in_ptr1 +(x0 +20 *r0_1 ),r0_mask &xmask ,other =0.0 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp7 =2.0 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =tmp5 *tmp8 \n    tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n    tmp12 =tl .where (r0_mask &xmask ,tmp10 ,float (\"-inf\"))\n    tmp13 =triton_helpers .max2 (tmp12 ,1 )[:,None ]\n    tmp14 =tmp9 -tmp13 \n    tmp15 =tl_math .exp (tmp14 )\n    tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ,R0_BLOCK ])\n    tmp18 =tl .where (r0_mask &xmask ,tmp16 ,0 )\n    tmp19 =tl .sum (tmp18 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr3 +(x0 ),tmp19 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_abs_mean_sub_6 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =200 \n    R0_BLOCK :tl .constexpr =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_2 =r0_index \n    r0_0 =(r0_index %20 )\n    tmp0 =tl .load (in_ptr0 +(r0_2 ),r0_mask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 ).to (tl .int1 )\n    tmp6 =tl .load (in_ptr2 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp9 =tl .load (in_ptr3 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =2.0 \n    tmp4 =tmp2 *tmp3 \n    tmp5 =tmp0 *tmp4 \n    tmp7 =tmp5 -tmp6 \n    tmp8 =tl_math .exp (tmp7 )\n    tmp10 =tmp8 /tmp9 \n    tmp11 =tl_math .abs (tmp10 )\n    tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n    tmp14 =tl .where (r0_mask ,tmp12 ,0 )\n    tmp15 =tl .sum (tmp14 ,1 )[:,None ]\n    tmp16 =200.0 \n    tmp17 =tmp15 /tmp16 \n    tl .store (out_ptr0 +(tl .broadcast_to (r0_2 ,[XBLOCK ,R0_BLOCK ])),tmp10 ,r0_mask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp17 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 =args \n    args .clear ()\n    assert_size_stride (primals_2 ,(1 ,20 ,10 ),(200 ,10 ,1 ))\n    assert_size_stride (primals_3 ,(20 ,10 ),(10 ,1 ))\n    assert_size_stride (primals_4 ,(20 ,20 ),(20 ,1 ))\n    assert_size_stride (primals_5 ,(20 ,),(1 ,))\n    assert_size_stride (primals_6 ,(20 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_0 [grid (20 )](buf0 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((1 ,10 ,1 ,10 ),(100 ,1 ,100 ,10 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_1 [grid (100 )](primals_2 ,buf1 ,100 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n        buf2 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf0 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),0 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf3 )\n        buf4 =buf2 ;del buf2 \n        buf41 =empty_strided_cuda ((1 ,200 ),(200 ,1 ),torch .float32 )\n        buf31 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),0 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_2 [grid (20 )](buf4 ,primals_6 ,buf3 ,primals_5 ,buf31 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf5 =buf3 ;del buf3 \n\n        extern_kernels .mm (buf4 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf5 )\n        buf6 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),10 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf6 )\n        buf7 =buf5 ;del buf5 \n        buf32 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),20 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_3 [grid (20 )](buf7 ,primals_6 ,buf6 ,primals_5 ,buf32 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf8 =buf6 ;del buf6 \n\n        extern_kernels .mm (buf7 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf8 )\n        buf9 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),20 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf9 )\n        buf10 =buf8 ;del buf8 \n        buf33 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),40 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_3 [grid (20 )](buf10 ,primals_6 ,buf9 ,primals_5 ,buf33 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf11 =buf9 ;del buf9 \n\n        extern_kernels .mm (buf10 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf11 )\n        buf12 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),30 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf12 )\n        buf13 =buf11 ;del buf11 \n        buf34 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),60 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_3 [grid (20 )](buf13 ,primals_6 ,buf12 ,primals_5 ,buf34 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf14 =buf12 ;del buf12 \n\n        extern_kernels .mm (buf13 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf14 )\n        buf15 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),40 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf15 )\n        buf16 =buf14 ;del buf14 \n        buf35 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),80 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_2 [grid (20 )](buf16 ,primals_6 ,buf15 ,primals_5 ,buf35 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf17 =buf15 ;del buf15 \n\n        extern_kernels .mm (buf16 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf17 )\n        buf18 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),50 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf18 )\n        buf19 =buf17 ;del buf17 \n        buf36 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),100 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_3 [grid (20 )](buf19 ,primals_6 ,buf18 ,primals_5 ,buf36 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf20 =buf18 ;del buf18 \n\n        extern_kernels .mm (buf19 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf20 )\n        buf21 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),60 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf21 )\n        buf22 =buf20 ;del buf20 \n        buf37 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),120 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_3 [grid (20 )](buf22 ,primals_6 ,buf21 ,primals_5 ,buf37 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf23 =buf21 ;del buf21 \n\n        extern_kernels .mm (buf22 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf23 )\n        buf24 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),70 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf24 )\n        buf25 =buf23 ;del buf23 \n        buf38 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),140 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_3 [grid (20 )](buf25 ,primals_6 ,buf24 ,primals_5 ,buf38 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf26 =buf24 ;del buf24 \n\n        extern_kernels .mm (buf25 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf26 )\n        buf27 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),80 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf27 )\n        buf28 =buf26 ;del buf26 \n        buf39 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),160 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_2 [grid (20 )](buf28 ,primals_6 ,buf27 ,primals_5 ,buf39 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf29 =buf27 ;del buf27 \n\n        extern_kernels .mm (buf28 ,reinterpret_tensor (primals_4 ,(20 ,20 ),(1 ,20 ),0 ),out =buf29 )\n        buf30 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(1 ,10 ),(0 ,1 ),90 ),reinterpret_tensor (primals_3 ,(10 ,20 ),(1 ,10 ),0 ),out =buf30 )\n        del primals_3 \n        buf40 =reinterpret_tensor (buf41 ,(1 ,20 ),(200 ,1 ),180 )\n        buf49 =empty_strided_cuda ((1 ,20 ),(20 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_tanh_backward_4 [grid (20 )](buf29 ,primals_6 ,buf30 ,primals_5 ,buf40 ,buf49 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del primals_5 \n        del primals_6 \n        del buf31 \n        del buf32 \n        del buf33 \n        del buf34 \n        del buf35 \n        del buf36 \n        del buf37 \n        del buf38 \n        del buf39 \n        del buf40 \n        buf42 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf42 )\n        buf44 =empty_strided_cuda ((1 ,20 ,1 ),(20 ,1 ,1 ),torch .bool )\n        buf45 =reinterpret_tensor (buf30 ,(1 ,1 ,20 ),(20 ,20 ,1 ),0 );del buf30 \n        buf46 =reinterpret_tensor (buf29 ,(1 ,1 ,20 ),(20 ,20 ,1 ),0 );del buf29 \n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_bernoulli_5 [grid (20 )](buf42 ,buf41 ,buf44 ,buf45 ,buf46 ,0 ,20 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf42 \n        buf47 =empty_strided_cuda ((1 ,10 ,20 ),(200 ,20 ,1 ),torch .float32 )\n        buf48 =empty_strided_cuda ((),(),torch .float32 )\n        buf50 =buf48 ;del buf48 \n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_abs_mean_sub_6 [grid (1 )](buf50 ,buf41 ,buf44 ,buf45 ,buf46 ,buf47 ,1 ,200 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf41 \n        del buf45 \n        del buf46 \n    return (buf47 ,buf50 ,buf0 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),0 ),buf4 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),10 ),buf7 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),20 ),buf10 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),30 ),buf13 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),40 ),buf16 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),50 ),buf19 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),60 ),buf22 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),70 ),buf25 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),80 ),buf28 ,reinterpret_tensor (buf1 ,(1 ,10 ),(100 ,1 ),90 ),buf44 ,buf47 ,buf49 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =20 \n    primals_2 =rand_strided ((1 ,20 ,10 ),(200 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((20 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((20 ,20 ),(20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "67c0e538-4a70-415b-8680-da2eeb0ba29a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReLU', 'InstanceNorm3d', 'LazyConvTranspose2d', 'MaxUnpool2d', 'Fold']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.instance_norm = nn.InstanceNorm3d(10)\n        self.conv_transpose = nn.LazyConvTranspose2d(out_channels=20, kernel_size=3, stride=2)\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.fold = nn.Fold(output_size=(28, 28), kernel_size=(5, 5))\n        \n        # Additional layers to repeat modules up to 5 times\n        self.relu2 = nn.ReLU()\n        self.instance_norm2 = nn.InstanceNorm3d(20)\n        self.conv_transpose2 = nn.LazyConvTranspose2d(out_channels=30, kernel_size=3, stride=2)\n        self.max_unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.fold2 = nn.Fold(output_size=(14, 14), kernel_size=(3, 3))\n\n    def forward(self, x):\n        # Initial processing\n        x = self.relu(x)\n        x = x.unsqueeze(1)  # Add a channel dimension for InstanceNorm3d\n        x = self.instance_norm(x)\n        x = x.squeeze(1)  # Remove the channel dimension\n        \n        # Reshape for ConvTranspose2d\n        x = x.view(-1, 10, 16, 16)  # Arbitrary shape to fit ConvTranspose2d\n        x = self.conv_transpose(x)\n        \n        # MaxUnpool2d requires indices from a previous MaxPool2d operation\n        # For simplicity, we assume indices are available\n        pool_output, indices = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool(pool_output, indices)\n        \n        # Fold operation\n        x = x.view(x.size(0), -1)  # Flatten for Fold\n        x = self.fold(x)\n        \n        # Repeat some modules\n        x = self.relu2(x)\n        x = x.unsqueeze(1)  # Add a channel dimension for InstanceNorm3d\n        x = self.instance_norm2(x)\n        x = x.squeeze(1)  # Remove the channel dimension\n        \n        # Reshape for ConvTranspose2d\n        x = x.view(-1, 20, 8, 8)  # Arbitrary shape to fit ConvTranspose2d\n        x = self.conv_transpose2(x)\n        \n        # MaxUnpool2d again\n        pool_output, indices = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool2(pool_output, indices)\n        \n        # Final Fold operation\n        x = x.view(x.size(0), -1)  # Flatten for Fold\n        x = self.fold2(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 32, 32).cuda()  # Arbitrary input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_relu_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_relu_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_relu_0 [grid (triton_poi_fused_relu_0_xnumel )](arg3_1 ,buf0 ,10240 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "6873743f-c49e-4e4b-aed8-f1d162813c71",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad2d', 'L1Loss', 'Tanhshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ConstantPad2d(2, 3.0)  # Padding with value 3.0\n        self.pad2 = nn.ConstantPad2d(1, 1.5)  # Padding with value 1.5\n        self.tanhshrink = nn.Tanhshrink()\n        self.l1_loss = nn.L1Loss()\n\n    def forward(self, x):\n        # Apply padding layers\n        x = self.pad1(x)\n        x = self.pad2(x)\n        \n        # Apply Tanhshrink activation\n        x = self.tanhshrink(x)\n        \n        # Compute L1 loss with respect to a zero tensor of the same shape\n        target = torch.zeros_like(x)\n        loss = self.l1_loss(x, target)\n        \n        # Return the loss as part of the output\n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_sub_tanh_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x3 =xindex \n    tmp0 =(-1 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =4 +ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =4 +ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =(-3 )+x1 \n    tmp13 =tl .full ([1 ],0 ,tl .int64 )\n    tmp14 =tmp12 >=tmp13 \n    tmp15 =tl .broadcast_to (ks2 ,[XBLOCK ])\n    tmp16 =tmp12 <tmp15 \n    tmp17 =(-3 )+x0 \n    tmp18 =tmp17 >=tmp13 \n    tmp19 =tl .broadcast_to (ks3 ,[XBLOCK ])\n    tmp20 =tmp17 <tmp19 \n    tmp21 =tmp14 &tmp16 \n    tmp22 =tmp21 &tmp18 \n    tmp23 =tmp22 &tmp20 \n    tmp24 =tmp23 &tmp11 \n    tmp25 =tl .load (in_ptr0 +((-3 )+x0 +((-3 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp24 &xmask ,eviction_policy ='evict_last',other =3.0 )\n    tmp26 =tl .full (tmp25 .shape ,1.5 ,tmp25 .dtype )\n    tmp27 =tl .where (tmp11 ,tmp25 ,tmp26 )\n    tmp28 =libdevice .tanh (tmp27 )\n    tmp29 =tmp27 -tmp28 \n    tl .store (out_ptr0 +(x3 ),tmp29 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_abs_mean_sub_zeros_like_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp10 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+18 *ks0 *x0 +3 *ks0 *ks1 *x0 +3 *ks0 *ks2 *x0 \n        tmp1 =36 *ks0 +6 *ks0 *ks1 +6 *ks0 *ks2 +ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(6 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+18 *ks0 *x0 +3 *ks0 *ks1 *x0 +3 *ks0 *ks2 *x0 )//ks3 )%ks4 ))+36 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+18 *ks0 *x0 +3 *ks0 *ks1 *x0 +3 *ks0 *ks2 *x0 )//(36 +6 *ks1 +6 *ks2 +ks1 *ks2 ))%ks0 ))+ks2 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+18 *ks0 *x0 +3 *ks0 *ks1 *x0 +3 *ks0 *ks2 *x0 )//ks3 )%ks4 ))+6 *ks1 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+18 *ks0 *x0 +3 *ks0 *ks1 *x0 +3 *ks0 *ks2 *x0 )//(36 +6 *ks1 +6 *ks2 +ks1 *ks2 ))%ks0 ))+6 *ks2 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+18 *ks0 *x0 +3 *ks0 *ks1 *x0 +3 *ks0 *ks2 *x0 )//(36 +6 *ks1 +6 *ks2 +ks1 *ks2 ))%ks0 ))+ks1 *ks2 *((((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+18 *ks0 *x0 +3 *ks0 *ks1 *x0 +3 *ks0 *ks2 *x0 )//(36 +6 *ks1 +6 *ks2 +ks1 *ks2 ))%ks0 ))+(((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )+18 *ks0 *x0 +3 *ks0 *ks1 *x0 +3 *ks0 *ks2 *x0 )%ks3 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =0.0 \n        tmp5 =tmp3 -tmp4 \n        tmp6 =tl_math .abs (tmp5 )\n        tmp7 =tl .full (tmp6 .shape ,0 ,tmp6 .dtype )\n        tmp8 =tl .where (tmp2 ,tmp6 ,tmp7 )\n        tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n        tmp11 =_tmp10 +tmp9 \n        _tmp10 =tl .where (r0_mask &xmask ,tmp11 ,_tmp10 )\n    tmp10 =tl .sum (_tmp10 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_abs_mean_sub_zeros_like_2 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =36 *ks0 +6 *ks0 *ks1 +6 *ks0 *ks2 +ks0 *ks1 *ks2 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp6 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        6 +s2 \n        6 +s1 \n        36 +6 *s1 +6 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,6 +s1 ,6 +s2 ),(36 *s0 +6 *s0 *s1 +6 *s0 *s2 +s0 *s1 *s2 ,36 +6 *s1 +6 *s2 +s1 *s2 ,6 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_sub_tanh_0_xnumel =36 *s0 +6 *s0 *s1 +6 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_sub_tanh_0 [grid (triton_poi_fused_constant_pad_nd_sub_tanh_0_xnumel )](arg3_1 ,buf0 ,70 ,70 ,64 ,64 ,4900 ,14700 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((2 ,),(1 ,),torch .float32 )\n\n        18 *s0 +3 *s0 *s1 +3 *s0 *s2 +((1 +s0 *s1 *s2 )//2 )\n        get_raw_stream (0 )\n        triton_red_fused_abs_mean_sub_zeros_like_1 [grid (2 )](buf0 ,buf1 ,3 ,64 ,64 ,70 ,70 ,2 ,7350 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((),(),torch .float32 )\n        buf3 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_per_fused_abs_mean_sub_zeros_like_2 [grid (1 )](buf3 ,buf1 ,3 ,64 ,64 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n    return (buf0 ,buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "6a0fdb70-2e2a-4004-aa3d-d2cf590686f5",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LPPool1d', 'ReflectionPad1d', 'PoissonNLLLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ReflectionPad1d(2)\n        self.pool1 = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.pad2 = nn.ReflectionPad1d(1)\n        self.pool2 = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Ensure input is 3D (batch_size, channels, length)\n        if x.dim() == 2:\n            x = x.unsqueeze(1)  # Add channel dimension if missing\n        elif x.dim() == 1:\n            x = x.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions if missing\n        \n        x = self.pad1(x)\n        x = self.pool1(x)\n        x = self.pad2(x)\n        x = self.pool2(x)\n        \n        # Dummy target for PoissonNLLLoss (assuming x is the input to the loss)\n        target = torch.randint_like(x, low=0, high=10).float()\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 64).cuda()  # Example input: (batch_size, channels, length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_mul_relu_sign_0 (in_ptr0 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+2 *x0 )))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+2 *x0 ))))),xmask ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-1 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-1 )+2 *x0 )))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-1 )+2 *x0 ))))),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+2 *x0 ))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+2 *x0 ))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+2 *x0 )))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tmp0 *tmp0 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tmp3 +tmp1 \n    tmp6 =tmp5 *tmp5 \n    tmp7 =tmp6 +tmp4 \n    tmp8 =0.3333333333333333 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =tl .full ([1 ],0 ,tl .int32 )\n    tmp11 =tmp10 <tmp9 \n    tmp12 =tmp11 .to (tl .int8 )\n    tmp13 =tmp9 <tmp10 \n    tmp14 =tmp13 .to (tl .int8 )\n    tmp15 =tmp12 -tmp14 \n    tmp16 =tmp15 .to (tmp9 .dtype )\n    tmp17 =tl_math .abs (tmp9 )\n    tmp18 =triton_helpers .maximum (tmp10 ,tmp17 )\n    tmp19 =tmp16 *tmp18 \n    tmp20 =3.0 \n    tmp21 =tmp19 *tmp20 \n    tl .store (out_ptr0 +(x0 ),tmp21 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_abs_exp_mean_mul_pow_randint_like_relu_sign_sub_1 (in_out_ptr1 ,in_ptr0 ,in_ptr1 ,ks0 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp36 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(tl .where (((-1 )*tl_math .abs (((-1 )*((1 +ks0 )//2 ))+tl_math .abs ((-1 )+2 *r0_0 )))+((1 +ks0 )//2 )<0 ,((-1 )*tl_math .abs (((-1 )*((1 +ks0 )//2 ))+tl_math .abs ((-1 )+2 *r0_0 )))+((1 +ks0 )//2 )+((3 +ks0 )//2 ),((-1 )*tl_math .abs (((-1 )*((1 +ks0 )//2 ))+tl_math .abs ((-1 )+2 *r0_0 )))+((1 +ks0 )//2 ))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tl .load (in_ptr0 +(tl .where (((-1 )*tl_math .abs (((-1 )*((1 +ks0 )//2 ))+2 *r0_0 ))+((1 +ks0 )//2 )<0 ,((-1 )*tl_math .abs (((-1 )*((1 +ks0 )//2 ))+2 *r0_0 ))+((1 +ks0 )//2 )+((3 +ks0 )//2 ),((-1 )*tl_math .abs (((-1 )*((1 +ks0 )//2 ))+2 *r0_0 ))+((1 +ks0 )//2 ))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp7 =tl .load (in_ptr0 +(tl .where (((-1 )*tl_math .abs (1 +((-1 )*((1 +ks0 )//2 ))+2 *r0_0 ))+((1 +ks0 )//2 )<0 ,((-1 )*tl_math .abs (1 +((-1 )*((1 +ks0 )//2 ))+2 *r0_0 ))+((1 +ks0 )//2 )+((3 +ks0 )//2 ),((-1 )*tl_math .abs (1 +((-1 )*((1 +ks0 )//2 ))+2 *r0_0 ))+((1 +ks0 )//2 ))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =libdevice .sqrt (tmp0 )\n        tmp2 =tmp1 *tmp1 \n        tmp4 =libdevice .sqrt (tmp3 )\n        tmp5 =tmp4 *tmp4 \n        tmp6 =tmp5 +tmp2 \n        tmp8 =libdevice .sqrt (tmp7 )\n        tmp9 =tmp8 *tmp8 \n        tmp10 =tmp9 +tmp6 \n        tmp11 =0.3333333333333333 \n        tmp12 =tmp10 *tmp11 \n        tmp13 =tl_math .abs (tmp12 )\n        tmp14 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp15 =triton_helpers .maximum (tmp14 ,tmp13 )\n        tmp16 =tmp14 <tmp12 \n        tmp17 =tmp16 .to (tl .int8 )\n        tmp18 =tmp12 <tmp14 \n        tmp19 =tmp18 .to (tl .int8 )\n        tmp20 =tmp17 -tmp19 \n        tmp21 =tmp20 .to (tmp12 .dtype )\n        tmp22 =tmp21 *tmp15 \n        tmp23 =3.0 \n        tmp24 =tmp22 *tmp23 \n        tmp25 =libdevice .sqrt (tmp24 )\n        tmp26 =tl_math .exp (tmp25 )\n        tmp27 =tl .load (in_ptr1 +load_seed_offset )\n        tmp28 =r0_0 \n        tmp29 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp30 =tl .full ([1 ,1 ],10 ,tl .int64 )\n        tmp31 =triton_helpers .randint64 (tmp27 ,(tmp28 ).to (tl .uint32 ),tmp29 ,tmp30 )\n        tmp32 =tmp31 .to (tl .float32 )\n        tmp33 =tmp32 *tmp25 \n        tmp34 =tmp26 -tmp33 \n        tmp35 =tl .broadcast_to (tmp34 ,[XBLOCK ,R0_BLOCK ])\n        tmp37 =_tmp36 +tmp35 \n        _tmp36 =tl .where (r0_mask ,tmp37 ,_tmp36 )\n    tmp36 =tl .sum (_tmp36 ,1 )[:,None ]\n    tmp38 =1 +((1 +ks0 )//4 )\n    tmp39 =tmp38 .to (tl .float32 )\n    tmp40 =tmp36 /tmp39 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp40 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg1_1 ,(1 ,1 ,s0 ),(s0 ,s0 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,(3 +s0 )//2 ),((3 +s0 )//2 ,(3 +s0 )//2 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_mul_relu_sign_0_xnumel =(3 +s0 )//2 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_mul_relu_sign_0 [grid (triton_poi_fused_abs_mul_relu_sign_0_xnumel )](arg1_1 ,buf0 ,64 ,33 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del arg1_1 \n        buf3 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf3 )\n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf5 =buf4 ;del buf4 \n\n        1 +((1 +s0 )//4 )\n        get_raw_stream (0 )\n        triton_red_fused_abs_exp_mean_mul_pow_randint_like_relu_sign_sub_1 [grid (1 )](buf5 ,buf0 ,buf3 ,64 ,0 ,1 ,17 ,XBLOCK =1 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n        del buf3 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =rand_strided ((1 ,1 ,64 ),(64 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "6d4573cc-ca91-4ce1-a881-4789fb8e24a3",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AlphaDropout', 'LogSigmoid', 'UpsamplingBilinear2d', 'FeatureAlphaDropout', 'Flatten', 'ReflectionPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.alpha_dropout = nn.AlphaDropout(p=0.5)\n        self.log_sigmoid = nn.LogSigmoid()\n        self.upsampling_bilinear = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.flatten = nn.Flatten()\n        self.reflection_pad3d = nn.ReflectionPad3d(padding=1)\n\n    def forward(self, x):\n        # Apply ReflectionPad3d to the input\n        x = self.reflection_pad3d(x)\n        \n        # Apply UpsamplingBilinear2d to the padded input\n        x = self.upsampling_bilinear(x)\n        \n        # Apply FeatureAlphaDropout to the upsampled input\n        x = self.feature_alpha_dropout(x)\n        \n        # Apply AlphaDropout to the output of FeatureAlphaDropout\n        x = self.alpha_dropout(x)\n        \n        # Flatten the output to prepare for LogSigmoid\n        x = self.flatten(x)\n        \n        # Apply LogSigmoid to the flattened output\n        x = self.log_sigmoid(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape for ReflectionPad3d\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_reflection_pad3d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =((xindex //ks2 )%ks3 )\n    x3 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks7 *(tl .where ((-1 )+ks6 +((-1 )*tl_math .abs (1 +((-1 )*ks6 )+tl_math .abs ((-1 )+x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks6 )+tl_math .abs ((-1 )+x1 )))+2 *ks6 ,(-1 )+ks6 +((-1 )*tl_math .abs (1 +((-1 )*ks6 )+tl_math .abs ((-1 )+x1 )))))+ks6 *ks7 *(tl .where ((-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+x2 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+x2 )))+2 *ks5 ,(-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+x2 )))))+ks5 *ks6 *ks7 *x3 +(tl .where ((-1 )+ks7 +((-1 )*tl_math .abs (1 +((-1 )*ks7 )+tl_math .abs ((-1 )+x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks7 )+tl_math .abs ((-1 )+x0 )))+2 *ks7 ,(-1 )+ks7 +((-1 )*tl_math .abs (1 +((-1 )*ks7 )+tl_math .abs ((-1 )+x0 )))))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x4 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +s3 \n        2 +s2 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        2 +s1 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_reflection_pad3d_0_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_reflection_pad3d_0 [grid (triton_poi_fused_reflection_pad3d_0_xnumel )](arg4_1 ,buf0 ,34 ,34 ,1156 ,34 ,39304 ,32 ,32 ,32 ,117912 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg4_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "6d7e96a0-81c4-429c-97c9-188e199676fc",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad2d', 'MaxUnpool2d', 'Softmax2d', 'Embedding', 'BCELoss', 'MaxPool2d', 'UpsamplingBilinear2d', 'CircularPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.embedding = nn.Embedding(1000, 128)  # Embedding layer\n        self.constant_pad = nn.ConstantPad2d(2, 3.0)  # ConstantPad2d layer\n        self.max_pool = nn.MaxPool2d(kernel_size=2, return_indices=True)  # MaxPool2d layer\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2)  # MaxUnpool2d layer\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)  # UpsamplingBilinear2d layer\n        self.softmax = nn.Softmax2d()  # Softmax2d layer\n        self.circular_pad = nn.CircularPad3d(1)  # CircularPad3d layer\n        self.bce_loss = nn.BCELoss()  # BCELoss layer\n\n    def forward(self, x):\n        # Assume x is a 4D tensor (batch_size, channels, height, width)\n        x = self.constant_pad(x)  # Apply ConstantPad2d\n        x, indices = self.max_pool(x)  # Apply MaxPool2d and get indices\n        x = self.max_unpool(x, indices)  # Apply MaxUnpool2d using the indices\n        x = self.upsample(x)  # Apply UpsamplingBilinear2d\n        x = self.softmax(x)  # Apply Softmax2d\n        \n        # Reshape x to 5D tensor for CircularPad3d\n        x = x.unsqueeze(2)  # Add a new dimension\n        x = self.circular_pad(x)  # Apply CircularPad3d\n        \n        # Reshape back to 4D tensor\n        x = x.squeeze(2)\n        \n        # Apply BCELoss (assuming x is the input and we have a target tensor)\n        target = torch.rand_like(x)  # Random target tensor for demonstration\n        loss = self.bce_loss(x, target)  # Apply BCELoss\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randint(0, 1000, (1, 3, 64, 64)).cuda()  # Random input tensor for Embedding\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-2 )+2 *x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+2 *x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+((-2 )*ks3 )+2 *x0 +2 *ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =3 )\n    tmp13 =(-1 )+2 *x0 \n    tmp14 =tmp13 >=tmp1 \n    tmp15 =tmp13 <tmp7 \n    tmp16 =tmp9 &tmp14 \n    tmp17 =tmp16 &tmp15 \n    tmp18 =tl .load (in_ptr0 +((-1 )+((-2 )*ks3 )+2 *x0 +2 *ks3 *x1 +ks2 *ks3 *x2 ),tmp17 &xmask ,eviction_policy ='evict_last',other =3 )\n    tmp19 =triton_helpers .maximum (tmp18 ,tmp12 )\n    tmp20 =(-1 )+2 *x1 \n    tmp21 =tmp20 >=tmp1 \n    tmp22 =tmp20 <tmp3 \n    tmp23 =tmp21 &tmp22 \n    tmp24 =tmp23 &tmp6 \n    tmp25 =tmp24 &tmp8 \n    tmp26 =tl .load (in_ptr0 +((-2 )+((-1 )*ks3 )+2 *x0 +2 *ks3 *x1 +ks2 *ks3 *x2 ),tmp25 &xmask ,eviction_policy ='evict_last',other =3 )\n    tmp27 =triton_helpers .maximum (tmp26 ,tmp19 )\n    tmp28 =tmp23 &tmp14 \n    tmp29 =tmp28 &tmp15 \n    tmp30 =tl .load (in_ptr0 +((-1 )+((-1 )*ks3 )+2 *x0 +2 *ks3 *x1 +ks2 *ks3 *x2 ),tmp29 &xmask ,eviction_policy ='evict_last',other =3 )\n    tmp31 =triton_helpers .maximum (tmp30 ,tmp27 )\n    tmp32 =tmp18 >tmp12 \n    tmp33 =tl .full ([1 ],1 ,tl .int8 )\n    tmp34 =tl .full ([1 ],0 ,tl .int8 )\n    tmp35 =tl .where (tmp32 ,tmp33 ,tmp34 )\n    tmp36 =tmp26 >tmp19 \n    tmp37 =tl .full ([1 ],2 ,tl .int8 )\n    tmp38 =tl .where (tmp36 ,tmp37 ,tmp35 )\n    tmp39 =tmp30 >tmp27 \n    tmp40 =tl .full ([1 ],3 ,tl .int8 )\n    tmp41 =tl .where (tmp39 ,tmp40 ,tmp38 )\n    tl .store (out_ptr0 +(x4 ),tmp31 ,xmask )\n    tl .store (out_ptr1 +(x4 ),tmp41 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *(((x0 //ks0 )%ks1 ))+4 *(triton_helpers .div_floor_integer (x0 ,4 +2 *(ks2 //2 )+2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )))+(ks3 //2 )*(((x0 //ks0 )%ks1 ))+2 *(ks2 //2 )*(triton_helpers .div_floor_integer (x0 ,4 +2 *(ks2 //2 )+2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )))+2 *(ks3 //2 )*(triton_helpers .div_floor_integer (x0 ,4 +2 *(ks2 //2 )+2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )))+(ks2 //2 )*(ks3 //2 )*(triton_helpers .div_floor_integer (x0 ,4 +2 *(ks2 //2 )+2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )))+((x0 %ks0 ))),xmask ,eviction_policy ='evict_last')\n    tmp19 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],2 ,tl .int32 )\n    tmp2 =tl .where ((tmp0 <0 )!=(tmp1 <0 ),tl .where (tmp0 %tmp1 !=0 ,tmp0 //tmp1 -1 ,tmp0 //tmp1 ),tmp0 //tmp1 )\n    tmp3 =tmp2 *tmp1 \n    tmp4 =tmp0 -tmp3 \n    tmp5 =2 *(((x0 //ks0 )%ks1 ))\n    tmp6 =tmp5 +tmp2 \n    tmp7 =2 *((x0 %ks0 ))\n    tmp8 =tmp7 +tmp4 \n    tmp9 =4 +ks3 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =tmp10 +tmp8 \n    tmp12 =16 *(triton_helpers .div_floor_integer (x0 ,4 +2 *(ks2 //2 )+2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )))+8 *(ks2 //2 )*(triton_helpers .div_floor_integer (x0 ,4 +2 *(ks2 //2 )+2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )))+8 *(ks3 //2 )*(triton_helpers .div_floor_integer (x0 ,4 +2 *(ks2 //2 )+2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )))+4 *(ks2 //2 )*(ks3 //2 )*(triton_helpers .div_floor_integer (x0 ,4 +2 *(ks2 //2 )+2 *(ks3 //2 )+(ks2 //2 )*(ks3 //2 )))\n    tmp13 =tmp11 +tmp12 \n    tmp14 =16 *ks4 +8 *ks4 *(ks2 //2 )+8 *ks4 *(ks3 //2 )+4 *ks4 *(ks2 //2 )*(ks3 //2 )\n    tmp15 =tmp13 +tmp14 \n    tmp16 =tmp13 <0 \n    tmp17 =tl .where (tmp16 ,tmp15 ,tmp13 )\n    tl .device_assert (((0 <=tmp17 )&(tmp17 <16 *ks4 +8 *ks4 *(ks2 //2 )+8 *ks4 *(ks3 //2 )+4 *ks4 *(ks2 //2 )*(ks3 //2 )))|~(xmask ),\"index out of bounds: 0 <= tmp17 < 16*ks4 + 8*ks4*(ks2 // 2) + 8*ks4*(ks3 // 2) + 4*ks4*(ks2 // 2)*(ks3 // 2)\")\n    tl .store (out_ptr0 +(tl .broadcast_to (4 *(((tmp17 //(4 +2 *(ks3 //2 )))%(4 +2 *(ks2 //2 ))))+16 *(((tmp17 //(16 +8 *(ks2 //2 )+8 *(ks3 //2 )+4 *(ks2 //2 )*(ks3 //2 )))%ks4 ))+2 *(ks3 //2 )*(((tmp17 //(4 +2 *(ks3 //2 )))%(4 +2 *(ks2 //2 ))))+8 *(ks2 //2 )*(((tmp17 //(16 +8 *(ks2 //2 )+8 *(ks3 //2 )+4 *(ks2 //2 )*(ks3 //2 )))%ks4 ))+8 *(ks3 //2 )*(((tmp17 //(16 +8 *(ks2 //2 )+8 *(ks3 //2 )+4 *(ks2 //2 )*(ks3 //2 )))%ks4 ))+4 *(ks2 //2 )*(ks3 //2 )*(((tmp17 //(16 +8 *(ks2 //2 )+8 *(ks3 //2 )+4 *(ks2 //2 )*(ks3 //2 )))%ks4 ))+((tmp17 %(4 +2 *(ks3 //2 )))),[XBLOCK ])),tmp19 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_clamp_sub_3 (out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =ks0 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =2.0 \n    tmp3 =tmp1 /tmp2 \n    tmp4 =libdevice .floor (tmp3 )\n    tmp5 =tmp2 *tmp4 \n    tmp6 =4.0 \n    tmp7 =tmp6 +tmp5 \n    tmp8 =tmp7 .to (tl .float64 )\n    tmp9 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp10 =tmp9 +tmp8 \n    tmp11 =tmp6 *tmp4 \n    tmp12 =8.0 \n    tmp13 =tmp12 +tmp11 \n    tmp14 =tmp13 .to (tl .float64 )\n    tmp15 =tmp9 +tmp14 \n    tmp16 =tmp10 /tmp15 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =x0 \n    tmp19 =tmp18 .to (tl .float32 )\n    tmp20 =tmp19 *tmp17 \n    tmp21 =0.0 \n    tmp22 =triton_helpers .maximum (tmp20 ,tmp21 )\n    tmp23 =tmp22 .to (tl .int64 )\n    tmp24 =tmp23 .to (tl .float32 )\n    tmp25 =tmp22 -tmp24 \n    tmp26 =triton_helpers .maximum (tmp25 ,tmp21 )\n    tmp27 =1.0 \n    tmp28 =triton_helpers .minimum (tmp26 ,tmp27 )\n    tl .store (out_ptr0 +(x0 ),tmp28 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_clamp_mul_round_sub_4 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr4 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x5 =xindex //ks4 \n    x6 =xindex \n    tmp60 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp66 =tl .load (in_ptr2 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =ks0 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =2.0 \n    tmp3 =tmp1 /tmp2 \n    tmp4 =libdevice .floor (tmp3 )\n    tmp5 =tmp2 *tmp4 \n    tmp6 =4.0 \n    tmp7 =tmp6 +tmp5 \n    tmp8 =tmp7 .to (tl .float64 )\n    tmp9 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp10 =tmp9 +tmp8 \n    tmp11 =tmp6 *tmp4 \n    tmp12 =8.0 \n    tmp13 =tmp12 +tmp11 \n    tmp14 =tmp13 .to (tl .float64 )\n    tmp15 =tmp9 +tmp14 \n    tmp16 =tmp10 /tmp15 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =x1 \n    tmp19 =tmp18 .to (tl .float32 )\n    tmp20 =tmp19 *tmp17 \n    tmp21 =0.0 \n    tmp22 =triton_helpers .maximum (tmp20 ,tmp21 )\n    tmp23 =tmp22 .to (tl .int64 )\n    tmp24 =ks3 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 /tmp2 \n    tmp27 =libdevice .floor (tmp26 )\n    tmp28 =tmp2 *tmp27 \n    tmp29 =tmp6 +tmp28 \n    tmp30 =tmp29 .to (tl .float64 )\n    tmp31 =tmp9 +tmp30 \n    tmp32 =tmp6 *tmp27 \n    tmp33 =tmp12 +tmp32 \n    tmp34 =tmp33 .to (tl .float64 )\n    tmp35 =tmp9 +tmp34 \n    tmp36 =tmp31 /tmp35 \n    tmp37 =tmp36 .to (tl .float32 )\n    tmp38 =x0 \n    tmp39 =tmp38 .to (tl .float32 )\n    tmp40 =tmp39 *tmp37 \n    tmp41 =triton_helpers .maximum (tmp40 ,tmp21 )\n    tmp42 =tmp41 .to (tl .int64 )\n    tmp43 =tl .load (in_ptr0 +(4 *((((tmp42 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks0 //2 ))))+16 *((((tmp42 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+2 *(ks3 //2 )*((((tmp42 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks0 //2 ))))+8 *(ks0 //2 )*((((tmp42 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+8 *(ks3 //2 )*((((tmp42 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+4 *(ks0 //2 )*(ks3 //2 )*((((tmp42 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+((tmp42 %(4 +2 *(ks3 //2 ))))),xmask ,eviction_policy ='evict_last')\n    tmp44 =tmp43 .to (tl .float32 )\n    tmp45 =tl .full ([1 ],1 ,tl .int64 )\n    tmp46 =tmp23 +tmp45 \n    tmp47 =3 +2 *(ks0 //2 )\n    tmp48 =triton_helpers .minimum (tmp46 ,tmp47 )\n    tmp49 =tl .load (in_ptr0 +(4 *((((tmp42 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks0 //2 ))))+16 *((((tmp42 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+2 *(ks3 //2 )*((((tmp42 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks0 //2 ))))+8 *(ks0 //2 )*((((tmp42 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+8 *(ks3 //2 )*((((tmp42 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+4 *(ks0 //2 )*(ks3 //2 )*((((tmp42 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+((tmp42 %(4 +2 *(ks3 //2 ))))),xmask ,eviction_policy ='evict_last')\n    tmp50 =tmp49 .to (tl .float32 )\n    tmp51 =tmp42 +tmp45 \n    tmp52 =3 +2 *(ks3 //2 )\n    tmp53 =triton_helpers .minimum (tmp51 ,tmp52 )\n    tmp54 =tl .load (in_ptr0 +(4 *((((tmp53 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks0 //2 ))))+16 *((((tmp53 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+2 *(ks3 //2 )*((((tmp53 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks0 //2 ))))+8 *(ks0 //2 )*((((tmp53 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+8 *(ks3 //2 )*((((tmp53 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+4 *(ks0 //2 )*(ks3 //2 )*((((tmp53 +4 *tmp48 +16 *x5 +2 *tmp48 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+((tmp53 %(4 +2 *(ks3 //2 ))))),xmask ,eviction_policy ='evict_last')\n    tmp55 =tmp54 .to (tl .float32 )\n    tmp56 =tmp55 -tmp50 \n    tmp57 =tl .load (in_ptr0 +(4 *((((tmp53 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks0 //2 ))))+16 *((((tmp53 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+2 *(ks3 //2 )*((((tmp53 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks0 //2 ))))+8 *(ks0 //2 )*((((tmp53 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+8 *(ks3 //2 )*((((tmp53 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+4 *(ks0 //2 )*(ks3 //2 )*((((tmp53 +4 *tmp23 +16 *x5 +2 *tmp23 *(ks3 //2 )+8 *x5 *(ks0 //2 )+8 *x5 *(ks3 //2 )+4 *x5 *(ks0 //2 )*(ks3 //2 ))//(16 +8 *(ks0 //2 )+8 *(ks3 //2 )+4 *(ks0 //2 )*(ks3 //2 )))%ks5 ))+((tmp53 %(4 +2 *(ks3 //2 ))))),xmask ,eviction_policy ='evict_last')\n    tmp58 =tmp57 .to (tl .float32 )\n    tmp59 =tmp58 -tmp44 \n    tmp61 =tmp59 *tmp60 \n    tmp62 =tmp44 +tmp61 \n    tmp63 =tmp56 *tmp60 \n    tmp64 =tmp50 +tmp63 \n    tmp65 =tmp64 -tmp62 \n    tmp67 =tmp65 *tmp66 \n    tmp68 =tmp62 +tmp67 \n    tmp69 =libdevice .nearbyint (tmp68 )\n    tmp70 =tmp69 .to (tl .int64 )\n    tl .store (out_ptr4 +(x6 ),tmp70 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_5 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +64 *r0_1 +32 *r0_1 *(ks0 //2 )+32 *r0_1 *(ks1 //2 )+16 *r0_1 *(ks0 //2 )*(ks1 //2 )),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask &xmask ,tmp1 ,-9223372036854775808 )\n    tmp4 =triton_helpers .max2 (tmp3 ,1 )[:,None ]\n    tmp5 =tmp0 -tmp4 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tl_math .exp (tmp6 )\n    tmp8 =tl .broadcast_to (tmp7 ,[XBLOCK ,R0_BLOCK ])\n    tmp10 =tl .where (r0_mask &xmask ,tmp8 ,0 )\n    tmp11 =tl .sum (tmp10 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_6 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks2 )\n    x2 =((xindex //ks4 )%3 )\n    x3 =xindex //ks5 \n    x4 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =9 +4 *(ks1 //2 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =x1 \n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 >=tmp7 \n    tmp9 =tl .broadcast_to (9 +4 *(ks3 //2 ),[XBLOCK ])\n    tmp10 =tmp6 <tmp9 \n    tmp11 =tmp8 &tmp10 \n    tmp12 =tmp11 &tmp5 \n    tmp13 =x2 \n    tmp14 =tl .full ([1 ],1 ,tl .int64 )\n    tmp15 =tmp13 >=tmp14 \n    tmp16 =tl .full ([1 ],2 ,tl .int64 )\n    tmp17 =tmp13 <tmp16 \n    tmp18 =tmp15 &tmp17 \n    tmp19 =tmp18 &tmp12 \n    tmp20 =tl .load (in_ptr0 +((-9 )+x0 +((-4 )*(ks1 //2 ))+8 *x1 +64 *x3 +4 *x1 *(ks1 //2 )+32 *x3 *(ks1 //2 )+32 *x3 *(ks3 //2 )+16 *x3 *(ks1 //2 )*(ks3 //2 )),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr1 +((-9 )+x0 +((-4 )*(ks1 //2 ))+8 *x1 +4 *x1 *(ks1 //2 )),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tmp20 -tmp21 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =tl_math .exp (tmp23 )\n    tmp25 =tl .load (in_ptr2 +((-9 )+x0 +((-4 )*(ks1 //2 ))+8 *x1 +4 *x1 *(ks1 //2 )),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp26 =tmp24 /tmp25 \n    tmp27 =tmp26 .to (tl .int64 )\n    tmp28 =tl .full (tmp27 .shape ,0 ,tmp27 .dtype )\n    tmp29 =tl .where (tmp19 ,tmp27 ,tmp28 )\n    tmp30 =tl .load (in_ptr3 +(x4 ),tmp12 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp31 =tl .where (tmp18 ,tmp29 ,tmp30 )\n    tmp32 =tl .full (tmp31 .shape ,0 ,tmp31 .dtype )\n    tmp33 =tl .where (tmp12 ,tmp31 ,tmp32 )\n    tmp34 =tl .load (in_ptr3 +(x4 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp35 =tl .where (tmp11 ,tmp33 ,tmp34 )\n    tmp36 =tl .full (tmp35 .shape ,0 ,tmp35 .dtype )\n    tmp37 =tl .where (tmp5 ,tmp35 ,tmp36 )\n    tmp38 =tl .full ([1 ],0 ,tl .int64 )\n    tmp39 =tl .where (tmp5 ,tmp37 ,tmp38 )\n    tl .store (out_ptr0 +(x4 ),tmp39 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_7 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x4 =xindex //ks0 \n    x3 =xindex \n    tmp39 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =x0 \n    tmp4 =tl .broadcast_to (9 +4 *(ks2 //2 ),[XBLOCK ])\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =(-8 )+x0 +((-4 )*(ks2 //2 ))\n    tmp8 =tl .full ([1 ],1 ,tl .int64 )\n    tmp9 =tmp7 <tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(88 +10 *x4 +36 *(ks2 //2 )+40 *(ks3 //2 )+4 *x4 *(ks2 //2 )+16 *(ks2 //2 )*(ks3 //2 )),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(72 +x3 +28 *(ks2 //2 )+40 *(ks3 //2 )+16 *(ks2 //2 )*(ks3 //2 )),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =tl .full ([1 ],1 ,tl .int64 )\n    tmp17 =tmp3 <tmp16 \n    tmp18 =tmp17 &tmp2 \n    tmp19 =tl .load (in_ptr0 +(88 +10 *x4 +36 *(ks2 //2 )+40 *(ks3 //2 )+4 *x4 *(ks2 //2 )+16 *(ks2 //2 )*(ks3 //2 )),tmp18 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp20 =tl .load (in_ptr0 +(80 +x3 +32 *(ks2 //2 )+40 *(ks3 //2 )+16 *(ks2 //2 )*(ks3 //2 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .where (tmp17 ,tmp19 ,tmp20 )\n    tmp22 =tl .where (tmp5 ,tmp15 ,tmp21 )\n    tmp23 =tl .full (tmp22 .shape ,0 ,tmp22 .dtype )\n    tmp24 =tl .where (tmp2 ,tmp22 ,tmp23 )\n    tmp25 =x0 \n    tmp26 =9 +4 *(ks2 //2 )\n    tmp27 =tmp25 >=tmp26 \n    tmp28 =(-8 )+x0 +((-4 )*(ks2 //2 ))\n    tmp29 =tl .full ([1 ],1 ,tl .int64 )\n    tmp30 =tmp28 <tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(8 +4 *(ks2 //2 )+10 *x4 +4 *x4 *(ks2 //2 )),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +((-8 )+x3 +((-4 )*(ks2 //2 ))),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =tmp25 <tmp1 \n    tmp38 =tl .load (in_ptr0 +(8 +4 *(ks2 //2 )+10 *x4 +4 *x4 *(ks2 //2 )),tmp37 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp40 =tl .where (tmp37 ,tmp38 ,tmp39 )\n    tmp41 =tl .where (tmp27 ,tmp36 ,tmp40 )\n    tmp42 =tl .where (tmp2 ,tmp24 ,tmp41 )\n    tl .store (out_ptr0 +(x3 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_like_8 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_9 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =((xindex //ks0 )%3 )\n    x6 =((xindex //ks1 )%3 )\n    x1 =((xindex //ks2 )%ks3 )\n    x0 =(xindex %ks2 )\n    x9 =xindex //ks0 \n    x8 =xindex \n    tmp41 =tl .load (in_ptr0 +(x8 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x2 \n    tmp1 =tl .full ([1 ],2 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(-1 )+x6 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x1 \n    tmp8 =tl .broadcast_to (9 +4 *(ks4 //2 ),[XBLOCK ])\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(10 +x0 +4 *(ks5 //2 )+100 *x9 +40 *x9 *(ks4 //2 )+40 *x9 *(ks5 //2 )+16 *x9 *(ks4 //2 )*(ks5 //2 )),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x8 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =x1 \n    tmp17 =tl .broadcast_to (9 +4 *(ks4 //2 ),[XBLOCK ])\n    tmp18 =tmp16 >=tmp17 \n    tmp19 =tmp18 &tmp2 \n    tmp20 =tl .load (in_ptr0 +((-90 )+x0 +((-40 )*(ks4 //2 ))+((-36 )*(ks5 //2 ))+100 *x9 +((-16 )*(ks4 //2 )*(ks5 //2 ))+40 *x9 *(ks4 //2 )+40 *x9 *(ks5 //2 )+16 *x9 *(ks4 //2 )*(ks5 //2 )),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr0 +((-100 )+x8 +((-40 )*(ks4 //2 ))+((-40 )*(ks5 //2 ))+((-16 )*(ks4 //2 )*(ks5 //2 ))),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\n    tmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],1 ,tl .int64 )\n    tmp27 =tmp0 <tmp26 \n    tmp28 =x1 \n    tmp29 =tl .broadcast_to (9 +4 *(ks4 //2 ),[XBLOCK ])\n    tmp30 =tmp28 >=tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(110 +x0 +40 *(ks4 //2 )+44 *(ks5 //2 )+100 *x9 +16 *(ks4 //2 )*(ks5 //2 )+40 *x9 *(ks4 //2 )+40 *x9 *(ks5 //2 )+16 *x9 *(ks4 //2 )*(ks5 //2 )),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(100 +x8 +40 *(ks4 //2 )+40 *(ks5 //2 )+16 *(ks4 //2 )*(ks5 //2 )),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =x1 \n    tmp38 =9 +4 *(ks4 //2 )\n    tmp39 =tmp37 >=tmp38 \n    tmp40 =tl .load (in_ptr0 +(10 +x0 +4 *(ks5 //2 )+100 *x9 +40 *x9 *(ks4 //2 )+40 *x9 *(ks5 //2 )+16 *x9 *(ks4 //2 )*(ks5 //2 )),tmp39 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tl .where (tmp39 ,tmp40 ,tmp41 )\n    tmp43 =tl .where (tmp27 ,tmp36 ,tmp42 )\n    tmp44 =tl .where (tmp2 ,tmp25 ,tmp43 )\n    tl .store (out_ptr0 +(x8 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_binary_cross_entropy_rand_like_10 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =21 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp24 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 ))\n        tmp1 =300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 )\n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(10 *((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks4 )%ks5 ))+100 *((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks3 )%(3 *ks0 )))+4 *(ks2 //2 )*((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks4 )%ks5 ))+40 *(ks1 //2 )*((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks3 )%(3 *ks0 )))+40 *(ks2 //2 )*((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks3 )%(3 *ks0 )))+16 *(ks1 //2 )*(ks2 //2 )*((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks3 )%(3 *ks0 )))+(((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))%ks4 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tmp3 .to (tl .int64 )\n        tmp5 =tl .full ([1 ,1 ],1 ,tl .int64 )\n        tmp6 =tmp4 -tmp5 \n        tmp7 =tmp6 .to (tl .float32 )\n        tmp8 =tl .load (in_ptr1 +(10 *((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks4 )%ks5 ))+100 *((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks3 )%(3 *ks0 )))+4 *(ks2 //2 )*((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks4 )%ks5 ))+40 *(ks1 //2 )*((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks3 )%(3 *ks0 )))+40 *(ks2 //2 )*((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks3 )%(3 *ks0 )))+16 *(ks1 //2 )*(ks2 //2 )*((((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))//ks3 )%(3 *ks0 )))+(((r0_1 +x0 *(triton_helpers .div_floor_integer (20 +300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 ),21 )))%ks4 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp9 =-tmp8 \n        tmp10 =tmp9 .to (tl .float32 )\n        tmp11 =libdevice .log1p (tmp10 )\n        tmp12 =-100.0 \n        tmp13 =triton_helpers .maximum (tmp11 ,tmp12 )\n        tmp14 =tmp7 *tmp13 \n        tmp15 =tmp4 .to (tl .float32 )\n        tmp16 =tmp8 .to (tl .float32 )\n        tmp17 =tl_math .log (tmp16 )\n        tmp18 =triton_helpers .maximum (tmp17 ,tmp12 )\n        tmp19 =tmp15 *tmp18 \n        tmp20 =tmp14 -tmp19 \n        tmp21 =tl .full (tmp20 .shape ,0 ,tmp20 .dtype )\n        tmp22 =tl .where (tmp2 ,tmp20 ,tmp21 )\n        tmp23 =tl .broadcast_to (tmp22 ,[XBLOCK ,R0_BLOCK ])\n        tmp25 =_tmp24 +tmp23 \n        _tmp24 =tl .where (r0_mask &xmask ,tmp25 ,_tmp24 )\n    tmp24 =tl .sum (_tmp24 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp24 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_binary_cross_entropy_rand_like_11 (in_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =21 \n    R0_BLOCK :tl .constexpr =32 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =300 *ks0 +120 *ks0 *(ks1 //2 )+120 *ks0 *(ks2 //2 )+48 *ks0 *(ks1 //2 )*(ks2 //2 )\n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tmp4 /tmp6 \n    tmp8 =tmp7 .to (tl .int64 )\n    tl .store (out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp8 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +(s2 //2 )\n        2 +(s1 //2 )\n        4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +(s1 //2 ),2 +(s2 //2 )),(4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 ),2 +(s2 //2 ),1 ),torch .int64 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,2 +(s1 //2 ),2 +(s2 //2 )),(4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 ),2 +(s2 //2 ),1 ),torch .int8 )\n\n        triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_0_xnumel =4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_0 [grid (triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_0_xnumel )](arg3_1 ,buf0 ,buf1 ,34 ,34 ,64 ,64 ,1156 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf2 =empty_strided_cuda ((1 ,s0 ,4 +2 *(s1 //2 ),4 +2 *(s2 //2 )),(16 *s0 +8 *s0 *(s1 //2 )+8 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 ),16 +8 *(s1 //2 )+8 *(s2 //2 )+4 *(s1 //2 )*(s2 //2 ),4 +2 *(s2 //2 ),1 ),torch .int64 )\n\n        triton_poi_fused_max_unpool2d_1_xnumel =16 *s0 +8 *s0 *(s1 //2 )+8 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_1 [grid (triton_poi_fused_max_unpool2d_1_xnumel )](buf2 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool2d_2_xnumel =4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_2 [grid (triton_poi_fused_max_unpool2d_2_xnumel )](buf1 ,buf0 ,buf2 ,34 ,34 ,64 ,64 ,3 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        del buf1 \n        buf4 =empty_strided_cuda ((1 ,s0 ,3 ,10 +4 *(s1 //2 ),10 +4 *(s2 //2 )),(300 *s0 +120 *s0 *(s1 //2 )+120 *s0 *(s2 //2 )+48 *s0 *(s1 //2 )*(s2 //2 ),300 +120 *(s1 //2 )+120 *(s2 //2 )+48 *(s1 //2 )*(s2 //2 ),100 +40 *(s1 //2 )+40 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 ),10 +4 *(s2 //2 ),1 ),torch .int64 )\n        buf10 =empty_strided_cuda ((8 +4 *(s1 //2 ),1 ),(1 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_clamp_sub_3_xnumel =8 +4 *(s1 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_clamp_sub_3 [grid (triton_poi_fused__to_copy_clamp_sub_3_xnumel )](buf10 ,64 ,136 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf7 =empty_strided_cuda ((8 +4 *(s2 //2 ),),(1 ,),torch .float32 )\n\n        triton_poi_fused__to_copy_clamp_sub_3_xnumel =8 +4 *(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_clamp_sub_3 [grid (triton_poi_fused__to_copy_clamp_sub_3_xnumel )](buf7 ,64 ,136 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        8 +4 *(s2 //2 )\n        8 +4 *(s1 //2 )\n        64 +32 *(s1 //2 )+32 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 )\n        buf11 =empty_strided_cuda ((1 ,s0 ,8 +4 *(s1 //2 ),8 +4 *(s2 //2 )),(64 *s0 +32 *s0 *(s1 //2 )+32 *s0 *(s2 //2 )+16 *s0 *(s1 //2 )*(s2 //2 ),64 +32 *(s1 //2 )+32 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 ),8 +4 *(s2 //2 ),1 ),torch .int64 )\n\n        triton_poi_fused__to_copy__unsafe_index_add_clamp_mul_round_sub_4_xnumel =64 *s0 +32 *s0 *(s1 //2 )+32 *s0 *(s2 //2 )+16 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_clamp_mul_round_sub_4 [grid (triton_poi_fused__to_copy__unsafe_index_add_clamp_mul_round_sub_4_xnumel )](buf2 ,buf7 ,buf10 ,buf11 ,64 ,136 ,136 ,64 ,18496 ,3 ,55488 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf10 \n        del buf2 \n        del buf7 \n        buf12 =empty_strided_cuda ((1 ,1 ,8 +4 *(s1 //2 ),8 +4 *(s2 //2 )),(64 +32 *(s1 //2 )+32 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 ),64 +32 *(s1 //2 )+32 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 ),8 +4 *(s2 //2 ),1 ),torch .int64 )\n        buf13 =empty_strided_cuda ((1 ,1 ,8 +4 *(s1 //2 ),8 +4 *(s2 //2 )),(64 +32 *(s1 //2 )+32 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 ),64 +32 *(s1 //2 )+32 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 ),8 +4 *(s2 //2 ),1 ),torch .float32 )\n\n        triton_per_fused__softmax_5_xnumel =64 +32 *(s1 //2 )+32 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_per_fused__softmax_5 [grid (triton_per_fused__softmax_5_xnumel )](buf11 ,buf12 ,buf13 ,64 ,64 ,18496 ,3 ,XBLOCK =8 ,num_warps =2 ,num_stages =1 )\n        10 +4 *(s2 //2 )\n        10 +4 *(s1 //2 )\n        100 +40 *(s1 //2 )+40 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 )\n        300 +120 *(s1 //2 )+120 *(s2 //2 )+48 *(s1 //2 )*(s2 //2 )\n        buf14 =empty_strided_cuda ((1 ,s0 ,3 ,10 +4 *(s1 //2 ),10 +4 *(s2 //2 )),(300 *s0 +120 *s0 *(s1 //2 )+120 *s0 *(s2 //2 )+48 *s0 *(s1 //2 )*(s2 //2 ),300 +120 *(s1 //2 )+120 *(s2 //2 )+48 *(s1 //2 )*(s2 //2 ),100 +40 *(s1 //2 )+40 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 ),10 +4 *(s2 //2 ),1 ),torch .int64 )\n\n        triton_poi_fused_copy_6_xnumel =300 *s0 +120 *s0 *(s1 //2 )+120 *s0 *(s2 //2 )+48 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_copy_6 [grid (triton_poi_fused_copy_6_xnumel )](buf11 ,buf12 ,buf13 ,buf4 ,buf14 ,138 ,64 ,138 ,64 ,19044 ,57132 ,171396 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf11 \n        del buf12 \n        del buf13 \n        buf15 =buf4 ;del buf4 \n\n        triton_poi_fused_7_xnumel =300 *s0 +120 *s0 *(s1 //2 )+120 *s0 *(s2 //2 )+48 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_7 [grid (triton_poi_fused_7_xnumel )](buf14 ,buf15 ,138 ,138 ,64 ,64 ,171396 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        buf16 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf16 )\n        buf17 =empty_strided_cuda ((1 ,s0 ,3 ,10 +4 *(s1 //2 ),10 +4 *(s2 //2 )),(300 *s0 +120 *s0 *(s1 //2 )+120 *s0 *(s2 //2 )+48 *s0 *(s1 //2 )*(s2 //2 ),300 +120 *(s1 //2 )+120 *(s2 //2 )+48 *(s1 //2 )*(s2 //2 ),100 +40 *(s1 //2 )+40 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 ),10 +4 *(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_rand_like_8_xnumel =300 *s0 +120 *s0 *(s1 //2 )+120 *s0 *(s2 //2 )+48 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_rand_like_8 [grid (triton_poi_fused_rand_like_8_xnumel )](buf16 ,buf17 ,0 ,171396 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        100 +40 *(s1 //2 )+40 *(s2 //2 )+16 *(s1 //2 )*(s2 //2 )\n        buf18 =buf14 ;del buf14 \n\n        triton_poi_fused_9_xnumel =300 *s0 +120 *s0 *(s1 //2 )+120 *s0 *(s2 //2 )+48 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_9 [grid (triton_poi_fused_9_xnumel )](buf15 ,buf18 ,19044 ,19044 ,138 ,138 ,64 ,64 ,171396 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf15 \n        buf19 =empty_strided_cuda ((21 ,),(1 ,),torch .float32 )\n\n        (20 +300 *s0 +120 *s0 *(s1 //2 )+120 *s0 *(s2 //2 )+48 *s0 *(s1 //2 )*(s2 //2 ))//21 \n        get_raw_stream (0 )\n        triton_red_fused_binary_cross_entropy_rand_like_10 [grid (21 )](buf17 ,buf18 ,buf19 ,3 ,64 ,64 ,19044 ,138 ,138 ,21 ,8162 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf17 \n        del buf18 \n        buf21 =reinterpret_tensor (buf16 ,(),(),0 );del buf16 \n\n        get_raw_stream (0 )\n        triton_per_fused_binary_cross_entropy_rand_like_11 [grid (1 )](buf19 ,buf21 ,3 ,64 ,64 ,1 ,21 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf19 \n    return (buf21 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .int64 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "6fe5f6dd-60cb-4e44-9790-e248a3d21db9",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool3d', 'NLLLoss', 'BCEWithLogitsLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n        self.maxpool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n        self.nll_loss = nn.NLLLoss()\n        self.bce_loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        # Apply MaxPool3d twice\n        x = self.maxpool1(x)\n        x = self.maxpool2(x)\n        \n        # Flatten the output for loss computation\n        x = x.view(x.size(0), -1)\n        \n        # Dummy target for NLLLoss (assuming classification task)\n        target_nll = torch.randint(0, x.size(1), (x.size(0),)).to(x.device)\n        \n        # Dummy target for BCEWithLogitsLoss (assuming binary classification task)\n        target_bce = torch.randint(0, 2, (x.size(0), x.size(1))).float().to(x.device)\n        \n        # Compute NLLLoss\n        nll_loss = self.nll_loss(F.log_softmax(x, dim=1), target_nll)\n        \n        # Compute BCEWithLogitsLoss\n        bce_loss = self.bce_loss(x, target_bce)\n        \n        # Return the sum of the losses (for demonstration purposes)\n        return nll_loss + bce_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 32, 32, 32).cuda()  # Arbitrary 3D input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nfrom ctypes import c_void_p ,c_long ,c_int \nimport torch \nimport math \nimport random \nimport os \nimport tempfile \nfrom math import inf ,nan \nfrom cmath import nanj \nfrom torch ._inductor .hooks import run_intermediate_hooks \nfrom torch ._inductor .utils import maybe_profile \nfrom torch ._inductor .codegen .memory_planning import _align as align \nfrom torch import device ,empty_strided \nfrom torch ._inductor .async_compile import AsyncCompile \nfrom torch ._inductor .select_algorithm import extern_kernels \nfrom torch ._inductor .codegen .multi_kernel import MultiKernelCall \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\nsplit_scan_grid ,\ngrid_combo_kernels ,\nstart_graph ,\nend_graph ,\ncooperative_reduction_grid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nextern \"C\"void kernel (const int64_t *in_ptr0 ,\nint64_t *out_ptr0 ,\nconst int64_t ks0 ,\nconst int64_t ks1 ,\nconst int64_t ks2 )\n{\n{\n{\n{\nauto tmp0 =in_ptr0 [static_cast <int64_t >(0 L )];\nauto tmp1 =static_cast <int32_t >(0 );\nauto tmp2 =static_cast <int64_t >(0 );\nauto tmp3 =(c10 ::div_floor_integer (static_cast <int64_t >(ks0 ),static_cast <int64_t >(4 L )))*(c10 ::div_floor_integer (static_cast <int64_t >(ks1 ),static_cast <int64_t >(4 L )))*(c10 ::div_floor_integer (static_cast <int64_t >(ks2 ),static_cast <int64_t >(4 L )));\nauto tmp4 =c10 ::convert <int64_t >(tmp3 );\nauto tmp5 =randint64_cpu (tmp0 ,tmp1 ,tmp2 ,tmp4 );\nout_ptr0 [static_cast <int64_t >(0 L )]=tmp5 ;\n}\n}\n}\n}\n''')\n\n#include \"/tmp/torchinductor_sahanp/3b/c3bi5gk6mslf6u4iaqafhxm64z6u65e3eain4xlary5blqnvv6xx.h\"\nextern \"C\"  void kernel(const int64_t* in_ptr0,\n                       float* out_ptr0,\n                       const int64_t ks0,\n                       const int64_t ks1,\n                       const int64_t ks2)\n{\n    {\n        #pragma GCC ivdep\n        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>((c10::div_floor_integer(static_cast<int64_t>(ks0), static_cast<int64_t>(4L)))*(c10::div_floor_integer(static_cast<int64_t>(ks1), static_cast<int64_t>(4L)))*(c10::div_floor_integer(static_cast<int64_t>(ks2), static_cast<int64_t>(4L)))); x0+=static_cast<int64_t>(1L))\n        {\n            {\n                {\n                    auto tmp0 = in_ptr0[static_cast<int64_t>(1L)];\n                    auto tmp1 = x0;\n                    auto tmp2 = c10::convert<int32_t>(tmp1);\n                    auto tmp3 = static_cast<int64_t>(0);\n                    auto tmp4 = static_cast<int64_t>(2);\n                    auto tmp5 = randint64_cpu(tmp0, tmp2, tmp3, tmp4);\n                    auto tmp6 = c10::convert<float>(tmp5);\n                    out_ptr0[static_cast<int64_t>(x0)] = tmp6;\n                }\n            }\n        }\n    }\n}\n''')\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax_add_binary_cross_entropy_with_logits_nll_loss_forward_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =1 \n    rnumel =r0_numel \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    rbase =r0_base \n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        rindex =r0_index \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =triton_helpers .maximum (_tmp2 ,tmp1 )\n        _tmp2 =tl .where (r0_mask ,tmp3 ,_tmp2 )\n    tmp2 =triton_helpers .max2 (_tmp2 ,1 )[:,None ]\n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp23 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        rindex =r0_index \n        r0_0 =r0_index \n        tmp4 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp10 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp5 =tmp4 -tmp2 \n        tmp6 =tl_math .exp (tmp5 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =_tmp8 +tmp7 \n        _tmp8 =tl .where (r0_mask ,tmp9 ,_tmp8 )\n        tmp11 =1.0 \n        tmp12 =tmp11 -tmp10 \n        tmp13 =tmp12 *tmp4 \n        tmp14 =0.0 \n        tmp15 =triton_helpers .minimum (tmp14 ,tmp4 )\n        tmp16 =tl_math .abs (tmp4 )\n        tmp17 =-tmp16 \n        tmp18 =tl_math .exp (tmp17 )\n        tmp19 =libdevice .log1p (tmp18 )\n        tmp20 =tmp15 -tmp19 \n        tmp21 =tmp13 -tmp20 \n        tmp22 =tl .broadcast_to (tmp21 ,[XBLOCK ,R0_BLOCK ])\n        tmp24 =_tmp23 +tmp22 \n        _tmp23 =tl .where (r0_mask ,tmp24 ,_tmp23 )\n    tmp8 =tl .sum (_tmp8 ,1 )[:,None ]\n    tmp23 =tl .sum (_tmp23 ,1 )[:,None ]\n    tmp25 =tl .load (in_ptr2 +(0 ))\n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,1 ])\n    tmp27 =tl .full ([1 ,1 ],-100 ,tl .int64 )\n    tmp28 =tmp26 !=tmp27 \n    tmp29 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp30 =tl .where (tmp28 ,tmp26 ,tmp29 )\n    tmp31 =(ks0 //4 )*(ks1 //4 )*(ks2 //4 )\n    tmp32 =tmp30 +tmp31 \n    tmp33 =tmp30 <0 \n    tmp34 =tl .where (tmp33 ,tmp32 ,tmp30 )\n    tl .device_assert ((0 <=tmp34 )&(tmp34 <(ks0 //4 )*(ks1 //4 )*(ks2 //4 )),\"index out of bounds: 0 <= tmp34 < (ks0 // 4)*(ks1 // 4)*(ks2 // 4)\")\n    tmp36 =tl .load (in_ptr0 +(tmp34 ),None ,eviction_policy ='evict_last')\n    tmp37 =tmp36 -tmp2 \n    tmp38 =tl_math .log (tmp8 )\n    tmp39 =tmp37 -tmp38 \n    tmp40 =-tmp39 \n    tmp41 =0.0 \n    tmp42 =tl .where (tmp28 ,tmp40 ,tmp41 )\n    tmp43 =tmp28 .to (tl .int64 )\n    tmp44 =tmp43 .to (tl .float32 )\n    tmp45 =tmp42 /tmp44 \n    tmp46 =tmp31 .to (tl .float32 )\n    tmp47 =tmp23 /tmp46 \n    tmp48 =tmp45 +tmp47 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp48 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .max_pool3d_with_indices .default (arg3_1 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del arg3_1 \n        buf1 =buf0 [0 ]\n        del buf0 \n\n        buf3 =torch .ops .aten .max_pool3d_with_indices .default (buf1 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del buf1 \n        buf4 =buf3 [0 ]\n        del buf3 \n    buf6 =empty_strided_cpu ((2 ,),(1 ,),torch .int64 )\n\n    aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf6 )\n    buf7 =empty_strided_cpu ((1 ,),(1 ,),torch .int64 )\n    cpp_fused_randint_0 (buf6 ,buf7 ,s0 ,s1 ,s2 )\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf8 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n        buf8 .copy_ (buf7 ,False )\n        del buf7 \n    buf11 =empty_strided_cpu ((1 ,(s0 //4 )*(s1 //4 )*(s2 //4 )),((s0 //4 )*(s1 //4 )*(s2 //4 ),1 ),torch .float32 )\n    cpp_fused__to_copy_randint_1 (buf6 ,buf11 ,s0 ,s1 ,s2 )\n    del buf6 \n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf12 =empty_strided_cuda ((1 ,(s0 //4 )*(s1 //4 )*(s2 //4 )),((s0 //4 )*(s1 //4 )*(s2 //4 ),1 ),torch .float32 )\n        buf12 .copy_ (buf11 ,False )\n        del buf11 \n        buf9 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf14 =reinterpret_tensor (buf9 ,(),(),0 );del buf9 \n\n        triton_red_fused__log_softmax_add_binary_cross_entropy_with_logits_nll_loss_forward_2_r0_numel =(s0 //4 )*(s1 //4 )*(s2 //4 )\n        stream0 =get_raw_stream (0 )\n        triton_red_fused__log_softmax_add_binary_cross_entropy_with_logits_nll_loss_forward_2 [grid (1 )](buf14 ,buf4 ,buf12 ,buf8 ,32 ,32 ,32 ,1 ,512 ,XBLOCK =1 ,R0_BLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del buf12 \n        del buf4 \n        del buf8 \n    return (buf14 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =32 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,1 ,32 ,32 ,32 ),(32768 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "708cf089-09f1-4ea2-b65e-e818442a373f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['InstanceNorm2d', 'Mish', 'LazyInstanceNorm2d', 'MaxUnpool2d', 'CrossMapLRN2d', 'LazyConvTranspose2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.instance_norm1 = nn.InstanceNorm2d(64)\n        self.mish1 = nn.Mish()\n        self.lazy_instance_norm1 = nn.LazyInstanceNorm2d()\n        self.max_unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.cross_map_lrn1 = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.lazy_conv_transpose1 = nn.LazyConvTranspose2d(out_channels=32, kernel_size=3, stride=2, padding=1)\n        self.instance_norm2 = nn.InstanceNorm2d(32)\n        self.mish2 = nn.Mish()\n        self.lazy_instance_norm2 = nn.LazyInstanceNorm2d()\n        self.max_unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.cross_map_lrn2 = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.lazy_conv_transpose2 = nn.LazyConvTranspose2d(out_channels=16, kernel_size=3, stride=2, padding=1)\n        self.instance_norm3 = nn.InstanceNorm2d(16)\n        self.mish3 = nn.Mish()\n        self.lazy_instance_norm3 = nn.LazyInstanceNorm2d()\n        self.max_unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.cross_map_lrn3 = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.lazy_conv_transpose3 = nn.LazyConvTranspose2d(out_channels=8, kernel_size=3, stride=2, padding=1)\n        self.instance_norm4 = nn.InstanceNorm2d(8)\n        self.mish4 = nn.Mish()\n        self.lazy_instance_norm4 = nn.LazyInstanceNorm2d()\n        self.max_unpool4 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.cross_map_lrn4 = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.lazy_conv_transpose4 = nn.LazyConvTranspose2d(out_channels=4, kernel_size=3, stride=2, padding=1)\n        self.instance_norm5 = nn.InstanceNorm2d(4)\n        self.mish5 = nn.Mish()\n        self.lazy_instance_norm5 = nn.LazyInstanceNorm2d()\n        self.max_unpool5 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.cross_map_lrn5 = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.lazy_conv_transpose5 = nn.LazyConvTranspose2d(out_channels=1, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.instance_norm1(x)\n        x = self.mish1(x)\n        x = self.lazy_instance_norm1(x)\n        x = self.max_unpool1(x)\n        x = self.cross_map_lrn1(x)\n        x = self.lazy_conv_transpose1(x)\n        x = self.instance_norm2(x)\n        x = self.mish2(x)\n        x = self.lazy_instance_norm2(x)\n        x = self.max_unpool2(x)\n        x = self.cross_map_lrn2(x)\n        x = self.lazy_conv_transpose2(x)\n        x = self.instance_norm3(x)\n        x = self.mish3(x)\n        x = self.lazy_instance_norm3(x)\n        x = self.max_unpool3(x)\n        x = self.cross_map_lrn3(x)\n        x = self.lazy_conv_transpose3(x)\n        x = self.instance_norm4(x)\n        x = self.mish4(x)\n        x = self.lazy_instance_norm4(x)\n        x = self.max_unpool4(x)\n        x = self.cross_map_lrn4(x)\n        x = self.lazy_conv_transpose4(x)\n        x = self.instance_norm5(x)\n        x = self.mish5(x)\n        x = self.lazy_instance_norm5(x)\n        x = self.max_unpool5(x)\n        x = self.cross_map_lrn5(x)\n        x = self.lazy_conv_transpose5(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit__native_batch_norm_legit_functional_mean_mish_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr1 ,out_ptr3 ,out_ptr4 ,out_ptr6 ,out_ptr8 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \n    r0_numel =4096 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tmp8 =4096.0 \n    tmp9 =tmp3 /tmp8 \n    tmp10 =1e-05 \n    tmp11 =tmp9 +tmp10 \n    tmp12 =libdevice .rsqrt (tmp11 )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp12 ,xmask )\n    tmp15_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp15_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp15_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp13 =tl .load (in_ptr0 +(r0_1 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n        tmp15_mean_next ,tmp15_m2_next ,tmp15_weight_next =triton_helpers .welford_reduce (\n        tmp14 ,tmp15_mean ,tmp15_m2 ,tmp15_weight ,roffset ==0 \n        )\n        tmp15_mean =tl .where (r0_mask &xmask ,tmp15_mean_next ,tmp15_mean )\n        tmp15_m2 =tl .where (r0_mask &xmask ,tmp15_m2_next ,tmp15_m2 )\n        tmp15_weight =tl .where (r0_mask &xmask ,tmp15_weight_next ,tmp15_weight )\n    tmp18 ,tmp19 ,tmp20 =triton_helpers .welford (tmp15_mean ,tmp15_m2 ,tmp15_weight ,1 )\n    tmp15 =tmp18 [:,None ]\n    tmp16 =tmp19 [:,None ]\n    tmp17 =tmp20 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp15 ,xmask )\n    tmp32_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp32_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp32_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp21 =tl .load (in_ptr0 +(r0_1 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp22 =tmp21 -tmp15 \n        tmp23 =tmp22 *tmp12 \n        tmp24 =20.0 \n        tmp25 =tmp23 >tmp24 \n        tmp26 =tl_math .exp (tmp23 )\n        tmp27 =libdevice .log1p (tmp26 )\n        tmp28 =tl .where (tmp25 ,tmp23 ,tmp27 )\n        tmp29 =libdevice .tanh (tmp28 )\n        tmp30 =tmp23 *tmp29 \n        tmp31 =tl .broadcast_to (tmp30 ,[XBLOCK ,R0_BLOCK ])\n        tmp32_mean_next ,tmp32_m2_next ,tmp32_weight_next =triton_helpers .welford_reduce (\n        tmp31 ,tmp32_mean ,tmp32_m2 ,tmp32_weight ,roffset ==0 \n        )\n        tmp32_mean =tl .where (r0_mask &xmask ,tmp32_mean_next ,tmp32_mean )\n        tmp32_m2 =tl .where (r0_mask &xmask ,tmp32_m2_next ,tmp32_m2 )\n        tmp32_weight =tl .where (r0_mask &xmask ,tmp32_weight_next ,tmp32_weight )\n    tmp35 ,tmp36 ,tmp37 =triton_helpers .welford (tmp32_mean ,tmp32_m2 ,tmp32_weight ,1 )\n    tmp32 =tmp35 [:,None ]\n    tmp33 =tmp36 [:,None ]\n    tmp34 =tmp37 [:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp32 ,xmask )\n    tmp55 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp57 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp38 =tl .load (in_ptr0 +(r0_1 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp39 =tmp38 -tmp15 \n        tmp40 =tmp39 *tmp12 \n        tmp41 =20.0 \n        tmp42 =tmp40 >tmp41 \n        tmp43 =tl_math .exp (tmp40 )\n        tmp44 =libdevice .log1p (tmp43 )\n        tmp45 =tl .where (tmp42 ,tmp40 ,tmp44 )\n        tmp46 =libdevice .tanh (tmp45 )\n        tmp47 =tmp40 *tmp46 \n        tmp48 =tmp47 -tmp32 \n        tmp49 =4096.0 \n        tmp50 =tmp33 /tmp49 \n        tmp51 =1e-05 \n        tmp52 =tmp50 +tmp51 \n        tmp53 =libdevice .rsqrt (tmp52 )\n        tmp54 =tmp48 *tmp53 \n        tmp56 =tmp54 *tmp55 \n        tmp58 =tmp56 +tmp57 \n        tl .store (out_ptr3 +(r0_1 +4096 *x0 ),tmp58 ,r0_mask &xmask )\n    tmp68 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp75 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp59 =4096.0 \n    tmp60 =tmp33 /tmp59 \n    tmp61 =1e-05 \n    tmp62 =tmp60 +tmp61 \n    tmp63 =libdevice .rsqrt (tmp62 )\n    tmp64 =1.0002442002442002 \n    tmp65 =tmp60 *tmp64 \n    tmp66 =0.1 \n    tmp67 =tmp65 *tmp66 \n    tmp69 =0.9 \n    tmp70 =tmp68 *tmp69 \n    tmp71 =tmp67 +tmp70 \n    tmp72 =1.0 \n    tmp73 =tmp71 /tmp72 \n    tmp74 =tmp32 *tmp66 \n    tmp76 =tmp75 *tmp69 \n    tmp77 =tmp74 +tmp76 \n    tmp78 =tmp77 /tmp72 \n    tl .store (out_ptr4 +(x0 ),tmp63 ,xmask )\n    tl .store (out_ptr6 +(x0 ),tmp73 ,xmask )\n    tl .store (out_ptr8 +(x0 ),tmp78 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,64 ,64 ,64 ),(262144 ,4096 ,64 ,1 ))\n    assert_size_stride (primals_2 ,(64 ,),(1 ,))\n    assert_size_stride (primals_3 ,(64 ,),(1 ,))\n    assert_size_stride (primals_4 ,(64 ,),(1 ,))\n    assert_size_stride (primals_5 ,(64 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,64 ,1 ,1 ),(64 ,1 ,64 ,64 ),torch .float32 )\n        buf3 =reinterpret_tensor (buf1 ,(1 ,64 ,1 ,1 ),(64 ,1 ,1 ,1 ),0 );del buf1 \n        buf0 =empty_strided_cuda ((1 ,64 ,1 ,1 ),(64 ,1 ,1 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,64 ,1 ,1 ),(64 ,1 ,64 ,64 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,64 ,64 ,64 ),(262144 ,4096 ,64 ,1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,64 ,1 ,1 ),(64 ,1 ,64 ,64 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit__native_batch_norm_legit_functional_mean_mish_0 [grid (64 )](buf3 ,primals_1 ,primals_4 ,primals_5 ,primals_3 ,primals_2 ,buf0 ,buf4 ,buf8 ,buf7 ,primals_3 ,primals_2 ,64 ,4096 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del primals_2 \n        del primals_3 \n        del primals_4 \n        del primals_5 \n    return (buf8 ,primals_1 ,buf0 ,buf3 ,reinterpret_tensor (buf7 ,(64 ,),(1 ,),0 ),reinterpret_tensor (buf4 ,(1 ,64 ,1 ,1 ),(64 ,1 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,64 ,64 ,64 ),(262144 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "7194e3a4-27a8-486f-a7d1-633cfee5d789",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['UpsamplingNearest2d', 'PoissonNLLLoss', 'UpsamplingBilinear2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample_nearest = nn.UpsamplingNearest2d(scale_factor=2)\n        self.upsample_bilinear = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.poisson_nll_loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Apply nearest neighbor upsampling\n        x = self.upsample_nearest(x)\n        \n        # Apply bilinear upsampling\n        x = self.upsample_bilinear(x)\n        \n        # Flatten the tensor to compute PoissonNLLLoss\n        x = x.view(-1)\n        \n        # Generate a target tensor with the same shape as x\n        target = torch.ones_like(x)\n        \n        # Compute PoissonNLLLoss\n        loss = self.poisson_nll_loss(x, target)\n        \n        # Return the loss as the output\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input with arbitrary shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0 (in_out_ptr1 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks4 \n    x5 =xindex \n    tmp0 =2.0 \n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tmp3 .to (tl .float64 )\n    tmp5 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp6 =tmp5 +tmp4 \n    tmp7 =4.0 \n    tmp8 =tmp7 *tmp2 \n    tmp9 =tmp8 .to (tl .float64 )\n    tmp10 =tmp5 +tmp9 \n    tmp11 =tmp6 /tmp10 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =x1 \n    tmp14 =tmp13 .to (tl .float32 )\n    tmp15 =tmp14 *tmp12 \n    tmp16 =0.0 \n    tmp17 =triton_helpers .maximum (tmp15 ,tmp16 )\n    tmp18 =tmp17 .to (tl .int64 )\n    tmp19 =ks3 \n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =tmp0 *tmp20 \n    tmp22 =tmp21 .to (tl .float64 )\n    tmp23 =tmp5 +tmp22 \n    tmp24 =tmp7 *tmp20 \n    tmp25 =tmp24 .to (tl .float64 )\n    tmp26 =tmp5 +tmp25 \n    tmp27 =tmp23 /tmp26 \n    tmp28 =tmp27 .to (tl .float32 )\n    tmp29 =x0 \n    tmp30 =tmp29 .to (tl .float32 )\n    tmp31 =tmp30 *tmp28 \n    tmp32 =triton_helpers .maximum (tmp31 ,tmp16 )\n    tmp33 =tmp32 .to (tl .int64 )\n    tmp34 =tl .full ([1 ],2.0 ,tl .float64 )\n    tmp35 =tmp1 .to (tl .float64 )\n    tmp36 =tmp34 *tmp35 \n    tmp37 =tmp35 /tmp36 \n    tmp38 =tmp37 .to (tl .float32 )\n    tmp39 =tmp18 \n    tmp40 =tmp39 .to (tl .float32 )\n    tmp41 =tmp40 *tmp38 \n    tmp42 =tmp41 .to (tl .int64 )\n    tmp43 =tmp42 +tmp1 \n    tmp44 =tmp42 <0 \n    tmp45 =tl .where (tmp44 ,tmp43 ,tmp42 )\n    tmp46 =tmp19 .to (tl .float64 )\n    tmp47 =tmp34 *tmp46 \n    tmp48 =tmp46 /tmp47 \n    tmp49 =tmp48 .to (tl .float32 )\n    tmp50 =tmp33 \n    tmp51 =tmp50 .to (tl .float32 )\n    tmp52 =tmp51 *tmp49 \n    tmp53 =tmp52 .to (tl .int64 )\n    tmp54 =tmp53 +tmp19 \n    tmp55 =tmp53 <0 \n    tmp56 =tl .where (tmp55 ,tmp54 ,tmp53 )\n    tmp57 =tl .load (in_ptr0 +(tmp56 +ks3 *tmp45 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp58 =tl .full ([1 ],1 ,tl .int64 )\n    tmp59 =tmp18 +tmp58 \n    tmp60 =(-1 )+2 *ks0 \n    tmp61 =triton_helpers .minimum (tmp59 ,tmp60 )\n    tmp62 =tmp61 \n    tmp63 =tmp62 .to (tl .float32 )\n    tmp64 =tmp63 *tmp38 \n    tmp65 =tmp64 .to (tl .int64 )\n    tmp66 =tmp65 +tmp1 \n    tmp67 =tmp65 <0 \n    tmp68 =tl .where (tmp67 ,tmp66 ,tmp65 )\n    tmp69 =tl .load (in_ptr0 +(tmp56 +ks3 *tmp68 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp70 =tmp33 +tmp58 \n    tmp71 =(-1 )+2 *ks3 \n    tmp72 =triton_helpers .minimum (tmp70 ,tmp71 )\n    tmp73 =tmp72 \n    tmp74 =tmp73 .to (tl .float32 )\n    tmp75 =tmp74 *tmp49 \n    tmp76 =tmp75 .to (tl .int64 )\n    tmp77 =tmp76 +tmp19 \n    tmp78 =tmp76 <0 \n    tmp79 =tl .where (tmp78 ,tmp77 ,tmp76 )\n    tmp80 =tl .load (in_ptr0 +(tmp79 +ks3 *tmp68 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp81 =tmp80 -tmp69 \n    tmp82 =tl .load (in_ptr0 +(tmp79 +ks3 *tmp45 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp83 =tmp82 -tmp57 \n    tmp84 =tmp33 .to (tl .float32 )\n    tmp85 =tmp32 -tmp84 \n    tmp86 =triton_helpers .maximum (tmp85 ,tmp16 )\n    tmp87 =1.0 \n    tmp88 =triton_helpers .minimum (tmp86 ,tmp87 )\n    tmp89 =tmp81 *tmp88 \n    tmp90 =tmp69 +tmp89 \n    tmp91 =tmp83 *tmp88 \n    tmp92 =tmp57 +tmp91 \n    tmp93 =tmp90 -tmp92 \n    tmp94 =tmp18 .to (tl .float32 )\n    tmp95 =tmp17 -tmp94 \n    tmp96 =triton_helpers .maximum (tmp95 ,tmp16 )\n    tmp97 =triton_helpers .minimum (tmp96 ,tmp87 )\n    tmp98 =tmp93 *tmp97 \n    tmp99 =tmp92 +tmp98 \n    tl .store (in_out_ptr1 +(x5 ),tmp99 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_exp_mean_mul_ones_like_sub_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =6 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((5 +16 *ks0 *ks1 *ks2 )//6 )\n        tmp1 =16 *ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(r0_1 +x0 *((5 +16 *ks0 *ks1 *ks2 )//6 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl_math .exp (tmp3 )\n        tmp5 =1.0 \n        tmp6 =tmp5 *tmp3 \n        tmp7 =tmp4 -tmp6 \n        tmp8 =tl .full (tmp7 .shape ,0 ,tmp7 .dtype )\n        tmp9 =tl .where (tmp2 ,tmp7 ,tmp8 )\n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =_tmp11 +tmp10 \n        _tmp11 =tl .where (r0_mask &xmask ,tmp12 ,_tmp11 )\n    tmp11 =tl .sum (_tmp11 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_exp_mean_mul_ones_like_sub_2 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =6 \n    R0_BLOCK :tl .constexpr =8 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =16 *ks0 *ks1 *ks2 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tmp4 /tmp6 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp7 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 *s2 \n        4 *s1 \n        16 *s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 ,4 *s1 ,4 *s2 ),(16 *s0 *s1 *s2 ,16 *s1 *s2 ,4 *s2 ,1 ),torch .float32 )\n        buf5 =buf2 ;del buf2 \n\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0_xnumel =16 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0 [grid (triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0_xnumel )](buf5 ,arg3_1 ,32 ,128 ,128 ,32 ,16384 ,49152 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf6 =empty_strided_cuda ((6 ,),(1 ,),torch .float32 )\n\n        (5 +16 *s0 *s1 *s2 )//6 \n        get_raw_stream (0 )\n        triton_red_fused_exp_mean_mul_ones_like_sub_1 [grid (6 )](buf5 ,buf6 ,3 ,32 ,32 ,6 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf5 \n        buf7 =empty_strided_cuda ((),(),torch .float32 )\n        buf8 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_per_fused_exp_mean_mul_ones_like_sub_2 [grid (1 )](buf8 ,buf6 ,3 ,32 ,32 ,1 ,6 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf6 \n    return (buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "722dbdb1-c098-4004-9dda-189ac8e00ade",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FeatureAlphaDropout', 'ParameterList', 'InstanceNorm2d', 'AdaptiveMaxPool3d', 'Dropout2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.feature_alpha_dropout1 = nn.FeatureAlphaDropout(p=0.5)\n        self.feature_alpha_dropout2 = nn.FeatureAlphaDropout(p=0.5)\n        self.instance_norm2d = nn.InstanceNorm2d(10)\n        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((5, 5, 5))\n        self.dropout2d = nn.Dropout2d(p=0.5)\n        self.parameter_list = nn.ParameterList([nn.Parameter(torch.randn(10)) for _ in range(5)])\n\n    def forward(self, x):\n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout1(x)\n        \n        # Reshape to 4D tensor for InstanceNorm2d\n        x = x.view(-1, 10, x.size(-2), x.size(-1))\n        x = self.instance_norm2d(x)\n        \n        # Apply Dropout2d\n        x = self.dropout2d(x)\n        \n        # Reshape to 5D tensor for AdaptiveMaxPool3d\n        x = x.unsqueeze(2)  # Add a new dimension\n        x = self.adaptive_max_pool3d(x)\n        \n        # Apply FeatureAlphaDropout again\n        x = self.feature_alpha_dropout2(x)\n        \n        # Use ParameterList (just summing the parameters for demonstration)\n        param_sum = sum(self.parameter_list)\n        x = x + param_sum.view(1, -1, 1, 1, 1)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit__to_copy_add_bernoulli_div_mul_view_0 (in_ptr0 ,in_ptr1 ,out_ptr4 ,load_seed_offset ,load_seed_offset1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =10 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =tl .load (in_ptr0 +load_seed_offset1 )\n    tmp4 =tl .rand (tmp3 ,(tmp1 ).to (tl .uint32 ))\n    tmp20_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp20_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp20_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp5 =tl .load (in_ptr1 +(r0_1 +ks2 *ks3 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp6 =0.5 \n        tmp7 =tmp4 <tmp6 \n        tmp8 =tmp7 .to (tl .float32 )\n        tmp9 =0.8864048946659319 \n        tmp10 =tmp8 *tmp9 \n        tmp11 =tmp5 *tmp10 \n        tmp12 =-1.0 \n        tmp13 =tmp8 +tmp12 \n        tmp14 =1.558387861036063 \n        tmp15 =tmp13 *tmp14 \n        tmp16 =0.7791939305180315 \n        tmp17 =tmp15 +tmp16 \n        tmp18 =tmp11 +tmp17 \n        tmp19 =tl .broadcast_to (tmp18 ,[XBLOCK ,R0_BLOCK ])\n        tmp20_mean_next ,tmp20_m2_next ,tmp20_weight_next =triton_helpers .welford_reduce (\n        tmp19 ,tmp20_mean ,tmp20_m2 ,tmp20_weight ,roffset ==0 \n        )\n        tmp20_mean =tl .where (r0_mask &xmask ,tmp20_mean_next ,tmp20_mean )\n        tmp20_m2 =tl .where (r0_mask &xmask ,tmp20_m2_next ,tmp20_m2 )\n        tmp20_weight =tl .where (r0_mask &xmask ,tmp20_weight_next ,tmp20_weight )\n    tmp23 ,tmp24 ,tmp25 =triton_helpers .welford (tmp20_mean ,tmp20_m2 ,tmp20_weight ,1 )\n    tmp20 =tmp23 [:,None ]\n    tmp21 =tmp24 [:,None ]\n    tmp22 =tmp25 [:,None ]\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp26 =tl .load (in_ptr1 +(r0_1 +ks2 *ks3 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp27 =0.5 \n        tmp28 =tmp4 <tmp27 \n        tmp29 =tmp28 .to (tl .float32 )\n        tmp30 =0.8864048946659319 \n        tmp31 =tmp29 *tmp30 \n        tmp32 =tmp26 *tmp31 \n        tmp33 =-1.0 \n        tmp34 =tmp29 +tmp33 \n        tmp35 =1.558387861036063 \n        tmp36 =tmp34 *tmp35 \n        tmp37 =0.7791939305180315 \n        tmp38 =tmp36 +tmp37 \n        tmp39 =tmp32 +tmp38 \n        tmp40 =tmp39 -tmp20 \n        tmp41 =ks2 *ks3 \n        tmp42 =tmp41 .to (tl .float32 )\n        tmp43 =tmp21 /tmp42 \n        tmp44 =1e-05 \n        tmp45 =tmp43 +tmp44 \n        tmp46 =libdevice .rsqrt (tmp45 )\n        tmp47 =tmp40 *tmp46 \n        tmp48 =tmp2 <tmp27 \n        tmp49 =tmp48 .to (tl .float32 )\n        tmp50 =2.0 \n        tmp51 =tmp49 *tmp50 \n        tmp52 =tmp47 *tmp51 \n        tl .store (out_ptr4 +(r0_1 +ks2 *ks3 *x0 ),tmp52 ,r0_mask &xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp5 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp7 =tl .load (in_ptr3 +(x0 ),xmask )\n    tmp9 =tl .load (in_ptr4 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =tmp0 +tmp1 \n    tmp4 =tmp2 +tmp3 \n    tmp6 =tmp4 +tmp5 \n    tmp8 =tmp6 +tmp7 \n    tmp10 =tmp8 +tmp9 \n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_mul_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1250 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //125 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =0.5 \n    tmp3 =tmp1 <tmp2 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =0.8864048946659319 \n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp0 *tmp6 \n    tmp8 =-1.0 \n    tmp9 =tmp4 +tmp8 \n    tmp10 =1.558387861036063 \n    tmp11 =tmp9 *tmp10 \n    tmp12 =0.7791939305180315 \n    tmp13 =tmp11 +tmp12 \n    tmp14 =tmp7 +tmp13 \n    tmp16 =tmp14 +tmp15 \n    tl .store (in_out_ptr0 +(x2 ),tmp16 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 =args \n    args .clear ()\n    s1 =primals_1 \n    s2 =primals_2 \n    assert_size_stride (primals_3 ,(1 ,10 ,s1 ,s2 ),(10 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    assert_size_stride (primals_4 ,(10 ,),(1 ,))\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    assert_size_stride (primals_6 ,(10 ,),(1 ,))\n    assert_size_stride (primals_7 ,(10 ,),(1 ,))\n    assert_size_stride (primals_8 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((3 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[3 ],out =buf0 )\n        buf6 =empty_strided_cuda ((1 ,10 ,s1 ,s2 ),(10 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit__to_copy_add_bernoulli_div_mul_view_0 [grid (10 )](buf0 ,primals_3 ,buf6 ,1 ,0 ,32 ,32 ,10 ,1024 ,XBLOCK =1 ,R0_BLOCK =1024 ,num_warps =8 ,num_stages =1 )\n        del primals_3 \n\n        buf7 =torch .ops .aten .adaptive_max_pool3d .default (reinterpret_tensor (buf6 ,(1 ,10 ,1 ,s1 ,s2 ),(10 *s1 *s2 ,s1 *s2 ,s1 *s2 ,s2 ,1 ),0 ),[5 ,5 ,5 ])\n        del buf6 \n        buf8 =buf7 [0 ]\n        del buf7 \n        buf10 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,10 ,10 ,10 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_1 [grid (10 )](buf0 ,buf10 ,2 ,10 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf11 =empty_strided_cuda ((10 ,),(1 ,),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_2 [grid (10 )](primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,buf11 ,10 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del primals_4 \n        del primals_5 \n        del primals_6 \n        del primals_7 \n        del primals_8 \n        buf12 =buf8 ;del buf8 \n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_mul_3 [grid (1250 )](buf12 ,buf10 ,buf11 ,1250 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf10 \n        del buf11 \n    return (buf12 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =32 \n    primals_2 =32 \n    primals_3 =rand_strided ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "76dc45d8-4ce0-4ecd-881d-8238e2a05f58",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['L1Loss', 'CTCLoss', 'UpsamplingNearest2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample1 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.upsample2 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.l1_loss = nn.L1Loss()\n        self.ctc_loss = nn.CTCLoss()\n\n    def forward(self, x):\n        # Upsample the input twice\n        x = self.upsample1(x)\n        x = self.upsample2(x)\n        \n        # Compute L1 loss between the upsampled output and a target tensor of the same shape\n        target_l1 = torch.zeros_like(x)\n        l1_loss_value = self.l1_loss(x, target_l1)\n        \n        # Compute CTC loss between the upsampled output and a target sequence\n        # Assuming the input is of shape (batch_size, channels, height, width)\n        # We reshape it to (sequence_length, batch_size, num_classes) for CTC loss\n        x_reshaped = x.view(x.size(0), x.size(1), -1).permute(2, 0, 1)  # (sequence_length, batch_size, num_classes)\n        target_ctc = torch.randint(0, x.size(1), (x.size(0), 10), dtype=torch.long)  # Random target sequence\n        input_lengths = torch.full((x.size(0),), x_reshaped.size(0), dtype=torch.long)\n        target_lengths = torch.full((x.size(0),), 10, dtype=torch.long)\n        ctc_loss_value = self.ctc_loss(x_reshaped, target_ctc, input_lengths, target_lengths)\n        \n        # Return both loss values\n        return l1_loss_value, ctc_loss_value\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nfrom ctypes import c_void_p ,c_long ,c_int \nimport torch \nimport math \nimport random \nimport os \nimport tempfile \nfrom math import inf ,nan \nfrom cmath import nanj \nfrom torch ._inductor .hooks import run_intermediate_hooks \nfrom torch ._inductor .utils import maybe_profile \nfrom torch ._inductor .codegen .memory_planning import _align as align \nfrom torch import device ,empty_strided \nfrom torch ._inductor .async_compile import AsyncCompile \nfrom torch ._inductor .select_algorithm import extern_kernels \nfrom torch ._inductor .codegen .multi_kernel import MultiKernelCall \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\nsplit_scan_grid ,\ngrid_combo_kernels ,\nstart_graph ,\nend_graph ,\ncooperative_reduction_grid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nextern \"C\"void kernel (int64_t *out_ptr0 ,\nint64_t *out_ptr1 ,\nconst int64_t ks0 ,\nconst int64_t ks1 )\n{\n{\n{\n{\nauto tmp0 =16 L *ks0 *ks1 ;\nauto tmp1 =c10 ::convert <int64_t >(tmp0 );\nout_ptr0 [static_cast <int64_t >(0 L )]=tmp1 ;\n}\n}\n}\n{\n{\n{\nauto tmp0 =static_cast <int64_t >(10 );\nout_ptr1 [static_cast <int64_t >(0 L )]=tmp0 ;\n}\n}\n}\n}\n''')\n\n# kernel path: /tmp/torchinductor_sahanp/qc/cqcv3nfsbzczh3orr37rg34zil4thtyfsiohpkus3ky62jefb6cf.py\n# Topologically Sorted Source Nodes: [x, l1_loss_value], Original ATen: [aten._unsafe_index, aten.sub]\n# Source node to ATen node mapping:\n#   l1_loss_value => _unsafe_index_1\n#   x => _unsafe_index\n# Graph fragment:\n#   %_unsafe_index : [num_users=1] = call_function[target=torch.ops.aten._unsafe_index.Tensor](args = (%arg3_1, [None, None, %unsqueeze, %convert_element_type_3]), kwargs = {})\n#   %_unsafe_index_1 : [num_users=2] = call_function[target=torch.ops.aten._unsafe_index.Tensor](args = (%_unsafe_index, [None, None, %unsqueeze_1, %convert_element_type_7]), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_poi_fused__unsafe_index_sub_1(in_ptr0, out_ptr0, ks0, ks1, ks2, ks3, ks4, xnumel, XBLOCK : tl.constexpr):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = ((xindex // ks1) % ks2)\n    x0 = (xindex % ks1)\n    x2 = xindex // ks4\n    x3 = xindex\n    tmp0 = 2.0\n    tmp1 = ks0\n    tmp2 = tmp1.to(tl.float32)\n    tmp3 = tmp0 * tmp2\n    tmp4 = tmp3.to(tl.float64)\n    tmp5 = tl.full([1], 2.0, tl.float64)\n    tmp6 = tmp5 * tmp4\n    tmp7 = tmp4 / tmp6\n    tmp8 = tmp7.to(tl.float32)\n    tmp9 = x1\n    tmp10 = tmp9.to(tl.float32)\n    tmp11 = tmp10 * tmp8\n    tmp12 = tmp11.to(tl.int64)\n    tmp13 = 2*ks0\n    tmp14 = tmp12 + tmp13\n    tmp15 = tmp12 < 0\n    tmp16 = tl.where(tmp15, tmp14, tmp12)\n    tmp17 = ks3\n    tmp18 = tmp17.to(tl.float32)\n    tmp19 = tmp0 * tmp18\n    tmp20 = tmp19.to(tl.float64)\n    tmp21 = tmp5 * tmp20\n    tmp22 = tmp20 / tmp21\n    tmp23 = tmp22.to(tl.float32)\n    tmp24 = x0\n    tmp25 = tmp24.to(tl.float32)\n    tmp26 = tmp25 * tmp23\n    tmp27 = tmp26.to(tl.int64)\n    tmp28 = 2*ks3\n    tmp29 = tmp27 + tmp28\n    tmp30 = tmp27 < 0\n    tmp31 = tl.where(tmp30, tmp29, tmp27)\n    tmp32 = tmp1.to(tl.float64)\n    tmp33 = tmp5 * tmp32\n    tmp34 = tmp32 / tmp33\n    tmp35 = tmp34.to(tl.float32)\n    tmp36 = tmp16\n    tmp37 = tmp36.to(tl.float32)\n    tmp38 = tmp37 * tmp35\n    tmp39 = tmp38.to(tl.int64)\n    tmp40 = tmp39 + tmp1\n    tmp41 = tmp39 < 0\n    tmp42 = tl.where(tmp41, tmp40, tmp39)\n    tmp43 = tmp17.to(tl.float64)\n    tmp44 = tmp5 * tmp43\n    tmp45 = tmp43 / tmp44\n    tmp46 = tmp45.to(tl.float32)\n    tmp47 = tmp31\n    tmp48 = tmp47.to(tl.float32)\n    tmp49 = tmp48 * tmp46\n    tmp50 = tmp49.to(tl.int64)\n    tmp51 = tmp50 + tmp17\n    tmp52 = tmp50 < 0\n    tmp53 = tl.where(tmp52, tmp51, tmp50)\n    tmp54 = tl.load(in_ptr0 + (tmp53 + ks3*tmp42 + ks0*ks3*x2), xmask, eviction_policy='evict_last')\n    tl.store(out_ptr0 + (x3), tmp54, xmask)\n\n# kernel path: /tmp/torchinductor_sahanp/55/c55uwnhvmh3uwyx57x65gxhsk7sfoewf53h66tdn2oe5xz6mgjhw.py\n# Topologically Sorted Source Nodes: [l1_loss_value], Original ATen: [aten.abs, aten.mean]\n# Source node to ATen node mapping:\n#   l1_loss_value => abs_1, mean\n# Graph fragment:\n#   %abs_1 : [num_users=1] = call_function[target=torch.ops.aten.abs.default](args = (%_unsafe_index_1,), kwargs = {})\n#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%abs_1,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_red_fused_abs_mean_2(in_ptr0, out_ptr0, ks0, ks1, ks2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):\n    xnumel = 6\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    r0_base = tl.arange(0, R0_BLOCK)[None, :]\n    rbase = r0_base\n    x0 = xindex\n    _tmp8 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)\n    for r0_offset in range(0, r0_numel, R0_BLOCK):\n        r0_index = r0_offset + r0_base\n        r0_mask = r0_index < r0_numel\n        roffset = r0_offset\n        rindex = r0_index\n        r0_1 = r0_index\n        tmp0 = r0_1 + x0*((5 + 16*ks0*ks1*ks2) // 6)\n        tmp1 = 16*ks0*ks1*ks2\n        tmp2 = tmp0 < tmp1\n        tmp3 = tl.load(in_ptr0 + (((r0_1 + x0*((5 + 16*ks0*ks1*ks2) // 6)) % (16*ks0*ks1*ks2))), r0_mask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)\n        tmp4 = tl_math.abs(tmp3)\n        tmp5 = tl.full(tmp4.shape, 0, tmp4.dtype)\n        tmp6 = tl.where(tmp2, tmp4, tmp5)\n        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])\n        tmp9 = _tmp8 + tmp7\n        _tmp8 = tl.where(r0_mask & xmask, tmp9, _tmp8)\n    tmp8 = tl.sum(_tmp8, 1)[:, None]\n    tl.store(out_ptr0 + (x0), tmp8, xmask)\n\n# kernel path: /tmp/torchinductor_sahanp/kp/ckpniguokvk7xri2v4b2uf3olbhdnjblwpplklh7bva6as55qkuk.py\n# Topologically Sorted Source Nodes: [l1_loss_value], Original ATen: [aten.abs, aten.mean]\n# Source node to ATen node mapping:\n#   l1_loss_value => abs_1, mean\n# Graph fragment:\n#   %abs_1 : [num_users=1] = call_function[target=torch.ops.aten.abs.default](args = (%_unsafe_index_1,), kwargs = {})\n#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%abs_1,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_per_fused_abs_mean_3(in_out_ptr0, in_ptr0, ks0, ks1, ks2, xnumel, r0_numel, XBLOCK : tl.constexpr):\n    xnumel = 1\n    r0_numel = 6\n    R0_BLOCK: tl.constexpr = 8\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_index = tl.arange(0, R0_BLOCK)[None, :]\n    r0_offset = 0\n    r0_mask = r0_index < r0_numel\n    roffset = r0_offset\n    rindex = r0_index\n    r0_0 = r0_index\n    tmp0 = tl.load(in_ptr0 + (r0_0), r0_mask, other=0.0)\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])\n    tmp3 = tl.where(r0_mask, tmp1, 0)\n    tmp4 = tl.sum(tmp3, 1)[:, None]\n    tmp5 = 16*ks0*ks1*ks2\n    tmp6 = tmp5.to(tl.float32)\n    tmp7 = tmp4 / tmp6\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (tl.full([XBLOCK, 1], 0, tl.int32)), tmp7, None)\n\n#include \"/tmp/torchinductor_sahanp/3b/c3bi5gk6mslf6u4iaqafhxm64z6u65e3eain4xlary5blqnvv6xx.h\"\nextern \"C\"  void kernel(const int64_t* in_ptr0,\n                       int64_t* out_ptr0,\n                       const int64_t ks0)\n{\n    {\n        #pragma GCC ivdep\n        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(10L); x0+=static_cast<int64_t>(1L))\n        {\n            {\n                {\n                    auto tmp0 = in_ptr0[static_cast<int64_t>(0L)];\n                    auto tmp1 = x0;\n                    auto tmp2 = c10::convert<int32_t>(tmp1);\n                    auto tmp3 = static_cast<int64_t>(0);\n                    auto tmp4 = ks0;\n                    auto tmp5 = c10::convert<int64_t>(tmp4);\n                    auto tmp6 = randint64_cpu(tmp0, tmp2, tmp3, tmp5);\n                    out_ptr0[static_cast<int64_t>(x0)] = tmp6;\n                }\n            }\n        }\n    }\n}\n''')\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    buf1 =empty_strided_cpu ((1 ,),(1 ,),torch .int64 )\n\n    aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf1 )\n    buf5 =empty_strided_cpu ((1 ,),(1 ,),torch .int64 )\n    buf6 =empty_strided_cpu ((1 ,),(1 ,),torch .int64 )\n    cpp_fused_full_0 (buf5 ,buf6 ,s1 ,s2 )\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        ps0 =4 *s2 \n        ps1 =4 *s1 \n        ps2 =16 *s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 *s1 ,4 *s2 ),(16 *s0 *s1 *s2 ,16 *s1 *s2 ,4 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__unsafe_index_sub_1_xnumel =16 *s0 *s1 *s2 \n        stream0 =get_raw_stream (0 )\n        triton_poi_fused__unsafe_index_sub_1 [grid (triton_poi_fused__unsafe_index_sub_1_xnumel )](arg3_1 ,buf0 ,32 ,128 ,128 ,32 ,16384 ,49152 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf3 =empty_strided_cuda ((6 ,),(1 ,),torch .float32 )\n\n        triton_red_fused_abs_mean_2_r0_numel =(5 +16 *s0 *s1 *s2 )//6 \n        stream0 =get_raw_stream (0 )\n        triton_red_fused_abs_mean_2 [grid (6 )](buf0 ,buf3 ,3 ,32 ,32 ,6 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf7 =buf4 ;del buf4 \n\n        stream0 =get_raw_stream (0 )\n        triton_per_fused_abs_mean_3 [grid (1 )](buf7 ,buf3 ,3 ,32 ,32 ,1 ,6 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf3 \n    buf2 =empty_strided_cpu ((1 ,10 ),(10 ,1 ),torch .int64 )\n    cpp_fused_randint_4 (buf1 ,buf2 ,s0 )\n    return (reinterpret_tensor (buf0 ,(16 *s1 *s2 ,1 ,s0 ),(1 ,16 *s0 *s1 *s2 ,16 *s1 *s2 ),0 ),buf2 ,buf5 ,buf6 ,buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "77a7a281-9e6c-41ba-8dc2-c6deb97cb864",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AvgPool3d', 'ReplicationPad2d', 'TransformerEncoder', 'Softshrink', 'ReLU6']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.avgpool3d = nn.AvgPool3d(kernel_size=2, stride=2)\n        self.replicationpad2d = nn.ReplicationPad2d(padding=1)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=64, nhead=8), num_layers=3\n        )\n        self.softshrink = nn.Softshrink(lambd=0.5)\n        self.relu6 = nn.ReLU6()\n\n    def forward(self, x):\n        # Assuming input is 5D (batch, channels, depth, height, width)\n        x = self.avgpool3d(x)  # Reduce spatial dimensions\n        x = x.squeeze(2)  # Remove depth dimension to make it 4D (batch, channels, height, width)\n        x = self.replicationpad2d(x)  # Pad the 2D spatial dimensions\n        x = x.permute(2, 3, 0, 1)  # Reshape for TransformerEncoder (seq_len, batch, features)\n        x = self.transformer_encoder(x)  # Apply TransformerEncoder\n        x = x.permute(2, 3, 0, 1)  # Reshape back to (batch, channels, height, width)\n        x = self.softshrink(x)  # Apply Softshrink\n        x = self.relu6(x)  # Apply ReLU6\n        return x\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 64, 64).cuda()  # Example input: (batch, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool3d_squeeze_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =((xindex //ks2 )%ks3 )\n    x3 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks7 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +ks7 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(1 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(ks7 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(1 +ks7 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp8 =tmp7 +tmp6 \n    tmp10 =tmp9 +tmp8 \n    tmp12 =tmp11 +tmp10 \n    tmp14 =tmp13 +tmp12 \n    tmp15 =0.125 \n    tmp16 =tmp14 *tmp15 \n    tl .store (out_ptr0 +(x4 ),tmp16 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s3 //2 \n        s2 //2 \n        (s2 //2 )*(s3 //2 )\n        s1 //2 \n        (s1 //2 )*(s2 //2 )*(s3 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ,s3 //2 ),(s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),(s1 //2 )*(s2 //2 )*(s3 //2 ),(s2 //2 )*(s3 //2 ),s3 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool3d_squeeze_0_xnumel =s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool3d_squeeze_0 [grid (triton_poi_fused_avg_pool3d_squeeze_0_xnumel )](arg4_1 ,buf0 ,32 ,32 ,1024 ,16 ,16384 ,32 ,64 ,64 ,49152 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg4_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =64 \n    arg3_1 =64 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,64 ,64 ),(393216 ,131072 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "78e214c2-da7d-43f7-badb-caaa8a9cc3ef",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['BCELoss', 'Sigmoid', 'AdaptiveMaxPool1d', 'Dropout', 'UpsamplingBilinear2d', 'RNN', 'TransformerEncoderLayer']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool1d = nn.AdaptiveMaxPool1d(output_size=10)\n        self.dropout = nn.Dropout(p=0.5)\n        self.upsampling_bilinear2d = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=20, nhead=4)\n        self.sigmoid = nn.Sigmoid()\n        self.bce_loss = nn.BCELoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        # Reshape to (batch_size, channels, height * width) for AdaptiveMaxPool1d\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1)\n        \n        # Apply AdaptiveMaxPool1d\n        x = self.adaptive_max_pool1d(x)\n        \n        # Reshape back to (batch_size, channels, height, width) for UpsamplingBilinear2d\n        x = x.view(batch_size, channels, 10, 1)\n        \n        # Apply UpsamplingBilinear2d\n        x = self.upsampling_bilinear2d(x)\n        \n        # Reshape to (batch_size, height * width, channels) for RNN\n        x = x.view(batch_size, -1, channels)\n        \n        # Apply RNN\n        x, _ = self.rnn(x)\n        \n        # Reshape to (batch_size, channels, height, width) for TransformerEncoderLayer\n        x = x.view(batch_size, channels, -1)\n        \n        # Apply TransformerEncoderLayer\n        x = self.transformer_encoder_layer(x)\n        \n        # Apply Dropout\n        x = self.dropout(x)\n        \n        # Apply Sigmoid\n        x = self.sigmoid(x)\n        \n        # Assuming target is a binary tensor of the same shape as x\n        target = torch.randint(0, 2, x.shape).float().to(x.device)\n        \n        # Compute BCELoss\n        loss = self.bce_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //2 )%20 )\n    x2 =xindex //40 \n    x4 =xindex \n    tmp0 =x1 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.47368421052631576 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],9 ,tl .int64 )\n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tmp11 =tl .load (in_ptr0 +(tmp10 +10 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tmp11 -tmp11 \n    tmp13 =tmp12 *tmp4 \n    tmp14 =tmp11 +tmp13 \n    tmp15 =tl .load (in_ptr0 +(tmp6 +10 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp16 =tmp15 -tmp15 \n    tmp17 =tmp16 *tmp4 \n    tmp18 =tmp15 +tmp17 \n    tmp19 =tmp14 -tmp18 \n    tmp20 =tmp6 .to (tl .float32 )\n    tmp21 =tmp5 -tmp20 \n    tmp22 =triton_helpers .maximum (tmp21 ,tmp4 )\n    tmp23 =1.0 \n    tmp24 =triton_helpers .minimum (tmp22 ,tmp23 )\n    tmp25 =tmp19 *tmp24 \n    tmp26 =tmp18 +tmp25 \n    tl .store (in_out_ptr0 +(x4 ),tmp26 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .adaptive_max_pool2d .default (reinterpret_tensor (arg3_1 ,(1 ,s0 ,1 ,s1 *s2 ),(s0 *s1 *s2 ,s1 *s2 ,s1 *s2 ,1 ),0 ),[1 ,10 ])\n        del arg3_1 \n        buf1 =buf0 [0 ]\n        del buf0 \n        buf3 =empty_strided_cuda ((1 ,s0 ,20 ,2 ),(40 *s0 ,40 ,2 ,1 ),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_0_xnumel =40 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_0 [grid (triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_0_xnumel )](buf4 ,buf1 ,120 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n    return (reinterpret_tensor (buf4 ,(1 ,40 ,s0 ),(40 *s0 ,s0 ,1 ),0 ),s0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "790eb292-ee74-475d-aeaa-8fe8500b8aad",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxUnpool3d', 'Dropout3d', 'InstanceNorm1d', 'Softplus', 'LazyBatchNorm1d', 'MultiLabelSoftMarginLoss', 'ZeroPad2d', 'LSTM', 'MSELoss', 'InstanceNorm3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad2d = nn.ZeroPad2d(2)\n        self.dropout3d = nn.Dropout3d(0.5)\n        self.instance_norm1d = nn.InstanceNorm1d(64)\n        self.lazy_batch_norm1d = nn.LazyBatchNorm1d()\n        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.instance_norm3d = nn.InstanceNorm3d(32)\n        self.max_unpool3d = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.softplus = nn.Softplus()\n        self.multi_label_soft_margin_loss = nn.MultiLabelSoftMarginLoss()\n        self.mse_loss = nn.MSELoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        x = self.zero_pad2d(x)  # Shape: (batch_size, channels, height+4, width+4)\n        \n        # Reshape for 3D operations\n        x = x.unsqueeze(2)  # Shape: (batch_size, channels, 1, height+4, width+4)\n        x = self.dropout3d(x)  # Shape: (batch_size, channels, 1, height+4, width+4)\n        \n        # Reshape for 1D operations\n        x = x.view(x.size(0), x.size(1), -1)  # Shape: (batch_size, channels, (1 * (height+4) * (width+4)))\n        x = self.instance_norm1d(x)  # Shape: (batch_size, channels, (1 * (height+4) * (width+4)))\n        x = self.lazy_batch_norm1d(x)  # Shape: (batch_size, channels, (1 * (height+4) * (width+4)))\n        \n        # Reshape for LSTM\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, (1 * (height+4) * (width+4)), channels)\n        x, _ = self.lstm(x)  # Shape: (batch_size, (1 * (height+4) * (width+4)), 128)\n        \n        # Reshape for 3D operations\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, 128, (1 * (height+4) * (width+4)))\n        x = x.view(x.size(0), 32, 4, x.size(2)//4, x.size(2)//4)  # Shape: (batch_size, 32, 4, height+4, width+4)\n        x = self.instance_norm3d(x)  # Shape: (batch_size, 32, 4, height+4, width+4)\n        \n        # MaxUnpool3d requires indices from a previous MaxPool3d operation\n        # For simplicity, we assume a dummy pooling operation here\n        pool_output, indices = F.max_pool3d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool3d(pool_output, indices)  # Shape: (batch_size, 32, 4, height+4, width+4)\n        \n        x = self.softplus(x)  # Shape: (batch_size, 32, 4, height+4, width+4)\n        \n        # Reshape for loss computation\n        x = x.view(x.size(0), -1)  # Shape: (batch_size, 32 * 4 * (height+4) * (width+4))\n        \n        # Dummy target for loss computation\n        target = torch.randint(0, 2, (x.size(0), x.size(1))).float()\n        loss = self.multi_label_soft_margin_loss(x, target)\n        \n        # Dummy target for MSE loss\n        mse_target = torch.randn_like(x)\n        mse_loss = self.mse_loss(x, mse_target)\n        \n        return x, loss, mse_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp13 =tl .load (in_ptr1 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp14 =0.5 \n    tmp15 =tmp13 <tmp14 \n    tmp16 =tmp15 .to (tl .float32 )\n    tmp17 =2.0 \n    tmp18 =tmp16 *tmp17 \n    tmp19 =tmp12 *tmp18 \n    tl .store (out_ptr0 +(x4 ),tmp19 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_view_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *(x0 //ks1 )+16 *x1 +ks3 *(x0 //ks1 )+4 *ks2 *x1 +4 *ks3 *x1 +ks2 *ks3 *x1 +((x0 %ks1 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 ,1 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_bernoulli_div_mul_1_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_1 [grid (triton_poi_fused__to_copy_bernoulli_div_mul_1_xnumel )](arg3_1 ,buf1 ,buf2 ,68 ,68 ,64 ,64 ,4624 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf3 =empty_strided_cuda ((1 ,s0 ,16 +4 *s1 +4 *s2 +s1 *s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_bernoulli_div_mul_view_2_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_view_2 [grid (triton_poi_fused__to_copy_bernoulli_div_mul_view_2_xnumel )](buf2 ,buf3 ,4624 ,68 ,64 ,64 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "7a364d67-9960-41b6-b50b-e4b28780f8c8",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PairwiseDistance', 'Softsign', 'PoissonNLLLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pairwise_distance = nn.PairwiseDistance(p=2)\n        self.softsign = nn.Softsign()\n        self.poisson_nll_loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Assuming x is a batch of tensors, we split it into two tensors for PairwiseDistance\n        x1, x2 = x.chunk(2, dim=1)\n        x1 = x1.view(x1.size(0), -1)  # Flatten x1\n        x2 = x2.view(x2.size(0), -1)  # Flatten x2\n        \n        # Apply PairwiseDistance\n        x = self.pairwise_distance(x1, x2)\n        \n        # Apply Softsign\n        x = self.softsign(x)\n        \n        # Apply PoissonNLLLoss (assuming target is a tensor of ones for simplicity)\n        target = torch.ones_like(x)\n        x = self.poisson_nll_loss(x, target)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 2, 64, 64).cuda()  # Input shape adjusted for PairwiseDistance\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_abs_add_exp_mean_mul_norm_sub_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp7 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +(r0_0 +ks0 *ks1 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp3 =1e-06 \n        tmp4 =tmp2 +tmp3 \n        tmp5 =tmp4 *tmp4 \n        tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n        tmp8 =_tmp7 +tmp6 \n        _tmp7 =tl .where (r0_mask ,tmp8 ,_tmp7 )\n    tmp7 =tl .sum (_tmp7 ,1 )[:,None ]\n    tmp9 =libdevice .sqrt (tmp7 )\n    tmp10 =tl_math .abs (tmp9 )\n    tmp11 =1.0 \n    tmp12 =tmp10 +tmp11 \n    tmp13 =tmp9 /tmp12 \n    tmp14 =tl_math .exp (tmp13 )\n    tmp15 =tmp14 -tmp13 \n    tmp16 =tmp15 /tmp11 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp16 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    s2 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,2 ,s1 ,s2 ),(2 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf1 =reinterpret_tensor (buf0 ,(),(),0 );del buf0 \n\n        s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused_abs_add_exp_mean_mul_norm_sub_0 [grid (1 )](buf1 ,arg2_1 ,64 ,64 ,1 ,4096 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg2_1 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,2 ,64 ,64 ),(8192 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "7a6e03ea-52a4-47d9-b927-6d58086bf235",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CTCLoss', 'RNNCell', 'LazyConv1d', 'LazyInstanceNorm3d', 'Mish', 'MaxPool1d', 'PReLU', 'Softmax2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.LazyConv1d(out_channels=32, kernel_size=3)\n        self.instance_norm = nn.LazyInstanceNorm3d()\n        self.rnn_cell = nn.RNNCell(input_size=32, hidden_size=64)\n        self.mish = nn.Mish()\n        self.max_pool = nn.MaxPool1d(kernel_size=2)\n        self.prelu = nn.PReLU()\n        self.softmax = nn.Softmax2d()\n        self.ctc_loss = nn.CTCLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, sequence_length)\n        x = self.conv1(x)  # Shape: (batch_size, 32, sequence_length - 2)\n        x = x.unsqueeze(2).unsqueeze(3)  # Shape: (batch_size, 32, 1, 1, sequence_length - 2)\n        x = self.instance_norm(x)  # Shape: (batch_size, 32, 1, 1, sequence_length - 2)\n        x = x.squeeze(3).squeeze(2)  # Shape: (batch_size, 32, sequence_length - 2)\n        x = self.max_pool(x)  # Shape: (batch_size, 32, (sequence_length - 2) // 2)\n        x = self.mish(x)  # Shape: (batch_size, 32, (sequence_length - 2) // 2)\n        \n        # Reshape for RNNCell\n        x = x.permute(2, 0, 1)  # Shape: ((sequence_length - 2) // 2, batch_size, 32)\n        hx = torch.zeros(x.size(1), 64).to(x.device)  # Initial hidden state\n        outputs = []\n        for i in range(x.size(0)):\n            hx = self.rnn_cell(x[i], hx)\n            outputs.append(hx)\n        x = torch.stack(outputs, dim=0)  # Shape: ((sequence_length - 2) // 2, batch_size, 64)\n        x = x.permute(1, 2, 0)  # Shape: (batch_size, 64, (sequence_length - 2) // 2)\n        \n        x = self.prelu(x)  # Shape: (batch_size, 64, (sequence_length - 2) // 2)\n        x = x.unsqueeze(1)  # Shape: (batch_size, 1, 64, (sequence_length - 2) // 2)\n        x = self.softmax(x)  # Shape: (batch_size, 1, 64, (sequence_length - 2) // 2)\n        \n        # Assuming we have a target for CTC loss\n        target = torch.randint(1, 10, (x.size(0), 10), dtype=torch.long).to(x.device)\n        input_lengths = torch.full((x.size(0),), x.size(3), dtype=torch.long).to(x.device)\n        target_lengths = torch.randint(1, 10, (x.size(0),), dtype=torch.long).to(x.device)\n        \n        x = x.squeeze(1)  # Shape: (batch_size, 64, (sequence_length - 2) // 2)\n        x = x.log_softmax(2)  # Shape: (batch_size, 64, (sequence_length - 2) // 2)\n        loss = self.ctc_loss(x, target, input_lengths, target_lengths)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 128).cuda()  # Example input shape: (batch_size, channels, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nfrom ctypes import c_void_p ,c_long ,c_int \nimport torch \nimport math \nimport random \nimport os \nimport tempfile \nfrom math import inf ,nan \nfrom cmath import nanj \nfrom torch ._inductor .hooks import run_intermediate_hooks \nfrom torch ._inductor .utils import maybe_profile \nfrom torch ._inductor .codegen .memory_planning import _align as align \nfrom torch import device ,empty_strided \nfrom torch ._inductor .async_compile import AsyncCompile \nfrom torch ._inductor .select_algorithm import extern_kernels \nfrom torch ._inductor .codegen .multi_kernel import MultiKernelCall \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\nsplit_scan_grid ,\ngrid_combo_kernels ,\nstart_graph ,\nend_graph ,\ncooperative_reduction_grid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_convolution_mean_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    r0_numel =126 \n    R0_BLOCK :tl .constexpr =128 \n    rnumel =r0_numel \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_offset =0 \n    r0_mask =r0_index <r0_numel \n    roffset =r0_offset \n    rindex =r0_index \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(r0_1 +126 *x0 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp26 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp28 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp32 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp41 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .where (r0_mask &xmask ,tmp3 ,0 )\n    tmp6 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n    tmp8 =tl .where (r0_mask &xmask ,tmp6 ,0 )\n    tmp9 =tl .sum (tmp8 ,1 )[:,None ]\n    tmp10 =tl .full ([XBLOCK ,1 ],126 ,tl .int32 )\n    tmp11 =tmp10 .to (tl .float32 )\n    tmp12 =tmp9 /tmp11 \n    tmp13 =tmp3 -tmp12 \n    tmp14 =tmp13 *tmp13 \n    tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp17 =tl .where (r0_mask &xmask ,tmp15 ,0 )\n    tmp18 =tl .sum (tmp17 ,1 )[:,None ]\n    tmp19 =tmp2 -tmp12 \n    tmp20 =126.0 \n    tmp21 =tmp18 /tmp20 \n    tmp22 =1e-05 \n    tmp23 =tmp21 +tmp22 \n    tmp24 =libdevice .rsqrt (tmp23 )\n    tmp25 =tmp19 *tmp24 \n    tmp27 =tmp25 *tmp26 \n    tmp29 =tmp27 +tmp28 \n    tmp30 =0.1 \n    tmp31 =tmp12 *tmp30 \n    tmp33 =0.9 \n    tmp34 =tmp32 *tmp33 \n    tmp35 =tmp31 +tmp34 \n    tmp36 =1.0 \n    tmp37 =tmp35 /tmp36 \n    tmp38 =1.008 \n    tmp39 =tmp21 *tmp38 \n    tmp40 =tmp39 *tmp30 \n    tmp42 =tmp41 *tmp33 \n    tmp43 =tmp40 +tmp42 \n    tmp44 =tmp43 /tmp36 \n    tl .store (in_out_ptr0 +(r0_1 +126 *x0 ),tmp2 ,r0_mask &xmask )\n    tl .store (out_ptr2 +(r0_1 +126 *x0 ),tmp29 ,r0_mask &xmask )\n    tl .store (out_ptr3 +(x0 ),tmp24 ,xmask )\n    tl .store (out_ptr5 +(x0 ),tmp37 ,xmask )\n    tl .store (out_ptr7 +(x0 ),tmp44 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_mish_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =2016 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp7 =20.0 \n    tmp8 =tmp6 >tmp7 \n    tmp9 =tl_math .exp (tmp6 )\n    tmp10 =libdevice .log1p (tmp9 )\n    tmp11 =tl .where (tmp8 ,tmp6 ,tmp10 )\n    tmp12 =libdevice .tanh (tmp11 )\n    tmp13 =tmp6 *tmp12 \n    tl .store (out_ptr0 +(x0 ),tmp5 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_2 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_stack_tanh_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tl .store (in_out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_5 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(1 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_6 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_7 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(3 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_8 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_9 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(5 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_10 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(6 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_11 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(7 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_12 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(8 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_13 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(9 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_14 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(10 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_15 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(11 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_16 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(12 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_17 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(13 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_18 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(14 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_19 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(15 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_20 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(16 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_21 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(17 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_22 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(18 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_23 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(19 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_24 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(20 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_25 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(21 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_26 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(22 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_27 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(23 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_28 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(24 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_29 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(25 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_30 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(26 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_31 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(27 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_32 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(28 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_33 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(29 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_34 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(30 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_35 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(31 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_36 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(32 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_37 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(33 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_38 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(34 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_39 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(35 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_40 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(36 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_41 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(37 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_42 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(38 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_43 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(39 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_44 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(40 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_45 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(41 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_46 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(42 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_47 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(43 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_48 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(44 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_49 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(45 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_50 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(46 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_51 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(47 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_52 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(48 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_53 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(49 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_54 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(50 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_55 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(51 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_56 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(52 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_57 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(53 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_58 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(54 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_59 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(55 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_60 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(56 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_61 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(57 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_62 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(58 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_63 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(59 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_64 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(60 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_65 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(61 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_66 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(62 +63 *x0 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_tanh_tanh_backward_67 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr3 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tmp8 =tmp7 *tmp7 \n    tmp9 =1.0 \n    tmp10 =tmp9 -tmp8 \n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp10 ,xmask )\n\nextern \"C\"void kernel (const int64_t *in_ptr0 ,\nint64_t *out_ptr0 )\n{\n{\n\nfor (int64_t x0 =static_cast <int64_t >(0 L );x0 <static_cast <int64_t >(10 L );x0 +=static_cast <int64_t >(1 L ))\n{\n{\n{\nauto tmp0 =in_ptr0 [static_cast <int64_t >(0 L )];\nauto tmp1 =x0 ;\nauto tmp2 =c10 ::convert <int32_t >(tmp1 );\nauto tmp3 =static_cast <int64_t >(1 );\nauto tmp4 =static_cast <int64_t >(10 );\nauto tmp5 =randint64_cpu (tmp0 ,tmp2 ,tmp3 ,tmp4 );\nout_ptr0 [static_cast <int64_t >(x0 )]=tmp5 ;\n}\n}\n}\n}\n}\n''')\n\n#include \"/tmp/torchinductor_sahanp/3b/c3bi5gk6mslf6u4iaqafhxm64z6u65e3eain4xlary5blqnvv6xx.h\"\nextern \"C\"  void kernel(const int64_t* in_ptr0,\n                       int64_t* out_ptr0)\n{\n    {\n        {\n            {\n                auto tmp0 = in_ptr0[static_cast<int64_t>(1L)];\n                auto tmp1 = static_cast<int32_t>(0);\n                auto tmp2 = static_cast<int64_t>(1);\n                auto tmp3 = static_cast<int64_t>(10);\n                auto tmp4 = randint64_cpu(tmp0, tmp1, tmp2, tmp3);\n                out_ptr0[static_cast<int64_t>(0L)] = tmp4;\n            }\n        }\n    }\n}\n''')\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__log_softmax_70 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    r0_numel =63 \n    R0_BLOCK :tl .constexpr =64 \n    rnumel =r0_numel \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_offset =0 \n    r0_mask =r0_index <r0_numel \n    roffset =r0_offset \n    rindex =r0_index \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +64 *r0_1 ),r0_mask &xmask ,other =0.0 )\n    tmp3 =tl .load (in_ptr1 +(0 ))\n    tmp4 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp5 =tmp4 *tmp0 \n    tmp6 =tl .where (tmp2 ,tmp0 ,tmp5 )\n    tmp7 =tmp6 -tmp6 \n    tmp8 =tl_math .exp (tmp7 )\n    tmp9 =tmp8 /tmp8 \n    tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n    tmp12 =tl .where (r0_mask &xmask ,tmp10 ,float (\"-inf\"))\n    tmp13 =triton_helpers .max2 (tmp12 ,1 )[:,None ]\n    tmp14 =tmp9 -tmp13 \n    tmp15 =tl_math .exp (tmp14 )\n    tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ,R0_BLOCK ])\n    tmp18 =tl .where (r0_mask &xmask ,tmp16 ,0 )\n    tmp19 =tl .sum (tmp18 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp19 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__log_softmax_71 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    ynumel =63 \n    xnumel =64 \n    yoffset =tl .program_id (1 )*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x1 =xindex \n    y0 =yindex \n    tmp0 =tl .load (in_ptr0 +(x1 +64 *y0 ),xmask &ymask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr1 +(0 ))\n    tmp4 =tl .broadcast_to (tmp3 ,[XBLOCK ,YBLOCK ])\n    tmp10 =tl .load (in_ptr2 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr3 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp5 =tmp4 *tmp0 \n    tmp6 =tl .where (tmp2 ,tmp0 ,tmp5 )\n    tmp7 =tmp6 -tmp6 \n    tmp8 =tl_math .exp (tmp7 )\n    tmp9 =tmp8 /tmp8 \n    tmp11 =tmp9 -tmp10 \n    tmp13 =tl_math .log (tmp12 )\n    tmp14 =tmp11 -tmp13 \n    tl .store (out_ptr0 +(y0 +63 *x1 ),tmp14 ,xmask &ymask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_fill_mish_mul_sigmoid_sub_72 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =2016 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp3 =20.0 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tl_math .exp (tmp2 )\n    tmp6 =libdevice .log1p (tmp5 )\n    tmp7 =tl .where (tmp4 ,tmp2 ,tmp6 )\n    tmp8 =libdevice .tanh (tmp7 )\n    tmp9 =tl .sigmoid (tmp2 )\n    tmp10 =tmp2 *tmp9 \n    tmp11 =tmp8 *tmp8 \n    tmp12 =1.0 \n    tmp13 =tmp12 -tmp11 \n    tmp14 =tmp10 *tmp13 \n    tmp15 =tmp8 +tmp14 \n    tl .store (out_ptr0 +(x0 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_73 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .full ([1 ],63 ,tl .int64 )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp0 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(32 ,1 ,3 ),(3 ,3 ,1 ))\n    assert_size_stride (primals_2 ,(32 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,1 ,128 ),(128 ,128 ,1 ))\n    assert_size_stride (primals_4 ,(32 ,),(1 ,))\n    assert_size_stride (primals_5 ,(32 ,),(1 ,))\n    assert_size_stride (primals_6 ,(32 ,),(1 ,))\n    assert_size_stride (primals_7 ,(32 ,),(1 ,))\n    assert_size_stride (primals_8 ,(64 ,32 ),(32 ,1 ))\n    assert_size_stride (primals_9 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_10 ,(64 ,),(1 ,))\n    assert_size_stride (primals_11 ,(64 ,),(1 ,))\n    assert_size_stride (primals_12 ,(1 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,),padding =(0 ,),dilation =(1 ,),transposed =False ,output_padding =(0 ,),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,32 ,126 ),(4032 ,126 ,1 ))\n        buf1 =buf0 ;del buf0 \n        buf2 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,32 ,32 ,32 ),torch .float32 )\n        buf6 =empty_strided_cuda ((1 ,32 ,1 ,1 ,126 ),(4032 ,126 ,126 ,126 ,1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,32 ,32 ,32 ),torch .float32 )\n\n        stream0 =get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_convolution_mean_0 [grid (32 )](buf1 ,primals_2 ,primals_6 ,primals_7 ,primals_4 ,primals_5 ,buf2 ,buf6 ,buf5 ,primals_4 ,primals_5 ,32 ,126 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_2 \n        del primals_4 \n        del primals_5 \n        del primals_7 \n        buf7 =empty_strided_cuda ((1 ,32 ,1 ,63 ),(2048 ,63 ,63 ,1 ),torch .int8 )\n        buf9 =empty_strided_cuda ((1 ,32 ,63 ),(2016 ,63 ,1 ),torch .float32 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_mish_1 [grid (2016 )](buf6 ,buf7 ,buf9 ,2016 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf8 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused__to_copy_2 [grid (64 )](buf8 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf10 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf8 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf10 )\n        buf11 =empty_strided_cuda ((1 ,32 ),(32 ,1 ),torch .float32 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_3 [grid (32 )](buf9 ,buf11 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf12 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf11 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf12 )\n        buf13 =buf10 ;del buf10 \n        buf324 =empty_strided_cuda ((63 ,64 ),(64 ,1 ),torch .float32 )\n        buf261 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),0 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf13 ,primals_11 ,buf12 ,primals_10 ,buf261 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf14 =buf12 ;del buf12 \n\n        extern_kernels .mm (buf13 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf14 )\n        buf15 =buf11 ;del buf11 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_5 [grid (32 )](buf9 ,buf15 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf16 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf15 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf16 )\n        buf17 =buf14 ;del buf14 \n        buf262 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),64 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf17 ,primals_11 ,buf16 ,primals_10 ,buf262 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf18 =buf16 ;del buf16 \n\n        extern_kernels .mm (buf17 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf18 )\n        buf19 =buf15 ;del buf15 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_6 [grid (32 )](buf9 ,buf19 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf20 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf19 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf20 )\n        buf21 =buf18 ;del buf18 \n        buf263 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),128 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf21 ,primals_11 ,buf20 ,primals_10 ,buf263 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf22 =buf20 ;del buf20 \n\n        extern_kernels .mm (buf21 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf22 )\n        buf23 =buf19 ;del buf19 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_7 [grid (32 )](buf9 ,buf23 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf24 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf23 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf24 )\n        buf25 =buf22 ;del buf22 \n        buf264 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),192 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf25 ,primals_11 ,buf24 ,primals_10 ,buf264 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf26 =buf24 ;del buf24 \n\n        extern_kernels .mm (buf25 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf26 )\n        buf27 =buf23 ;del buf23 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_8 [grid (32 )](buf9 ,buf27 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf28 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf27 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf28 )\n        buf29 =buf26 ;del buf26 \n        buf265 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),256 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf29 ,primals_11 ,buf28 ,primals_10 ,buf265 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf30 =buf28 ;del buf28 \n\n        extern_kernels .mm (buf29 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf30 )\n        buf31 =buf27 ;del buf27 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_9 [grid (32 )](buf9 ,buf31 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf32 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf31 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf32 )\n        buf33 =buf30 ;del buf30 \n        buf266 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),320 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf33 ,primals_11 ,buf32 ,primals_10 ,buf266 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf34 =buf32 ;del buf32 \n\n        extern_kernels .mm (buf33 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf34 )\n        buf35 =buf31 ;del buf31 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_10 [grid (32 )](buf9 ,buf35 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf36 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf35 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf36 )\n        buf37 =buf34 ;del buf34 \n        buf267 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),384 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf37 ,primals_11 ,buf36 ,primals_10 ,buf267 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf38 =buf36 ;del buf36 \n\n        extern_kernels .mm (buf37 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf38 )\n        buf39 =buf35 ;del buf35 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_11 [grid (32 )](buf9 ,buf39 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf40 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf39 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf40 )\n        buf41 =buf38 ;del buf38 \n        buf268 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),448 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf41 ,primals_11 ,buf40 ,primals_10 ,buf268 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf42 =buf40 ;del buf40 \n\n        extern_kernels .mm (buf41 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf42 )\n        buf43 =buf39 ;del buf39 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_12 [grid (32 )](buf9 ,buf43 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf44 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf43 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf44 )\n        buf45 =buf42 ;del buf42 \n        buf269 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),512 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf45 ,primals_11 ,buf44 ,primals_10 ,buf269 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf46 =buf44 ;del buf44 \n\n        extern_kernels .mm (buf45 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf46 )\n        buf47 =buf43 ;del buf43 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_13 [grid (32 )](buf9 ,buf47 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf48 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf47 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf48 )\n        buf49 =buf46 ;del buf46 \n        buf270 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),576 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf49 ,primals_11 ,buf48 ,primals_10 ,buf270 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf50 =buf48 ;del buf48 \n\n        extern_kernels .mm (buf49 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf50 )\n        buf51 =buf47 ;del buf47 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_14 [grid (32 )](buf9 ,buf51 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf52 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf51 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf52 )\n        buf53 =buf50 ;del buf50 \n        buf271 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),640 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf53 ,primals_11 ,buf52 ,primals_10 ,buf271 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf54 =buf52 ;del buf52 \n\n        extern_kernels .mm (buf53 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf54 )\n        buf55 =buf51 ;del buf51 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_15 [grid (32 )](buf9 ,buf55 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf56 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf55 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf56 )\n        buf57 =buf54 ;del buf54 \n        buf272 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),704 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf57 ,primals_11 ,buf56 ,primals_10 ,buf272 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf58 =buf56 ;del buf56 \n\n        extern_kernels .mm (buf57 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf58 )\n        buf59 =buf55 ;del buf55 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_16 [grid (32 )](buf9 ,buf59 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf60 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf59 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf60 )\n        buf61 =buf58 ;del buf58 \n        buf273 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),768 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf61 ,primals_11 ,buf60 ,primals_10 ,buf273 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf62 =buf60 ;del buf60 \n\n        extern_kernels .mm (buf61 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf62 )\n        buf63 =buf59 ;del buf59 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_17 [grid (32 )](buf9 ,buf63 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf64 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf63 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf64 )\n        buf65 =buf62 ;del buf62 \n        buf274 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),832 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf65 ,primals_11 ,buf64 ,primals_10 ,buf274 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf66 =buf64 ;del buf64 \n\n        extern_kernels .mm (buf65 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf66 )\n        buf67 =buf63 ;del buf63 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_18 [grid (32 )](buf9 ,buf67 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf68 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf67 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf68 )\n        buf69 =buf66 ;del buf66 \n        buf275 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),896 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf69 ,primals_11 ,buf68 ,primals_10 ,buf275 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf70 =buf68 ;del buf68 \n\n        extern_kernels .mm (buf69 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf70 )\n        buf71 =buf67 ;del buf67 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_19 [grid (32 )](buf9 ,buf71 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf72 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf71 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf72 )\n        buf73 =buf70 ;del buf70 \n        buf276 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),960 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf73 ,primals_11 ,buf72 ,primals_10 ,buf276 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf74 =buf72 ;del buf72 \n\n        extern_kernels .mm (buf73 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf74 )\n        buf75 =buf71 ;del buf71 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_20 [grid (32 )](buf9 ,buf75 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf76 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf75 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf76 )\n        buf77 =buf74 ;del buf74 \n        buf277 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1024 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf77 ,primals_11 ,buf76 ,primals_10 ,buf277 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf78 =buf76 ;del buf76 \n\n        extern_kernels .mm (buf77 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf78 )\n        buf79 =buf75 ;del buf75 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_21 [grid (32 )](buf9 ,buf79 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf80 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf79 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf80 )\n        buf81 =buf78 ;del buf78 \n        buf278 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1088 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf81 ,primals_11 ,buf80 ,primals_10 ,buf278 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf82 =buf80 ;del buf80 \n\n        extern_kernels .mm (buf81 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf82 )\n        buf83 =buf79 ;del buf79 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_22 [grid (32 )](buf9 ,buf83 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf84 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf83 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf84 )\n        buf85 =buf82 ;del buf82 \n        buf279 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1152 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf85 ,primals_11 ,buf84 ,primals_10 ,buf279 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf86 =buf84 ;del buf84 \n\n        extern_kernels .mm (buf85 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf86 )\n        buf87 =buf83 ;del buf83 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_23 [grid (32 )](buf9 ,buf87 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf88 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf87 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf88 )\n        buf89 =buf86 ;del buf86 \n        buf280 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1216 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf89 ,primals_11 ,buf88 ,primals_10 ,buf280 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf90 =buf88 ;del buf88 \n\n        extern_kernels .mm (buf89 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf90 )\n        buf91 =buf87 ;del buf87 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_24 [grid (32 )](buf9 ,buf91 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf92 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf91 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf92 )\n        buf93 =buf90 ;del buf90 \n        buf281 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1280 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf93 ,primals_11 ,buf92 ,primals_10 ,buf281 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf94 =buf92 ;del buf92 \n\n        extern_kernels .mm (buf93 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf94 )\n        buf95 =buf91 ;del buf91 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_25 [grid (32 )](buf9 ,buf95 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf96 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf95 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf96 )\n        buf97 =buf94 ;del buf94 \n        buf282 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1344 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf97 ,primals_11 ,buf96 ,primals_10 ,buf282 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf98 =buf96 ;del buf96 \n\n        extern_kernels .mm (buf97 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf98 )\n        buf99 =buf95 ;del buf95 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_26 [grid (32 )](buf9 ,buf99 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf100 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf99 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf100 )\n        buf101 =buf98 ;del buf98 \n        buf283 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1408 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf101 ,primals_11 ,buf100 ,primals_10 ,buf283 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf102 =buf100 ;del buf100 \n\n        extern_kernels .mm (buf101 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf102 )\n        buf103 =buf99 ;del buf99 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_27 [grid (32 )](buf9 ,buf103 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf104 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf103 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf104 )\n        buf105 =buf102 ;del buf102 \n        buf284 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1472 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf105 ,primals_11 ,buf104 ,primals_10 ,buf284 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf106 =buf104 ;del buf104 \n\n        extern_kernels .mm (buf105 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf106 )\n        buf107 =buf103 ;del buf103 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_28 [grid (32 )](buf9 ,buf107 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf108 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf107 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf108 )\n        buf109 =buf106 ;del buf106 \n        buf285 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1536 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf109 ,primals_11 ,buf108 ,primals_10 ,buf285 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf110 =buf108 ;del buf108 \n\n        extern_kernels .mm (buf109 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf110 )\n        buf111 =buf107 ;del buf107 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_29 [grid (32 )](buf9 ,buf111 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf112 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf111 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf112 )\n        buf113 =buf110 ;del buf110 \n        buf286 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1600 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf113 ,primals_11 ,buf112 ,primals_10 ,buf286 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf114 =buf112 ;del buf112 \n\n        extern_kernels .mm (buf113 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf114 )\n        buf115 =buf111 ;del buf111 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_30 [grid (32 )](buf9 ,buf115 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf116 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf115 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf116 )\n        buf117 =buf114 ;del buf114 \n        buf287 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1664 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf117 ,primals_11 ,buf116 ,primals_10 ,buf287 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf118 =buf116 ;del buf116 \n\n        extern_kernels .mm (buf117 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf118 )\n        buf119 =buf115 ;del buf115 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_31 [grid (32 )](buf9 ,buf119 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf120 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf119 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf120 )\n        buf121 =buf118 ;del buf118 \n        buf288 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1728 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf121 ,primals_11 ,buf120 ,primals_10 ,buf288 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf122 =buf120 ;del buf120 \n\n        extern_kernels .mm (buf121 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf122 )\n        buf123 =buf119 ;del buf119 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_32 [grid (32 )](buf9 ,buf123 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf124 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf123 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf124 )\n        buf125 =buf122 ;del buf122 \n        buf289 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1792 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf125 ,primals_11 ,buf124 ,primals_10 ,buf289 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf126 =buf124 ;del buf124 \n\n        extern_kernels .mm (buf125 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf126 )\n        buf127 =buf123 ;del buf123 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_33 [grid (32 )](buf9 ,buf127 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf128 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf127 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf128 )\n        buf129 =buf126 ;del buf126 \n        buf290 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1856 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf129 ,primals_11 ,buf128 ,primals_10 ,buf290 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf130 =buf128 ;del buf128 \n\n        extern_kernels .mm (buf129 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf130 )\n        buf131 =buf127 ;del buf127 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_34 [grid (32 )](buf9 ,buf131 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf132 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf131 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf132 )\n        buf133 =buf130 ;del buf130 \n        buf291 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1920 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf133 ,primals_11 ,buf132 ,primals_10 ,buf291 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf134 =buf132 ;del buf132 \n\n        extern_kernels .mm (buf133 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf134 )\n        buf135 =buf131 ;del buf131 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_35 [grid (32 )](buf9 ,buf135 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf136 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf135 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf136 )\n        buf137 =buf134 ;del buf134 \n        buf292 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),1984 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf137 ,primals_11 ,buf136 ,primals_10 ,buf292 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf138 =buf136 ;del buf136 \n\n        extern_kernels .mm (buf137 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf138 )\n        buf139 =buf135 ;del buf135 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_36 [grid (32 )](buf9 ,buf139 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf140 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf139 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf140 )\n        buf141 =buf138 ;del buf138 \n        buf293 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2048 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf141 ,primals_11 ,buf140 ,primals_10 ,buf293 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf142 =buf140 ;del buf140 \n\n        extern_kernels .mm (buf141 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf142 )\n        buf143 =buf139 ;del buf139 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_37 [grid (32 )](buf9 ,buf143 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf144 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf143 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf144 )\n        buf145 =buf142 ;del buf142 \n        buf294 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2112 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf145 ,primals_11 ,buf144 ,primals_10 ,buf294 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf146 =buf144 ;del buf144 \n\n        extern_kernels .mm (buf145 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf146 )\n        buf147 =buf143 ;del buf143 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_38 [grid (32 )](buf9 ,buf147 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf148 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf147 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf148 )\n        buf149 =buf146 ;del buf146 \n        buf295 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2176 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf149 ,primals_11 ,buf148 ,primals_10 ,buf295 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf150 =buf148 ;del buf148 \n\n        extern_kernels .mm (buf149 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf150 )\n        buf151 =buf147 ;del buf147 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_39 [grid (32 )](buf9 ,buf151 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf152 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf151 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf152 )\n        buf153 =buf150 ;del buf150 \n        buf296 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2240 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf153 ,primals_11 ,buf152 ,primals_10 ,buf296 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf154 =buf152 ;del buf152 \n\n        extern_kernels .mm (buf153 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf154 )\n        buf155 =buf151 ;del buf151 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_40 [grid (32 )](buf9 ,buf155 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf156 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf155 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf156 )\n        buf157 =buf154 ;del buf154 \n        buf297 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2304 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf157 ,primals_11 ,buf156 ,primals_10 ,buf297 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf158 =buf156 ;del buf156 \n\n        extern_kernels .mm (buf157 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf158 )\n        buf159 =buf155 ;del buf155 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_41 [grid (32 )](buf9 ,buf159 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf160 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf159 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf160 )\n        buf161 =buf158 ;del buf158 \n        buf298 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2368 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf161 ,primals_11 ,buf160 ,primals_10 ,buf298 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf162 =buf160 ;del buf160 \n\n        extern_kernels .mm (buf161 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf162 )\n        buf163 =buf159 ;del buf159 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_42 [grid (32 )](buf9 ,buf163 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf164 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf163 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf164 )\n        buf165 =buf162 ;del buf162 \n        buf299 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2432 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf165 ,primals_11 ,buf164 ,primals_10 ,buf299 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf166 =buf164 ;del buf164 \n\n        extern_kernels .mm (buf165 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf166 )\n        buf167 =buf163 ;del buf163 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_43 [grid (32 )](buf9 ,buf167 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf168 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf167 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf168 )\n        buf169 =buf166 ;del buf166 \n        buf300 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2496 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf169 ,primals_11 ,buf168 ,primals_10 ,buf300 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf170 =buf168 ;del buf168 \n\n        extern_kernels .mm (buf169 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf170 )\n        buf171 =buf167 ;del buf167 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_44 [grid (32 )](buf9 ,buf171 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf172 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf171 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf172 )\n        buf173 =buf170 ;del buf170 \n        buf301 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2560 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf173 ,primals_11 ,buf172 ,primals_10 ,buf301 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf174 =buf172 ;del buf172 \n\n        extern_kernels .mm (buf173 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf174 )\n        buf175 =buf171 ;del buf171 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_45 [grid (32 )](buf9 ,buf175 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf176 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf175 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf176 )\n        buf177 =buf174 ;del buf174 \n        buf302 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2624 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf177 ,primals_11 ,buf176 ,primals_10 ,buf302 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf178 =buf176 ;del buf176 \n\n        extern_kernels .mm (buf177 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf178 )\n        buf179 =buf175 ;del buf175 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_46 [grid (32 )](buf9 ,buf179 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf180 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf179 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf180 )\n        buf181 =buf178 ;del buf178 \n        buf303 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2688 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf181 ,primals_11 ,buf180 ,primals_10 ,buf303 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf182 =buf180 ;del buf180 \n\n        extern_kernels .mm (buf181 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf182 )\n        buf183 =buf179 ;del buf179 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_47 [grid (32 )](buf9 ,buf183 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf184 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf183 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf184 )\n        buf185 =buf182 ;del buf182 \n        buf304 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2752 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf185 ,primals_11 ,buf184 ,primals_10 ,buf304 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf186 =buf184 ;del buf184 \n\n        extern_kernels .mm (buf185 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf186 )\n        buf187 =buf183 ;del buf183 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_48 [grid (32 )](buf9 ,buf187 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf188 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf187 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf188 )\n        buf189 =buf186 ;del buf186 \n        buf305 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2816 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf189 ,primals_11 ,buf188 ,primals_10 ,buf305 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf190 =buf188 ;del buf188 \n\n        extern_kernels .mm (buf189 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf190 )\n        buf191 =buf187 ;del buf187 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_49 [grid (32 )](buf9 ,buf191 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf192 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf191 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf192 )\n        buf193 =buf190 ;del buf190 \n        buf306 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2880 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf193 ,primals_11 ,buf192 ,primals_10 ,buf306 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf194 =buf192 ;del buf192 \n\n        extern_kernels .mm (buf193 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf194 )\n        buf195 =buf191 ;del buf191 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_50 [grid (32 )](buf9 ,buf195 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf196 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf195 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf196 )\n        buf197 =buf194 ;del buf194 \n        buf307 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),2944 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf197 ,primals_11 ,buf196 ,primals_10 ,buf307 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf198 =buf196 ;del buf196 \n\n        extern_kernels .mm (buf197 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf198 )\n        buf199 =buf195 ;del buf195 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_51 [grid (32 )](buf9 ,buf199 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf200 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf199 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf200 )\n        buf201 =buf198 ;del buf198 \n        buf308 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3008 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf201 ,primals_11 ,buf200 ,primals_10 ,buf308 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf202 =buf200 ;del buf200 \n\n        extern_kernels .mm (buf201 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf202 )\n        buf203 =buf199 ;del buf199 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_52 [grid (32 )](buf9 ,buf203 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf204 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf203 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf204 )\n        buf205 =buf202 ;del buf202 \n        buf309 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3072 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf205 ,primals_11 ,buf204 ,primals_10 ,buf309 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf206 =buf204 ;del buf204 \n\n        extern_kernels .mm (buf205 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf206 )\n        buf207 =buf203 ;del buf203 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_53 [grid (32 )](buf9 ,buf207 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf208 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf207 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf208 )\n        buf209 =buf206 ;del buf206 \n        buf310 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3136 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf209 ,primals_11 ,buf208 ,primals_10 ,buf310 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf210 =buf208 ;del buf208 \n\n        extern_kernels .mm (buf209 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf210 )\n        buf211 =buf207 ;del buf207 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_54 [grid (32 )](buf9 ,buf211 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf212 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf211 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf212 )\n        buf213 =buf210 ;del buf210 \n        buf311 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3200 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf213 ,primals_11 ,buf212 ,primals_10 ,buf311 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf214 =buf212 ;del buf212 \n\n        extern_kernels .mm (buf213 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf214 )\n        buf215 =buf211 ;del buf211 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_55 [grid (32 )](buf9 ,buf215 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf216 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf215 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf216 )\n        buf217 =buf214 ;del buf214 \n        buf312 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3264 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf217 ,primals_11 ,buf216 ,primals_10 ,buf312 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf218 =buf216 ;del buf216 \n\n        extern_kernels .mm (buf217 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf218 )\n        buf219 =buf215 ;del buf215 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_56 [grid (32 )](buf9 ,buf219 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf220 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf219 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf220 )\n        buf221 =buf218 ;del buf218 \n        buf313 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3328 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf221 ,primals_11 ,buf220 ,primals_10 ,buf313 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf222 =buf220 ;del buf220 \n\n        extern_kernels .mm (buf221 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf222 )\n        buf223 =buf219 ;del buf219 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_57 [grid (32 )](buf9 ,buf223 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf224 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf223 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf224 )\n        buf225 =buf222 ;del buf222 \n        buf314 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3392 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf225 ,primals_11 ,buf224 ,primals_10 ,buf314 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf226 =buf224 ;del buf224 \n\n        extern_kernels .mm (buf225 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf226 )\n        buf227 =buf223 ;del buf223 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_58 [grid (32 )](buf9 ,buf227 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf228 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf227 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf228 )\n        buf229 =buf226 ;del buf226 \n        buf315 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3456 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf229 ,primals_11 ,buf228 ,primals_10 ,buf315 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf230 =buf228 ;del buf228 \n\n        extern_kernels .mm (buf229 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf230 )\n        buf231 =buf227 ;del buf227 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_59 [grid (32 )](buf9 ,buf231 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf232 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf231 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf232 )\n        buf233 =buf230 ;del buf230 \n        buf316 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3520 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf233 ,primals_11 ,buf232 ,primals_10 ,buf316 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf234 =buf232 ;del buf232 \n\n        extern_kernels .mm (buf233 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf234 )\n        buf235 =buf231 ;del buf231 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_60 [grid (32 )](buf9 ,buf235 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf236 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf235 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf236 )\n        buf237 =buf234 ;del buf234 \n        buf317 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3584 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf237 ,primals_11 ,buf236 ,primals_10 ,buf317 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf238 =buf236 ;del buf236 \n\n        extern_kernels .mm (buf237 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf238 )\n        buf239 =buf235 ;del buf235 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_61 [grid (32 )](buf9 ,buf239 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf240 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf239 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf240 )\n        buf241 =buf238 ;del buf238 \n        buf318 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3648 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf241 ,primals_11 ,buf240 ,primals_10 ,buf318 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf242 =buf240 ;del buf240 \n\n        extern_kernels .mm (buf241 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf242 )\n        buf243 =buf239 ;del buf239 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_62 [grid (32 )](buf9 ,buf243 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf244 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf243 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf244 )\n        buf245 =buf242 ;del buf242 \n        buf319 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3712 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf245 ,primals_11 ,buf244 ,primals_10 ,buf319 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf246 =buf244 ;del buf244 \n\n        extern_kernels .mm (buf245 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf246 )\n        buf247 =buf243 ;del buf243 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_63 [grid (32 )](buf9 ,buf247 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf248 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf247 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf248 )\n        buf249 =buf246 ;del buf246 \n        buf320 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3776 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf249 ,primals_11 ,buf248 ,primals_10 ,buf320 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf250 =buf248 ;del buf248 \n\n        extern_kernels .mm (buf249 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf250 )\n        buf251 =buf247 ;del buf247 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_64 [grid (32 )](buf9 ,buf251 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf252 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf251 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf252 )\n        buf253 =buf250 ;del buf250 \n        buf321 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3840 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf253 ,primals_11 ,buf252 ,primals_10 ,buf321 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf254 =buf252 ;del buf252 \n\n        extern_kernels .mm (buf253 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf254 )\n        buf255 =buf251 ;del buf251 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_65 [grid (32 )](buf9 ,buf255 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf256 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf255 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf256 )\n        buf257 =buf254 ;del buf254 \n        buf322 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3904 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_4 [grid (64 )](buf257 ,primals_11 ,buf256 ,primals_10 ,buf322 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf258 =buf256 ;del buf256 \n\n        extern_kernels .mm (buf257 ,reinterpret_tensor (primals_9 ,(64 ,64 ),(1 ,64 ),0 ),out =buf258 )\n        buf259 =buf255 ;del buf255 \n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_addmm_66 [grid (32 )](buf9 ,buf259 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf260 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf259 ,reinterpret_tensor (primals_8 ,(32 ,64 ),(1 ,32 ),0 ),out =buf260 )\n        del buf259 \n        buf323 =reinterpret_tensor (buf324 ,(1 ,64 ),(64 ,1 ),3968 )\n        buf333 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_tanh_backward_67 [grid (64 )](buf258 ,primals_11 ,buf260 ,primals_10 ,buf323 ,buf333 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del primals_10 \n        del primals_11 \n    buf325 =empty_strided_cpu ((2 ,),(1 ,),torch .int64 )\n\n    aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf325 )\n    buf326 =empty_strided_cpu ((1 ,10 ),(10 ,1 ),torch .int64 )\n    cpp_fused_randint_68 (buf325 ,buf326 )\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf327 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .int64 )\n        buf327 .copy_ (buf326 ,False )\n        del buf326 \n    buf328 =empty_strided_cpu ((1 ,),(1 ,),torch .int64 )\n    cpp_fused_randint_69 (buf325 ,buf328 )\n    del buf325 \n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf329 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n        buf329 .copy_ (buf328 ,False )\n        del buf328 \n        buf330 =reinterpret_tensor (buf260 ,(1 ,64 ,1 ),(64 ,1 ,64 ),0 );del buf260 \n        buf331 =reinterpret_tensor (buf258 ,(1 ,64 ,1 ),(64 ,1 ,64 ),0 );del buf258 \n\n        stream0 =get_raw_stream (0 )\n        triton_per_fused__log_softmax_70 [grid (64 )](buf324 ,primals_12 ,buf330 ,buf331 ,64 ,63 ,XBLOCK =32 ,num_warps =8 ,num_stages =1 )\n        buf332 =empty_strided_cuda ((1 ,64 ,63 ),(4032 ,63 ,1 ),torch .float32 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused__log_softmax_71 [grid (63 ,64 )](buf324 ,primals_12 ,buf330 ,buf331 ,buf332 ,63 ,64 ,XBLOCK =64 ,YBLOCK =4 ,num_warps =4 ,num_stages =1 )\n        del buf330 \n        del buf331 \n        buf334 =empty_strided_cuda ((1 ,32 ,63 ),(2016 ,63 ,1 ),torch .float32 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_add_fill_mish_mul_sigmoid_sub_72 [grid (2016 )](buf6 ,buf334 ,2016 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf343 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused__to_copy_73 [grid (1 )](buf343 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n    return (buf332 ,buf327 ,buf343 ,buf329 ,primals_1 ,primals_3 ,primals_6 ,primals_12 ,buf1 ,reinterpret_tensor (buf5 ,(32 ,),(1 ,),0 ),reinterpret_tensor (buf6 ,(1 ,32 ,1 ,126 ),(4032 ,126 ,126 ,1 ),0 ),buf7 ,buf8 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),0 ),buf13 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),1 ),buf17 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),2 ),buf21 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),3 ),buf25 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),4 ),buf29 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),5 ),buf33 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),6 ),buf37 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),7 ),buf41 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),8 ),buf45 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),9 ),buf49 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),10 ),buf53 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),11 ),buf57 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),12 ),buf61 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),13 ),buf65 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),14 ),buf69 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),15 ),buf73 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),16 ),buf77 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),17 ),buf81 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),18 ),buf85 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),19 ),buf89 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),20 ),buf93 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),21 ),buf97 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),22 ),buf101 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),23 ),buf105 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),24 ),buf109 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),25 ),buf113 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),26 ),buf117 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),27 ),buf121 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),28 ),buf125 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),29 ),buf129 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),30 ),buf133 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),31 ),buf137 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),32 ),buf141 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),33 ),buf145 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),34 ),buf149 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),35 ),buf153 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),36 ),buf157 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),37 ),buf161 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),38 ),buf165 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),39 ),buf169 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),40 ),buf173 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),41 ),buf177 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),42 ),buf181 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),43 ),buf185 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),44 ),buf189 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),45 ),buf193 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),46 ),buf197 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),47 ),buf201 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),48 ),buf205 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),49 ),buf209 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),50 ),buf213 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),51 ),buf217 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),52 ),buf221 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),53 ),buf225 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),54 ),buf229 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),55 ),buf233 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),56 ),buf237 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),57 ),buf241 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),58 ),buf245 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),59 ),buf249 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),60 ),buf253 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),61 ),buf257 ,reinterpret_tensor (buf9 ,(1 ,32 ),(2016 ,63 ),62 ),buf324 ,buf332 ,buf333 ,primals_8 ,primals_9 ,buf334 ,reinterpret_tensor (buf2 ,(1 ,32 ,1 ,1 ,1 ),(32 ,1 ,1 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((32 ,1 ,3 ),(3 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,1 ,128 ),(128 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((64 ,32 ),(32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((1 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "7c6eff5e-804d-4aa8-8b16-ea7c232cbf6a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MultiLabelSoftMarginLoss', 'AvgPool2d', 'CrossMapLRN2d', 'GRUCell', 'ReplicationPad2d', 'MaxPool3d', 'Softmax2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.cross_map_lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.gru_cell = nn.GRUCell(input_size=64, hidden_size=128)\n        self.replication_pad = nn.ReplicationPad2d(padding=2)\n        self.max_pool3d = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n        self.softmax2d = nn.Softmax2d()\n        self.loss = nn.MultiLabelSoftMarginLoss()\n\n    def forward(self, x):\n        # Apply AvgPool2d\n        x = self.avg_pool(x)\n        \n        # Apply CrossMapLRN2d\n        x = self.cross_map_lrn(x)\n        \n        # Reshape for GRUCell\n        batch_size, channels, height, width = x.size()\n        x = x.view(batch_size, -1)  # Flatten spatial dimensions\n        x = self.gru_cell(x, torch.zeros(batch_size, 128).to(x.device))  # GRUCell requires hidden state\n        \n        # Reshape back to 4D for ReplicationPad2d\n        x = x.view(batch_size, channels, height, width)\n        \n        # Apply ReplicationPad2d\n        x = self.replication_pad(x)\n        \n        # Reshape for MaxPool3d (add a dummy temporal dimension)\n        x = x.unsqueeze(2)  # Add a temporal dimension\n        x = self.max_pool3d(x)\n        \n        # Remove the temporal dimension\n        x = x.squeeze(2)\n        \n        # Apply Softmax2d\n        x = self.softmax2d(x)\n        \n        # Compute loss (dummy target for demonstration)\n        target = torch.randint(0, 2, (batch_size, channels, height, width)).float().to(x.device)\n        loss = self.loss(x, target)\n        \n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels, 64x64 spatial dimensions\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tl .store (out_ptr0 +(x3 ),tmp8 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //2 \n        s1 //2 \n        (s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_0_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_0 [grid (triton_poi_fused_avg_pool2d_0_xnumel )](arg3_1 ,buf0 ,32 ,32 ,1024 ,64 ,64 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "7dd819c6-5078-4fc7-b267-708075fa0478",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LogSoftmax', 'ParameterList', 'AdaptiveAvgPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.parameter_list = nn.ParameterList([\n            nn.Parameter(torch.randn(10)),\n            nn.Parameter(torch.randn(10)),\n            nn.Parameter(torch.randn(10))\n        ])\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        # Apply AdaptiveAvgPool2d to reduce spatial dimensions to 1x1\n        x = self.adaptive_avg_pool(x)\n        \n        # Flatten the output to match the shape required for the ParameterList\n        x = x.view(x.size(0), -1)\n        \n        # Apply the parameters from the ParameterList\n        for param in self.parameter_list:\n            x = x + param.unsqueeze(0).expand_as(x)\n        \n        # Apply LogSoftmax to the final output\n        x = self.log_softmax(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_mean_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =10 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =_tmp2 +tmp1 \n        _tmp2 =tl .where (r0_mask &xmask ,tmp3 ,_tmp2 )\n    tmp2 =tl .sum (_tmp2 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__log_softmax_add_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp4 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp6 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp8 =tl .load (in_ptr2 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =ks0 *ks1 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =tmp0 /tmp2 \n    tmp5 =tmp3 +tmp4 \n    tmp7 =tmp5 +tmp6 \n    tmp9 =tmp7 +tmp8 \n    tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n    tmp12 =tl .where (r0_mask ,tmp10 ,float (\"-inf\"))\n    tmp13 =triton_helpers .max2 (tmp12 ,1 )[:,None ]\n    tmp14 =tmp9 -tmp13 \n    tmp15 =tl_math .exp (tmp14 )\n    tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ,R0_BLOCK ])\n    tmp18 =tl .where (r0_mask ,tmp16 ,0 )\n    tmp19 =tl .sum (tmp18 ,1 )[:,None ]\n    tmp20 =tl_math .log (tmp19 )\n    tmp21 =tmp14 -tmp20 \n    tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp21 ,r0_mask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 =args \n    args .clear ()\n    s1 =primals_1 \n    s2 =primals_2 \n    assert_size_stride (primals_3 ,(1 ,10 ,s1 ,s2 ),(10 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    assert_size_stride (primals_4 ,(10 ,),(1 ,))\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    assert_size_stride (primals_6 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,10 ,10 ),torch .float32 )\n\n        s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused_mean_0 [grid (10 )](primals_3 ,buf0 ,64 ,64 ,10 ,4096 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del primals_3 \n        buf2 =reinterpret_tensor (buf0 ,(1 ,10 ),(10 ,1 ),0 );del buf0 \n        buf4 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_per_fused__log_softmax_add_1 [grid (1 )](buf4 ,primals_4 ,primals_5 ,primals_6 ,64 ,64 ,1 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_4 \n        del primals_5 \n        del primals_6 \n    return (buf4 ,buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =64 \n    primals_2 =64 \n    primals_3 =rand_strided ((1 ,10 ,64 ,64 ),(40960 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "7e060fcb-2053-46d7-ba14-5b3de51c3218",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CrossMapLRN2d', 'SiLU', 'LazyInstanceNorm3d', 'BCEWithLogitsLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lrn1 = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.silu1 = nn.SiLU()\n        self.lrn2 = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.silu2 = nn.SiLU()\n        self.norm1 = nn.LazyInstanceNorm3d()\n        self.norm2 = nn.LazyInstanceNorm3d()\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        # Assuming input is 4D (batch, channels, height, width)\n        x = self.lrn1(x)\n        x = self.silu1(x)\n        x = self.lrn2(x)\n        x = self.silu2(x)\n        \n        # Reshape to 5D for LazyInstanceNorm3d\n        x = x.unsqueeze(2)  # Add a dummy depth dimension\n        x = self.norm1(x)\n        x = self.norm2(x)\n        \n        # Remove the dummy depth dimension\n        x = x.squeeze(2)\n        \n        # Compute loss (assuming target is provided externally)\n        target = torch.zeros_like(x)  # Dummy target for demonstration\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_silu_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .sigmoid (tmp0 )\n    tmp2 =tmp0 *tmp1 \n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_silu_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_silu_0 [grid (triton_poi_fused_silu_0_xnumel )](arg3_1 ,buf0 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "7e4d30aa-2469-4432-89ae-81efa34ee111",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['SoftMarginLoss', 'FractionalMaxPool3d', 'MaxUnpool1d', 'HingeEmbeddingLoss', 'LayerNorm']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))\n        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)\n        self.layer_norm = nn.LayerNorm([8, 8, 8])\n        self.soft_margin_loss = nn.SoftMarginLoss()\n        self.hinge_embedding_loss = nn.HingeEmbeddingLoss()\n\n    def forward(self, x):\n        # Assuming input is 5D (batch, channels, depth, height, width)\n        x = self.fractional_max_pool3d(x)\n        \n        # Reshape for MaxUnpool1d (assuming we unpool along the depth dimension)\n        batch, channels, depth, height, width = x.shape\n        x = x.view(batch, channels * height * width, depth)\n        x, indices = F.max_pool1d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool1d(x, indices)\n        \n        # Reshape back to original 5D shape\n        x = x.view(batch, channels, depth, height, width)\n        \n        # Apply LayerNorm\n        x = self.layer_norm(x)\n        \n        # Compute losses (assuming we have a target tensor for demonstration)\n        target = torch.ones_like(x)\n        soft_margin_loss = self.soft_margin_loss(x, target)\n        hinge_embedding_loss = self.hinge_embedding_loss(x, target)\n        \n        # Return the losses as a tuple (for demonstration purposes)\n        return soft_margin_loss, hinge_embedding_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 16, 16, 16).cuda()  # Example input: (batch, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =9 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1536 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_2 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =768 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp7 =tl .full ([1 ],2 ,tl .int32 )\n    tmp8 =tl .where ((tmp5 <0 )!=(tmp7 <0 ),tl .where (tmp5 %tmp7 !=0 ,tmp5 //tmp7 -1 ,tmp5 //tmp7 ),tmp5 //tmp7 )\n    tmp9 =tmp8 *tmp7 \n    tmp10 =tmp5 -tmp9 \n    tmp11 =tl .full ([1 ],0 ,tl .int64 )\n    tmp12 =tmp11 +tmp8 \n    tmp13 =2 *((x0 %4 ))\n    tmp14 =tmp13 +tmp10 \n    tmp15 =tl .full ([1 ],8 ,tl .int64 )\n    tmp16 =tmp12 *tmp15 \n    tmp17 =tmp16 +tmp14 \n    tmp18 =8 *(x0 //4 )\n    tmp19 =tmp17 +tmp18 \n    tmp20 =tl .full ([XBLOCK ],1536 ,tl .int32 )\n    tmp21 =tmp19 +tmp20 \n    tmp22 =tmp19 <0 \n    tmp23 =tl .where (tmp22 ,tmp21 ,tmp19 )\n    tl .device_assert (((0 <=tmp23 )&(tmp23 <1536 ))|~(xmask ),\"index out of bounds: 0 <= tmp23 < 1536\")\n    tl .store (out_ptr0 +(tl .broadcast_to (tmp23 ,[XBLOCK ])),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_native_layer_norm_3 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =512 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +512 *x0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[R0_BLOCK ])\n    tmp3 =tl .broadcast_to (tmp1 ,[R0_BLOCK ])\n    tmp5 =triton_helpers .promote_to_tensor (tl .sum (tmp3 ,0 ))\n    tmp6 =tl .full ([1 ],512 ,tl .int32 )\n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp5 /tmp7 \n    tmp9 =tmp1 -tmp8 \n    tmp10 =tmp9 *tmp9 \n    tmp11 =tl .broadcast_to (tmp10 ,[R0_BLOCK ])\n    tmp13 =triton_helpers .promote_to_tensor (tl .sum (tmp11 ,0 ))\n    tmp14 =512.0 \n    tmp15 =tmp13 /tmp14 \n    tmp16 =1e-05 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =libdevice .rsqrt (tmp17 )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp18 ,None )\n    tl .store (out_ptr0 +(x0 ),tmp8 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_fill_mean_native_layer_norm_ne_soft_margin_loss_soft_margin_loss_backward_sub_where_zeros_like_4 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    r0_numel =1536 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp13 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp25 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        r0_1 =r0_index //512 \n        r0_0 =(r0_index %512 )\n        tmp0 =tl .load (in_ptr0 +(r0_2 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .load (in_ptr1 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tl .load (in_ptr2 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tl .load (in_ptr3 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp7 =tl .load (in_ptr4 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp4 =tmp2 *tmp3 \n        tmp6 =tmp4 *tmp5 \n        tmp8 =tmp6 +tmp7 \n        tmp9 =-tmp8 \n        tmp10 =tl_math .exp (tmp9 )\n        tmp11 =libdevice .log1p (tmp10 )\n        tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n        tmp14 =_tmp13 +tmp12 \n        _tmp13 =tl .where (r0_mask ,tmp14 ,_tmp13 )\n        tmp15 =1.0 \n        tmp16 =tmp15 -tmp8 \n        tmp17 =0.0 \n        tmp18 =triton_helpers .maximum (tmp16 ,tmp17 )\n        tmp19 =tl .full ([1 ,1 ],False ,tl .int1 )\n        tmp20 =tl .where (tmp19 ,tmp18 ,tmp17 )\n        tmp21 =tl .full ([1 ,1 ],True ,tl .int1 )\n        tmp22 =tl .where (tmp21 ,tmp8 ,tmp17 )\n        tmp23 =tmp20 +tmp22 \n        tmp24 =tl .broadcast_to (tmp23 ,[XBLOCK ,R0_BLOCK ])\n        tmp26 =_tmp25 +tmp24 \n        _tmp25 =tl .where (r0_mask ,tmp26 ,_tmp25 )\n    tmp13 =tl .sum (_tmp13 ,1 )[:,None ]\n    tmp25 =tl .sum (_tmp25 ,1 )[:,None ]\n    tmp27 =1536.0 \n    tmp28 =tmp13 /tmp27 \n    tmp29 =tmp25 /tmp27 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp28 ,None )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp29 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,3 ,16 ,16 ,16 ),(12288 ,4096 ,256 ,16 ,1 ))\n    assert_size_stride (primals_2 ,(8 ,8 ,8 ),(64 ,8 ,1 ))\n    assert_size_stride (primals_3 ,(8 ,8 ,8 ),(64 ,8 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,3 ,3 ),(9 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (9 )](buf0 ,buf1 ,0 ,9 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n\n        buf2 =torch .ops .aten .fractional_max_pool3d .default (primals_1 ,[2 ,2 ,2 ],[8 ,8 ,8 ],buf1 )\n        del buf1 \n        del primals_1 \n        buf3 =buf2 [0 ]\n        del buf2 \n        buf5 =empty_strided_cuda ((1536 ,),(1 ,),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_1 [grid (1536 )](buf5 ,1536 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_2 [grid (768 )](buf3 ,buf5 ,768 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        buf7 =empty_strided_cuda ((1 ,3 ,1 ,1 ,1 ),(3 ,1 ,1 ,1 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,3 ,1 ,1 ,1 ),(3 ,1 ,3 ,3 ,3 ),torch .float32 )\n        buf10 =reinterpret_tensor (buf8 ,(1 ,3 ,1 ,1 ,1 ),(3 ,1 ,1 ,1 ,1 ),0 );del buf8 \n\n        get_raw_stream (0 )\n        triton_per_fused_native_layer_norm_3 [grid (3 )](buf10 ,buf5 ,buf7 ,3 ,512 ,num_warps =4 ,num_stages =1 )\n        buf12 =empty_strided_cuda ((),(),torch .float32 )\n        buf13 =empty_strided_cuda ((),(),torch .float32 )\n        buf14 =buf12 ;del buf12 \n        buf15 =buf13 ;del buf13 \n\n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_fill_mean_native_layer_norm_ne_soft_margin_loss_soft_margin_loss_backward_sub_where_zeros_like_4 [grid (1 )](buf14 ,buf15 ,buf5 ,buf7 ,buf10 ,primals_2 ,primals_3 ,1 ,1536 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n    return (buf14 ,buf15 ,primals_2 ,primals_3 ,buf5 ,buf7 ,buf10 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,3 ,16 ,16 ,16 ),(12288 ,4096 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((8 ,8 ,8 ),(64 ,8 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((8 ,8 ,8 ),(64 ,8 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "7f8208ab-4993-485f-9f40-0e8badf26294",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AlphaDropout', 'AdaptiveMaxPool2d', 'LSTM']\nimport torch\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.alpha_dropout = nn.AlphaDropout(p=0.5)\n        self.adaptive_max_pool = nn.AdaptiveMaxPool2d((5, 5))\n        self.lstm = nn.LSTM(input_size=25, hidden_size=128, num_layers=2, batch_first=True)\n        self.fc = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # Apply AlphaDropout\n        x = self.alpha_dropout(x)\n        \n        # Apply AdaptiveMaxPool2d to reduce spatial dimensions\n        x = self.adaptive_max_pool(x)\n        \n        # Reshape for LSTM input\n        batch_size = x.size(0)\n        x = x.view(batch_size, -1, 25)  # Reshape to (batch_size, sequence_length, input_size)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Take the output of the last time step\n        x = x[:, -1, :]\n        \n        # Apply a fully connected layer\n        x = self.fc(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_mul_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    tmp3 =tl .load (in_ptr1 +(x0 ),None )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp4 =0.5 \n    tmp5 =tmp2 <tmp4 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =0.8864048946659319 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =tmp3 *tmp8 \n    tmp10 =-1.0 \n    tmp11 =tmp6 +tmp10 \n    tmp12 =1.558387861036063 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.7791939305180315 \n    tmp15 =tmp13 +tmp14 \n    tmp16 =tmp9 +tmp15 \n    tl .store (in_out_ptr0 +(x0 ),tmp16 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,64 ,64 ),(4096 *s0 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,64 ,64 ),(4096 *s0 ,4096 ,64 ,1 ),torch .float32 )\n        buf2 =buf1 ;del buf1 \n\n        triton_poi_fused__to_copy_add_bernoulli_mul_0_xnumel =4096 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_mul_0 [grid (triton_poi_fused__to_copy_add_bernoulli_mul_0_xnumel )](buf2 ,buf0 ,arg3_1 ,0 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf0 \n\n        buf3 =torch .ops .aten .adaptive_max_pool2d .default (buf2 ,[5 ,5 ])\n        del buf2 \n        buf4 =buf3 [0 ]\n        del buf3 \n    return (reinterpret_tensor (buf4 ,(1 ,s0 ,25 ),(25 *s0 ,25 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "811e3774-4f3f-4c70-bd5b-7619773eb375",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['KLDivLoss', 'Softmax', 'Hardtanh']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softmax = nn.Softmax(dim=1)\n        self.hardtanh1 = nn.Hardtanh(min_val=-1.0, max_val=1.0)\n        self.hardtanh2 = nn.Hardtanh(min_val=-0.5, max_val=0.5)\n        self.hardtanh3 = nn.Hardtanh(min_val=-0.2, max_val=0.2)\n        self.kldivloss = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, x):\n        # Apply Hardtanh layers\n        x = self.hardtanh1(x)\n        x = self.hardtanh2(x)\n        x = self.hardtanh3(x)\n        \n        # Reshape the input to match the expected shape for Softmax\n        x = x.view(x.size(0), -1)  # Flatten the input\n        \n        # Apply Softmax\n        x = self.softmax(x)\n        \n        # Generate a target distribution for KLDivLoss\n        target = torch.ones_like(x) / x.size(1)  # Uniform distribution\n        \n        # Compute KLDivLoss\n        loss = self.kldivloss(x.log(), target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_div_log_mul_ones_like_sub_sum_xlogy_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp14 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =-1.0 \n        tmp2 =triton_helpers .maximum (tmp0 ,tmp1 )\n        tmp3 =1.0 \n        tmp4 =triton_helpers .minimum (tmp2 ,tmp3 )\n        tmp5 =-0.5 \n        tmp6 =triton_helpers .maximum (tmp4 ,tmp5 )\n        tmp7 =0.5 \n        tmp8 =triton_helpers .minimum (tmp6 ,tmp7 )\n        tmp9 =-0.2 \n        tmp10 =triton_helpers .maximum (tmp8 ,tmp9 )\n        tmp11 =0.2 \n        tmp12 =triton_helpers .minimum (tmp10 ,tmp11 )\n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =triton_helpers .maximum (_tmp14 ,tmp13 )\n        _tmp14 =tl .where (r0_mask ,tmp15 ,_tmp14 )\n    tmp14 =triton_helpers .max2 (_tmp14 ,1 )[:,None ]\n    _tmp32 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp16 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp17 =-1.0 \n        tmp18 =triton_helpers .maximum (tmp16 ,tmp17 )\n        tmp19 =1.0 \n        tmp20 =triton_helpers .minimum (tmp18 ,tmp19 )\n        tmp21 =-0.5 \n        tmp22 =triton_helpers .maximum (tmp20 ,tmp21 )\n        tmp23 =0.5 \n        tmp24 =triton_helpers .minimum (tmp22 ,tmp23 )\n        tmp25 =-0.2 \n        tmp26 =triton_helpers .maximum (tmp24 ,tmp25 )\n        tmp27 =0.2 \n        tmp28 =triton_helpers .minimum (tmp26 ,tmp27 )\n        tmp29 =tmp28 -tmp14 \n        tmp30 =tl_math .exp (tmp29 )\n        tmp31 =tl .broadcast_to (tmp30 ,[XBLOCK ,R0_BLOCK ])\n        tmp33 =_tmp32 +tmp31 \n        _tmp32 =tl .where (r0_mask ,tmp33 ,_tmp32 )\n    tmp32 =tl .sum (_tmp32 ,1 )[:,None ]\n    _tmp65 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp46 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp34 =1.0 \n        tmp35 =ks0 *ks1 *ks2 \n        tmp36 =tmp35 .to (tl .float32 )\n        tmp37 =tmp34 /tmp36 \n        tmp38 =libdevice .isnan (tmp37 ).to (tl .int1 )\n        tmp39 =0.0 \n        tmp40 =tmp37 ==tmp39 \n        tmp41 =tl_math .log (tmp37 )\n        tmp42 =tmp37 *tmp41 \n        tmp43 =tl .where (tmp40 ,tmp39 ,tmp42 )\n        tmp44 =float (\"nan\")\n        tmp45 =tl .where (tmp38 ,tmp44 ,tmp43 )\n        tmp47 =-1.0 \n        tmp48 =triton_helpers .maximum (tmp46 ,tmp47 )\n        tmp49 =triton_helpers .minimum (tmp48 ,tmp34 )\n        tmp50 =-0.5 \n        tmp51 =triton_helpers .maximum (tmp49 ,tmp50 )\n        tmp52 =0.5 \n        tmp53 =triton_helpers .minimum (tmp51 ,tmp52 )\n        tmp54 =-0.2 \n        tmp55 =triton_helpers .maximum (tmp53 ,tmp54 )\n        tmp56 =0.2 \n        tmp57 =triton_helpers .minimum (tmp55 ,tmp56 )\n        tmp58 =tmp57 -tmp14 \n        tmp59 =tl_math .exp (tmp58 )\n        tmp60 =tmp59 /tmp32 \n        tmp61 =tl_math .log (tmp60 )\n        tmp62 =tmp37 *tmp61 \n        tmp63 =tmp45 -tmp62 \n        tmp64 =tl .broadcast_to (tmp63 ,[XBLOCK ,R0_BLOCK ])\n        tmp66 =_tmp65 +tmp64 \n        _tmp65 =tl .where (r0_mask ,tmp66 ,_tmp65 )\n    tmp65 =tl .sum (_tmp65 ,1 )[:,None ]\n    tmp67 =1.0 \n    tmp68 =tmp65 *tmp67 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp68 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf2 =reinterpret_tensor (buf0 ,(),(),0 );del buf0 \n        buf3 =buf2 ;del buf2 \n\n        s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_div_log_mul_ones_like_sub_sum_xlogy_0 [grid (1 )](buf3 ,arg3_1 ,3 ,32 ,32 ,1 ,3072 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg3_1 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "814b8ba1-9d17-46cb-b17f-0f4ae4865b65",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReflectionPad1d', 'Mish', 'Flatten', 'Softmax', 'ReplicationPad3d', 'AdaptiveMaxPool3d', 'RNN', 'Container', 'Bilinear', 'AdaptiveAvgPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reflection_pad1d = nn.ReflectionPad1d(2)\n        self.mish = nn.Mish()\n        self.flatten = nn.Flatten()\n        self.softmax = nn.Softmax(dim=1)\n        self.replication_pad3d = nn.ReplicationPad3d(1)\n        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((8, 8, 8))\n        self.rnn = nn.RNN(input_size=8, hidden_size=16, num_layers=2, batch_first=True)\n        self.container = nn.Sequential(\n            nn.Linear(16, 32),\n            nn.ReLU(),\n            nn.Linear(32, 64)\n        )\n        self.bilinear = nn.Bilinear(64, 64, 128)\n        self.adaptive_avg_pool2d = nn.AdaptiveAvgPool2d((4, 4))\n\n    def forward(self, x):\n        # Assuming input is 1D, pad it\n        x = self.reflection_pad1d(x)\n        x = self.mish(x)\n        \n        # Flatten the input\n        x = self.flatten(x)\n        \n        # Apply softmax\n        x = self.softmax(x)\n        \n        # Reshape for 3D operations\n        x = x.view(-1, 1, 1, x.size(1))\n        x = self.replication_pad3d(x)\n        x = self.adaptive_max_pool3d(x)\n        \n        # Reshape for RNN\n        x = x.view(x.size(0), -1, 8)\n        x, _ = self.rnn(x)\n        \n        # Pass through container\n        x = self.container(x[:, -1, :])\n        \n        # Apply bilinear transformation\n        x = self.bilinear(x, x)\n        \n        # Reshape for 2D pooling\n        x = x.view(x.size(0), 1, 16, 8)\n        x = self.adaptive_avg_pool2d(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_mish_reflection_pad1d_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =14 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(9 +((-1 )*tl_math .abs ((-9 )+tl_math .abs ((-2 )+r0_0 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp1 =20.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =tl_math .exp (tmp0 )\n    tmp4 =libdevice .log1p (tmp3 )\n    tmp5 =tl .where (tmp2 ,tmp0 ,tmp4 )\n    tmp6 =libdevice .tanh (tmp5 )\n    tmp7 =tmp0 *tmp6 \n    tmp8 =tl .broadcast_to (tmp7 ,[XBLOCK ,R0_BLOCK ])\n    tmp10 =tl .where (r0_mask ,tmp8 ,float (\"-inf\"))\n    tmp11 =triton_helpers .max2 (tmp10 ,1 )[:,None ]\n    tmp12 =tmp7 -tmp11 \n    tmp13 =tl_math .exp (tmp12 )\n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tmp16 =tl .where (r0_mask ,tmp14 ,0 )\n    tmp17 =tl .sum (tmp16 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp11 ,None )\n    tl .store (out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp17 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_replication_pad3d_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =144 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %16 )\n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(9 +((-1 )*tl_math .abs ((-9 )+tl_math .abs ((-2 )+((13 )*((13 )<=(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))<(13 ))))))),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr1 +(0 ))\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ])\n    tmp12 =tl .load (in_ptr2 +(0 ))\n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ])\n    tmp1 =20.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =tl_math .exp (tmp0 )\n    tmp4 =libdevice .log1p (tmp3 )\n    tmp5 =tl .where (tmp2 ,tmp0 ,tmp4 )\n    tmp6 =libdevice .tanh (tmp5 )\n    tmp7 =tmp0 *tmp6 \n    tmp10 =tmp7 -tmp9 \n    tmp11 =tl_math .exp (tmp10 )\n    tmp14 =tmp11 /tmp13 \n    tl .store (out_ptr0 +(x2 ),tmp14 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,10 ),(10 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_mish_reflection_pad1d_0 [grid (1 )](arg0_1 ,buf0 ,buf1 ,1 ,14 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,3 ,3 ,16 ),(144 ,48 ,16 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_replication_pad3d_1 [grid (144 )](arg0_1 ,buf0 ,buf1 ,buf2 ,144 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n        del buf0 \n        del buf1 \n\n        buf3 =torch .ops .aten .adaptive_max_pool3d .default (buf2 ,[8 ,8 ,8 ])\n        del buf2 \n        buf4 =buf3 [0 ]\n        del buf3 \n    return (reinterpret_tensor (buf4 ,(1 ,64 ,8 ),(512 ,8 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "8182cdb7-809e-4eba-8d0b-e90b5c85d8b4",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CircularPad3d', 'Tanhshrink', 'CosineEmbeddingLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.circular_pad1 = nn.CircularPad3d(1)\n        self.circular_pad2 = nn.CircularPad3d(2)\n        self.tanhshrink = nn.Tanhshrink()\n        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()\n\n    def forward(self, x):\n        # Apply circular padding\n        x = self.circular_pad1(x)\n        x = self.circular_pad2(x)\n        \n        # Apply Tanhshrink activation\n        x = self.tanhshrink(x)\n        \n        # Reshape the tensor to match the expected input shape for CosineEmbeddingLoss\n        # Assuming the input is 5D (batch, channel, depth, height, width)\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size, -1)  # Flatten the tensor\n        \n        # Create a dummy target tensor for CosineEmbeddingLoss\n        target = torch.ones(batch_size, x.size(1)).to(x.device)\n        \n        # Compute the CosineEmbeddingLoss\n        loss = self.cosine_embedding_loss(x, target, torch.ones(batch_size).to(x.device))\n        \n        # Return the loss as the output (this is unusual but fits the requirement)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 10, 10, 10).cuda()  # Example input: 5D tensor (batch, channel, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =5184 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %12 )\n    x1 =((xindex //12 )%12 )\n    x2 =((xindex //144 )%12 )\n    x3 =xindex //1728 \n    x5 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =10 +x0 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .full ([1 ],11 ,tl .int64 )\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =x1 \n    tmp11 =tl .full ([1 ],1 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .full ([1 ],11 ,tl .int64 )\n    tmp14 =tmp10 <tmp13 \n    tmp15 =tmp12 &tmp14 \n    tmp16 =tmp15 &tmp9 \n    tmp17 =x2 \n    tmp18 =tl .full ([1 ],1 ,tl .int64 )\n    tmp19 =tmp17 >=tmp18 \n    tmp20 =tl .full ([1 ],11 ,tl .int64 )\n    tmp21 =tmp17 <tmp20 \n    tmp22 =tmp19 &tmp21 \n    tmp23 =tmp22 &tmp16 \n    tmp24 =tl .load (in_ptr0 +((-101 )+x0 +10 *x1 +100 *x2 +1000 *x3 ),tmp23 &xmask ,other =0.0 )\n    tmp25 =tl .load (in_ptr1 +(10 +x5 ),tmp16 &xmask ,other =0.0 )\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tl .full (tmp26 .shape ,0.0 ,tmp26 .dtype )\n    tmp28 =tl .where (tmp16 ,tmp26 ,tmp27 )\n    tmp29 =tl .load (in_ptr1 +(10 +x5 ),tmp9 &xmask ,other =0.0 )\n    tmp30 =tl .where (tmp15 ,tmp28 ,tmp29 )\n    tmp31 =tl .full (tmp30 .shape ,0.0 ,tmp30 .dtype )\n    tmp32 =tl .where (tmp9 ,tmp30 ,tmp31 )\n    tmp33 =float (\"nan\")\n    tmp34 =tl .where (tmp8 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp2 ,tmp34 ,tmp35 )\n    tmp37 =tmp0 >=tmp1 \n    tmp38 =tl .full ([1 ],11 ,tl .int64 )\n    tmp39 =tmp0 <tmp38 \n    tmp40 =tmp37 &tmp39 \n    tmp41 =x1 \n    tmp42 =tl .full ([1 ],1 ,tl .int64 )\n    tmp43 =tmp41 >=tmp42 \n    tmp44 =tl .full ([1 ],11 ,tl .int64 )\n    tmp45 =tmp41 <tmp44 \n    tmp46 =tmp43 &tmp45 \n    tmp47 =tmp46 &tmp40 \n    tmp48 =x2 \n    tmp49 =tl .full ([1 ],1 ,tl .int64 )\n    tmp50 =tmp48 >=tmp49 \n    tmp51 =tl .full ([1 ],11 ,tl .int64 )\n    tmp52 =tmp48 <tmp51 \n    tmp53 =tmp50 &tmp52 \n    tmp54 =tmp53 &tmp47 \n    tmp55 =tl .load (in_ptr0 +((-111 )+x0 +10 *x1 +100 *x2 +1000 *x3 ),tmp54 &xmask ,other =0.0 )\n    tmp56 =tl .load (in_ptr1 +(x5 ),tmp47 &xmask ,other =0.0 )\n    tmp57 =tl .where (tmp53 ,tmp55 ,tmp56 )\n    tmp58 =tl .full (tmp57 .shape ,0.0 ,tmp57 .dtype )\n    tmp59 =tl .where (tmp47 ,tmp57 ,tmp58 )\n    tmp60 =tl .load (in_ptr1 +(x5 ),tmp40 &xmask ,other =0.0 )\n    tmp61 =tl .where (tmp46 ,tmp59 ,tmp60 )\n    tmp62 =tl .full (tmp61 .shape ,0.0 ,tmp61 .dtype )\n    tmp63 =tl .where (tmp40 ,tmp61 ,tmp62 )\n    tmp64 =float (\"nan\")\n    tmp65 =tl .where (tmp40 ,tmp63 ,tmp64 )\n    tmp66 =tl .where (tmp2 ,tmp36 ,tmp65 )\n    tl .store (out_ptr0 +(x5 ),tmp66 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =5184 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //12 )%12 )\n    x0 =(xindex %12 )\n    x3 =xindex //12 \n    x4 =xindex \n    tmp40 =tl .load (in_ptr0 +(x4 ),xmask )\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],11 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(-10 )+x1 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .full ([1 ],11 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(1 +12 *x3 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x4 ),tmp6 &xmask ,other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =x0 \n    tmp17 =tl .full ([1 ],11 ,tl .int64 )\n    tmp18 =tmp16 >=tmp17 \n    tmp19 =tmp18 &tmp2 \n    tmp20 =tl .load (in_ptr0 +((-119 )+12 *x3 ),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr0 +((-120 )+x4 ),tmp2 &xmask ,other =0.0 )\n    tmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\n    tmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],1 ,tl .int64 )\n    tmp27 =tmp0 <tmp26 \n    tmp28 =x0 \n    tmp29 =tl .full ([1 ],11 ,tl .int64 )\n    tmp30 =tmp28 >=tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(121 +12 *x3 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(120 +x4 ),tmp27 &xmask ,other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =x0 \n    tmp38 =tmp37 >=tmp1 \n    tmp39 =tl .load (in_ptr0 +(1 +12 *x3 ),tmp38 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp41 =tl .where (tmp38 ,tmp39 ,tmp40 )\n    tmp42 =tl .where (tmp27 ,tmp36 ,tmp41 )\n    tmp43 =tl .where (tmp2 ,tmp25 ,tmp42 )\n    tl .store (out_ptr0 +(x4 ),tmp43 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =9216 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //12 )%16 )\n    x2 =((xindex //192 )%16 )\n    x3 =xindex //3072 \n    x4 =(xindex %192 )\n    x0 =(xindex %12 )\n    x5 =xindex //12 \n    x6 =xindex \n    tmp39 =tl .load (in_ptr1 +(2 +x0 +16 *x5 ),xmask )\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],2 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],14 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =x2 \n    tmp7 =tl .full ([1 ],2 ,tl .int64 )\n    tmp8 =tmp6 >=tmp7 \n    tmp9 =tl .full ([1 ],14 ,tl .int64 )\n    tmp10 =tmp6 <tmp9 \n    tmp11 =tmp8 &tmp10 \n    tmp12 =tmp11 &tmp5 \n    tmp13 =(-2 )+x2 \n    tmp14 =tl .full ([1 ],11 ,tl .int64 )\n    tmp15 =tmp13 >=tmp14 \n    tmp16 =tmp15 &tmp12 \n    tmp17 =(-12 )+x2 \n    tmp18 =tl .full ([1 ],1 ,tl .int64 )\n    tmp19 =tmp17 <tmp18 \n    tmp20 =tmp19 &tmp16 \n    tmp21 =tl .load (in_ptr0 +(1416 +x4 +1728 *x3 ),tmp20 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tl .load (in_ptr0 +((-1752 )+x4 +144 *x2 +1728 *x3 ),tmp16 &xmask ,other =0.0 )\n    tmp23 =tl .where (tmp19 ,tmp21 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp16 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],1 ,tl .int64 )\n    tmp27 =tmp13 <tmp26 \n    tmp28 =tmp27 &tmp12 \n    tmp29 =tl .load (in_ptr0 +(1416 +x4 +1728 *x3 ),tmp28 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp30 =tl .load (in_ptr0 +((-312 )+x4 +144 *x2 +1728 *x3 ),tmp12 &xmask ,other =0.0 )\n    tmp31 =tl .where (tmp27 ,tmp29 ,tmp30 )\n    tmp32 =tl .where (tmp15 ,tmp25 ,tmp31 )\n    tmp33 =tl .full (tmp32 .shape ,0.0 ,tmp32 .dtype )\n    tmp34 =tl .where (tmp12 ,tmp32 ,tmp33 )\n    tmp35 =tl .load (in_ptr1 +(2 +x0 +16 *x5 ),tmp5 &xmask ,other =0.0 )\n    tmp36 =tl .where (tmp11 ,tmp34 ,tmp35 )\n    tmp37 =tl .full (tmp36 .shape ,0.0 ,tmp36 .dtype )\n    tmp38 =tl .where (tmp5 ,tmp36 ,tmp37 )\n    tmp40 =tl .where (tmp5 ,tmp38 ,tmp39 )\n    tl .store (out_ptr0 +(x6 ),tmp40 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =(xindex %16 )\n    x1 =xindex //16 \n    x2 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],14 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(-12 )+x0 \n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .full ([1 ],2 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tl .full ([1 ],14 ,tl .int64 )\n    tmp11 =tmp7 <tmp10 \n    tmp12 =tmp9 &tmp11 \n    tmp13 =tmp12 &tmp6 \n    tmp14 =tl .load (in_ptr0 +((-2 )+x0 +12 *x1 ),tmp13 ,other =0.0 )\n    tmp15 =float (\"nan\")\n    tmp16 =tl .where (tmp12 ,tmp14 ,tmp15 )\n    tmp17 =tl .full (tmp16 .shape ,0.0 ,tmp16 .dtype )\n    tmp18 =tl .where (tmp6 ,tmp16 ,tmp17 )\n    tmp19 =tmp3 >=tmp4 \n    tmp20 =tl .full ([1 ],14 ,tl .int64 )\n    tmp21 =tmp3 <tmp20 \n    tmp22 =tmp19 &tmp21 \n    tmp23 =tmp22 &tmp2 \n    tmp24 =tl .load (in_ptr0 +((-14 )+x0 +12 *x1 ),tmp23 ,other =0.0 )\n    tmp25 =float (\"nan\")\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tl .where (tmp5 ,tmp18 ,tmp26 )\n    tmp28 =tl .full (tmp27 .shape ,0.0 ,tmp27 .dtype )\n    tmp29 =tl .where (tmp2 ,tmp27 ,tmp28 )\n    tmp30 =tl .full ([1 ],2 ,tl .int64 )\n    tmp31 =tmp0 <tmp30 \n    tmp32 =12 +x0 \n    tmp33 =tl .full ([1 ],2 ,tl .int64 )\n    tmp34 =tmp32 >=tmp33 \n    tmp35 =tl .full ([1 ],14 ,tl .int64 )\n    tmp36 =tmp32 <tmp35 \n    tmp37 =tmp34 &tmp36 \n    tmp38 =tmp37 &tmp31 \n    tmp39 =tl .load (in_ptr0 +(10 +x0 +12 *x1 ),tmp38 ,other =0.0 )\n    tmp40 =float (\"nan\")\n    tmp41 =tl .where (tmp37 ,tmp39 ,tmp40 )\n    tmp42 =tl .full (tmp41 .shape ,0.0 ,tmp41 .dtype )\n    tmp43 =tl .where (tmp31 ,tmp41 ,tmp42 )\n    tmp44 =tmp0 >=tmp30 \n    tmp45 =tmp0 <tmp1 \n    tmp46 =tmp44 &tmp45 \n    tmp47 =tl .load (in_ptr0 +((-2 )+x0 +12 *x1 ),tmp46 ,other =0.0 )\n    tmp48 =float (\"nan\")\n    tmp49 =tl .where (tmp46 ,tmp47 ,tmp48 )\n    tmp50 =tl .where (tmp31 ,tmp43 ,tmp49 )\n    tmp51 =tl .where (tmp2 ,tmp29 ,tmp50 )\n    tl .store (out_ptr0 +(x2 ),tmp51 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_4 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =((xindex //256 )%16 )\n    x1 =((xindex //16 )%16 )\n    x5 =xindex \n    tmp39 =tl .load (in_ptr0 +(x5 ),None )\n    tmp0 =x2 \n    tmp1 =tl .full ([1 ],2 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =x1 \n    tmp4 =tl .full ([1 ],14 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =(-12 )+x1 \n    tmp8 =tl .full ([1 ],2 ,tl .int64 )\n    tmp9 =tmp7 <tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(3072 +x5 ),tmp10 ,other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(2880 +x5 ),tmp6 ,other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =tl .full ([1 ],2 ,tl .int64 )\n    tmp17 =tmp3 <tmp16 \n    tmp18 =tmp17 &tmp2 \n    tmp19 =tl .load (in_ptr0 +(3264 +x5 ),tmp18 ,other =0.0 )\n    tmp20 =tl .load (in_ptr0 +(3072 +x5 ),tmp2 ,other =0.0 )\n    tmp21 =tl .where (tmp17 ,tmp19 ,tmp20 )\n    tmp22 =tl .where (tmp5 ,tmp15 ,tmp21 )\n    tmp23 =tl .full (tmp22 .shape ,0.0 ,tmp22 .dtype )\n    tmp24 =tl .where (tmp2 ,tmp22 ,tmp23 )\n    tmp25 =x1 \n    tmp26 =tl .full ([1 ],14 ,tl .int64 )\n    tmp27 =tmp25 >=tmp26 \n    tmp28 =(-12 )+x1 \n    tmp29 =tl .full ([1 ],2 ,tl .int64 )\n    tmp30 =tmp28 <tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(x5 ),tmp31 ,other =0.0 )\n    tmp33 =tl .load (in_ptr0 +((-192 )+x5 ),tmp27 ,other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =tmp25 <tmp1 \n    tmp38 =tl .load (in_ptr0 +(192 +x5 ),tmp37 ,other =0.0 )\n    tmp40 =tl .where (tmp37 ,tmp38 ,tmp39 )\n    tmp41 =tl .where (tmp27 ,tmp36 ,tmp40 )\n    tmp42 =tl .where (tmp2 ,tmp24 ,tmp41 )\n    tl .store (out_ptr0 +(x5 ),tmp42 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__to_copy_mul_ones_sum_5 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =6144 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp15 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp4 =tl .load (in_ptr0 +(r0_1 +6144 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp0 =(((r0_1 +6144 *x0 )//256 )%16 )\n        tmp1 =tl .full ([1 ,1 ],14 ,tl .int64 )\n        tmp2 =tmp0 >=tmp1 \n        tmp3 =tl .load (in_ptr0 +((-3072 )+r0_1 +6144 *x0 ),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n        tmp6 =libdevice .tanh (tmp5 )\n        tmp7 =tmp5 -tmp6 \n        tmp8 =1.0 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =_tmp11 +tmp10 \n        _tmp11 =tl .where (r0_mask &xmask ,tmp12 ,_tmp11 )\n        tmp13 =tmp7 *tmp7 \n        tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n        tmp16 =_tmp15 +tmp14 \n        _tmp15 =tl .where (r0_mask &xmask ,tmp16 ,_tmp15 )\n    tmp11 =tl .sum (_tmp11 ,1 )[:,None ]\n    tmp15 =tl .sum (_tmp15 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp11 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_mul_sum_6 (out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =6144 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    x0 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        tmp0 =1.0 \n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =_tmp2 +tmp1 \n        _tmp2 =tl .where (r0_mask &xmask ,tmp3 ,_tmp2 )\n    tmp2 =tl .sum (_tmp2 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__to_copy_add_clamp_min_div_eq_fill_mean_mul_ones_sqrt_sub_sum_where_zeros_like_7 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp4 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp8 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .sum (tmp5 ,1 )[:,None ]\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n    tmp11 =tl .sum (tmp9 ,1 )[:,None ]\n    tmp12 =9.999999960041972e-13 \n    tmp13 =tmp7 +tmp12 \n    tmp14 =tmp11 +tmp12 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =libdevice .sqrt (tmp15 )\n    tmp17 =tmp3 /tmp16 \n    tmp18 =1.0 \n    tmp19 =tmp18 -tmp17 \n    tmp20 =tl .full ([1 ,1 ],True ,tl .int1 )\n    tmp21 =0.0 \n    tmp22 =tl .where (tmp20 ,tmp19 ,tmp21 )\n    tmp23 =tmp17 -tmp21 \n    tmp24 =triton_helpers .maximum (tmp23 ,tmp21 )\n    tmp25 =tl .full ([1 ,1 ],False ,tl .int1 )\n    tmp26 =tl .where (tmp25 ,tmp24 ,tmp21 )\n    tmp27 =tmp22 +tmp26 \n    tmp28 =tmp27 /tmp18 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp28 ,None )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,3 ,10 ,10 ,10 ),(3000 ,1000 ,100 ,10 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,12 ,12 ,12 ),(5184 ,1728 ,144 ,12 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,3 ,12 ,12 ,12 ),(5184 ,1728 ,144 ,12 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (5184 )](arg0_1 ,buf0 ,buf1 ,5184 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n        buf2 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_1 [grid (5184 )](buf1 ,buf2 ,5184 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        buf3 =empty_strided_cuda ((1 ,3 ,16 ,16 ,16 ),(12288 ,4096 ,256 ,16 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,3 ,16 ,16 ,12 ),(9216 ,3072 ,192 ,12 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_copy_2 [grid (9216 )](buf2 ,buf3 ,buf4 ,9216 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        buf5 =buf3 ;del buf3 \n\n        get_raw_stream (0 )\n        triton_poi_fused_3 [grid (12288 )](buf4 ,buf5 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n        buf6 =empty_strided_cuda ((1 ,3 ,16 ,16 ,16 ),(12288 ,4096 ,256 ,16 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_4 [grid (12288 )](buf5 ,buf6 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf5 \n        buf7 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n        buf9 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__to_copy_mul_ones_sum_5 [grid (2 )](buf6 ,buf7 ,buf9 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf6 \n        buf11 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused_mul_sum_6 [grid (2 )](buf11 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf8 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf13 =reinterpret_tensor (buf8 ,(),(),0 );del buf8 \n\n        get_raw_stream (0 )\n        triton_per_fused__to_copy_add_clamp_min_div_eq_fill_mean_mul_ones_sqrt_sub_sum_where_zeros_like_7 [grid (1 )](buf13 ,buf7 ,buf9 ,buf11 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf11 \n        del buf7 \n        del buf9 \n    return (buf13 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,3 ,10 ,10 ,10 ),(3000 ,1000 ,100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "81de5cb7-31ec-4116-858e-68d9d749d9c6",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad3d', 'FractionalMaxPool3d', 'Hardsigmoid', 'FeatureAlphaDropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ConstantPad3d(padding=1, value=0.5)\n        self.pool1 = nn.FractionalMaxPool3d(kernel_size=2, output_size=(10, 10, 10))\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.dropout1 = nn.FeatureAlphaDropout(p=0.5)\n        self.pad2 = nn.ConstantPad3d(padding=2, value=0.25)\n        self.pool2 = nn.FractionalMaxPool3d(kernel_size=3, output_size=(5, 5, 5))\n        self.dropout2 = nn.FeatureAlphaDropout(p=0.3)\n\n    def forward(self, x):\n        x = self.pad1(x)\n        x = self.pool1(x)\n        x = self.hardsigmoid(x)\n        x = self.dropout1(x)\n        x = self.pad2(x)\n        x = self.pool2(x)\n        x = self.hardsigmoid(x)\n        x = self.dropout2(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 20, 20, 20).cuda()  # Example input shape for 3D operations\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =9 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =31944 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =((xindex //484 )%22 )\n    x1 =((xindex //22 )%22 )\n    x0 =(xindex %22 )\n    x3 =xindex //10648 \n    x7 =(xindex %10648 )\n    tmp0 =(-1 )+x2 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],20 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =(-1 )+x0 \n    tmp9 =tmp8 >=tmp1 \n    tmp10 =tmp8 <tmp3 \n    tmp11 =tmp2 &tmp4 \n    tmp12 =tmp11 &tmp6 \n    tmp13 =tmp12 &tmp7 \n    tmp14 =tmp13 &tmp9 \n    tmp15 =tmp14 &tmp10 \n    tmp16 =tl .load (in_ptr0 +((-421 )+x0 +20 *x1 +400 *x2 +8000 *x3 ),tmp15 &xmask ,other =0.5 )\n    tl .store (out_ptr0 +(x7 +10656 *x3 ),tmp16 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_constant_pad_nd_hardsigmoid_mul_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =8232 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =((xindex //196 )%14 )\n    x1 =((xindex //14 )%14 )\n    x0 =(xindex %14 )\n    x3 =xindex //2744 \n    x7 =(xindex %2744 )\n    tmp0 =(-2 )+x2 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],10 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =(-2 )+x0 \n    tmp9 =tmp8 >=tmp1 \n    tmp10 =tmp8 <tmp3 \n    tmp11 =tmp2 &tmp4 \n    tmp12 =tmp11 &tmp6 \n    tmp13 =tmp12 &tmp7 \n    tmp14 =tmp13 &tmp9 \n    tmp15 =tmp14 &tmp10 \n    tmp16 =tl .load (in_ptr0 +((-222 )+x0 +10 *x1 +100 *x2 +1000 *x3 ),tmp15 &xmask ,other =0.0 )\n    tmp17 =3.0 \n    tmp18 =tmp16 +tmp17 \n    tmp19 =0.0 \n    tmp20 =triton_helpers .maximum (tmp18 ,tmp19 )\n    tmp21 =6.0 \n    tmp22 =triton_helpers .minimum (tmp20 ,tmp21 )\n    tmp23 =0.16666666666666666 \n    tmp24 =tmp22 *tmp23 \n    tmp25 =tl .load (in_ptr1 +(x3 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp26 =0.5 \n    tmp27 =tmp25 <tmp26 \n    tmp28 =tmp27 .to (tl .float32 )\n    tmp29 =0.8864048946659319 \n    tmp30 =tmp28 *tmp29 \n    tmp31 =tmp24 *tmp30 \n    tmp32 =-1.0 \n    tmp33 =tmp28 +tmp32 \n    tmp34 =1.558387861036063 \n    tmp35 =tmp33 *tmp34 \n    tmp36 =0.7791939305180315 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =tmp31 +tmp37 \n    tmp39 =tl .full (tmp38 .shape ,0.25 ,tmp38 .dtype )\n    tmp40 =tl .where (tmp15 ,tmp38 ,tmp39 )\n    tl .store (out_ptr0 +(x7 +2752 *x3 ),tmp40 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_4 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_hardsigmoid_mul_5 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =375 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //125 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp9 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =3.0 \n    tmp2 =tmp0 +tmp1 \n    tmp3 =0.0 \n    tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n    tmp5 =6.0 \n    tmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\n    tmp7 =0.16666666666666666 \n    tmp8 =tmp6 *tmp7 \n    tmp10 =0.7 \n    tmp11 =tmp9 <tmp10 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =0.8609526162463561 \n    tmp14 =tmp12 *tmp13 \n    tmp15 =tmp8 *tmp14 \n    tmp16 =-1.0 \n    tmp17 =tmp12 +tmp16 \n    tmp18 =1.513640227123543 \n    tmp19 =tmp17 *tmp18 \n    tmp20 =0.4540920681370629 \n    tmp21 =tmp19 +tmp20 \n    tmp22 =tmp15 +tmp21 \n    tl .store (in_out_ptr0 +(x2 ),tmp22 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,3 ,20 ,20 ,20 ),(24000 ,8000 ,400 ,20 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((4 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[4 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,3 ,3 ),(9 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (9 )](buf0 ,buf1 ,2 ,9 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,3 ,22 ,22 ,22 ),(31968 ,10656 ,484 ,22 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_1 [grid (31944 )](arg0_1 ,buf2 ,31944 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n\n        buf3 =torch .ops .aten .fractional_max_pool3d .default (buf2 ,[2 ,2 ,2 ],[10 ,10 ,10 ],buf1 )\n        del buf2 \n        buf4 =buf3 [0 ]\n        del buf3 \n        buf6 =empty_strided_cuda ((1 ,3 ,1 ,1 ,1 ),(3 ,1 ,3 ,3 ,3 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_2 [grid (3 )](buf0 ,buf6 ,1 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf7 =empty_strided_cuda ((1 ,3 ,14 ,14 ,14 ),(8256 ,2752 ,196 ,14 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_constant_pad_nd_hardsigmoid_mul_3 [grid (8232 )](buf4 ,buf6 ,buf7 ,8232 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n        buf8 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (9 )](buf0 ,buf8 ,2 ,9 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n\n        buf9 =torch .ops .aten .fractional_max_pool3d .default (buf7 ,[3 ,3 ,3 ],[5 ,5 ,5 ],buf8 )\n        del buf7 \n        del buf8 \n        buf10 =buf9 [0 ]\n        del buf9 \n        buf12 =buf6 ;del buf6 \n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_4 [grid (3 )](buf0 ,buf12 ,3 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf13 =buf10 ;del buf10 \n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_hardsigmoid_mul_5 [grid (375 )](buf13 ,buf12 ,375 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf12 \n    return (buf13 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,3 ,20 ,20 ,20 ),(24000 ,8000 ,400 ,20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "8215e615-e3a6-43bb-8050-864c07d989a4",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Dropout2d', 'Softplus', 'AdaptiveMaxPool1d', 'Conv1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv1d(1, 10, kernel_size=5)\n        self.dropout2d = nn.Dropout2d(p=0.5)\n        self.adaptive_max_pool1d = nn.AdaptiveMaxPool1d(output_size=10)\n        self.conv2 = nn.Conv1d(10, 20, kernel_size=5)\n        self.softplus = nn.Softplus()\n\n    def forward(self, x):\n        # Assuming input is of shape (batch_size, channels, sequence_length)\n        x = self.conv1(x)  # Shape: (batch_size, 10, sequence_length - 4)\n        x = x.unsqueeze(2)  # Shape: (batch_size, 10, 1, sequence_length - 4)\n        x = self.dropout2d(x)  # Shape: (batch_size, 10, 1, sequence_length - 4)\n        x = x.squeeze(2)  # Shape: (batch_size, 10, sequence_length - 4)\n        x = self.adaptive_max_pool1d(x)  # Shape: (batch_size, 10, 10)\n        x = self.conv2(x)  # Shape: (batch_size, 20, 6)\n        x = self.softplus(x)  # Shape: (batch_size, 20, 6)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 64).cuda()  # Example input shape: (batch_size, channels, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_div_mul_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =600 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //60 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last').to (tl .int1 )\n    tmp2 =tmp0 +tmp1 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =2.0 \n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp2 *tmp6 \n    tl .store (in_out_ptr0 +(x2 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_adaptive_max_pool2d_2 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =100 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(6 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +6 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(2 +6 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr0 +(3 +6 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr0 +(4 +6 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp22 =tl .load (in_ptr0 +(5 +6 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp8 =tmp7 >tmp6 \n    tmp9 =tl .full ([1 ],2 ,tl .int8 )\n    tmp10 =tl .where (tmp8 ,tmp9 ,tmp5 )\n    tmp11 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp13 =tmp12 >tmp11 \n    tmp14 =tl .full ([1 ],3 ,tl .int8 )\n    tmp15 =tl .where (tmp13 ,tmp14 ,tmp10 )\n    tmp16 =triton_helpers .maximum (tmp12 ,tmp11 )\n    tmp18 =tmp17 >tmp16 \n    tmp19 =tl .full ([1 ],4 ,tl .int8 )\n    tmp20 =tl .where (tmp18 ,tmp19 ,tmp15 )\n    tmp21 =triton_helpers .maximum (tmp17 ,tmp16 )\n    tmp23 =tmp22 >tmp21 \n    tmp24 =tl .full ([1 ],5 ,tl .int8 )\n    tmp25 =tl .where (tmp23 ,tmp24 ,tmp20 )\n    tmp26 =triton_helpers .maximum (tmp22 ,tmp21 )\n    tl .store (out_ptr0 +(x0 ),tmp25 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp26 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_softplus_3 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =120 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //6 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =1.0 \n    tmp4 =tmp2 *tmp3 \n    tmp5 =20.0 \n    tmp6 =tmp4 >tmp5 \n    tmp7 =tl_math .exp (tmp4 )\n    tmp8 =libdevice .log1p (tmp7 )\n    tmp9 =tmp8 *tmp3 \n    tmp10 =tl .where (tmp6 ,tmp2 ,tmp9 )\n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n    tl .store (out_ptr0 +(x2 ),tmp10 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(10 ,1 ,5 ),(5 ,5 ,1 ))\n    assert_size_stride (primals_2 ,(10 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,1 ,64 ),(64 ,64 ,1 ))\n    assert_size_stride (primals_4 ,(20 ,10 ,5 ),(50 ,5 ,1 ))\n    assert_size_stride (primals_5 ,(20 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,),padding =(0 ,),dilation =(1 ,),transposed =False ,output_padding =(0 ,),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,10 ,60 ),(600 ,60 ,1 ))\n        buf1 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf1 )\n        buf3 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (10 )](buf1 ,buf3 ,0 ,10 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf1 \n        buf4 =reinterpret_tensor (buf0 ,(1 ,10 ,1 ,60 ),(600 ,60 ,60 ,1 ),0 );del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_div_mul_1 [grid (600 )](buf4 ,primals_2 ,buf3 ,600 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n        buf5 =empty_strided_cuda ((1 ,10 ,1 ,10 ),(100 ,10 ,10 ,1 ),torch .int8 )\n        buf6 =empty_strided_cuda ((1 ,10 ,1 ,10 ),(100 ,10 ,10 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_adaptive_max_pool2d_2 [grid (100 )](buf4 ,buf5 ,buf6 ,100 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n\n        buf7 =extern_kernels .convolution (reinterpret_tensor (buf6 ,(1 ,10 ,10 ),(0 ,10 ,1 ),0 ),primals_4 ,stride =(1 ,),padding =(0 ,),dilation =(1 ,),transposed =False ,output_padding =(0 ,),groups =1 ,bias =None )\n        assert_size_stride (buf7 ,(1 ,20 ,6 ),(120 ,6 ,1 ))\n        buf8 =buf7 ;del buf7 \n        buf9 =empty_strided_cuda ((1 ,20 ,6 ),(120 ,6 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_softplus_3 [grid (120 )](buf8 ,primals_5 ,buf9 ,120 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_5 \n    return (buf9 ,primals_1 ,primals_3 ,primals_4 ,buf3 ,buf4 ,buf5 ,reinterpret_tensor (buf6 ,(1 ,10 ,10 ),(100 ,10 ,1 ),0 ),buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((10 ,1 ,5 ),(5 ,5 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,1 ,64 ),(64 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((20 ,10 ,5 ),(50 ,5 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "821a2975-6f59-43e8-a7ee-d428e1d7e0fe",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PoissonNLLLoss', 'Transformer', 'Dropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.transformer = nn.Transformer(d_model=64, nhead=8, num_encoder_layers=3, num_decoder_layers=3)\n        self.dropout = nn.Dropout(p=0.5)\n        self.loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, seq_len, d_model)\n        # Transformer expects input of shape (seq_len, batch_size, d_model)\n        x = x.permute(1, 0, 2)\n        \n        # Apply transformer\n        x = self.transformer(x, x)\n        \n        # Apply dropout\n        x = self.dropout(x)\n        \n        # Permute back to (batch_size, seq_len, d_model)\n        x = x.permute(1, 0, 2)\n        \n        # Compute PoissonNLLLoss (assuming target is the same as input for simplicity)\n        loss = self.loss(x, x)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(10, 20, 64).cuda()  # (batch_size, seq_len, d_model)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =12800 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %64 )\n    x1 =((xindex //64 )%10 )\n    x2 =xindex //640 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +64 *x2 +1280 *x1 ),xmask )\n    tl .store (out_ptr0 +(x3 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =38400 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %64 )\n    x1 =((xindex //64 )%200 )\n    x2 =xindex //12800 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +64 *x2 +192 *x1 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 +64 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_2 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =12800 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(x4 ),xmask )\n    tl .store (out_ptr0 +(x4 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =12800 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(12800 +x4 ),xmask )\n    tl .store (out_ptr0 +(x4 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_4 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =12800 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(25600 +x4 ),xmask )\n    tl .store (out_ptr0 +(x4 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,in_ptr6 ,in_ptr7 ,in_ptr8 ,in_ptr9 ,out_ptr1 ,out_ptr3 ,out_ptr6 ,out_ptr9 ,out_ptr10 ,out_ptr11 ,out_ptr12 ,out_ptr13 ,load_seed_offset ,load_seed_offset1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =200 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    x2 =(xindex %10 )\n    x3 =xindex //10 \n    tmp8 =tl .load (in_ptr1 +(r0_1 +64 *x3 +1280 *x2 ),xmask ,other =0.0 )\n    tmp10 =tl .load (in_ptr2 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp11 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp40 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp42 =tl .load (in_ptr5 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp45 =tl .load (in_ptr6 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp46 =tl .load (in_ptr7 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp70 =tl .load (in_ptr8 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp72 =tl .load (in_ptr9 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +64 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tl .load (in_ptr0 +load_seed_offset1 )\n    tmp6 =tl .rand (tmp5 ,(tmp1 ).to (tl .uint32 ))\n    tmp7 =tmp6 >tmp3 \n    tmp9 =tmp7 .to (tl .float32 )\n    tmp12 =tmp10 +tmp11 \n    tmp13 =tmp9 *tmp12 \n    tmp14 =1.1111111111111112 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =tmp8 +tmp15 \n    tmp17 =tl .broadcast_to (tmp16 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp17 ,0 )\n    tmp20 =tl .broadcast_to (tmp17 ,[XBLOCK ,R0_BLOCK ])\n    tmp22 =tl .where (xmask ,tmp20 ,0 )\n    tmp23 =tl .sum (tmp22 ,1 )[:,None ]\n    tmp24 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp23 /tmp25 \n    tmp27 =tmp17 -tmp26 \n    tmp28 =tmp27 *tmp27 \n    tmp29 =tl .broadcast_to (tmp28 ,[XBLOCK ,R0_BLOCK ])\n    tmp31 =tl .where (xmask ,tmp29 ,0 )\n    tmp32 =tl .sum (tmp31 ,1 )[:,None ]\n    tmp33 =tmp16 -tmp26 \n    tmp34 =64.0 \n    tmp35 =tmp32 /tmp34 \n    tmp36 =1e-05 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =libdevice .rsqrt (tmp37 )\n    tmp39 =tmp33 *tmp38 \n    tmp41 =tmp39 *tmp40 \n    tmp43 =tmp41 +tmp42 \n    tmp44 =tmp4 .to (tl .float32 )\n    tmp47 =tmp45 +tmp46 \n    tmp48 =tmp44 *tmp47 \n    tmp49 =tmp48 *tmp14 \n    tmp50 =tmp8 +tmp49 \n    tmp51 =tl .broadcast_to (tmp50 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp51 ,0 )\n    tmp54 =tl .broadcast_to (tmp51 ,[XBLOCK ,R0_BLOCK ])\n    tmp56 =tl .where (xmask ,tmp54 ,0 )\n    tmp57 =tl .sum (tmp56 ,1 )[:,None ]\n    tmp58 =tmp57 /tmp25 \n    tmp59 =tmp51 -tmp58 \n    tmp60 =tmp59 *tmp59 \n    tmp61 =tl .broadcast_to (tmp60 ,[XBLOCK ,R0_BLOCK ])\n    tmp63 =tl .where (xmask ,tmp61 ,0 )\n    tmp64 =tl .sum (tmp63 ,1 )[:,None ]\n    tmp65 =tmp50 -tmp58 \n    tmp66 =tmp64 /tmp34 \n    tmp67 =tmp66 +tmp36 \n    tmp68 =libdevice .rsqrt (tmp67 )\n    tmp69 =tmp65 *tmp68 \n    tmp71 =tmp69 *tmp70 \n    tmp73 =tmp71 +tmp72 \n    tmp74 =0.015625 \n    tmp75 =tmp68 *tmp74 \n    tmp76 =tmp38 *tmp74 \n    tl .store (out_ptr1 +(r0_1 +64 *x0 ),tmp4 ,xmask )\n    tl .store (out_ptr3 +(r0_1 +64 *x0 ),tmp7 ,xmask )\n    tl .store (out_ptr6 +(r0_1 +64 *x0 ),tmp43 ,xmask )\n    tl .store (out_ptr9 +(r0_1 +64 *x0 ),tmp73 ,xmask )\n    tl .store (out_ptr10 +(r0_1 +64 *x3 +1280 *x2 ),tmp69 ,xmask )\n    tl .store (out_ptr11 +(r0_1 +64 *x3 +1280 *x2 ),tmp39 ,xmask )\n    tl .store (out_ptr12 +(x0 ),tmp75 ,xmask )\n    tl .store (out_ptr13 +(x0 ),tmp76 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_relu_threshold_backward_6 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =xindex \n    x0 =(xindex %2048 )\n    tmp0 =tl .load (in_ptr0 +(x2 ),None )\n    tmp1 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp5 =0.0 \n    tmp6 =tmp4 <=tmp5 \n    tl .store (out_ptr0 +(x2 ),tmp6 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_7 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    x1 =(xindex %2048 )\n    tmp6 =tl .load (in_out_ptr0 +(x0 ),None )\n    tmp7 =tl .load (in_ptr1 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,None )\n    tl .store (in_out_ptr0 +(x0 ),tmp13 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =200 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp7 =tl .load (in_out_ptr0 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp8 =tl .load (in_ptr2 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +64 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp14 ,0 )\n    tmp17 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp19 =tl .where (xmask ,tmp17 ,0 )\n    tmp20 =tl .sum (tmp19 ,1 )[:,None ]\n    tmp21 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 /tmp22 \n    tmp24 =tmp14 -tmp23 \n    tmp25 =tmp24 *tmp24 \n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (xmask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =tmp13 -tmp23 \n    tmp31 =64.0 \n    tmp32 =tmp29 /tmp31 \n    tmp33 =1e-05 \n    tmp34 =tmp32 +tmp33 \n    tmp35 =libdevice .rsqrt (tmp34 )\n    tmp36 =tmp30 *tmp35 \n    tmp38 =tmp36 *tmp37 \n    tmp40 =tmp38 +tmp39 \n    tmp41 =0.015625 \n    tmp42 =tmp35 *tmp41 \n    tl .store (out_ptr1 +(r0_1 +64 *x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(r0_1 +64 *x0 ),tmp36 ,xmask )\n    tl .store (out_ptr4 +(r0_1 +64 *x0 ),tmp40 ,xmask )\n    tl .store (out_ptr5 +(x0 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_9 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    x1 =(xindex %2048 )\n    tmp6 =tl .load (in_out_ptr0 +(x0 ),None )\n    tmp7 =tl .load (in_ptr1 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,None )\n    tl .store (in_out_ptr0 +(x0 ),tmp13 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_10 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,in_ptr6 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,out_ptr6 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =200 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp7 =tl .load (in_out_ptr0 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp8 =tl .load (in_ptr2 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp62 =tl .load (in_ptr5 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp64 =tl .load (in_ptr6 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +64 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp14 ,0 )\n    tmp17 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp19 =tl .where (xmask ,tmp17 ,0 )\n    tmp20 =tl .sum (tmp19 ,1 )[:,None ]\n    tmp21 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 /tmp22 \n    tmp24 =tmp14 -tmp23 \n    tmp25 =tmp24 *tmp24 \n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (xmask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =tmp13 -tmp23 \n    tmp31 =64.0 \n    tmp32 =tmp29 /tmp31 \n    tmp33 =1e-05 \n    tmp34 =tmp32 +tmp33 \n    tmp35 =libdevice .rsqrt (tmp34 )\n    tmp36 =tmp30 *tmp35 \n    tmp38 =tmp36 *tmp37 \n    tmp40 =tmp38 +tmp39 \n    tmp41 =tl .broadcast_to (tmp40 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp41 ,0 )\n    tmp44 =tl .broadcast_to (tmp41 ,[XBLOCK ,R0_BLOCK ])\n    tmp46 =tl .where (xmask ,tmp44 ,0 )\n    tmp47 =tl .sum (tmp46 ,1 )[:,None ]\n    tmp48 =tmp47 /tmp22 \n    tmp49 =tmp41 -tmp48 \n    tmp50 =tmp49 *tmp49 \n    tmp51 =tl .broadcast_to (tmp50 ,[XBLOCK ,R0_BLOCK ])\n    tmp53 =tl .where (xmask ,tmp51 ,0 )\n    tmp54 =tl .sum (tmp53 ,1 )[:,None ]\n    tmp55 =tmp54 /tmp31 \n    tmp56 =tmp55 +tmp33 \n    tmp57 =libdevice .rsqrt (tmp56 )\n    tmp58 =0.015625 \n    tmp59 =tmp35 *tmp58 \n    tmp60 =tmp40 -tmp48 \n    tmp61 =tmp60 *tmp57 \n    tmp63 =tmp61 *tmp62 \n    tmp65 =tmp63 +tmp64 \n    tl .store (out_ptr1 +(r0_1 +64 *x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(r0_1 +64 *x0 ),tmp36 ,xmask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(x0 ),tmp57 ,xmask )\n    tl .store (out_ptr5 +(x0 ),tmp59 ,xmask )\n    tl .store (out_ptr6 +(r0_1 +64 *x0 ),tmp65 ,xmask )\n    tl .store (out_ptr4 +(x0 ),tmp48 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_11 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =25600 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %64 )\n    x1 =((xindex //64 )%200 )\n    x2 =xindex //12800 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +64 *x2 +128 *x1 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(64 +x0 +64 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_threshold_backward_12 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    x1 =(xindex %2048 )\n    tmp6 =tl .load (in_ptr1 +(x0 ),None )\n    tmp7 =tl .load (in_ptr2 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.0 \n    tmp15 =tmp10 <=tmp14 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,None )\n    tl .store (out_ptr2 +(x0 ),tmp13 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp15 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_13 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,in_ptr6 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,out_ptr7 ,out_ptr8 ,load_seed_offset ,load_seed_offset1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =200 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp7 =tl .load (in_out_ptr0 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp8 =tl .load (in_ptr2 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp67 =tl .load (in_ptr5 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp69 =tl .load (in_ptr6 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +64 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp14 ,0 )\n    tmp17 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp19 =tl .where (xmask ,tmp17 ,0 )\n    tmp20 =tl .sum (tmp19 ,1 )[:,None ]\n    tmp21 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 /tmp22 \n    tmp24 =tmp14 -tmp23 \n    tmp25 =tmp24 *tmp24 \n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (xmask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =tmp13 -tmp23 \n    tmp31 =64.0 \n    tmp32 =tmp29 /tmp31 \n    tmp33 =1e-05 \n    tmp34 =tmp32 +tmp33 \n    tmp35 =libdevice .rsqrt (tmp34 )\n    tmp36 =tmp30 *tmp35 \n    tmp38 =tmp36 *tmp37 \n    tmp40 =tmp38 +tmp39 \n    tmp41 =tl .broadcast_to (tmp40 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp41 ,0 )\n    tmp44 =tl .broadcast_to (tmp41 ,[XBLOCK ,R0_BLOCK ])\n    tmp46 =tl .where (xmask ,tmp44 ,0 )\n    tmp47 =tl .sum (tmp46 ,1 )[:,None ]\n    tmp48 =tmp47 /tmp22 \n    tmp49 =tmp41 -tmp48 \n    tmp50 =tmp49 *tmp49 \n    tmp51 =tl .broadcast_to (tmp50 ,[XBLOCK ,R0_BLOCK ])\n    tmp53 =tl .where (xmask ,tmp51 ,0 )\n    tmp54 =tl .sum (tmp53 ,1 )[:,None ]\n    tmp55 =tmp54 /tmp31 \n    tmp56 =tmp55 +tmp33 \n    tmp57 =libdevice .rsqrt (tmp56 )\n    tmp58 =0.015625 \n    tmp59 =tmp35 *tmp58 \n    tmp60 =tl .load (in_ptr0 +load_seed_offset1 )\n    tmp61 =tl .rand (tmp60 ,(tmp1 ).to (tl .uint32 ))\n    tmp62 =0.5 \n    tmp63 =tmp61 >tmp62 \n    tmp64 =tmp63 .to (tl .float32 )\n    tmp65 =tmp40 -tmp48 \n    tmp66 =tmp65 *tmp57 \n    tmp68 =tmp66 *tmp67 \n    tmp70 =tmp68 +tmp69 \n    tmp71 =tmp64 *tmp70 \n    tmp72 =2.0 \n    tmp73 =tmp71 *tmp72 \n    tl .store (out_ptr1 +(r0_1 +64 *x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(r0_1 +64 *x0 ),tmp36 ,xmask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(x0 ),tmp57 ,xmask )\n    tl .store (out_ptr5 +(x0 ),tmp59 ,xmask )\n    tl .store (out_ptr7 +(r0_1 +64 *x0 ),tmp63 ,xmask )\n    tl .store (out_ptr8 +(r0_1 +64 *x0 ),tmp73 ,xmask )\n    tl .store (out_ptr4 +(x0 ),tmp48 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_exp_mean_mul_sub_14 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =200 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_2 =r0_index \n    x0 =(xindex %10 )\n    x1 =xindex //10 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_2 +64 *((r0_2 +64 *x0 +640 *x1 )//1280 )+640 *(((x0 +10 *x1 )%20 ))),xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp1 =tl_math .exp (tmp0 )\n    tmp2 =tmp0 *tmp0 \n    tmp3 =tmp1 -tmp2 \n    tmp4 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n    tmp6 =tl .where (xmask ,tmp4 ,0 )\n    tmp7 =tl .sum (tmp6 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_exp_mean_mul_sub_15 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =200 \n    R0_BLOCK :tl .constexpr =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =12800.0 \n    tmp6 =tmp4 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp6 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 ,primals_27 ,primals_28 ,primals_29 ,primals_30 ,primals_31 ,primals_32 ,primals_33 ,primals_34 ,primals_35 ,primals_36 ,primals_37 ,primals_38 ,primals_39 ,primals_40 ,primals_41 ,primals_42 ,primals_43 ,primals_44 ,primals_45 ,primals_46 ,primals_47 ,primals_48 ,primals_49 ,primals_50 ,primals_51 ,primals_52 ,primals_53 ,primals_54 ,primals_55 ,primals_56 ,primals_57 ,primals_58 ,primals_59 ,primals_60 ,primals_61 ,primals_62 ,primals_63 ,primals_64 ,primals_65 ,primals_66 ,primals_67 ,primals_68 ,primals_69 ,primals_70 ,primals_71 ,primals_72 ,primals_73 ,primals_74 ,primals_75 ,primals_76 ,primals_77 ,primals_78 ,primals_79 ,primals_80 ,primals_81 ,primals_82 ,primals_83 ,primals_84 ,primals_85 ,primals_86 ,primals_87 ,primals_88 ,primals_89 ,primals_90 ,primals_91 ,primals_92 ,primals_93 ,primals_94 ,primals_95 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(10 ,20 ,64 ),(1280 ,64 ,1 ))\n    assert_size_stride (primals_2 ,(192 ,),(1 ,))\n    assert_size_stride (primals_3 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_4 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_5 ,(64 ,),(1 ,))\n    assert_size_stride (primals_6 ,(64 ,),(1 ,))\n    assert_size_stride (primals_7 ,(64 ,),(1 ,))\n    assert_size_stride (primals_8 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_9 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_10 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_11 ,(64 ,),(1 ,))\n    assert_size_stride (primals_12 ,(64 ,),(1 ,))\n    assert_size_stride (primals_13 ,(64 ,),(1 ,))\n    assert_size_stride (primals_14 ,(192 ,),(1 ,))\n    assert_size_stride (primals_15 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_16 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_17 ,(64 ,),(1 ,))\n    assert_size_stride (primals_18 ,(64 ,),(1 ,))\n    assert_size_stride (primals_19 ,(64 ,),(1 ,))\n    assert_size_stride (primals_20 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_21 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_22 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_23 ,(64 ,),(1 ,))\n    assert_size_stride (primals_24 ,(64 ,),(1 ,))\n    assert_size_stride (primals_25 ,(64 ,),(1 ,))\n    assert_size_stride (primals_26 ,(192 ,),(1 ,))\n    assert_size_stride (primals_27 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_28 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_29 ,(64 ,),(1 ,))\n    assert_size_stride (primals_30 ,(64 ,),(1 ,))\n    assert_size_stride (primals_31 ,(64 ,),(1 ,))\n    assert_size_stride (primals_32 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_33 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_34 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_35 ,(64 ,),(1 ,))\n    assert_size_stride (primals_36 ,(64 ,),(1 ,))\n    assert_size_stride (primals_37 ,(64 ,),(1 ,))\n    assert_size_stride (primals_38 ,(64 ,),(1 ,))\n    assert_size_stride (primals_39 ,(64 ,),(1 ,))\n    assert_size_stride (primals_40 ,(192 ,),(1 ,))\n    assert_size_stride (primals_41 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_42 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_43 ,(64 ,),(1 ,))\n    assert_size_stride (primals_44 ,(64 ,),(1 ,))\n    assert_size_stride (primals_45 ,(64 ,),(1 ,))\n    assert_size_stride (primals_46 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_47 ,(192 ,),(1 ,))\n    assert_size_stride (primals_48 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_49 ,(64 ,),(1 ,))\n    assert_size_stride (primals_50 ,(64 ,),(1 ,))\n    assert_size_stride (primals_51 ,(64 ,),(1 ,))\n    assert_size_stride (primals_52 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_53 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_54 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_55 ,(64 ,),(1 ,))\n    assert_size_stride (primals_56 ,(64 ,),(1 ,))\n    assert_size_stride (primals_57 ,(64 ,),(1 ,))\n    assert_size_stride (primals_58 ,(192 ,),(1 ,))\n    assert_size_stride (primals_59 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_60 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_61 ,(64 ,),(1 ,))\n    assert_size_stride (primals_62 ,(64 ,),(1 ,))\n    assert_size_stride (primals_63 ,(64 ,),(1 ,))\n    assert_size_stride (primals_64 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_65 ,(192 ,),(1 ,))\n    assert_size_stride (primals_66 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_67 ,(64 ,),(1 ,))\n    assert_size_stride (primals_68 ,(64 ,),(1 ,))\n    assert_size_stride (primals_69 ,(64 ,),(1 ,))\n    assert_size_stride (primals_70 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_71 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_72 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_73 ,(64 ,),(1 ,))\n    assert_size_stride (primals_74 ,(64 ,),(1 ,))\n    assert_size_stride (primals_75 ,(64 ,),(1 ,))\n    assert_size_stride (primals_76 ,(192 ,),(1 ,))\n    assert_size_stride (primals_77 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_78 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_79 ,(64 ,),(1 ,))\n    assert_size_stride (primals_80 ,(64 ,),(1 ,))\n    assert_size_stride (primals_81 ,(64 ,),(1 ,))\n    assert_size_stride (primals_82 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_83 ,(192 ,),(1 ,))\n    assert_size_stride (primals_84 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_85 ,(64 ,),(1 ,))\n    assert_size_stride (primals_86 ,(64 ,),(1 ,))\n    assert_size_stride (primals_87 ,(64 ,),(1 ,))\n    assert_size_stride (primals_88 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_89 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_90 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_91 ,(64 ,),(1 ,))\n    assert_size_stride (primals_92 ,(64 ,),(1 ,))\n    assert_size_stride (primals_93 ,(64 ,),(1 ,))\n    assert_size_stride (primals_94 ,(64 ,),(1 ,))\n    assert_size_stride (primals_95 ,(64 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf13 =empty_strided_cuda ((22 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[22 ],out =buf13 )\n        buf0 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](primals_1 ,buf0 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((200 ,192 ),(192 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf0 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf1 )\n        del primals_3 \n        buf2 =empty_strided_cuda ((3 ,20 ,10 ,64 ),(12800 ,640 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_1 [grid (38400 )](buf1 ,primals_2 ,buf2 ,38400 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n        buf3 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (12800 )](buf2 ,buf3 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (12800 )](buf2 ,buf4 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_4 [grid (12800 )](buf2 ,buf5 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        buf6 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (buf3 ,buf4 ,buf5 ,None ,True ,0.1 )\n        buf7 =buf6 [0 ]\n        buf8 =buf6 [1 ]\n        buf9 =buf6 [2 ]\n        buf10 =buf6 [3 ]\n        del buf6 \n        buf11 =empty_strided_cuda ((20 ,10 ,8 ,8 ),(640 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](buf7 ,buf11 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf12 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf11 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_4 ,(64 ,64 ),(1 ,64 ),0 ),out =buf12 )\n        buf97 =reinterpret_tensor (buf2 ,(200 ,192 ),(192 ,1 ),0 );del buf2 \n\n        extern_kernels .mm (reinterpret_tensor (buf0 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_41 ,(64 ,192 ),(1 ,64 ),0 ),out =buf97 )\n        del primals_41 \n        buf98 =reinterpret_tensor (buf1 ,(3 ,20 ,10 ,64 ),(12800 ,640 ,64 ,1 ),0 );del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_1 [grid (38400 )](buf97 ,primals_40 ,buf98 ,38400 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del primals_40 \n        buf100 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (12800 )](buf98 ,buf100 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf101 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_4 [grid (12800 )](buf98 ,buf101 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf99 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (12800 )](buf98 ,buf99 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n\n        buf102 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (buf99 ,buf100 ,buf101 ,None ,True ,0.1 )\n        buf103 =buf102 [0 ]\n        buf104 =buf102 [1 ]\n        buf105 =buf102 [2 ]\n        buf106 =buf102 [3 ]\n        del buf102 \n        buf107 =empty_strided_cuda ((20 ,10 ,8 ,8 ),(640 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](buf103 ,buf107 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf108 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf107 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_42 ,(64 ,64 ),(1 ,64 ),0 ),out =buf108 )\n        buf110 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf15 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf19 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf114 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf266 =empty_strided_cuda ((20 ,10 ,64 ),(64 ,1280 ,1 ),torch .float32 )\n        buf276 =empty_strided_cuda ((20 ,10 ,64 ),(64 ,1280 ,1 ),torch .float32 )\n        buf267 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n        buf277 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_5 [grid (200 )](buf13 ,primals_1 ,buf12 ,primals_5 ,primals_6 ,primals_7 ,buf108 ,primals_43 ,primals_44 ,primals_45 ,buf110 ,buf15 ,buf19 ,buf114 ,buf266 ,buf276 ,buf267 ,buf277 ,9 ,0 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_1 \n        del primals_43 \n        del primals_45 \n        del primals_5 \n        del primals_7 \n        buf115 =buf12 ;del buf12 \n\n        extern_kernels .addmm (reinterpret_tensor (primals_47 ,(64 ,),(1 ,),0 ),reinterpret_tensor (buf114 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_46 ,(64 ,64 ),(1 ,64 ),0 ),alpha =1 ,beta =1 ,out =buf115 )\n        buf20 =empty_strided_cuda ((200 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf19 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_8 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf20 )\n        buf275 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_relu_threshold_backward_6 [grid (409600 )](buf20 ,primals_9 ,buf275 ,409600 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        buf22 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n        buf23 =reinterpret_tensor (buf20 ,(20 ,10 ,2048 ),(20480 ,2048 ,1 ),0 );del buf20 \n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_7 [grid (409600 )](buf23 ,buf13 ,primals_9 ,buf22 ,1 ,409600 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_9 \n        buf24 =buf108 ;del buf108 \n\n        extern_kernels .mm (reinterpret_tensor (buf23 ,(200 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_10 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf24 )\n        buf26 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf30 =reinterpret_tensor (buf24 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf24 \n        buf31 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf274 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf30 ,buf13 ,buf19 ,primals_11 ,primals_12 ,primals_13 ,buf26 ,buf31 ,buf274 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_11 \n        del primals_13 \n        buf32 =reinterpret_tensor (buf98 ,(200 ,192 ),(192 ,1 ),0 );del buf98 \n\n        extern_kernels .mm (reinterpret_tensor (buf31 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_15 ,(64 ,192 ),(1 ,64 ),0 ),out =buf32 )\n        buf33 =reinterpret_tensor (buf97 ,(3 ,20 ,10 ,64 ),(12800 ,640 ,64 ,1 ),0 );del buf97 \n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_1 [grid (38400 )](buf32 ,primals_14 ,buf33 ,38400 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del primals_14 \n        buf34 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (12800 )](buf33 ,buf34 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf35 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (12800 )](buf33 ,buf35 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf36 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_4 [grid (12800 )](buf33 ,buf36 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        buf37 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (buf34 ,buf35 ,buf36 ,None ,True ,0.1 )\n        buf38 =buf37 [0 ]\n        buf39 =buf37 [1 ]\n        buf40 =buf37 [2 ]\n        buf41 =buf37 [3 ]\n        del buf37 \n        buf42 =empty_strided_cuda ((20 ,10 ,8 ,8 ),(640 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](buf38 ,buf42 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf43 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf42 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_16 ,(64 ,64 ),(1 ,64 ),0 ),out =buf43 )\n        buf45 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf49 =reinterpret_tensor (buf43 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf43 \n        buf50 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf273 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf49 ,buf13 ,buf31 ,primals_17 ,primals_18 ,primals_19 ,buf45 ,buf50 ,buf273 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_17 \n        del primals_19 \n        buf51 =empty_strided_cuda ((200 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf50 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_20 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf51 )\n        buf272 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_relu_threshold_backward_6 [grid (409600 )](buf51 ,primals_21 ,buf272 ,409600 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        buf53 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n        buf54 =reinterpret_tensor (buf51 ,(20 ,10 ,2048 ),(20480 ,2048 ,1 ),0 );del buf51 \n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_9 [grid (409600 )](buf54 ,buf13 ,primals_21 ,buf53 ,15 ,409600 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_21 \n        buf55 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf54 ,(200 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_22 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf55 )\n        buf57 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf61 =reinterpret_tensor (buf55 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf55 \n        buf62 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf271 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf61 ,buf13 ,buf50 ,primals_23 ,primals_24 ,primals_25 ,buf57 ,buf62 ,buf271 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_23 \n        del primals_25 \n        buf63 =reinterpret_tensor (buf33 ,(200 ,192 ),(192 ,1 ),0 );del buf33 \n\n        extern_kernels .mm (reinterpret_tensor (buf62 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_27 ,(64 ,192 ),(1 ,64 ),0 ),out =buf63 )\n        buf64 =reinterpret_tensor (buf32 ,(3 ,20 ,10 ,64 ),(12800 ,640 ,64 ,1 ),0 );del buf32 \n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_1 [grid (38400 )](buf63 ,primals_26 ,buf64 ,38400 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del primals_26 \n        buf65 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (12800 )](buf64 ,buf65 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf66 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (12800 )](buf64 ,buf66 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf67 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_4 [grid (12800 )](buf64 ,buf67 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        buf68 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (buf65 ,buf66 ,buf67 ,None ,True ,0.1 )\n        buf69 =buf68 [0 ]\n        buf70 =buf68 [1 ]\n        buf71 =buf68 [2 ]\n        buf72 =buf68 [3 ]\n        del buf68 \n        buf73 =empty_strided_cuda ((20 ,10 ,8 ,8 ),(640 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](buf69 ,buf73 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf74 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf73 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_28 ,(64 ,64 ),(1 ,64 ),0 ),out =buf74 )\n        buf76 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf80 =reinterpret_tensor (buf74 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf74 \n        buf81 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf270 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf80 ,buf13 ,buf62 ,primals_29 ,primals_30 ,primals_31 ,buf76 ,buf81 ,buf270 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_29 \n        del primals_31 \n        buf82 =empty_strided_cuda ((200 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf81 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_32 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf82 )\n        buf269 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_relu_threshold_backward_6 [grid (409600 )](buf82 ,primals_33 ,buf269 ,409600 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        buf84 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n        buf85 =reinterpret_tensor (buf82 ,(20 ,10 ,2048 ),(20480 ,2048 ,1 ),0 );del buf82 \n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_9 [grid (409600 )](buf85 ,buf13 ,primals_33 ,buf84 ,15 ,409600 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_33 \n        buf86 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf85 ,(200 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_34 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf86 )\n        buf88 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf92 =reinterpret_tensor (buf86 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf86 \n        buf93 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n        buf94 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,200 ),torch .float32 )\n        buf96 =reinterpret_tensor (buf94 ,(20 ,10 ,1 ),(10 ,1 ,1 ),0 );del buf94 \n        buf268 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n        buf116 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_10 [grid (200 )](buf92 ,buf96 ,buf13 ,buf81 ,primals_35 ,primals_36 ,primals_37 ,primals_38 ,primals_39 ,buf88 ,buf93 ,buf268 ,buf116 ,8 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_35 \n        del primals_39 \n        buf117 =empty_strided_cuda ((200 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf116 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_46 ,(64 ,128 ),(1 ,64 ),4096 ),out =buf117 )\n        buf118 =empty_strided_cuda ((2 ,20 ,10 ,64 ),(12800 ,640 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_11 [grid (25600 )](buf117 ,primals_47 ,buf118 ,25600 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_47 \n        buf119 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (12800 )](buf118 ,buf119 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf120 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (12800 )](buf118 ,buf120 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n\n        buf121 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf115 ,(10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),0 ),buf119 ,buf120 ,None ,True ,0.1 )\n        buf122 =buf121 [0 ]\n        buf123 =buf121 [1 ]\n        buf124 =buf121 [2 ]\n        buf125 =buf121 [3 ]\n        del buf121 \n        buf126 =empty_strided_cuda ((20 ,10 ,8 ,8 ),(640 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](buf122 ,buf126 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf127 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf126 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_48 ,(64 ,64 ),(1 ,64 ),0 ),out =buf127 )\n        buf129 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf133 =reinterpret_tensor (buf127 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf127 \n        buf134 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf265 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf133 ,buf13 ,buf114 ,primals_49 ,primals_50 ,primals_51 ,buf129 ,buf134 ,buf265 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_49 \n        del primals_51 \n        buf167 =reinterpret_tensor (buf118 ,(200 ,128 ),(128 ,1 ),0 );del buf118 \n\n        extern_kernels .mm (reinterpret_tensor (buf116 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_64 ,(64 ,128 ),(1 ,64 ),4096 ),out =buf167 )\n        buf168 =reinterpret_tensor (buf117 ,(2 ,20 ,10 ,64 ),(12800 ,640 ,64 ,1 ),0 );del buf117 \n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_11 [grid (25600 )](buf167 ,primals_65 ,buf168 ,25600 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf169 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (12800 )](buf168 ,buf169 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf170 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (12800 )](buf168 ,buf170 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf217 =reinterpret_tensor (buf168 ,(200 ,128 ),(128 ,1 ),0 );del buf168 \n\n        extern_kernels .mm (reinterpret_tensor (buf116 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_82 ,(64 ,128 ),(1 ,64 ),4096 ),out =buf217 )\n        buf218 =reinterpret_tensor (buf167 ,(2 ,20 ,10 ,64 ),(12800 ,640 ,64 ,1 ),0 );del buf167 \n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_11 [grid (25600 )](buf217 ,primals_83 ,buf218 ,25600 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf217 \n        buf219 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (12800 )](buf218 ,buf219 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf220 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (12800 )](buf218 ,buf220 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf218 \n        buf135 =empty_strided_cuda ((200 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf134 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_52 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf135 )\n        buf264 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_relu_threshold_backward_6 [grid (409600 )](buf135 ,primals_53 ,buf264 ,409600 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        buf137 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n        buf138 =reinterpret_tensor (buf135 ,(20 ,10 ,2048 ),(20480 ,2048 ,1 ),0 );del buf135 \n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_9 [grid (409600 )](buf138 ,buf13 ,primals_53 ,buf137 ,15 ,409600 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_53 \n        buf139 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf138 ,(200 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_54 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf139 )\n        buf141 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf145 =reinterpret_tensor (buf139 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf139 \n        buf146 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf263 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf145 ,buf13 ,buf134 ,primals_55 ,primals_56 ,primals_57 ,buf141 ,buf146 ,buf263 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_55 \n        del primals_57 \n        buf147 =reinterpret_tensor (buf64 ,(200 ,192 ),(192 ,1 ),0 );del buf64 \n\n        extern_kernels .mm (reinterpret_tensor (buf146 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_59 ,(64 ,192 ),(1 ,64 ),0 ),out =buf147 )\n        buf148 =reinterpret_tensor (buf63 ,(3 ,20 ,10 ,64 ),(12800 ,640 ,64 ,1 ),0 );del buf63 \n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_1 [grid (38400 )](buf147 ,primals_58 ,buf148 ,38400 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del primals_58 \n        buf149 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (12800 )](buf148 ,buf149 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf150 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (12800 )](buf148 ,buf150 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf151 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_4 [grid (12800 )](buf148 ,buf151 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        buf152 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (buf149 ,buf150 ,buf151 ,None ,True ,0.1 )\n        buf153 =buf152 [0 ]\n        buf154 =buf152 [1 ]\n        buf155 =buf152 [2 ]\n        buf156 =buf152 [3 ]\n        del buf152 \n        buf157 =empty_strided_cuda ((20 ,10 ,8 ,8 ),(640 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](buf153 ,buf157 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf158 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf157 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_60 ,(64 ,64 ),(1 ,64 ),0 ),out =buf158 )\n        buf160 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf164 =reinterpret_tensor (buf158 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf158 \n        buf165 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf262 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf164 ,buf13 ,buf146 ,primals_61 ,primals_62 ,primals_63 ,buf160 ,buf165 ,buf262 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_61 \n        del primals_63 \n        buf166 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (reinterpret_tensor (primals_65 ,(64 ,),(1 ,),0 ),reinterpret_tensor (buf165 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_64 ,(64 ,64 ),(1 ,64 ),0 ),alpha =1 ,beta =1 ,out =buf166 )\n        del primals_65 \n\n        buf171 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf166 ,(10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),0 ),buf169 ,buf170 ,None ,True ,0.1 )\n        buf172 =buf171 [0 ]\n        buf173 =buf171 [1 ]\n        buf174 =buf171 [2 ]\n        buf175 =buf171 [3 ]\n        del buf171 \n        buf176 =empty_strided_cuda ((20 ,10 ,8 ,8 ),(640 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](buf172 ,buf176 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf177 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf176 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_66 ,(64 ,64 ),(1 ,64 ),0 ),out =buf177 )\n        buf179 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf183 =reinterpret_tensor (buf177 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf177 \n        buf184 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf261 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf183 ,buf13 ,buf165 ,primals_67 ,primals_68 ,primals_69 ,buf179 ,buf184 ,buf261 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_67 \n        del primals_69 \n        buf185 =empty_strided_cuda ((200 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf184 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_70 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf185 )\n        buf260 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_relu_threshold_backward_6 [grid (409600 )](buf185 ,primals_71 ,buf260 ,409600 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        buf187 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n        buf188 =reinterpret_tensor (buf185 ,(20 ,10 ,2048 ),(20480 ,2048 ,1 ),0 );del buf185 \n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_9 [grid (409600 )](buf188 ,buf13 ,primals_71 ,buf187 ,15 ,409600 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_71 \n        buf189 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf188 ,(200 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_72 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf189 )\n        buf191 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf195 =reinterpret_tensor (buf189 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf189 \n        buf196 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf259 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf195 ,buf13 ,buf184 ,primals_73 ,primals_74 ,primals_75 ,buf191 ,buf196 ,buf259 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_73 \n        del primals_75 \n        buf197 =reinterpret_tensor (buf148 ,(200 ,192 ),(192 ,1 ),0 );del buf148 \n\n        extern_kernels .mm (reinterpret_tensor (buf196 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_77 ,(64 ,192 ),(1 ,64 ),0 ),out =buf197 )\n        buf198 =reinterpret_tensor (buf147 ,(3 ,20 ,10 ,64 ),(12800 ,640 ,64 ,1 ),0 );del buf147 \n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_1 [grid (38400 )](buf197 ,primals_76 ,buf198 ,38400 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del buf197 \n        del primals_76 \n        buf199 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_2 [grid (12800 )](buf198 ,buf199 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf200 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_3 [grid (12800 )](buf198 ,buf200 ,12800 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf201 =empty_strided_cuda ((10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_4 [grid (12800 )](buf198 ,buf201 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf198 \n\n        buf202 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (buf199 ,buf200 ,buf201 ,None ,True ,0.1 )\n        buf203 =buf202 [0 ]\n        buf204 =buf202 [1 ]\n        buf205 =buf202 [2 ]\n        buf206 =buf202 [3 ]\n        del buf202 \n        buf207 =empty_strided_cuda ((20 ,10 ,8 ,8 ),(640 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](buf203 ,buf207 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf208 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf207 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_78 ,(64 ,64 ),(1 ,64 ),0 ),out =buf208 )\n        buf210 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf214 =reinterpret_tensor (buf208 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf208 \n        buf215 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf258 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf214 ,buf13 ,buf196 ,primals_79 ,primals_80 ,primals_81 ,buf210 ,buf215 ,buf258 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_79 \n        del primals_81 \n        buf216 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (reinterpret_tensor (primals_83 ,(64 ,),(1 ,),0 ),reinterpret_tensor (buf215 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_82 ,(64 ,64 ),(1 ,64 ),0 ),alpha =1 ,beta =1 ,out =buf216 )\n        del primals_83 \n\n        buf221 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf216 ,(10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),0 ),buf219 ,buf220 ,None ,True ,0.1 )\n        buf222 =buf221 [0 ]\n        buf223 =buf221 [1 ]\n        buf224 =buf221 [2 ]\n        buf225 =buf221 [3 ]\n        del buf221 \n        buf226 =empty_strided_cuda ((20 ,10 ,8 ,8 ),(640 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12800 )](buf222 ,buf226 ,12800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf227 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf226 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_84 ,(64 ,64 ),(1 ,64 ),0 ),out =buf227 )\n        buf229 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf233 =reinterpret_tensor (buf227 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf227 \n        buf234 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n        buf257 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_8 [grid (200 )](buf233 ,buf13 ,buf215 ,primals_85 ,primals_86 ,primals_87 ,buf229 ,buf234 ,buf257 ,18 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_85 \n        del primals_87 \n        buf235 =empty_strided_cuda ((200 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf234 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_88 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf235 )\n        buf237 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n        buf238 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .float32 )\n        buf256 =empty_strided_cuda ((20 ,10 ,2048 ),(20480 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_12 [grid (409600 )](buf13 ,buf235 ,primals_89 ,buf237 ,buf238 ,buf256 ,19 ,409600 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf235 \n        del primals_89 \n        buf239 =empty_strided_cuda ((200 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf238 ,(200 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_90 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf239 )\n        buf241 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf245 =reinterpret_tensor (buf239 ,(20 ,10 ,64 ),(640 ,64 ,1 ),0 );del buf239 \n        buf246 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n        buf247 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,200 ),torch .float32 )\n        buf249 =reinterpret_tensor (buf247 ,(20 ,10 ,1 ),(10 ,1 ,1 ),0 );del buf247 \n        buf255 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n        buf251 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .bool )\n        buf252 =empty_strided_cuda ((20 ,10 ,64 ),(640 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_13 [grid (200 )](buf245 ,buf249 ,buf13 ,buf234 ,primals_91 ,primals_92 ,primals_93 ,primals_94 ,primals_95 ,buf241 ,buf246 ,buf255 ,buf251 ,buf252 ,20 ,21 ,200 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf13 \n        del primals_91 \n        buf253 =empty_strided_cuda ((20 ,10 ,1 ),(10 ,1 ,200 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_exp_mean_mul_sub_14 [grid (200 )](buf252 ,buf253 ,200 ,64 ,XBLOCK =32 ,num_warps =8 ,num_stages =1 )\n        del buf252 \n        buf254 =empty_strided_cuda ((),(),torch .float32 )\n        buf278 =buf254 ;del buf254 \n\n        get_raw_stream (0 )\n        triton_per_fused_exp_mean_mul_sub_15 [grid (1 )](buf278 ,buf253 ,1 ,200 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf253 \n    return (buf278 ,primals_6 ,primals_12 ,primals_18 ,primals_24 ,primals_30 ,primals_36 ,primals_37 ,primals_38 ,primals_44 ,primals_50 ,primals_56 ,primals_62 ,primals_68 ,primals_74 ,primals_80 ,primals_86 ,primals_92 ,primals_93 ,primals_94 ,primals_95 ,reinterpret_tensor (buf0 ,(200 ,64 ),(64 ,1 ),0 ),buf3 ,buf4 ,buf5 ,buf7 ,buf8 ,buf9 ,buf10 ,reinterpret_tensor (buf11 ,(200 ,64 ),(64 ,1 ),0 ),buf15 ,reinterpret_tensor (buf19 ,(200 ,64 ),(64 ,1 ),0 ),buf22 ,reinterpret_tensor (buf23 ,(200 ,2048 ),(2048 ,1 ),0 ),buf26 ,buf30 ,reinterpret_tensor (buf31 ,(200 ,64 ),(64 ,1 ),0 ),buf34 ,buf35 ,buf36 ,buf38 ,buf39 ,buf40 ,buf41 ,reinterpret_tensor (buf42 ,(200 ,64 ),(64 ,1 ),0 ),buf45 ,buf49 ,reinterpret_tensor (buf50 ,(200 ,64 ),(64 ,1 ),0 ),buf53 ,reinterpret_tensor (buf54 ,(200 ,2048 ),(2048 ,1 ),0 ),buf57 ,buf61 ,reinterpret_tensor (buf62 ,(200 ,64 ),(64 ,1 ),0 ),buf65 ,buf66 ,buf67 ,buf69 ,buf70 ,buf71 ,buf72 ,reinterpret_tensor (buf73 ,(200 ,64 ),(64 ,1 ),0 ),buf76 ,buf80 ,reinterpret_tensor (buf81 ,(200 ,64 ),(64 ,1 ),0 ),buf84 ,reinterpret_tensor (buf85 ,(200 ,2048 ),(2048 ,1 ),0 ),buf88 ,buf92 ,buf93 ,buf96 ,buf99 ,buf100 ,buf101 ,buf103 ,buf104 ,buf105 ,buf106 ,reinterpret_tensor (buf107 ,(200 ,64 ),(64 ,1 ),0 ),buf110 ,reinterpret_tensor (buf114 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (buf116 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (buf115 ,(10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),0 ),buf119 ,buf120 ,buf122 ,buf123 ,buf124 ,buf125 ,reinterpret_tensor (buf126 ,(200 ,64 ),(64 ,1 ),0 ),buf129 ,buf133 ,reinterpret_tensor (buf134 ,(200 ,64 ),(64 ,1 ),0 ),buf137 ,reinterpret_tensor (buf138 ,(200 ,2048 ),(2048 ,1 ),0 ),buf141 ,buf145 ,reinterpret_tensor (buf146 ,(200 ,64 ),(64 ,1 ),0 ),buf149 ,buf150 ,buf151 ,buf153 ,buf154 ,buf155 ,buf156 ,reinterpret_tensor (buf157 ,(200 ,64 ),(64 ,1 ),0 ),buf160 ,buf164 ,reinterpret_tensor (buf165 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (buf166 ,(10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),0 ),buf169 ,buf170 ,buf172 ,buf173 ,buf174 ,buf175 ,reinterpret_tensor (buf176 ,(200 ,64 ),(64 ,1 ),0 ),buf179 ,buf183 ,reinterpret_tensor (buf184 ,(200 ,64 ),(64 ,1 ),0 ),buf187 ,reinterpret_tensor (buf188 ,(200 ,2048 ),(2048 ,1 ),0 ),buf191 ,buf195 ,reinterpret_tensor (buf196 ,(200 ,64 ),(64 ,1 ),0 ),buf199 ,buf200 ,buf201 ,buf203 ,buf204 ,buf205 ,buf206 ,reinterpret_tensor (buf207 ,(200 ,64 ),(64 ,1 ),0 ),buf210 ,buf214 ,reinterpret_tensor (buf215 ,(200 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (buf216 ,(10 ,8 ,20 ,8 ),(64 ,8 ,640 ,1 ),0 ),buf219 ,buf220 ,buf222 ,buf223 ,buf224 ,buf225 ,reinterpret_tensor (buf226 ,(200 ,64 ),(64 ,1 ),0 ),buf229 ,buf233 ,reinterpret_tensor (buf234 ,(200 ,64 ),(64 ,1 ),0 ),buf237 ,reinterpret_tensor (buf238 ,(200 ,2048 ),(2048 ,1 ),0 ),buf241 ,buf245 ,buf246 ,buf249 ,buf251 ,buf255 ,primals_90 ,buf256 ,primals_88 ,buf257 ,primals_84 ,reinterpret_tensor (primals_82 ,(128 ,64 ),(64 ,1 ),4096 ),reinterpret_tensor (primals_82 ,(64 ,64 ),(64 ,1 ),0 ),buf258 ,primals_78 ,primals_77 ,buf259 ,primals_72 ,buf260 ,primals_70 ,buf261 ,primals_66 ,reinterpret_tensor (primals_64 ,(128 ,64 ),(64 ,1 ),4096 ),reinterpret_tensor (primals_64 ,(64 ,64 ),(64 ,1 ),0 ),buf262 ,primals_60 ,primals_59 ,buf263 ,primals_54 ,buf264 ,primals_52 ,buf265 ,primals_48 ,reinterpret_tensor (primals_46 ,(128 ,64 ),(64 ,1 ),4096 ),reinterpret_tensor (primals_46 ,(64 ,64 ),(64 ,1 ),0 ),buf266 ,buf267 ,primals_42 ,buf268 ,primals_34 ,buf269 ,primals_32 ,buf270 ,primals_28 ,primals_27 ,buf271 ,primals_22 ,buf272 ,primals_20 ,buf273 ,primals_16 ,primals_15 ,buf274 ,primals_10 ,buf275 ,primals_8 ,buf276 ,buf277 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((10 ,20 ,64 ),(1280 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_18 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_22 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_23 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_24 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_25 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_26 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_27 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_28 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_29 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_30 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_31 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_32 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_33 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_34 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_35 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_36 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_37 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_38 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_39 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_40 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_41 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_42 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_43 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_44 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_45 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_46 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_47 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_48 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_49 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_50 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_51 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_52 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_53 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_54 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_55 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_56 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_57 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_58 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_59 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_60 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_61 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_62 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_63 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_64 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_65 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_66 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_67 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_68 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_69 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_70 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_71 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_72 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_73 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_74 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_75 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_76 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_77 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_78 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_79 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_80 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_81 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_82 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_83 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_84 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_85 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_86 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_87 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_88 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_89 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_90 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_91 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_92 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_93 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_94 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_95 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 ,primals_27 ,primals_28 ,primals_29 ,primals_30 ,primals_31 ,primals_32 ,primals_33 ,primals_34 ,primals_35 ,primals_36 ,primals_37 ,primals_38 ,primals_39 ,primals_40 ,primals_41 ,primals_42 ,primals_43 ,primals_44 ,primals_45 ,primals_46 ,primals_47 ,primals_48 ,primals_49 ,primals_50 ,primals_51 ,primals_52 ,primals_53 ,primals_54 ,primals_55 ,primals_56 ,primals_57 ,primals_58 ,primals_59 ,primals_60 ,primals_61 ,primals_62 ,primals_63 ,primals_64 ,primals_65 ,primals_66 ,primals_67 ,primals_68 ,primals_69 ,primals_70 ,primals_71 ,primals_72 ,primals_73 ,primals_74 ,primals_75 ,primals_76 ,primals_77 ,primals_78 ,primals_79 ,primals_80 ,primals_81 ,primals_82 ,primals_83 ,primals_84 ,primals_85 ,primals_86 ,primals_87 ,primals_88 ,primals_89 ,primals_90 ,primals_91 ,primals_92 ,primals_93 ,primals_94 ,primals_95 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "83004efe-134e-4ba5-821c-2b9121230b27",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['InstanceNorm1d', 'AvgPool3d', 'UpsamplingBilinear2d', 'GELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.instance_norm1 = nn.InstanceNorm1d(64)\n        self.avg_pool3d = nn.AvgPool3d(kernel_size=2)\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.gelu = nn.GELU()\n        self.instance_norm2 = nn.InstanceNorm1d(32)\n        self.avg_pool3d_2 = nn.AvgPool3d(kernel_size=2)\n        self.upsample_2 = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.gelu_2 = nn.GELU()\n\n    def forward(self, x):\n        # Assuming input is of shape (batch_size, channels, *dims)\n        # First, reshape to 1D for InstanceNorm1d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, flattened_dims)\n        x = self.instance_norm1(x)\n        \n        # Reshape back to 3D for AvgPool3d\n        x = x.view(x.size(0), x.size(1), 8, 8, 8)  # Reshape to (batch_size, channels, 8, 8, 8)\n        x = self.avg_pool3d(x)\n        \n        # Reshape to 2D for UpsamplingBilinear2d\n        x = x.view(x.size(0), x.size(1), x.size(2), -1)  # Reshape to (batch_size, channels, 8, 8*8)\n        x = self.upsample(x)\n        \n        # Apply GELU\n        x = self.gelu(x)\n        \n        # Repeat the process with different shapes\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, flattened_dims)\n        x = self.instance_norm2(x)\n        \n        x = x.view(x.size(0), x.size(1), 4, 4, 4)  # Reshape to (batch_size, channels, 4, 4, 4)\n        x = self.avg_pool3d_2(x)\n        \n        x = x.view(x.size(0), x.size(1), x.size(2), -1)  # Reshape to (batch_size, channels, 4, 4*4)\n        x = self.upsample_2(x)\n        \n        x = self.gelu_2(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 8, 8, 8).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr2 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *ks2 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp8 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *ks2 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp9 =tmp8 -tmp2 \n        tmp10 =ks0 *ks1 *ks2 \n        tmp11 =tmp10 .to (tl .float32 )\n        tmp12 =tmp3 /tmp11 \n        tmp13 =1e-05 \n        tmp14 =tmp12 +tmp13 \n        tmp15 =libdevice .rsqrt (tmp14 )\n        tmp16 =tmp9 *tmp15 \n        tl .store (out_ptr2 +(r0_1 +ks0 *ks1 *ks2 *x0 ),tmp16 ,r0_mask &xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool3d_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =(xindex %4 )\n    x1 =((xindex //4 )%4 )\n    x2 =((xindex //16 )%4 )\n    x3 =xindex //64 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +16 *x1 +128 *x2 +ks0 *ks1 *ks2 *x3 ),None ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +16 *x1 +128 *x2 +ks0 *ks1 *ks2 *x3 ),None ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(8 +2 *x0 +16 *x1 +128 *x2 +ks0 *ks1 *ks2 *x3 ),None ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(9 +2 *x0 +16 *x1 +128 *x2 +ks0 *ks1 *ks2 *x3 ),None ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(64 +2 *x0 +16 *x1 +128 *x2 +ks0 *ks1 *ks2 *x3 ),None ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(65 +2 *x0 +16 *x1 +128 *x2 +ks0 *ks1 *ks2 *x3 ),None ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(72 +2 *x0 +16 *x1 +128 *x2 +ks0 *ks1 *ks2 *x3 ),None ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(73 +2 *x0 +16 *x1 +128 *x2 +ks0 *ks1 *ks2 *x3 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp8 =tmp7 +tmp6 \n    tmp10 =tmp9 +tmp8 \n    tmp12 =tmp11 +tmp10 \n    tmp14 =tmp13 +tmp12 \n    tmp15 =0.125 \n    tmp16 =tmp14 *tmp15 \n    tl .store (out_ptr0 +(x4 ),tmp16 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_arange_clamp_view_2 (out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =ks0 *ks1 *ks2 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =128.0 \n    tmp3 =tmp1 /tmp2 \n    tmp4 =libdevice .floor (tmp3 )\n    tmp5 =4.0 \n    tmp6 =tmp5 *tmp4 \n    tmp7 =tmp6 .to (tl .float64 )\n    tmp8 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp9 =tmp8 +tmp7 \n    tmp10 =8.0 \n    tmp11 =tmp10 *tmp4 \n    tmp12 =tmp11 .to (tl .float64 )\n    tmp13 =tmp8 +tmp12 \n    tmp14 =tmp9 /tmp13 \n    tmp15 =tmp14 .to (tl .float32 )\n    tmp16 =x0 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =tmp17 *tmp15 \n    tmp19 =0.0 \n    tmp20 =triton_helpers .maximum (tmp18 ,tmp19 )\n    tmp21 =tmp20 .to (tl .int64 )\n    tmp22 =tl .full ([1 ],1 ,tl .int64 )\n    tmp23 =tmp21 +tmp22 \n    tmp24 =(-1 )+4 *((ks0 *ks1 *ks2 )//128 )\n    tmp25 =triton_helpers .minimum (tmp23 ,tmp24 )\n    tl .store (out_ptr0 +(x0 ),tmp25 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_arange_clamp_sub_view_3 (out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =ks0 *ks1 *ks2 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =128.0 \n    tmp3 =tmp1 /tmp2 \n    tmp4 =libdevice .floor (tmp3 )\n    tmp5 =4.0 \n    tmp6 =tmp5 *tmp4 \n    tmp7 =tmp6 .to (tl .float64 )\n    tmp8 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp9 =tmp8 +tmp7 \n    tmp10 =8.0 \n    tmp11 =tmp10 *tmp4 \n    tmp12 =tmp11 .to (tl .float64 )\n    tmp13 =tmp8 +tmp12 \n    tmp14 =tmp9 /tmp13 \n    tmp15 =tmp14 .to (tl .float32 )\n    tmp16 =x0 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =tmp17 *tmp15 \n    tmp19 =0.0 \n    tmp20 =triton_helpers .maximum (tmp18 ,tmp19 )\n    tmp21 =tmp20 .to (tl .int64 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 -tmp22 \n    tmp24 =triton_helpers .maximum (tmp23 ,tmp19 )\n    tmp25 =1.0 \n    tmp26 =triton_helpers .minimum (tmp24 ,tmp25 )\n    tl .store (out_ptr0 +(x0 ),tmp26 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_clamp_gelu_mul_sub_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x1 =((xindex //ks0 )%8 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x3 =xindex \n    tmp34 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp41 =tl .load (in_ptr2 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.42857142857142855 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],3 ,tl .int64 )\n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tmp11 =ks1 *ks2 *ks3 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =128.0 \n    tmp14 =tmp12 /tmp13 \n    tmp15 =libdevice .floor (tmp14 )\n    tmp16 =4.0 \n    tmp17 =tmp16 *tmp15 \n    tmp18 =tmp17 .to (tl .float64 )\n    tmp19 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp20 =tmp19 +tmp18 \n    tmp21 =8.0 \n    tmp22 =tmp21 *tmp15 \n    tmp23 =tmp22 .to (tl .float64 )\n    tmp24 =tmp19 +tmp23 \n    tmp25 =tmp20 /tmp24 \n    tmp26 =tmp25 .to (tl .float32 )\n    tmp27 =x0 \n    tmp28 =tmp27 .to (tl .float32 )\n    tmp29 =tmp28 *tmp26 \n    tmp30 =triton_helpers .maximum (tmp29 ,tmp4 )\n    tmp31 =tmp30 .to (tl .int64 )\n    tmp32 =tl .load (in_ptr0 +(16 *tmp10 +64 *x2 +((tmp31 %16 ))),None ,eviction_policy ='evict_last')\n    tmp33 =tl .load (in_ptr0 +(16 *tmp6 +64 *x2 +((tmp31 %16 ))),None ,eviction_policy ='evict_last')\n    tmp35 =tl .full ([XBLOCK ],16 ,tl .int32 )\n    tmp36 =tmp34 +tmp35 \n    tmp37 =tmp34 <0 \n    tmp38 =tl .where (tmp37 ,tmp36 ,tmp34 )\n    tmp39 =tl .load (in_ptr0 +(16 *tmp6 +64 *x2 +((tmp38 %16 ))),None ,eviction_policy ='evict_last')\n    tmp40 =tmp39 -tmp33 \n    tmp42 =tmp40 *tmp41 \n    tmp43 =tmp33 +tmp42 \n    tmp44 =tl .load (in_ptr0 +(16 *tmp10 +64 *x2 +((tmp38 %16 ))),None ,eviction_policy ='evict_last')\n    tmp45 =tmp44 -tmp32 \n    tmp46 =tmp45 *tmp41 \n    tmp47 =tmp32 +tmp46 \n    tmp48 =tmp47 -tmp43 \n    tmp49 =tmp6 .to (tl .float32 )\n    tmp50 =tmp5 -tmp49 \n    tmp51 =triton_helpers .maximum (tmp50 ,tmp4 )\n    tmp52 =1.0 \n    tmp53 =triton_helpers .minimum (tmp51 ,tmp52 )\n    tmp54 =tmp48 *tmp53 \n    tmp55 =tmp43 +tmp54 \n    tmp56 =0.5 \n    tmp57 =tmp55 *tmp56 \n    tmp58 =0.7071067811865476 \n    tmp59 =tmp55 *tmp58 \n    tmp60 =libdevice .erf (tmp59 )\n    tmp61 =tmp60 +tmp52 \n    tmp62 =tmp57 *tmp61 \n    tl .store (in_out_ptr0 +(x3 ),tmp62 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    s2 =arg1_1 \n    s3 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,64 ,s1 ,s2 ,s3 ),(64 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf3 =empty_strided_cuda ((1 ,64 ,s1 *s2 *s3 ),(64 *s1 *s2 *s3 ,s1 *s2 *s3 ,1 ),torch .float32 )\n\n        s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_0 [grid (64 )](arg3_1 ,buf3 ,8 ,8 ,8 ,64 ,512 ,XBLOCK =1 ,R0_BLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf4 =empty_strided_cuda ((1 ,64 ,4 ,4 ,4 ),(4096 ,64 ,16 ,4 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool3d_1 [grid (4096 )](buf3 ,buf4 ,8 ,8 ,8 ,4096 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        buf5 =empty_strided_cuda ((8 *((s1 *s2 *s3 )//128 ),),(1 ,),torch .int64 )\n\n        triton_poi_fused__to_copy_add_arange_clamp_view_2_xnumel =8 *((s1 *s2 *s3 )//128 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_arange_clamp_view_2 [grid (triton_poi_fused__to_copy_add_arange_clamp_view_2_xnumel )](buf5 ,8 ,8 ,8 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf7 =empty_strided_cuda ((8 *((s1 *s2 *s3 )//128 ),),(1 ,),torch .float32 )\n\n        triton_poi_fused__to_copy_arange_clamp_sub_view_3_xnumel =8 *((s1 *s2 *s3 )//128 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_arange_clamp_sub_view_3 [grid (triton_poi_fused__to_copy_arange_clamp_sub_view_3_xnumel )](buf7 ,8 ,8 ,8 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        8 *((s1 *s2 *s3 )//128 )\n        64 *((s1 *s2 *s3 )//128 )\n        buf8 =empty_strided_cuda ((1 ,64 ,8 ,8 *((s1 *s2 *s3 )//128 )),(4096 *((s1 *s2 *s3 )//128 ),64 *((s1 *s2 *s3 )//128 ),8 *((s1 *s2 *s3 )//128 ),1 ),torch .float32 )\n        buf9 =buf8 ;del buf8 \n        buf10 =buf9 ;del buf9 \n\n        triton_poi_fused__to_copy__unsafe_index_add_clamp_gelu_mul_sub_4_xnumel =4096 *((s1 *s2 *s3 )//128 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_clamp_gelu_mul_sub_4 [grid (triton_poi_fused__to_copy__unsafe_index_add_clamp_gelu_mul_sub_4_xnumel )](buf10 ,buf4 ,buf5 ,buf7 ,32 ,8 ,8 ,8 ,256 ,16384 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n        del buf5 \n        del buf7 \n    return (reinterpret_tensor (buf10 ,(1 ,64 ,64 *((s1 *s2 *s3 )//128 )),(4096 *((s1 *s2 *s3 )//128 ),64 *((s1 *s2 *s3 )//128 ),1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =8 \n    arg1_1 =8 \n    arg2_1 =8 \n    arg3_1 =rand_strided ((1 ,64 ,8 ,8 ,8 ),(32768 ,512 ,64 ,8 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "833c02b1-824f-4d0b-acb4-5285de89ccb1",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardswish', 'Softmax2d', 'LogSigmoid']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardswish = nn.Hardswish()\n        self.softmax2d = nn.Softmax2d()\n        self.logsigmoid = nn.LogSigmoid()\n\n    def forward(self, x):\n        # Apply Hardswish activation\n        x = self.hardswish(x)\n        \n        # Reshape the input to have at least 2 spatial dimensions for Softmax2d\n        if x.dim() == 2:\n            x = x.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n        elif x.dim() == 3:\n            x = x.unsqueeze(1)  # Add channel dimension\n        \n        # Apply Softmax2d\n        x = self.softmax2d(x)\n        \n        # Reshape back to the original shape (excluding the added dimensions)\n        if x.dim() == 4:\n            x = x.squeeze(1)  # Remove channel dimension if it was added\n        elif x.dim() == 2:\n            x = x.squeeze(0)  # Remove batch dimension if it was added\n        \n        # Apply LogSigmoid\n        x = self.logsigmoid(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input with batch size 1, 3 channels, 32x32 spatial dimensions\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_hardswish_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(x0 +ks0 *ks1 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =3.0 \n        tmp2 =tmp0 +tmp1 \n        tmp3 =0.0 \n        tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n        tmp5 =6.0 \n        tmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\n        tmp7 =tmp0 *tmp6 \n        tmp8 =1.0 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =triton_helpers .maximum (_tmp11 ,tmp10 )\n        _tmp11 =tl .where (r0_mask &xmask ,tmp12 ,_tmp11 )\n    tmp11 =triton_helpers .max2 (_tmp11 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp11 ,xmask )\n    _tmp28 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp13 =tl .load (in_ptr0 +(x0 +ks0 *ks1 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp14 =3.0 \n        tmp15 =tmp13 +tmp14 \n        tmp16 =0.0 \n        tmp17 =triton_helpers .maximum (tmp15 ,tmp16 )\n        tmp18 =6.0 \n        tmp19 =triton_helpers .minimum (tmp17 ,tmp18 )\n        tmp20 =tmp13 *tmp19 \n        tmp21 =1.0 \n        tmp22 =tmp20 *tmp21 \n        tmp23 =tmp22 -tmp11 \n        tmp24 =0.16666666666666666 \n        tmp25 =tmp23 *tmp24 \n        tmp26 =tl_math .exp (tmp25 )\n        tmp27 =tl .broadcast_to (tmp26 ,[XBLOCK ,R0_BLOCK ])\n        tmp29 =_tmp28 +tmp27 \n        _tmp28 =tl .where (r0_mask &xmask ,tmp29 ,_tmp28 )\n    tmp28 =tl .sum (_tmp28 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp28 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_hardswish_log_sigmoid_forward_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %ks0 )\n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =3.0 \n    tmp2 =tmp0 +tmp1 \n    tmp3 =0.0 \n    tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n    tmp5 =6.0 \n    tmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\n    tmp7 =tmp0 *tmp6 \n    tmp8 =1.0 \n    tmp9 =tmp7 *tmp8 \n    tmp11 =tmp9 -tmp10 \n    tmp12 =0.16666666666666666 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =tl_math .exp (tmp13 )\n    tmp16 =tmp14 /tmp15 \n    tmp17 =triton_helpers .minimum (tmp3 ,tmp16 )\n    tmp18 =tl_math .abs (tmp16 )\n    tmp19 =-tmp18 \n    tmp20 =tl_math .exp (tmp19 )\n    tmp21 =libdevice .log1p (tmp20 )\n    tmp22 =tmp17 -tmp21 \n    tl .store (out_ptr0 +(x2 ),tmp22 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,s1 ,s2 ),(s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,1 ,s1 ,s2 ),(s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_red_fused__softmax_hardswish_0_xnumel =s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_hardswish_0 [grid (triton_red_fused__softmax_hardswish_0_xnumel )](arg3_1 ,buf0 ,buf1 ,32 ,32 ,1024 ,3 ,XBLOCK =128 ,R0_BLOCK =4 ,num_warps =4 ,num_stages =1 )\n        s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__softmax_hardswish_log_sigmoid_forward_1_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__softmax_hardswish_log_sigmoid_forward_1 [grid (triton_poi_fused__softmax_hardswish_log_sigmoid_forward_1_xnumel )](arg3_1 ,buf0 ,buf1 ,buf2 ,1024 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf0 \n        del buf1 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "842aef75-36c7-4f4f-a53a-3cce8cf0c217",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['UpsamplingBilinear2d', 'Hardswish', 'GRU', 'CircularPad2d', 'LazyConvTranspose1d', 'SoftMarginLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.hardswish = nn.Hardswish()\n        self.gru = nn.GRU(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.pad = nn.CircularPad2d(2)\n        self.conv_transpose = nn.LazyConvTranspose1d(out_channels=32, kernel_size=3, stride=2)\n        self.loss = nn.SoftMarginLoss()\n\n    def forward(self, x):\n        # Apply UpsamplingBilinear2d\n        x = self.upsample(x)\n        \n        # Apply Hardswish\n        x = self.hardswish(x)\n        \n        # Reshape for GRU\n        batch_size, channels, height, width = x.size()\n        x = x.view(batch_size, channels * height, width)\n        x = x.permute(0, 2, 1)  # (batch_size, width, channels * height)\n        \n        # Apply GRU\n        x, _ = self.gru(x)\n        \n        # Reshape for CircularPad2d\n        x = x.permute(0, 2, 1)  # (batch_size, hidden_size, width)\n        x = x.unsqueeze(1)  # (batch_size, 1, hidden_size, width)\n        \n        # Apply CircularPad2d\n        x = self.pad(x)\n        \n        # Reshape for LazyConvTranspose1d\n        x = x.squeeze(1)  # (batch_size, hidden_size, width + 2*padding)\n        x = x.permute(0, 2, 1)  # (batch_size, width + 2*padding, hidden_size)\n        \n        # Apply LazyConvTranspose1d\n        x = self.conv_transpose(x)\n        \n        # Reshape for SoftMarginLoss (assuming binary classification)\n        x = x.mean(dim=2)  # (batch_size, out_channels)\n        x = torch.sigmoid(x)  # Apply sigmoid for binary classification\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_hardswish_mul_sub_view_0 (in_out_ptr1 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float64 )\n    tmp3 =tmp0 +tmp2 \n    tmp4 =2.0 \n    tmp5 =tmp1 .to (tl .float32 )\n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp6 .to (tl .float64 )\n    tmp8 =tmp0 +tmp7 \n    tmp9 =tmp3 /tmp8 \n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =x1 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =tmp12 *tmp10 \n    tmp14 =0.0 \n    tmp15 =triton_helpers .maximum (tmp13 ,tmp14 )\n    tmp16 =tmp15 .to (tl .int64 )\n    tmp17 =ks3 \n    tmp18 =tmp17 .to (tl .float64 )\n    tmp19 =tmp0 +tmp18 \n    tmp20 =tmp17 .to (tl .float32 )\n    tmp21 =tmp4 *tmp20 \n    tmp22 =tmp21 .to (tl .float64 )\n    tmp23 =tmp0 +tmp22 \n    tmp24 =tmp19 /tmp23 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =x0 \n    tmp27 =tmp26 .to (tl .float32 )\n    tmp28 =tmp27 *tmp25 \n    tmp29 =triton_helpers .maximum (tmp28 ,tmp14 )\n    tmp30 =tmp29 .to (tl .int64 )\n    tmp31 =tl .load (in_ptr0 +(tmp30 +ks3 *tmp16 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp32 =tl .full ([1 ],1 ,tl .int64 )\n    tmp33 =tmp16 +tmp32 \n    tmp34 =(-1 )+ks0 \n    tmp35 =triton_helpers .minimum (tmp33 ,tmp34 )\n    tmp36 =tl .load (in_ptr0 +(tmp30 +ks3 *tmp35 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp37 =tmp30 +tmp32 \n    tmp38 =(-1 )+ks3 \n    tmp39 =triton_helpers .minimum (tmp37 ,tmp38 )\n    tmp40 =tl .load (in_ptr0 +(tmp39 +ks3 *tmp35 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp41 =tmp40 -tmp36 \n    tmp42 =tl .load (in_ptr0 +(tmp39 +ks3 *tmp16 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =tmp42 -tmp31 \n    tmp44 =tmp30 .to (tl .float32 )\n    tmp45 =tmp29 -tmp44 \n    tmp46 =triton_helpers .maximum (tmp45 ,tmp14 )\n    tmp47 =1.0 \n    tmp48 =triton_helpers .minimum (tmp46 ,tmp47 )\n    tmp49 =tmp41 *tmp48 \n    tmp50 =tmp36 +tmp49 \n    tmp51 =tmp43 *tmp48 \n    tmp52 =tmp31 +tmp51 \n    tmp53 =tmp50 -tmp52 \n    tmp54 =tmp16 .to (tl .float32 )\n    tmp55 =tmp15 -tmp54 \n    tmp56 =triton_helpers .maximum (tmp55 ,tmp14 )\n    tmp57 =triton_helpers .minimum (tmp56 ,tmp47 )\n    tmp58 =tmp53 *tmp57 \n    tmp59 =tmp52 +tmp58 \n    tmp60 =3.0 \n    tmp61 =tmp59 +tmp60 \n    tmp62 =triton_helpers .maximum (tmp61 ,tmp14 )\n    tmp63 =6.0 \n    tmp64 =triton_helpers .minimum (tmp62 ,tmp63 )\n    tmp65 =tmp59 *tmp64 \n    tmp66 =0.16666666666666666 \n    tmp67 =tmp65 *tmp66 \n    tl .store (in_out_ptr1 +(x4 ),tmp67 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 *s2 \n        2 *s1 \n        4 *s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 ,2 *s1 ,2 *s2 ),(4 *s0 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n        buf5 =buf2 ;del buf2 \n        buf6 =buf5 ;del buf5 \n\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_hardswish_mul_sub_view_0_xnumel =4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_hardswish_mul_sub_view_0 [grid (triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_hardswish_mul_sub_view_0_xnumel )](buf6 ,arg3_1 ,32 ,64 ,64 ,32 ,4096 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf6 ,(1 ,2 *s2 ,2 *s0 *s1 ),(4 *s0 *s1 *s2 ,1 ,2 *s2 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "842c5782-19b5-4aaa-9c76-4c7e5cfb6bb0",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FractionalMaxPool2d', 'UpsamplingBilinear2d', 'Hardsigmoid', 'ConstantPad2d', 'GRU', 'LocalResponseNorm', 'L1Loss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.fractional_max_pool = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.upsampling = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.pad = nn.ConstantPad2d(2, 3.0)\n        self.gru = nn.GRU(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n        self.local_response_norm = nn.LocalResponseNorm(size=5)\n        self.l1_loss = nn.L1Loss()\n\n    def forward(self, x):\n        # Apply FractionalMaxPool2d\n        x = self.fractional_max_pool(x)\n        \n        # Apply UpsamplingBilinear2d\n        x = self.upsampling(x)\n        \n        # Apply ConstantPad2d\n        x = self.pad(x)\n        \n        # Reshape for GRU\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # Reshape to (batch_size, seq_len, input_size)\n        \n        # Apply GRU\n        x, _ = self.gru(x)\n        \n        # Reshape back to 4D for LocalResponseNorm\n        x = x.permute(0, 2, 1).view(batch_size, -1, height, width)\n        \n        # Apply LocalResponseNorm\n        x = self.local_response_norm(x)\n        \n        # Apply Hardsigmoid\n        x = self.hardsigmoid(x)\n        \n        # Compute L1Loss with a dummy target\n        dummy_target = torch.zeros_like(x)\n        loss = self.l1_loss(x, dummy_target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 28, 28).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_fractional_max_pool2d_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex //196 \n    x1 =((xindex //14 )%14 )\n    x0 =(xindex %14 )\n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr0 +(1 +2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =((-2 )+ks0 )/13 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =x1 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =tmp4 +tmp0 \n    tmp6 =tmp5 *tmp2 \n    tmp7 =libdevice .floor (tmp6 )\n    tmp8 =tmp0 *tmp2 \n    tmp9 =libdevice .floor (tmp8 )\n    tmp10 =tmp7 -tmp9 \n    tmp11 =tmp10 .to (tl .int64 )\n    tmp12 =tl .full ([1 ],13 ,tl .int64 )\n    tmp13 =tmp4 <tmp12 \n    tmp14 =(-2 )+ks0 \n    tmp15 =tl .where (tmp13 ,tmp11 ,tmp14 )\n    tmp16 =ks0 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =tmp15 <0 \n    tmp19 =tl .where (tmp18 ,tmp17 ,tmp15 )\n    tl .device_assert (((0 <=tmp19 )&(tmp19 <ks0 ))|~(xmask ),\"index out of bounds: 0 <= tmp19 < ks0\")\n    tmp22 =((-2 )+ks1 )/13 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =x0 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 +tmp21 \n    tmp27 =tmp26 *tmp23 \n    tmp28 =libdevice .floor (tmp27 )\n    tmp29 =tmp21 *tmp23 \n    tmp30 =libdevice .floor (tmp29 )\n    tmp31 =tmp28 -tmp30 \n    tmp32 =tmp31 .to (tl .int64 )\n    tmp33 =tmp25 <tmp12 \n    tmp34 =(-2 )+ks1 \n    tmp35 =tl .where (tmp33 ,tmp32 ,tmp34 )\n    tmp36 =ks1 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =tmp35 <0 \n    tmp39 =tl .where (tmp38 ,tmp37 ,tmp35 )\n    tl .device_assert (((0 <=tmp39 )&(tmp39 <ks1 ))|~(xmask ),\"index out of bounds: 0 <= tmp39 < ks1\")\n    tmp41 =tl .load (in_ptr1 +(tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp42 =tl .load (in_ptr1 +(1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =triton_helpers .maximum (tmp42 ,tmp41 )\n    tmp44 =tl .load (in_ptr1 +(ks1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp45 =triton_helpers .maximum (tmp44 ,tmp43 )\n    tmp46 =tl .load (in_ptr1 +(1 +ks1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp47 =triton_helpers .maximum (tmp46 ,tmp45 )\n    tl .store (out_ptr0 +(x4 ),tmp47 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_2 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //28 )%28 )\n    x0 =(xindex %28 )\n    x2 =xindex //784 \n    x4 =xindex \n    tmp0 =x1 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.48148148148148145 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],13 ,tl .int64 )\n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tmp11 =x0 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =tmp12 *tmp2 \n    tmp14 =triton_helpers .maximum (tmp13 ,tmp4 )\n    tmp15 =tmp14 .to (tl .int32 )\n    tmp16 =tl .load (in_ptr0 +(tmp15 +14 *tmp10 +196 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tmp15 +tmp7 \n    tmp18 =triton_helpers .minimum (tmp17 ,tmp9 )\n    tmp19 =tl .load (in_ptr0 +(tmp18 +14 *tmp10 +196 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp20 =tmp19 -tmp16 \n    tmp21 =tmp15 .to (tl .float32 )\n    tmp22 =tmp14 -tmp21 \n    tmp23 =triton_helpers .maximum (tmp22 ,tmp4 )\n    tmp24 =1.0 \n    tmp25 =triton_helpers .minimum (tmp23 ,tmp24 )\n    tmp26 =tmp20 *tmp25 \n    tmp27 =tmp16 +tmp26 \n    tmp28 =tl .load (in_ptr0 +(tmp15 +14 *tmp6 +196 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr0 +(tmp18 +14 *tmp6 +196 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp30 =tmp29 -tmp28 \n    tmp31 =tmp30 *tmp25 \n    tmp32 =tmp28 +tmp31 \n    tmp33 =tmp27 -tmp32 \n    tmp34 =tmp6 .to (tl .float32 )\n    tmp35 =tmp5 -tmp34 \n    tmp36 =triton_helpers .maximum (tmp35 ,tmp4 )\n    tmp37 =triton_helpers .minimum (tmp36 ,tmp24 )\n    tmp38 =tmp33 *tmp37 \n    tmp39 =tmp32 +tmp38 \n    tl .store (in_out_ptr0 +(x4 ),tmp39 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //32 )%32 )\n    x0 =(xindex %32 )\n    x2 =xindex //1024 \n    x4 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],28 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =tmp2 &tmp4 \n    tmp9 =tmp8 &tmp6 \n    tmp10 =tmp9 &tmp7 \n    tmp11 =tl .load (in_ptr0 +((-58 )+x0 +28 *x1 +784 *x2 ),tmp10 &xmask ,other =3.0 )\n    tl .store (out_ptr0 +(x4 ),tmp11 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,2 ),(2 *s0 ,2 ,1 ),torch .float32 )\n\n        triton_poi_fused_rand_0_xnumel =2 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (triton_poi_fused_rand_0_xnumel )](buf0 ,buf1 ,0 ,6 ,XBLOCK =8 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,s0 ,14 ,14 ),(196 *s0 ,196 ,14 ,1 ),torch .float32 )\n\n        triton_poi_fused_fractional_max_pool2d_1_xnumel =196 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_fractional_max_pool2d_1 [grid (triton_poi_fused_fractional_max_pool2d_1_xnumel )](buf1 ,arg3_1 ,buf2 ,28 ,28 ,588 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        buf3 =empty_strided_cuda ((1 ,s0 ,28 ,28 ),(784 *s0 ,784 ,28 ,1 ),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_2_xnumel =784 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_2 [grid (triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_2_xnumel )](buf4 ,buf2 ,2352 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        buf5 =empty_strided_cuda ((1 ,s0 ,32 ,32 ),(1024 *s0 ,1024 ,32 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_3_xnumel =1024 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_3 [grid (triton_poi_fused_constant_pad_nd_3_xnumel )](buf4 ,buf5 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n    return (reinterpret_tensor (buf5 ,(1 ,1024 ,s0 ),(1024 *s0 ,1 ,1024 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =28 \n    arg2_1 =28 \n    arg3_1 =rand_strided ((1 ,3 ,28 ,28 ),(2352 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "843d0b7b-4a02-4fb1-bc4e-2570a506ffdb",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LPPool3d', 'AdaptiveAvgPool1d', 'Linear', 'HuberLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lp_pool3d = nn.LPPool3d(norm_type=2, kernel_size=3, stride=2)\n        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=10)\n        self.linear1 = nn.Linear(10, 50)\n        self.linear2 = nn.Linear(50, 10)\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Apply LPPool3d\n        x = self.lp_pool3d(x)\n        \n        # Reshape to fit AdaptiveAvgPool1d\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions\n        x = self.adaptive_avg_pool1d(x)\n        \n        # Reshape to fit Linear layers\n        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n        x = F.relu(self.linear1(x))\n        x = self.linear2(x)\n        \n        # Compute Huber loss (assuming target is a tensor of zeros for simplicity)\n        target = torch.zeros_like(x)\n        loss = self.huber_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 64, 64, 64).cuda()  # Arbitrary input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_pow_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tmp0 *tmp0 \n    tl .store (out_ptr0 +(x0 ),tmp1 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_mul_pow_relu_sign_1 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tmp1 <tmp0 \n    tmp3 =tmp2 .to (tl .int8 )\n    tmp4 =tmp0 <tmp1 \n    tmp5 =tmp4 .to (tl .int8 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =tmp6 .to (tmp0 .dtype )\n    tmp8 =tl_math .abs (tmp0 )\n    tmp9 =triton_helpers .maximum (tmp1 ,tmp8 )\n    tmp10 =tmp7 *tmp9 \n    tmp11 =27.0 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =libdevice .sqrt (tmp12 )\n    tl .store (in_out_ptr0 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +((triton_helpers .div_floor_integer (x0 ,1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))*(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))+(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))+(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))))*(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))+(triton_helpers .div_floor_integer (x0 ,1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))*(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))+(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))+(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))))*(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))+(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))*(((x0 //(1 +(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))))%(1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 )))))+(triton_helpers .div_floor_integer (x0 ,1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))*(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))+(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))+(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))))*(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))*(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))+(triton_helpers .div_floor_integer (x0 ,1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))*(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))+(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 ))+(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))))+((x0 %(1 +(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 )))))+(((x0 //(1 +(triton_helpers .div_floor_integer ((-3 )+ks1 ,2 ))))%(1 +(triton_helpers .div_floor_integer ((-3 )+ks0 ,2 )))))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_relu_3 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =50 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_huber_loss_4 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =1.0 \n    tmp3 =tmp1 <tmp2 \n    tmp4 =0.5 \n    tmp5 =tmp1 *tmp4 \n    tmp6 =tmp5 *tmp1 \n    tmp7 =tmp1 -tmp4 \n    tmp8 =tmp7 *tmp2 \n    tmp9 =tl .where (tmp3 ,tmp6 ,tmp8 )\n    tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n    tmp12 =tl .where (r0_mask ,tmp10 ,0 )\n    tmp13 =tl .sum (tmp12 ,1 )[:,None ]\n    tmp14 =10.0 \n    tmp15 =tmp13 /tmp14 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp15 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 =args \n    args .clear ()\n    s0 =primals_1 \n    s1 =primals_2 \n    s2 =primals_3 \n    assert_size_stride (primals_4 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    assert_size_stride (primals_5 ,(50 ,10 ),(10 ,1 ))\n    assert_size_stride (primals_6 ,(50 ,),(1 ,))\n    assert_size_stride (primals_7 ,(10 ,50 ),(50 ,1 ))\n    assert_size_stride (primals_8 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_pow_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_pow_0 [grid (triton_poi_fused_pow_0_xnumel )](primals_4 ,buf0 ,262144 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del primals_4 \n\n        buf1 =torch .ops .aten .avg_pool3d .default (buf0 ,[3 ,3 ,3 ],[2 ,2 ,2 ],[0 ,0 ,0 ],False ,True ,None )\n        del buf0 \n        buf2 =buf1 \n        del buf1 \n        buf3 =reinterpret_tensor (buf2 ,(1 ,1 ,1 +(((-3 )+s0 )//2 ),1 +(((-3 )+s1 )//2 ),1 +(((-3 )+s2 )//2 )),(1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 ,1 +(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 +(((-3 )+s2 )//2 ),1 ),0 );del buf2 \n\n        triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel =1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_mul_pow_relu_sign_1 [grid (triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel )](buf3 ,29791 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((1 ,1 ,1 ,1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 )),(1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused__adaptive_avg_pool2d_2_xnumel =1 +(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )*(((-3 )+s1 )//2 )*(((-3 )+s2 )//2 )+(((-3 )+s0 )//2 )+(((-3 )+s1 )//2 )+(((-3 )+s2 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_2 [grid (triton_poi_fused__adaptive_avg_pool2d_2_xnumel )](buf3 ,buf4 ,64 ,64 ,29791 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n\n        buf5 =torch .ops .aten ._adaptive_avg_pool2d .default (buf4 ,[1 ,10 ])\n        del buf4 \n        buf6 =buf5 \n        del buf5 \n        buf7 =empty_strided_cuda ((1 ,50 ),(50 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf6 ,(1 ,10 ),(10 ,1 ),0 ),reinterpret_tensor (primals_5 ,(10 ,50 ),(1 ,10 ),0 ),out =buf7 )\n        del primals_5 \n        buf8 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_relu_3 [grid (50 )](buf8 ,primals_6 ,50 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del primals_6 \n        buf9 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_8 ,buf8 ,reinterpret_tensor (primals_7 ,(50 ,10 ),(1 ,50 ),0 ),alpha =1 ,beta =1 ,out =buf9 )\n        del primals_8 \n        buf10 =empty_strided_cuda ((),(),torch .float32 )\n        buf11 =buf10 ;del buf10 \n\n        get_raw_stream (0 )\n        triton_per_fused_huber_loss_4 [grid (1 )](buf11 ,buf9 ,1 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n    return (buf11 ,reinterpret_tensor (buf6 ,(1 ,10 ),(10 ,1 ),0 ),buf8 ,buf9 ,primals_7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =64 \n    primals_2 =64 \n    primals_3 =64 \n    primals_4 =rand_strided ((1 ,1 ,64 ,64 ,64 ),(262144 ,262144 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((50 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((50 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((10 ,50 ),(50 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "84816763-ebd8-4341-9060-46fb03d40219",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FractionalMaxPool3d', 'CircularPad3d', 'NLLLoss2d', 'Conv3d', 'RReLU', 'GroupNorm', 'PixelUnshuffle', 'ReplicationPad2d', 'UpsamplingNearest2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv3d_1 = nn.Conv3d(1, 10, kernel_size=3)\n        self.circular_pad3d = nn.CircularPad3d(1)\n        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(10, 10, 10))\n        self.rrelu = nn.RReLU()\n        self.group_norm = nn.GroupNorm(2, 10)\n        self.pixel_unshuffle = nn.PixelUnshuffle(2)\n        self.replication_pad2d = nn.ReplicationPad2d(1)\n        self.upsampling_nearest2d = nn.UpsamplingNearest2d(scale_factor=2)\n        self.conv3d_2 = nn.Conv3d(10, 1, kernel_size=3)\n        self.nll_loss2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Assuming input is 5D (batch, channel, depth, height, width)\n        x = self.conv3d_1(x)\n        x = self.circular_pad3d(x)\n        x = self.fractional_max_pool3d(x)\n        x = self.rrelu(x)\n        x = self.group_norm(x)\n        \n        # Reshape to 4D for PixelUnshuffle\n        x = x.view(x.size(0), x.size(1), x.size(2), -1)\n        x = self.pixel_unshuffle(x)\n        \n        # Reshape back to 5D for Conv3d\n        x = x.view(x.size(0), x.size(1), x.size(2), x.size(3), -1)\n        x = self.replication_pad2d(x)\n        x = self.upsampling_nearest2d(x)\n        x = self.conv3d_2(x)\n        \n        # Reshape to 4D for NLLLoss2d\n        x = x.view(x.size(0), x.size(1), x.size(2), -1)\n        x = self.nll_loss2d(x, torch.zeros_like(x).long())\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 16, 16, 16).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =(xindex %16 )\n    x1 =((xindex //16 )%16 )\n    x2 =((xindex //256 )%16 )\n    x3 =xindex //4096 \n    x5 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =14 +x0 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .full ([1 ],15 ,tl .int64 )\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =x1 \n    tmp11 =tl .full ([1 ],1 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .full ([1 ],15 ,tl .int64 )\n    tmp14 =tmp10 <tmp13 \n    tmp15 =tmp12 &tmp14 \n    tmp16 =tmp15 &tmp9 \n    tmp17 =x2 \n    tmp18 =tl .full ([1 ],1 ,tl .int64 )\n    tmp19 =tmp17 >=tmp18 \n    tmp20 =tl .full ([1 ],15 ,tl .int64 )\n    tmp21 =tmp17 <tmp20 \n    tmp22 =tmp19 &tmp21 \n    tmp23 =tmp22 &tmp16 \n    tmp24 =tl .load (in_ptr0 +((-197 )+x0 +14 *x1 +196 *x2 +2744 *x3 ),tmp23 ,other =0.0 )\n    tmp25 =tl .load (in_ptr1 +(x3 ),tmp23 ,eviction_policy ='evict_last',other =0.0 )\n    tmp26 =tmp24 +tmp25 \n    tmp27 =tl .full (tmp26 .shape ,0.0 ,tmp26 .dtype )\n    tmp28 =tl .where (tmp23 ,tmp26 ,tmp27 )\n    tmp29 =tl .load (in_ptr2 +(14 +x5 ),tmp16 ,other =0.0 )\n    tmp30 =tl .where (tmp22 ,tmp28 ,tmp29 )\n    tmp31 =tl .full (tmp30 .shape ,0.0 ,tmp30 .dtype )\n    tmp32 =tl .where (tmp16 ,tmp30 ,tmp31 )\n    tmp33 =tl .load (in_ptr2 +(14 +x5 ),tmp9 ,other =0.0 )\n    tmp34 =tl .where (tmp15 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp9 ,tmp34 ,tmp35 )\n    tmp37 =float (\"nan\")\n    tmp38 =tl .where (tmp8 ,tmp36 ,tmp37 )\n    tmp39 =tl .full (tmp38 .shape ,0.0 ,tmp38 .dtype )\n    tmp40 =tl .where (tmp2 ,tmp38 ,tmp39 )\n    tmp41 =tmp0 >=tmp1 \n    tmp42 =tl .full ([1 ],15 ,tl .int64 )\n    tmp43 =tmp0 <tmp42 \n    tmp44 =tmp41 &tmp43 \n    tmp45 =x1 \n    tmp46 =tl .full ([1 ],1 ,tl .int64 )\n    tmp47 =tmp45 >=tmp46 \n    tmp48 =tl .full ([1 ],15 ,tl .int64 )\n    tmp49 =tmp45 <tmp48 \n    tmp50 =tmp47 &tmp49 \n    tmp51 =tmp50 &tmp44 \n    tmp52 =x2 \n    tmp53 =tl .full ([1 ],1 ,tl .int64 )\n    tmp54 =tmp52 >=tmp53 \n    tmp55 =tl .full ([1 ],15 ,tl .int64 )\n    tmp56 =tmp52 <tmp55 \n    tmp57 =tmp54 &tmp56 \n    tmp58 =tmp57 &tmp51 \n    tmp59 =tl .load (in_ptr0 +((-211 )+x0 +14 *x1 +196 *x2 +2744 *x3 ),tmp58 ,other =0.0 )\n    tmp60 =tl .load (in_ptr1 +(x3 ),tmp58 ,eviction_policy ='evict_last',other =0.0 )\n    tmp61 =tmp59 +tmp60 \n    tmp62 =tl .full (tmp61 .shape ,0.0 ,tmp61 .dtype )\n    tmp63 =tl .where (tmp58 ,tmp61 ,tmp62 )\n    tmp64 =tl .load (in_ptr2 +(x5 ),tmp51 ,other =0.0 )\n    tmp65 =tl .where (tmp57 ,tmp63 ,tmp64 )\n    tmp66 =tl .full (tmp65 .shape ,0.0 ,tmp65 .dtype )\n    tmp67 =tl .where (tmp51 ,tmp65 ,tmp66 )\n    tmp68 =tl .load (in_ptr2 +(x5 ),tmp44 ,other =0.0 )\n    tmp69 =tl .where (tmp50 ,tmp67 ,tmp68 )\n    tmp70 =tl .full (tmp69 .shape ,0.0 ,tmp69 .dtype )\n    tmp71 =tl .where (tmp44 ,tmp69 ,tmp70 )\n    tmp72 =float (\"nan\")\n    tmp73 =tl .where (tmp44 ,tmp71 ,tmp72 )\n    tmp74 =tl .where (tmp2 ,tmp40 ,tmp73 )\n    tl .store (out_ptr0 +(x5 ),tmp74 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x1 =((xindex //16 )%16 )\n    x0 =(xindex %16 )\n    x3 =xindex //16 \n    x4 =xindex \n    tmp40 =tl .load (in_ptr0 +(x4 ),None )\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],15 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(-14 )+x1 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .full ([1 ],15 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(1 +16 *x3 ),tmp10 ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x4 ),tmp6 ,other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =x0 \n    tmp17 =tl .full ([1 ],15 ,tl .int64 )\n    tmp18 =tmp16 >=tmp17 \n    tmp19 =tmp18 &tmp2 \n    tmp20 =tl .load (in_ptr0 +((-223 )+16 *x3 ),tmp19 ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr0 +((-224 )+x4 ),tmp2 ,other =0.0 )\n    tmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\n    tmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],1 ,tl .int64 )\n    tmp27 =tmp0 <tmp26 \n    tmp28 =x0 \n    tmp29 =tl .full ([1 ],15 ,tl .int64 )\n    tmp30 =tmp28 >=tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(225 +16 *x3 ),tmp31 ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(224 +x4 ),tmp27 ,other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =x0 \n    tmp38 =tmp37 >=tmp1 \n    tmp39 =tl .load (in_ptr0 +(1 +16 *x3 ),tmp38 ,eviction_policy ='evict_last',other =0.0 )\n    tmp41 =tl .where (tmp38 ,tmp39 ,tmp40 )\n    tmp42 =tl .where (tmp27 ,tmp36 ,tmp41 )\n    tmp43 =tl .where (tmp2 ,tmp25 ,tmp42 )\n    tl .store (out_ptr0 +(x4 ),tmp43 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_2 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x1 =((xindex //256 )%16 )\n    x0 =(xindex %256 )\n    x2 =xindex //4096 \n    x3 =xindex \n    tmp15 =tl .load (in_ptr0 +(x3 ),None )\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],15 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(-14 )+x1 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =tl .load (in_ptr0 +(3584 +x0 +4096 *x2 ),tmp6 ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =tl .load (in_ptr0 +((-3584 )+x3 ),tmp2 ,other =0.0 )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tmp10 =tl .full (tmp9 .shape ,0.0 ,tmp9 .dtype )\n    tmp11 =tl .where (tmp2 ,tmp9 ,tmp10 )\n    tmp12 =tl .full ([1 ],1 ,tl .int64 )\n    tmp13 =tmp0 <tmp12 \n    tmp14 =tl .load (in_ptr0 +(3584 +x0 +4096 *x2 ),tmp13 ,eviction_policy ='evict_last',other =0.0 )\n    tmp16 =tl .where (tmp13 ,tmp14 ,tmp15 )\n    tmp17 =tl .where (tmp2 ,tmp11 ,tmp16 )\n    tl .store (out_ptr0 +(x3 ),tmp17 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_3 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =30 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_native_group_norm_rrelu_with_noise_functional_4 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =5000 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp7_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp7_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp7_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_out_ptr0 +(r0_1 +5000 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp3 =tl .load (in_ptr0 +(r0_1 +5000 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =0.0 \n        tmp2 =tmp0 <=tmp1 \n        tmp4 =tmp0 *tmp3 \n        tmp5 =tl .where (tmp2 ,tmp4 ,tmp0 )\n        tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n        tmp7_mean_next ,tmp7_m2_next ,tmp7_weight_next =triton_helpers .welford_reduce (\n        tmp6 ,tmp7_mean ,tmp7_m2 ,tmp7_weight ,roffset ==0 \n        )\n        tmp7_mean =tl .where (r0_mask &xmask ,tmp7_mean_next ,tmp7_mean )\n        tmp7_m2 =tl .where (r0_mask &xmask ,tmp7_m2_next ,tmp7_m2 )\n        tmp7_weight =tl .where (r0_mask &xmask ,tmp7_weight_next ,tmp7_weight )\n        tl .store (out_ptr0 +(r0_1 +5000 *x0 ),tmp2 ,r0_mask &xmask )\n        tl .store (in_out_ptr0 +(r0_1 +5000 *x0 ),tmp5 ,r0_mask &xmask )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7_mean ,tmp7_m2 ,tmp7_weight ,1 )\n    tmp7 =tmp10 [:,None ]\n    tmp8 =tmp11 [:,None ]\n    tmp9 =tmp12 [:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp8 ,xmask )\n    tmp13 =5000.0 \n    tmp14 =tmp8 /tmp13 \n    tmp15 =1e-05 \n    tmp16 =tmp14 +tmp15 \n    tmp17 =libdevice .rsqrt (tmp16 )\n    tl .store (out_ptr3 +(x0 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_pixel_unshuffle_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    ynumel =40 \n    xnumel =250 \n    yoffset =tl .program_id (1 )*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x3 =(xindex %50 )\n    x4 =xindex //50 \n    y0 =(yindex %2 )\n    y1 =((yindex //2 )%2 )\n    y2 =yindex //4 \n    x6 =xindex \n    y7 =yindex \n    tmp0 =tl .load (in_ptr0 +(y0 +2 *x3 +100 *y1 +200 *x4 +1000 *y2 ),xmask &ymask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(y2 //5 ),ymask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(y2 //5 ),ymask ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr3 +(y2 ),ymask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr4 +(y2 ),ymask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp4 =5000.0 \n    tmp5 =tmp3 /tmp4 \n    tmp6 =1e-05 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =libdevice .rsqrt (tmp7 )\n    tmp9 =tmp2 *tmp8 \n    tmp11 =tmp9 *tmp10 \n    tmp13 =tmp11 +tmp12 \n    tl .store (out_ptr0 +(x6 +250 *y7 ),tmp13 ,xmask &ymask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(10 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_2 ,(10 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,1 ,16 ,16 ,16 ),(4096 ,4096 ,256 ,16 ,1 ))\n    assert_size_stride (primals_4 ,(10 ,),(1 ,))\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,1 ,1 ),padding =(0 ,0 ,0 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,10 ,14 ,14 ,14 ),(27440 ,2744 ,196 ,14 ,1 ))\n        buf1 =empty_strided_cuda ((1 ,10 ,16 ,16 ,16 ),(40960 ,4096 ,256 ,16 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,10 ,16 ,16 ,16 ),(40960 ,4096 ,256 ,16 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (40960 )](buf0 ,primals_2 ,buf1 ,buf2 ,40960 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        del primals_2 \n        buf3 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_1 [grid (40960 )](buf2 ,buf3 ,40960 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf4 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_2 [grid (40960 )](buf3 ,buf4 ,40960 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        buf5 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf5 )\n        buf6 =empty_strided_cuda ((1 ,10 ,3 ),(30 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_3 [grid (30 )](buf5 ,buf6 ,0 ,30 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del buf5 \n\n        buf7 =torch .ops .aten .fractional_max_pool3d .default (buf4 ,[2 ,2 ,2 ],[10 ,10 ,10 ],buf6 )\n        del buf6 \n        buf8 =buf7 [0 ]\n        buf9 =buf7 [1 ]\n        del buf7 \n\n        buf11 =torch .ops .aten .uniform .default (buf8 ,0.125 ,0.3333333333333333 )\n        buf12 =buf11 \n        del buf11 \n        buf10 =empty_strided_cuda ((1 ,10 ,10 ,10 ,10 ),(10112 ,1000 ,100 ,10 ,1 ),torch .bool )\n        buf13 =reinterpret_tensor (buf8 ,(1 ,10 ,10 ,10 ,10 ),(10016 ,1000 ,100 ,10 ,1 ),0 );del buf8 \n        buf14 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,2 ,2 ),torch .float32 )\n        buf15 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,2 ,2 ),torch .float32 )\n        buf17 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,2 ,2 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused_native_group_norm_rrelu_with_noise_functional_4 [grid (2 )](buf13 ,buf12 ,buf10 ,buf14 ,buf15 ,buf17 ,2 ,5000 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf18 =empty_strided_cuda ((1 ,10 ,2 ,2 ,5 ,50 ),(10000 ,1000 ,500 ,250 ,50 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_pixel_unshuffle_5 [grid (40 ,250 )](buf13 ,buf14 ,buf15 ,primals_4 ,primals_5 ,buf18 ,40 ,250 ,XBLOCK =256 ,YBLOCK =1 ,num_warps =4 ,num_stages =1 )\n        del buf15 \n        del primals_5 \n    return (reinterpret_tensor (buf18 ,(1 ,40 ,5 ,50 ,1 ),(10000 ,250 ,50 ,1 ,1 ),0 ),primals_1 ,primals_3 ,primals_4 ,buf4 ,buf9 ,buf10 ,buf12 ,buf13 ,reinterpret_tensor (buf14 ,(1 ,2 ),(2 ,1 ),0 ),reinterpret_tensor (buf17 ,(1 ,2 ),(2 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((10 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,1 ,16 ,16 ,16 ),(4096 ,4096 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "8671a537-023e-4281-9e42-78b793ecde6c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Unflatten', 'Softmax', 'Hardshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.unflatten1 = nn.Unflatten(1, (1, 28, 28))  # Assuming input is flattened 1D tensor\n        self.hardshrink1 = nn.Hardshrink()\n        self.softmax1 = nn.Softmax(dim=1)\n        self.hardshrink2 = nn.Hardshrink()\n        self.unflatten2 = nn.Unflatten(1, (1, 28, 28))  # Reshape back to original shape\n\n    def forward(self, x):\n        # Assuming input is a flattened 1D tensor\n        x = self.unflatten1(x)  # Unflatten to a 2D shape (e.g., 1x28x28)\n        x = self.hardshrink1(x)  # Apply Hardshrink\n        x = x.view(x.size(0), -1)  # Flatten again for Softmax\n        x = self.softmax1(x)  # Apply Softmax\n        x = self.hardshrink2(x)  # Apply Hardshrink again\n        x = self.unflatten2(x)  # Reshape back to original shape\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 784).cuda()  # Flattened input (e.g., 1x784 for a 28x28 image)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_abs_le_scalar_tensor_where_0 (in_ptr0 ,out_ptr2 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    r0_numel =784 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =0.5 \n    tmp3 =tmp1 <=tmp2 \n    tmp4 =0.0 \n    tmp5 =tl .where (tmp3 ,tmp4 ,tmp0 )\n    tmp6 =tl .broadcast_to (tmp5 ,[R0_BLOCK ])\n    tmp8 =tl .where (r0_mask ,tmp6 ,float (\"-inf\"))\n    tmp9 =triton_helpers .promote_to_tensor (triton_helpers .max2 (tmp8 ,0 ))\n    tmp10 =tmp5 -tmp9 \n    tmp11 =tl_math .exp (tmp10 )\n    tmp12 =tl .broadcast_to (tmp11 ,[R0_BLOCK ])\n    tmp14 =tl .where (r0_mask ,tmp12 ,0 )\n    tmp15 =triton_helpers .promote_to_tensor (tl .sum (tmp14 ,0 ))\n    tmp16 =tmp11 /tmp15 \n    tmp17 =tl_math .abs (tmp16 )\n    tmp18 =tmp17 <=tmp2 \n    tmp19 =tl .where (tmp18 ,tmp4 ,tmp16 )\n    tl .store (out_ptr2 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp19 ,r0_mask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,784 ),(784 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf2 =empty_strided_cuda ((1 ,784 ),(784 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_abs_le_scalar_tensor_where_0 [grid (1 )](arg0_1 ,buf2 ,1 ,784 ,num_warps =8 ,num_stages =1 )\n        del arg0_1 \n    return (reinterpret_tensor (buf2 ,(1 ,1 ,28 ,28 ),(784 ,784 ,28 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,784 ),(784 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "876b05a4-31b5-4c7e-a9b2-6f78f454ee87",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveAvgPool1d', 'RNNBase', 'AvgPool3d', 'CELU', 'Hardsigmoid']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=10)\n        self.rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.avg_pool3d = nn.AvgPool3d(kernel_size=2, stride=2)\n        self.celu = nn.CELU()\n        self.hardsigmoid = nn.Hardsigmoid()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, length)\n        # Reshape to 1D for AdaptiveAvgPool1d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, length)\n        x = self.adaptive_avg_pool1d(x)  # Output shape: (batch_size, channels, 10)\n        \n        # Reshape for RNN\n        x = x.permute(0, 2, 1)  # Reshape to (batch_size, 10, channels)\n        x, _ = self.rnn(x)  # Output shape: (batch_size, 10, 20)\n        \n        # Reshape for AvgPool3d\n        x = x.unsqueeze(1)  # Add a dimension for channels: (batch_size, 1, 10, 20)\n        x = x.unsqueeze(4)  # Add a dimension for depth: (batch_size, 1, 10, 20, 1)\n        x = self.avg_pool3d(x)  # Output shape: (batch_size, 1, 5, 10, 1)\n        \n        # Reshape for CELU\n        x = x.squeeze(4).squeeze(1)  # Remove extra dimensions: (batch_size, 5, 10)\n        x = self.celu(x)  # Output shape: (batch_size, 5, 10)\n        \n        # Reshape for Hardsigmoid\n        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 50)\n        x = self.hardsigmoid(x)  # Output shape: (batch_size, 50)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 100).cuda()  # Example input shape: (batch_size, channels, length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 +10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(3 +10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(4 +10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(5 +10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(6 +10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(7 +10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(8 +10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr0 +(9 +10 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp8 =tmp7 +tmp6 \n    tmp10 =tmp9 +tmp8 \n    tmp12 =tmp11 +tmp10 \n    tmp14 =tmp13 +tmp12 \n    tmp16 =tmp15 +tmp14 \n    tmp18 =tmp17 +tmp16 \n    tmp19 =0.1 \n    tmp20 =tmp18 *tmp19 \n    tl .store (out_ptr0 +(x0 ),tmp20 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,100 ),(100 *s0 ,100 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,1 ,10 ),(10 *s0 ,10 ,10 ,1 ),torch .float32 )\n\n        triton_poi_fused__adaptive_avg_pool2d_0_xnumel =10 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_0 [grid (triton_poi_fused__adaptive_avg_pool2d_0_xnumel )](arg2_1 ,buf0 ,30 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del arg2_1 \n    return (reinterpret_tensor (buf0 ,(1 ,10 ,s0 ),(10 *s0 ,1 ,10 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =100 \n    arg2_1 =rand_strided ((1 ,3 ,100 ),(300 ,100 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "8856d08d-5729-446a-8d81-12e42ee303fa",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['InstanceNorm1d', 'GLU', 'NLLLoss2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.instance_norm1 = nn.InstanceNorm1d(128)\n        self.instance_norm2 = nn.InstanceNorm1d(64)\n        self.glu1 = nn.GLU(dim=1)\n        self.glu2 = nn.GLU(dim=1)\n        self.nll_loss2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, sequence_length)\n        x = self.instance_norm1(x)\n        x = self.glu1(x)\n        x = self.instance_norm2(x)\n        x = self.glu2(x)\n        \n        # Reshape x to match the expected input shape for NLLLoss2d\n        # NLLLoss2d expects input of shape (batch_size, channels, height, width)\n        x = x.view(x.size(0), x.size(1), 1, -1)  # Reshape to (batch_size, channels, 1, sequence_length)\n        \n        # Assuming target is a tensor of class indices of shape (batch_size, 1, sequence_length)\n        target = torch.randint(0, x.size(1), (x.size(0), 1, x.size(3)), device=x.device)\n        \n        # Compute the loss using NLLLoss2d\n        loss = self.nll_loss2d(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 128, 100).cuda()  # Example input shape (batch_size=1, channels=128, sequence_length=100)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +ks0 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp3 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_glu_view_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp1 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr1 +(64 +x0 ),xmask ,eviction_policy ='evict_last')\n    tmp14 =tl .load (in_ptr2 +(64 +x0 ),xmask ,eviction_policy ='evict_last')\n    tmp22_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp22_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp22_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +ks0 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp11 =tl .load (in_ptr0 +(r0_1 +64 *ks0 +ks0 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp4 =ks0 \n        tmp5 =tmp4 .to (tl .float32 )\n        tmp6 =tmp3 /tmp5 \n        tmp7 =1e-05 \n        tmp8 =tmp6 +tmp7 \n        tmp9 =libdevice .rsqrt (tmp8 )\n        tmp10 =tmp2 *tmp9 \n        tmp13 =tmp11 -tmp12 \n        tmp15 =tmp14 /tmp5 \n        tmp16 =tmp15 +tmp7 \n        tmp17 =libdevice .rsqrt (tmp16 )\n        tmp18 =tmp13 *tmp17 \n        tmp19 =tl .sigmoid (tmp18 )\n        tmp20 =tmp10 *tmp19 \n        tmp21 =tl .broadcast_to (tmp20 ,[XBLOCK ,R0_BLOCK ])\n        tmp22_mean_next ,tmp22_m2_next ,tmp22_weight_next =triton_helpers .welford_reduce (\n        tmp21 ,tmp22_mean ,tmp22_m2 ,tmp22_weight ,roffset ==0 \n        )\n        tmp22_mean =tl .where (r0_mask &xmask ,tmp22_mean_next ,tmp22_mean )\n        tmp22_m2 =tl .where (r0_mask &xmask ,tmp22_m2_next ,tmp22_m2 )\n        tmp22_weight =tl .where (r0_mask &xmask ,tmp22_weight_next ,tmp22_weight )\n        tl .store (out_ptr0 +(r0_1 +ks0 *x0 ),tmp20 ,r0_mask &xmask )\n    tmp25 ,tmp26 ,tmp27 =triton_helpers .welford (tmp22_mean ,tmp22_m2 ,tmp22_weight ,1 )\n    tmp22 =tmp25 [:,None ]\n    tmp23 =tmp26 [:,None ]\n    tmp24 =tmp27 [:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp22 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp23 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_nll_loss2d_forward_randint_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,load_seed_offset ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp38 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp42 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int64 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =r0_0 \n        tmp2 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp3 =tl .full ([1 ,1 ],32 ,tl .int64 )\n        tmp4 =triton_helpers .randint64 (tmp0 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp3 )\n        tmp5 =tl .full ([1 ,1 ],-100 ,tl .int64 )\n        tmp6 =tmp4 !=tmp5 \n        tmp7 =tl .where (tmp6 ,tmp4 ,tmp2 )\n        tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],32 ,tl .int32 )\n        tmp9 =tmp7 +tmp8 \n        tmp10 =tmp7 <0 \n        tmp11 =tl .where (tmp10 ,tmp9 ,tmp7 )\n        tl .device_assert (((0 <=tmp11 )&(tmp11 <32 ))|~(r0_mask ),\"index out of bounds: 0 <= tmp11 < 32\")\n        tmp13 =tl .load (in_ptr1 +(r0_0 +ks1 *tmp11 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp14 =tl .load (in_ptr2 +(tmp11 ),r0_mask ,eviction_policy ='evict_last')\n        tmp15 =tmp13 -tmp14 \n        tmp16 =tl .load (in_ptr3 +(tmp11 ),r0_mask ,eviction_policy ='evict_last')\n        tmp17 =ks1 \n        tmp18 =tmp17 .to (tl .float32 )\n        tmp19 =tmp16 /tmp18 \n        tmp20 =1e-05 \n        tmp21 =tmp19 +tmp20 \n        tmp22 =libdevice .rsqrt (tmp21 )\n        tmp23 =tmp15 *tmp22 \n        tmp24 =tl .load (in_ptr1 +(r0_0 +32 *ks1 +ks1 *tmp11 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp25 =tl .load (in_ptr2 +(32 +tmp11 ),r0_mask ,eviction_policy ='evict_last')\n        tmp26 =tmp24 -tmp25 \n        tmp27 =tl .load (in_ptr3 +(32 +tmp11 ),r0_mask ,eviction_policy ='evict_last')\n        tmp28 =tmp27 /tmp18 \n        tmp29 =tmp28 +tmp20 \n        tmp30 =libdevice .rsqrt (tmp29 )\n        tmp31 =tmp26 *tmp30 \n        tmp32 =tl .sigmoid (tmp31 )\n        tmp33 =tmp23 *tmp32 \n        tmp34 =-tmp33 \n        tmp35 =0.0 \n        tmp36 =tl .where (tmp6 ,tmp34 ,tmp35 )\n        tmp37 =tl .broadcast_to (tmp36 ,[XBLOCK ,R0_BLOCK ])\n        tmp39 =_tmp38 +tmp37 \n        _tmp38 =tl .where (r0_mask ,tmp39 ,_tmp38 )\n        tmp40 =tmp6 .to (tl .int64 )\n        tmp41 =tl .broadcast_to (tmp40 ,[XBLOCK ,R0_BLOCK ])\n        tmp43 =_tmp42 +tmp41 \n        _tmp42 =tl .where (r0_mask ,tmp43 ,_tmp42 )\n    tmp38 =tl .sum (_tmp38 ,1 )[:,None ]\n    tmp42 =tl .sum (_tmp42 ,1 )[:,None ]\n    tmp44 =tmp42 .to (tl .float32 )\n    tmp45 =tmp38 /tmp44 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp45 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    assert_size_stride (arg1_1 ,(1 ,128 ,s1 ),(128 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,128 ,1 ),(128 ,1 ,128 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,128 ,1 ),(128 ,1 ,128 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_0 [grid (128 )](arg1_1 ,buf0 ,buf1 ,100 ,128 ,100 ,XBLOCK =1 ,R0_BLOCK =128 ,num_warps =2 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,64 ,s1 ),(64 *s1 ,s1 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,64 ,1 ),(64 ,1 ,64 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,64 ,1 ),(64 ,1 ,64 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_glu_view_1 [grid (64 )](arg1_1 ,buf0 ,buf1 ,buf3 ,buf4 ,buf5 ,100 ,64 ,100 ,XBLOCK =1 ,R0_BLOCK =128 ,num_warps =2 ,num_stages =1 )\n        del arg1_1 \n        del buf0 \n        del buf1 \n        buf7 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf7 )\n        buf9 =empty_strided_cuda ((),(),torch .float32 )\n        buf11 =buf9 ;del buf9 \n\n        get_raw_stream (0 )\n        triton_red_fused_nll_loss2d_forward_randint_2 [grid (1 )](buf11 ,buf7 ,buf3 ,buf4 ,buf5 ,0 ,100 ,1 ,100 ,XBLOCK =1 ,R0_BLOCK =128 ,num_warps =2 ,num_stages =1 )\n        del buf3 \n        del buf4 \n        del buf5 \n        del buf7 \n    return (buf11 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =100 \n    arg1_1 =rand_strided ((1 ,128 ,100 ),(12800 ,100 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "89c968e7-ad02-4bbb-ac3c-fb884575ef55",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MSELoss', 'FractionalMaxPool3d', 'Linear', 'Sequential', 'Conv2d', 'AvgPool1d', 'Softsign', 'ModuleList', 'LSTM', 'Module']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))\n        self.avg_pool1d = nn.AvgPool1d(kernel_size=2, stride=2)\n        self.softsign = nn.Softsign()\n        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.linear1 = nn.Linear(128, 64)\n        self.linear2 = nn.Linear(64, 10)\n        self.module_list = nn.ModuleList([nn.Linear(64, 64) for _ in range(3)])\n        self.sequential = nn.Sequential(\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 16)\n        )\n        self.mseloss = nn.MSELoss()\n\n    def forward(self, x):\n        # Conv2d\n        x = self.conv1(x)\n        \n        # Reshape for FractionalMaxPool3d\n        x = x.unsqueeze(1)  # Add a dummy dimension for 3D pooling\n        x = self.fractional_max_pool3d(x)\n        x = x.squeeze(1)  # Remove the dummy dimension\n        \n        # Reshape for AvgPool1d\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions\n        x = self.avg_pool1d(x)\n        \n        # Softsign\n        x = self.softsign(x)\n        \n        # Reshape for LSTM\n        x = x.permute(0, 2, 1)  # Swap dimensions for LSTM input\n        x, _ = self.lstm(x)\n        \n        # Linear layers\n        x = self.linear1(x[:, -1, :])  # Use the last time step's output\n        x = F.relu(x)\n        \n        # ModuleList\n        for layer in self.module_list:\n            x = layer(x)\n            x = F.relu(x)\n        \n        # Sequential\n        x = self.sequential(x)\n        \n        # Linear layer for final output\n        x = self.linear2(x)\n        \n        # MSELoss (assuming we have a target for demonstration)\n        target = torch.randn_like(x)\n        loss = self.mseloss(x, target)\n        \n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =xindex \n    x1 =xindex //1024 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),None )\n    tmp1 =tl .load (in_ptr0 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_avg_pool2d_div_2 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =256 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp3 =0.5 \n    tmp4 =tmp2 *tmp3 \n    tmp5 =tl_math .abs (tmp4 )\n    tmp6 =1.0 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =tmp4 /tmp7 \n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp8 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(16 ,3 ,3 ,3 ),(27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_2 ,(16 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,1 ),padding =(1 ,1 ),dilation =(1 ,1 ),transposed =False ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,16 ,32 ,32 ),(16384 ,1024 ,32 ,1 ))\n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_0 [grid (16384 )](buf1 ,primals_2 ,16384 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,1 ,3 ),(3 ,3 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_1 [grid (3 )](buf2 ,buf3 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf2 \n\n        buf4 =torch .ops .aten .fractional_max_pool3d .default (reinterpret_tensor (buf1 ,(1 ,1 ,16 ,32 ,32 ),(0 ,0 ,1024 ,32 ,1 ),0 ),[2 ,2 ,2 ],[8 ,8 ,8 ],buf3 )\n        del buf3 \n        buf5 =buf4 [0 ]\n        buf6 =buf4 [1 ]\n        del buf4 \n        buf7 =empty_strided_cuda ((1 ,8 ,1 ,32 ),(256 ,32 ,32 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,8 ,32 ),(256 ,32 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_avg_pool2d_div_2 [grid (256 )](buf5 ,buf7 ,buf8 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n    return (reinterpret_tensor (buf8 ,(1 ,32 ,8 ),(256 ,1 ,32 ),0 ),primals_1 ,primals_3 ,reinterpret_tensor (buf1 ,(1 ,1 ,16 ,32 ,32 ),(16384 ,16384 ,1024 ,32 ,1 ),0 ),buf6 ,reinterpret_tensor (buf5 ,(1 ,8 ,1 ,64 ),(512 ,64 ,64 ,1 ),0 ),buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((16 ,3 ,3 ,3 ),(27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "89f1e7ff-b6e6-4a6b-9678-f6d3817797f5",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['HingeEmbeddingLoss', 'Softmax', 'Bilinear']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.bilinear1 = nn.Bilinear(10, 10, 20)\n        self.bilinear2 = nn.Bilinear(20, 20, 10)\n        self.softmax = nn.Softmax(dim=1)\n        self.hinge_loss = nn.HingeEmbeddingLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, 10)\n        x1 = x.view(-1, 10)\n        x2 = x.view(-1, 10)\n        \n        # Apply Bilinear layers\n        x = self.bilinear1(x1, x2)\n        x = self.bilinear2(x, x)\n        \n        # Apply Softmax\n        x = self.softmax(x)\n        \n        # Dummy target for HingeEmbeddingLoss\n        target = torch.ones_like(x)\n        \n        # Apply HingeEmbeddingLoss\n        loss = self.hinge_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_view_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =20 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_add_clamp_min_fill_mean_ne_sub_where_zeros_like_1 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .where (r0_mask ,tmp3 ,float (\"-inf\"))\n    tmp6 =triton_helpers .max2 (tmp5 ,1 )[:,None ]\n    tmp7 =tmp2 -tmp6 \n    tmp8 =tl_math .exp (tmp7 )\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n    tmp11 =tl .where (r0_mask ,tmp9 ,0 )\n    tmp12 =tl .sum (tmp11 ,1 )[:,None ]\n    tmp13 =tmp8 /tmp12 \n    tmp14 =1.0 \n    tmp15 =tmp14 -tmp13 \n    tmp16 =0.0 \n    tmp17 =triton_helpers .maximum (tmp15 ,tmp16 )\n    tmp18 =tl .full ([1 ,1 ],False ,tl .int1 )\n    tmp19 =tl .where (tmp18 ,tmp17 ,tmp16 )\n    tmp20 =tl .full ([1 ,1 ],True ,tl .int1 )\n    tmp21 =tl .where (tmp20 ,tmp13 ,tmp16 )\n    tmp22 =tmp19 +tmp21 \n    tmp23 =tl .broadcast_to (tmp22 ,[XBLOCK ,R0_BLOCK ])\n    tmp25 =tl .where (r0_mask ,tmp23 ,0 )\n    tmp26 =tl .sum (tmp25 ,1 )[:,None ]\n    tmp27 =10.0 \n    tmp28 =tmp26 /tmp27 \n    tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp13 ,r0_mask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp28 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ),(10 ,1 ))\n    assert_size_stride (primals_2 ,(20 ,10 ,10 ),(100 ,10 ,1 ))\n    assert_size_stride (primals_3 ,(20 ,),(1 ,))\n    assert_size_stride (primals_4 ,(10 ,20 ,20 ),(400 ,20 ,1 ))\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten ._trilinear .default (primals_1 ,primals_2 ,primals_1 ,[1 ,3 ],[0 ],[1 ,2 ],[2 ,3 ])\n        del primals_2 \n        buf1 =buf0 \n        del buf0 \n        buf2 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_view_0 [grid (20 )](buf2 ,primals_3 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del primals_3 \n\n        buf3 =torch .ops .aten ._trilinear .default (buf2 ,primals_4 ,buf2 ,[1 ,3 ],[0 ],[1 ,2 ],[2 ,3 ])\n        buf4 =buf3 \n        del buf3 \n        buf7 =buf4 ;del buf4 \n        buf8 =empty_strided_cuda ((),(),torch .float32 )\n        buf9 =buf8 ;del buf8 \n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_add_clamp_min_fill_mean_ne_sub_where_zeros_like_1 [grid (1 )](buf7 ,buf9 ,primals_5 ,1 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_5 \n    return (buf9 ,primals_4 ,primals_1 ,buf2 ,buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((20 ,10 ,10 ),(100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,20 ,20 ),(400 ,20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "8a55272a-e49d-4c1c-a005-ec129e0135e2",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softsign', 'Unfold', 'MaxPool1d', 'Unflatten']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softsign = nn.Softsign()\n        self.unfold = nn.Unfold(kernel_size=(3, 3), stride=(1, 1))\n        self.maxpool1d = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.unflatten = nn.Unflatten(1, (1, -1))\n\n    def forward(self, x):\n        # Apply Softsign\n        x = self.softsign(x)\n        \n        # Apply Unfold to extract patches\n        x = self.unfold(x)\n        \n        # Reshape for MaxPool1d\n        x = x.view(x.size(0), x.size(1), -1)\n        \n        # Apply MaxPool1d\n        x = self.maxpool1d(x)\n        \n        # Reshape for Unflatten\n        x = x.view(x.size(0), -1)\n        \n        # Apply Unflatten\n        x = self.unflatten(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    yoffset =(tl .program_id (1 )+tl .program_id (2 )*tl .num_programs (1 ))*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x1 =xindex \n    y0 =yindex \n    tl .device_assert (((((x1 //3 )%3 ))+((((2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))<ks0 )|~(xmask &ymask ),\"index out of bounds: (((x1 // 3) % 3)) + ((((2*y0) // ((-2) + ks1)) % ((-2) + ks0))) < ks0\")\n    tl .device_assert ((((x1 %3 ))+(((2 *y0 )%((-2 )+ks1 )))<ks1 )|~(xmask &ymask ),\"index out of bounds: ((x1 % 3)) + (((2*y0) % ((-2) + ks1))) < ks1\")\n    tmp2 =tl .load (in_ptr0 +(ks1 *(((x1 //3 )%3 ))+ks1 *((((2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))+ks0 *ks1 *(x1 //9 )+((x1 %3 ))+(((2 *y0 )%((-2 )+ks1 )))),xmask &ymask ,eviction_policy ='evict_last')\n    tl .device_assert (((((x1 //3 )%3 ))+((((1 +2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))<ks0 )|~(xmask &ymask ),\"index out of bounds: (((x1 // 3) % 3)) + ((((1 + 2*y0) // ((-2) + ks1)) % ((-2) + ks0))) < ks0\")\n    tl .device_assert ((((x1 %3 ))+(((1 +2 *y0 )%((-2 )+ks1 )))<ks1 )|~(xmask &ymask ),\"index out of bounds: ((x1 % 3)) + (((1 + 2*y0) % ((-2) + ks1))) < ks1\")\n    tmp9 =tl .load (in_ptr0 +(ks1 *(((x1 //3 )%3 ))+ks1 *((((1 +2 *y0 )//((-2 )+ks1 ))%((-2 )+ks0 )))+ks0 *ks1 *(x1 //9 )+((x1 %3 ))+(((1 +2 *y0 )%((-2 )+ks1 )))),xmask &ymask ,eviction_policy ='evict_last')\n    tmp3 =tl_math .abs (tmp2 )\n    tmp4 =1.0 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 /tmp5 \n    tmp10 =tl_math .abs (tmp9 )\n    tmp11 =tmp10 +tmp4 \n    tmp12 =tmp9 /tmp11 \n    tmp13 =triton_helpers .maximum (tmp12 ,tmp6 )\n    tl .store (out_ptr0 +(y0 +2 *x1 +x1 *((ks0 *ks1 )//2 )+((-1 )*ks0 *x1 )+((-1 )*ks1 *x1 )),tmp13 ,xmask &ymask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *(triton_helpers .div_floor_integer (x0 ,2 +((-1 )*ks0 )+((-1 )*ks1 )+((ks0 *ks1 )//2 )))+(triton_helpers .div_floor_integer (x0 ,2 +((-1 )*ks0 )+((-1 )*ks1 )+((ks0 *ks1 )//2 )))*((ks0 *ks1 )//2 )+((-1 )*ks0 *(triton_helpers .div_floor_integer (x0 ,2 +((-1 )*ks0 )+((-1 )*ks1 )+((ks0 *ks1 )//2 ))))+((-1 )*ks1 *(triton_helpers .div_floor_integer (x0 ,2 +((-1 )*ks0 )+((-1 )*ks1 )+((ks0 *ks1 )//2 ))))+((x0 %(2 +((-1 )*ks0 )+((-1 )*ks1 )+((ks0 *ks1 )//2 ))))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,9 *s0 ,1 ,2 +((-1 )*s1 )+((-1 )*s2 )+((s1 *s2 )//2 )),(18 *s0 +((-9 )*s0 *s1 )+((-9 )*s0 *s2 )+9 *s0 *((s1 *s2 )//2 ),2 +((-1 )*s1 )+((-1 )*s2 )+((s1 *s2 )//2 ),2 +((-1 )*s1 )+((-1 )*s2 )+((s1 *s2 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_pool2d_with_indices_0_ynumel =2 +((-1 )*s1 )+((-1 )*s2 )+((s1 *s2 )//2 )\n        triton_poi_fused_max_pool2d_with_indices_0_xnumel =9 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_0 [grid (triton_poi_fused_max_pool2d_with_indices_0_ynumel ,triton_poi_fused_max_pool2d_with_indices_0_xnumel )](arg3_1 ,buf0 ,32 ,32 ,450 ,27 ,XBLOCK =1 ,YBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((1 ,1 ,18 *s0 +((-9 )*s0 *s1 )+((-9 )*s0 *s2 )+9 *s0 *((s1 *s2 )//2 )),(18 *s0 +((-9 )*s0 *s1 )+((-9 )*s0 *s2 )+9 *s0 *((s1 *s2 )//2 ),18 *s0 +((-9 )*s0 *s1 )+((-9 )*s0 *s2 )+9 *s0 *((s1 *s2 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_view_1_xnumel =18 *s0 +((-9 )*s0 *s1 )+((-9 )*s0 *s2 )+9 *s0 *((s1 *s2 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_view_1 [grid (triton_poi_fused_view_1_xnumel )](buf0 ,buf1 ,32 ,32 ,12150 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "8bec3772-dce8-4ab0-84de-7f9c2c982f51",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CosineEmbeddingLoss', 'MaxUnpool3d', 'FeatureAlphaDropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.max_unpool3d = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()\n\n    def forward(self, x):\n        # Assuming x is a 5D tensor (batch_size, channels, depth, height, width)\n        # Apply MaxUnpool3d\n        # First, we need to perform a MaxPool3d to get the indices\n        pool_output, indices = F.max_pool3d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool3d(pool_output, indices)\n        \n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Apply CosineEmbeddingLoss\n        # For CosineEmbeddingLoss, we need two input tensors and a target tensor\n        # Here, we use x as the first input and a random tensor as the second input\n        # The target tensor is a tensor of 1s (indicating similarity)\n        random_tensor = torch.randn_like(x)\n        target = torch.ones(x.size(0), dtype=torch.float32).to(x.device)\n        loss = self.cosine_embedding_loss(x.view(x.size(0), -1), random_tensor.view(random_tensor.size(0), -1), target)\n        \n        # Return the loss as the output (though typically, models return the processed tensor)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 16, 16, 16).cuda()  # Example input: 5D tensor\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp8 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp1 =8 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )*(triton_helpers .div_floor_integer (x0 ,(ks0 //2 )*(ks1 //2 )*(ks2 //2 )))\n    tmp2 =tmp0 +tmp1 \n    tmp3 =8 *ks3 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )\n    tmp4 =tmp2 +tmp3 \n    tmp5 =tmp2 <0 \n    tmp6 =tl .where (tmp5 ,tmp4 ,tmp2 )\n    tl .device_assert (((0 <=tmp6 )&(tmp6 <8 *ks3 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )))|~(xmask ),\"index out of bounds: 0 <= tmp6 < 8*ks3*(ks0 // 2)*(ks1 // 2)*(ks2 // 2)\")\n    tl .store (out_ptr0 +(tl .broadcast_to ((tmp6 %(8 *ks3 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 ))),[XBLOCK ])),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_mul_randn_like_sum_3 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp20 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp24 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp28 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp3 =tl .load (in_ptr1 +(2 *(ks4 //2 )*((((2 *(ks4 //2 )*(((r0_1 //(2 *(ks4 //2 )))%(2 *(ks3 //2 ))))+((r0_1 %(2 *(ks4 //2 )))))//(2 *(ks4 //2 )))%(2 *(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*((((2 *(ks4 //2 )*(((r0_1 //(2 *(ks4 //2 )))%(2 *(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*((((r0_1 +4 *ks1 *x0 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 ))//(4 *(ks3 //2 )*(ks4 //2 )))%(2 *(ks2 //2 ))))+((r0_1 %(2 *(ks4 //2 )))))//(4 *(ks3 //2 )*(ks4 //2 )))%(2 *(ks2 //2 ))))+8 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 )*((((2 *(ks4 //2 )*(((r0_1 //(2 *(ks4 //2 )))%(2 *(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*((((r0_1 +4 *ks1 *x0 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 ))//(4 *(ks3 //2 )*(ks4 //2 )))%(2 *(ks2 //2 ))))+8 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 )*((((r0_1 +4 *ks1 *x0 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 ))//(8 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 )))%ks1 ))+((r0_1 %(2 *(ks4 //2 )))))//(8 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 )))%ks1 ))+((((r0_1 %(2 *(ks4 //2 ))))%(2 *(ks4 //2 ))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr2 +((((r0_1 +4 *ks1 *x0 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 ))//(8 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 )))%ks1 )),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =r0_1 +4 *ks1 *x0 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 )\n        tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n        tmp5 =0.5 \n        tmp6 =tmp4 <tmp5 \n        tmp7 =tmp6 .to (tl .float32 )\n        tmp8 =0.8864048946659319 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tmp3 *tmp9 \n        tmp11 =-1.0 \n        tmp12 =tmp7 +tmp11 \n        tmp13 =1.558387861036063 \n        tmp14 =tmp12 *tmp13 \n        tmp15 =0.7791939305180315 \n        tmp16 =tmp14 +tmp15 \n        tmp17 =tmp10 +tmp16 \n        tmp18 =tmp17 *tmp2 \n        tmp19 =tl .broadcast_to (tmp18 ,[XBLOCK ,R0_BLOCK ])\n        tmp21 =_tmp20 +tmp19 \n        _tmp20 =tl .where (r0_mask &xmask ,tmp21 ,_tmp20 )\n        tmp22 =tmp17 *tmp17 \n        tmp23 =tl .broadcast_to (tmp22 ,[XBLOCK ,R0_BLOCK ])\n        tmp25 =_tmp24 +tmp23 \n        _tmp24 =tl .where (r0_mask &xmask ,tmp25 ,_tmp24 )\n        tmp26 =tmp2 *tmp2 \n        tmp27 =tl .broadcast_to (tmp26 ,[XBLOCK ,R0_BLOCK ])\n        tmp29 =_tmp28 +tmp27 \n        _tmp28 =tl .where (r0_mask &xmask ,tmp29 ,_tmp28 )\n    tmp20 =tl .sum (_tmp20 ,1 )[:,None ]\n    tmp24 =tl .sum (_tmp24 ,1 )[:,None ]\n    tmp28 =tl .sum (_tmp28 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp20 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp24 ,xmask )\n    tl .store (out_ptr3 +(x0 ),tmp28 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_clamp_min_div_eq_fill_mean_mul_sqrt_sub_sum_where_zeros_like_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp4 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp8 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .sum (tmp5 ,1 )[:,None ]\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n    tmp11 =tl .sum (tmp9 ,1 )[:,None ]\n    tmp12 =9.999999960041972e-13 \n    tmp13 =tmp7 +tmp12 \n    tmp14 =tmp11 +tmp12 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =libdevice .sqrt (tmp15 )\n    tmp17 =tmp3 /tmp16 \n    tmp18 =1.0 \n    tmp19 =tmp18 -tmp17 \n    tmp20 =tl .full ([1 ,1 ],True ,tl .int1 )\n    tmp21 =0.0 \n    tmp22 =tl .where (tmp20 ,tmp19 ,tmp21 )\n    tmp23 =tmp17 -tmp21 \n    tmp24 =triton_helpers .maximum (tmp23 ,tmp21 )\n    tmp25 =tl .full ([1 ,1 ],False ,tl .int1 )\n    tmp26 =tl .where (tmp25 ,tmp24 ,tmp21 )\n    tmp27 =tmp22 +tmp26 \n    tmp28 =tmp27 /tmp18 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp28 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .max_pool3d_with_indices .default (arg4_1 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del arg4_1 \n        buf1 =buf0 [0 ]\n        buf2 =buf0 [1 ]\n        del buf0 \n        buf3 =empty_strided_cuda ((1 ,s0 ,2 *(s1 //2 ),2 *(s2 //2 ),2 *(s3 //2 )),(8 *s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),8 *(s1 //2 )*(s2 //2 )*(s3 //2 ),4 *(s2 //2 )*(s3 //2 ),2 *(s3 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool3d_0_xnumel =8 *s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_0 [grid (triton_poi_fused_max_unpool3d_0_xnumel )](buf3 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool3d_1_xnumel =s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_1 [grid (triton_poi_fused_max_unpool3d_1_xnumel )](buf2 ,buf1 ,buf3 ,16 ,16 ,16 ,3 ,1536 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        del buf2 \n        buf5 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf5 )\n        buf6 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_2 [grid (s0 )](buf5 ,buf6 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf8 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n        buf10 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n        buf12 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n\n        4 *s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_red_fused_mul_randn_like_sum_3 [grid (2 )](buf5 ,buf3 ,buf6 ,buf8 ,buf10 ,buf12 ,1 ,3 ,16 ,16 ,16 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf3 \n        del buf5 \n        del buf6 \n        buf9 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf14 =reinterpret_tensor (buf9 ,(),(),0 );del buf9 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_clamp_min_div_eq_fill_mean_mul_sqrt_sub_sum_where_zeros_like_4 [grid (1 )](buf14 ,buf8 ,buf10 ,buf12 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf10 \n        del buf12 \n        del buf8 \n    return (buf14 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =16 \n    arg2_1 =16 \n    arg3_1 =16 \n    arg4_1 =rand_strided ((1 ,3 ,16 ,16 ,16 ),(12288 ,4096 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "8ff5df39-2fa7-4630-9a29-e71c7e8381ba",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GELU', 'Upsample', 'Tanh']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.gelu1 = nn.GELU()\n        self.gelu2 = nn.GELU()\n        self.tanh1 = nn.Tanh()\n        self.tanh2 = nn.Tanh()\n\n    def forward(self, x):\n        x = self.upsample1(x)\n        x = self.gelu1(x)\n        x = self.upsample2(x)\n        x = self.tanh1(x)\n        x = self.upsample3(x)\n        x = self.gelu2(x)\n        x = self.tanh2(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__unsafe_index_gelu_tanh_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks4 \n    x3 =xindex \n    tmp0 =4.0 \n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tmp3 .to (tl .float64 )\n    tmp5 =tl .full ([1 ],2.0 ,tl .float64 )\n    tmp6 =tmp5 *tmp4 \n    tmp7 =tmp4 /tmp6 \n    tmp8 =tmp7 .to (tl .float32 )\n    tmp9 =x1 \n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp10 *tmp8 \n    tmp12 =tmp11 .to (tl .int64 )\n    tmp13 =4 *ks0 \n    tmp14 =tmp12 +tmp13 \n    tmp15 =tmp12 <0 \n    tmp16 =tl .where (tmp15 ,tmp14 ,tmp12 )\n    tmp17 =ks3 \n    tmp18 =tmp17 .to (tl .float32 )\n    tmp19 =tmp0 *tmp18 \n    tmp20 =tmp19 .to (tl .float64 )\n    tmp21 =tmp5 *tmp20 \n    tmp22 =tmp20 /tmp21 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =x0 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 *tmp23 \n    tmp27 =tmp26 .to (tl .int64 )\n    tmp28 =4 *ks3 \n    tmp29 =tmp27 +tmp28 \n    tmp30 =tmp27 <0 \n    tmp31 =tl .where (tmp30 ,tmp29 ,tmp27 )\n    tmp32 =2.0 \n    tmp33 =tmp32 *tmp2 \n    tmp34 =tmp33 .to (tl .float64 )\n    tmp35 =tmp5 *tmp34 \n    tmp36 =tmp34 /tmp35 \n    tmp37 =tmp36 .to (tl .float32 )\n    tmp38 =tmp16 \n    tmp39 =tmp38 .to (tl .float32 )\n    tmp40 =tmp39 *tmp37 \n    tmp41 =tmp40 .to (tl .int64 )\n    tmp42 =2 *ks0 \n    tmp43 =tmp41 +tmp42 \n    tmp44 =tmp41 <0 \n    tmp45 =tl .where (tmp44 ,tmp43 ,tmp41 )\n    tmp46 =tmp32 *tmp18 \n    tmp47 =tmp46 .to (tl .float64 )\n    tmp48 =tmp5 *tmp47 \n    tmp49 =tmp47 /tmp48 \n    tmp50 =tmp49 .to (tl .float32 )\n    tmp51 =tmp31 \n    tmp52 =tmp51 .to (tl .float32 )\n    tmp53 =tmp52 *tmp50 \n    tmp54 =tmp53 .to (tl .int64 )\n    tmp55 =2 *ks3 \n    tmp56 =tmp54 +tmp55 \n    tmp57 =tmp54 <0 \n    tmp58 =tl .where (tmp57 ,tmp56 ,tmp54 )\n    tmp59 =tmp1 .to (tl .float64 )\n    tmp60 =tmp5 *tmp59 \n    tmp61 =tmp59 /tmp60 \n    tmp62 =tmp61 .to (tl .float32 )\n    tmp63 =tmp45 \n    tmp64 =tmp63 .to (tl .float32 )\n    tmp65 =tmp64 *tmp62 \n    tmp66 =tmp65 .to (tl .int64 )\n    tmp67 =tmp66 +tmp1 \n    tmp68 =tmp66 <0 \n    tmp69 =tl .where (tmp68 ,tmp67 ,tmp66 )\n    tmp70 =tmp17 .to (tl .float64 )\n    tmp71 =tmp5 *tmp70 \n    tmp72 =tmp70 /tmp71 \n    tmp73 =tmp72 .to (tl .float32 )\n    tmp74 =tmp58 \n    tmp75 =tmp74 .to (tl .float32 )\n    tmp76 =tmp75 *tmp73 \n    tmp77 =tmp76 .to (tl .int64 )\n    tmp78 =tmp77 +tmp17 \n    tmp79 =tmp77 <0 \n    tmp80 =tl .where (tmp79 ,tmp78 ,tmp77 )\n    tmp81 =tl .load (in_ptr0 +(tmp80 +ks3 *tmp69 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp82 =0.5 \n    tmp83 =tmp81 *tmp82 \n    tmp84 =0.7071067811865476 \n    tmp85 =tmp81 *tmp84 \n    tmp86 =libdevice .erf (tmp85 )\n    tmp87 =1.0 \n    tmp88 =tmp86 +tmp87 \n    tmp89 =tmp83 *tmp88 \n    tmp90 =libdevice .tanh (tmp89 )\n    tmp91 =tmp90 *tmp82 \n    tmp92 =tmp90 *tmp84 \n    tmp93 =libdevice .erf (tmp92 )\n    tmp94 =tmp93 +tmp87 \n    tmp95 =tmp91 *tmp94 \n    tmp96 =libdevice .tanh (tmp95 )\n    tl .store (in_out_ptr0 +(x3 ),tmp96 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        8 *s2 \n        8 *s1 \n        64 *s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,8 *s1 ,8 *s2 ),(64 *s0 *s1 *s2 ,64 *s1 *s2 ,8 *s2 ,1 ),torch .float32 )\n        buf1 =buf0 ;del buf0 \n\n        triton_poi_fused__unsafe_index_gelu_tanh_0_xnumel =64 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__unsafe_index_gelu_tanh_0 [grid (triton_poi_fused__unsafe_index_gelu_tanh_0_xnumel )](buf1 ,arg3_1 ,32 ,256 ,256 ,32 ,65536 ,196608 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg3_1 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "912abd39-0715-418e-b87f-59f8376a3eec",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LeakyReLU', 'Dropout1d', 'GRU', 'UpsamplingNearest2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.leaky_relu1 = nn.LeakyReLU(negative_slope=0.1)\n        self.dropout1d1 = nn.Dropout1d(p=0.5)\n        self.gru1 = nn.GRU(input_size=128, hidden_size=256, num_layers=2, batch_first=True)\n        self.upsample1 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.leaky_relu2 = nn.LeakyReLU(negative_slope=0.1)\n        self.dropout1d2 = nn.Dropout1d(p=0.5)\n        self.gru2 = nn.GRU(input_size=256, hidden_size=128, num_layers=1, batch_first=True)\n        self.upsample2 = nn.UpsamplingNearest2d(scale_factor=2)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        \n        # Reshape to (batch_size, channels * height, width) for Dropout1d\n        x = x.view(batch_size, channels * height, width)\n        x = self.dropout1d1(x)\n        \n        # Reshape back to (batch_size, channels, height, width)\n        x = x.view(batch_size, channels, height, width)\n        x = self.leaky_relu1(x)\n        \n        # Reshape to (batch_size, height, width * channels) for GRU\n        x = x.view(batch_size, height, width * channels)\n        x, _ = self.gru1(x)\n        \n        # Reshape to (batch_size, 256, height, 1) for UpsamplingNearest2d\n        x = x.view(batch_size, 256, height, 1)\n        x = self.upsample1(x)\n        \n        # Apply LeakyReLU and Dropout1d again\n        x = x.view(batch_size, 256 * height, 1)\n        x = self.dropout1d2(x)\n        x = x.view(batch_size, 256, height, 1)\n        x = self.leaky_relu2(x)\n        \n        # Reshape to (batch_size, height, 256) for GRU\n        x = x.view(batch_size, height, 256)\n        x, _ = self.gru2(x)\n        \n        # Reshape to (batch_size, 128, height, 1) for UpsamplingNearest2d\n        x = x.view(batch_size, 128, height, 1)\n        x = self.upsample2(x)\n        \n        # Reshape to (batch_size, 128, height * 2, 1)\n        x = x.view(batch_size, 128, height * 2, 1)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_leaky_relu_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =0.5 \n    tmp3 =tmp1 <tmp2 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =2.0 \n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp0 *tmp6 \n    tmp8 =0.0 \n    tmp9 =tmp7 >tmp8 \n    tmp10 =0.1 \n    tmp11 =tmp7 *tmp10 \n    tmp12 =tl .where (tmp9 ,tmp7 ,tmp11 )\n    tl .store (out_ptr0 +(x2 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_leaky_relu_view_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks3 *((((x0 +ks1 *ks3 *x1 )//ks3 )%(ks1 *ks2 )))+((x0 %ks3 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 *s1 ,1 ),(s0 *s1 ,1 ,s0 *s1 ),torch .float32 )\n\n        triton_poi_fused_bernoulli_0_xnumel =s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (triton_poi_fused_bernoulli_0_xnumel )](buf0 ,buf1 ,0 ,96 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_leaky_relu_1_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_leaky_relu_1 [grid (triton_poi_fused_leaky_relu_1_xnumel )](arg3_1 ,buf1 ,buf2 ,32 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        s0 *s2 \n        buf3 =empty_strided_cuda ((1 ,s1 ,s0 *s2 ),(s0 *s1 *s2 ,s0 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_leaky_relu_view_2_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_leaky_relu_view_2 [grid (triton_poi_fused_leaky_relu_view_2_xnumel )](buf2 ,buf3 ,96 ,3 ,32 ,32 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n    return (buf3 ,s1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "92e4778e-a4ba-43a4-953f-894b6681d22e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Sigmoid', 'Upsample', 'Softplus', 'BCEWithLogitsLoss', 'ZeroPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n        self.zero_pad = nn.ZeroPad1d(padding=2)\n        self.sigmoid = nn.Sigmoid()\n        self.softplus = nn.Softplus()\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        # Apply ZeroPad1d to the input\n        x = self.zero_pad(x)\n        \n        # Upsample the input\n        x = self.upsample(x)\n        \n        # Apply Softplus activation\n        x = self.softplus(x)\n        \n        # Apply Sigmoid activation\n        x = self.sigmoid(x)\n        \n        # Compute the loss (assuming a dummy target tensor for demonstration)\n        target = torch.zeros_like(x)\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__unsafe_index_binary_cross_entropy_with_logits_constant_pad_nd_sigmoid_softplus_zeros_like_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp39 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =4.0 \n        tmp1 =ks0 \n        tmp2 =tmp1 .to (tl .float32 )\n        tmp3 =tmp0 +tmp2 \n        tmp4 =tmp3 .to (tl .float64 )\n        tmp5 =tl .full ([1 ,1 ],2.0 ,tl .float64 )\n        tmp6 =tmp5 *tmp4 \n        tmp7 =tmp4 /tmp6 \n        tmp8 =tmp7 .to (tl .float32 )\n        tmp9 =r0_0 \n        tmp10 =tmp9 .to (tl .float32 )\n        tmp11 =tmp10 *tmp8 \n        tmp12 =tmp11 .to (tl .int64 )\n        tmp13 =(-2 )+tmp12 \n        tmp14 =tmp13 .to (tl .int32 )\n        tmp15 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp16 =tmp14 >=tmp15 \n        tmp17 =tmp14 <tmp1 \n        tmp18 =tmp16 &tmp17 \n        tmp19 =tl .load (in_ptr0 +(tl .broadcast_to ((-2 )+tmp12 ,[XBLOCK ,R0_BLOCK ])),r0_mask &tmp18 ,eviction_policy ='evict_last',other =0.0 )\n        tmp20 =1.0 \n        tmp21 =tmp19 *tmp20 \n        tmp22 =20.0 \n        tmp23 =tmp21 >tmp22 \n        tmp24 =tl_math .exp (tmp21 )\n        tmp25 =libdevice .log1p (tmp24 )\n        tmp26 =tmp25 *tmp20 \n        tmp27 =tl .where (tmp23 ,tmp19 ,tmp26 )\n        tmp28 =tl .sigmoid (tmp27 )\n        tmp29 =tmp20 *tmp28 \n        tmp30 =0.0 \n        tmp31 =triton_helpers .minimum (tmp30 ,tmp28 )\n        tmp32 =tl_math .abs (tmp28 )\n        tmp33 =-tmp32 \n        tmp34 =tl_math .exp (tmp33 )\n        tmp35 =libdevice .log1p (tmp34 )\n        tmp36 =tmp31 -tmp35 \n        tmp37 =tmp29 -tmp36 \n        tmp38 =tl .broadcast_to (tmp37 ,[XBLOCK ,R0_BLOCK ])\n        tmp40 =_tmp39 +tmp38 \n        _tmp39 =tl .where (r0_mask ,tmp40 ,_tmp39 )\n    tmp39 =tl .sum (_tmp39 ,1 )[:,None ]\n    tmp41 =8 +2 *ks0 \n    tmp42 =tmp41 .to (tl .float32 )\n    tmp43 =tmp39 /tmp42 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp43 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg1_1 ,(1 ,1 ,s0 ),(s0 ,s0 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((),(),torch .float32 )\n        buf2 =buf1 ;del buf1 \n\n        8 +2 *s0 \n        get_raw_stream (0 )\n        triton_red_fused__unsafe_index_binary_cross_entropy_with_logits_constant_pad_nd_sigmoid_softplus_zeros_like_0 [grid (1 )](buf2 ,arg1_1 ,32 ,1 ,72 ,XBLOCK =1 ,R0_BLOCK =128 ,num_warps =2 ,num_stages =1 )\n        del arg1_1 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =32 \n    arg1_1 =rand_strided ((1 ,1 ,32 ),(32 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "93669d60-969c-4f89-898c-be35dab5adfd",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MarginRankingLoss', 'Tanh', 'SELU', 'InstanceNorm1d', 'TransformerDecoder', 'Upsample', 'LSTM', 'Hardtanh', 'Tanhshrink', 'ZeroPad2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad = nn.ZeroPad2d(2)\n        self.instance_norm = nn.InstanceNorm1d(64)\n        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.transformer_decoder = nn.TransformerDecoder(\n            nn.TransformerDecoderLayer(d_model=128, nhead=8), num_layers=2\n        )\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n        self.tanh = nn.Tanh()\n        self.selu = nn.SELU()\n        self.hardtanh = nn.Hardtanh(min_val=-1.0, max_val=1.0)\n        self.tanhshrink = nn.Tanhshrink()\n        self.margin_ranking_loss = nn.MarginRankingLoss(margin=1.0)\n\n    def forward(self, x):\n        # Assume input x is of shape (batch_size, channels, height, width)\n        x = self.zero_pad(x)  # ZeroPad2d: (batch_size, channels, height+4, width+4)\n        \n        # Reshape for InstanceNorm1d\n        x = x.view(x.size(0), x.size(1), -1)  # (batch_size, channels, (height+4)*(width+4))\n        x = self.instance_norm(x)  # InstanceNorm1d: (batch_size, channels, (height+4)*(width+4))\n        \n        # Reshape for LSTM\n        x = x.permute(0, 2, 1)  # (batch_size, (height+4)*(width+4), channels)\n        x, _ = self.lstm(x)  # LSTM: (batch_size, (height+4)*(width+4), hidden_size)\n        \n        # Reshape for TransformerDecoder\n        x = x.permute(1, 0, 2)  # ((height+4)*(width+4), batch_size, hidden_size)\n        x = self.transformer_decoder(x, x)  # TransformerDecoder: ((height+4)*(width+4), batch_size, hidden_size)\n        \n        # Reshape for Upsample\n        x = x.permute(1, 2, 0)  # (batch_size, hidden_size, (height+4)*(width+4))\n        x = x.view(x.size(0), x.size(1), int(x.size(2)**0.5), int(x.size(2)**0.5))  # (batch_size, hidden_size, height+4, width+4)\n        x = self.upsample(x)  # Upsample: (batch_size, hidden_size, 2*(height+4), 2*(width+4))\n        \n        # Apply activation functions\n        x = self.tanh(x)  # Tanh: (batch_size, hidden_size, 2*(height+4), 2*(width+4))\n        x = self.selu(x)  # SELU: (batch_size, hidden_size, 2*(height+4), 2*(width+4))\n        x = self.hardtanh(x)  # Hardtanh: (batch_size, hidden_size, 2*(height+4), 2*(width+4))\n        x = self.tanhshrink(x)  # Tanhshrink: (batch_size, hidden_size, 2*(height+4), 2*(width+4))\n        \n        # MarginRankingLoss requires two inputs and a target\n        # For simplicity, we'll use the same tensor as both inputs and a dummy target\n        target = torch.ones(x.size(0)).to(x.device)\n        loss = self.margin_ranking_loss(x.view(x.size(0), -1), x.view(x.size(0), -1), target)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tl .store (out_ptr0 +(x4 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_view_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *(x0 //ks1 )+16 *x1 +ks3 *(x0 //ks1 )+4 *ks2 *x1 +4 *ks3 *x1 +ks2 *ks3 *x1 +((x0 %ks1 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg3_1 ,buf0 ,36 ,36 ,32 ,32 ,1296 ,3888 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf1 =empty_strided_cuda ((1 ,s0 ,16 +4 *s1 +4 *s2 +s1 *s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_view_1_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_view_1 [grid (triton_poi_fused_constant_pad_nd_view_1_xnumel )](buf0 ,buf1 ,1296 ,36 ,32 ,32 ,3888 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "94a74330-11f4-4b1a-9ebb-fe6046524f1c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AlphaDropout', 'Mish', 'MultiMarginLoss', 'Softplus']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.alpha_dropout1 = nn.AlphaDropout(p=0.5)\n        self.alpha_dropout2 = nn.AlphaDropout(p=0.5)\n        self.mish1 = nn.Mish()\n        self.mish2 = nn.Mish()\n        self.softplus1 = nn.Softplus()\n        self.softplus2 = nn.Softplus()\n        self.multi_margin_loss = nn.MultiMarginLoss()\n\n    def forward(self, x):\n        # Apply AlphaDropout\n        x = self.alpha_dropout1(x)\n        \n        # Apply Mish activation\n        x = self.mish1(x)\n        \n        # Apply Softplus activation\n        x = self.softplus1(x)\n        \n        # Reshape the input to a 2D tensor for MultiMarginLoss\n        x = x.view(x.size(0), -1)\n        \n        # Apply AlphaDropout again\n        x = self.alpha_dropout2(x)\n        \n        # Apply Mish activation again\n        x = self.mish2(x)\n        \n        # Apply Softplus activation again\n        x = self.softplus2(x)\n        \n        # Generate a target tensor for MultiMarginLoss\n        target = torch.randint(0, x.size(1), (x.size(0),), device=x.device)\n        \n        # Compute the MultiMarginLoss\n        loss = self.multi_margin_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_mul_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr0 ,load_seed_offset ,load_seed_offset1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =tl .load (in_ptr0 +load_seed_offset1 )\n    tmp4 =tl .rand (tmp3 ,(tmp1 ).to (tl .uint32 ))\n    tmp6 =0.5 \n    tmp7 =tmp2 <tmp6 \n    tmp8 =tmp7 .to (tl .float32 )\n    tmp9 =0.8864048946659319 \n    tmp10 =tmp8 *tmp9 \n    tmp11 =tmp5 *tmp10 \n    tmp12 =-1.0 \n    tmp13 =tmp8 +tmp12 \n    tmp14 =1.558387861036063 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =0.7791939305180315 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =tmp11 +tmp17 \n    tmp19 =20.0 \n    tmp20 =tmp18 >tmp19 \n    tmp21 =tl_math .exp (tmp18 )\n    tmp22 =libdevice .log1p (tmp21 )\n    tmp23 =tl .where (tmp20 ,tmp18 ,tmp22 )\n    tmp24 =libdevice .tanh (tmp23 )\n    tmp25 =tmp18 *tmp24 \n    tmp26 =1.0 \n    tmp27 =tmp25 *tmp26 \n    tmp28 =tmp27 >tmp19 \n    tmp29 =tl_math .exp (tmp27 )\n    tmp30 =libdevice .log1p (tmp29 )\n    tmp31 =tmp30 *tmp26 \n    tmp32 =tl .where (tmp28 ,tmp25 ,tmp31 )\n    tmp33 =tmp4 <tmp6 \n    tmp34 =tmp33 .to (tl .float32 )\n    tmp35 =tmp34 *tmp9 \n    tmp36 =tmp32 *tmp35 \n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(x0 ),tmp36 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_gather_mish_mul_softplus_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,load_seed_offset ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .full ([1 ],0 ,tl .int64 )\n    tmp3 =ks1 *ks2 *ks3 \n    tmp4 =triton_helpers .randint64 (tmp0 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp3 )\n    tmp5 =tmp4 +tmp3 \n    tmp6 =tmp4 <0 \n    tmp7 =tl .where (tmp6 ,tmp5 ,tmp4 )\n    tl .device_assert ((0 <=tmp7 )&(tmp7 <ks1 *ks2 *ks3 ),\"index out of bounds: 0 <= tmp7 < ks1*ks2*ks3\")\n    tmp9 =tl .load (in_ptr1 +(tmp7 ),None ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr2 +(tmp7 ),None ,eviction_policy ='evict_last')\n    tmp11 =0.5 \n    tmp12 =tmp10 <tmp11 \n    tmp13 =tmp12 .to (tl .float32 )\n    tmp14 =-1.0 \n    tmp15 =tmp13 +tmp14 \n    tmp16 =1.558387861036063 \n    tmp17 =tmp15 *tmp16 \n    tmp18 =0.7791939305180315 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =tmp9 +tmp19 \n    tmp21 =20.0 \n    tmp22 =tmp20 >tmp21 \n    tmp23 =tl_math .exp (tmp20 )\n    tmp24 =libdevice .log1p (tmp23 )\n    tmp25 =tl .where (tmp22 ,tmp20 ,tmp24 )\n    tmp26 =libdevice .tanh (tmp25 )\n    tmp27 =tmp20 *tmp26 \n    tmp28 =1.0 \n    tmp29 =tmp27 *tmp28 \n    tmp30 =tmp29 >tmp21 \n    tmp31 =tl_math .exp (tmp29 )\n    tmp32 =libdevice .log1p (tmp31 )\n    tmp33 =tmp32 *tmp28 \n    tmp34 =tl .where (tmp30 ,tmp27 ,tmp33 )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp34 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_clamp_min_mish_mul_rsub_softplus_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp4 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp5 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp2 =1.0 \n    tmp3 =tmp2 -tmp1 \n    tmp6 =0.5 \n    tmp7 =tmp5 <tmp6 \n    tmp8 =tmp7 .to (tl .float32 )\n    tmp9 =-1.0 \n    tmp10 =tmp8 +tmp9 \n    tmp11 =1.558387861036063 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =0.7791939305180315 \n    tmp14 =tmp12 +tmp13 \n    tmp15 =tmp4 +tmp14 \n    tmp16 =20.0 \n    tmp17 =tmp15 >tmp16 \n    tmp18 =tl_math .exp (tmp15 )\n    tmp19 =libdevice .log1p (tmp18 )\n    tmp20 =tl .where (tmp17 ,tmp15 ,tmp19 )\n    tmp21 =libdevice .tanh (tmp20 )\n    tmp22 =tmp15 *tmp21 \n    tmp23 =tmp22 *tmp2 \n    tmp24 =tmp23 >tmp16 \n    tmp25 =tl_math .exp (tmp23 )\n    tmp26 =libdevice .log1p (tmp25 )\n    tmp27 =tmp26 *tmp2 \n    tmp28 =tl .where (tmp24 ,tmp22 ,tmp27 )\n    tmp29 =tmp3 +tmp28 \n    tmp30 =0.0 \n    tmp31 =triton_helpers .maximum (tmp29 ,tmp30 )\n    tl .store (in_out_ptr0 +(x0 ),tmp31 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_arange_mean_ne_scalar_tensor_where_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp16 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )\n        tmp1 =ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +load_seed_offset )\n        tmp4 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp5 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp6 =tl .broadcast_to (ks0 *ks1 *ks2 ,[XBLOCK ,R0_BLOCK ])\n        tmp7 =triton_helpers .randint64 (tmp3 ,(tmp4 ).to (tl .uint32 ),tmp5 ,tmp6 )\n        tmp8 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )\n        tmp9 =tmp8 !=tmp7 \n        tmp10 =tl .load (in_ptr1 +(r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp11 =0.0 \n        tmp12 =tl .where (tmp9 ,tmp10 ,tmp11 )\n        tmp13 =tl .full (tmp12 .shape ,0 ,tmp12 .dtype )\n        tmp14 =tl .where (tmp2 ,tmp12 ,tmp13 )\n        tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n        tmp17 =_tmp16 +tmp15 \n        _tmp16 =tl .where (r0_mask &xmask ,tmp17 ,_tmp16 )\n    tmp16 =tl .sum (_tmp16 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp16 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_arange_mean_ne_scalar_tensor_where_4 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =ks0 *ks1 *ks2 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp6 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((3 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[3 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,s0 *s1 *s2 ),(s0 *s1 *s2 ,1 ),torch .float32 )\n        buf3 =reinterpret_tensor (buf1 ,(1 ,s0 *s1 *s2 ),(s0 *s1 *s2 ,1 ),0 );del buf1 \n\n        triton_poi_fused__to_copy_bernoulli_mul_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_mul_0 [grid (triton_poi_fused__to_copy_bernoulli_mul_0_xnumel )](buf3 ,buf0 ,arg3_1 ,buf2 ,0 ,1 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf4 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_gather_mish_mul_softplus_1 [grid (1 )](buf0 ,buf3 ,buf2 ,buf4 ,2 ,3 ,64 ,64 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        buf5 =buf3 ;del buf3 \n\n        triton_poi_fused__to_copy_add_bernoulli_clamp_min_mish_mul_rsub_softplus_2_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_clamp_min_mish_mul_rsub_softplus_2 [grid (triton_poi_fused__to_copy_add_bernoulli_clamp_min_mish_mul_rsub_softplus_2_xnumel )](buf5 ,buf4 ,buf2 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        buf6 =empty_strided_cuda ((2 ,),(1 ,),torch .float32 )\n\n        (1 +s0 *s1 *s2 )//2 \n        get_raw_stream (0 )\n        triton_red_fused_arange_mean_ne_scalar_tensor_where_3 [grid (2 )](buf0 ,buf5 ,buf6 ,3 ,64 ,64 ,2 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf0 \n        del buf5 \n        buf7 =reinterpret_tensor (buf4 ,(),(),0 );del buf4 \n        buf8 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_per_fused_arange_mean_ne_scalar_tensor_where_4 [grid (1 )](buf8 ,buf6 ,3 ,64 ,64 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf6 \n    return (buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "95c4f468-b673-4b1e-b2a7-09b647ace21f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ZeroPad1d', 'Tanh', 'LSTM', 'AvgPool2d', 'GroupNorm', 'ZeroPad3d', 'Transformer']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad1d = nn.ZeroPad1d(2)\n        self.tanh = nn.Tanh()\n        self.lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.avg_pool2d = nn.AvgPool2d(kernel_size=2)\n        self.group_norm = nn.GroupNorm(num_groups=2, num_channels=20)\n        self.zero_pad3d = nn.ZeroPad3d(1)\n        self.transformer = nn.Transformer(d_model=20, nhead=4, num_encoder_layers=2, num_decoder_layers=2)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, sequence_length)\n        x = self.zero_pad1d(x)  # Shape: (batch_size, channels, sequence_length + 4)\n        x = self.tanh(x)  # Shape: (batch_size, channels, sequence_length + 4)\n        \n        # Reshape for LSTM\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, sequence_length + 4, channels)\n        x, _ = self.lstm(x)  # Shape: (batch_size, sequence_length + 4, hidden_size)\n        \n        # Reshape for AvgPool2d\n        x = x.unsqueeze(1)  # Shape: (batch_size, 1, sequence_length + 4, hidden_size)\n        x = self.avg_pool2d(x)  # Shape: (batch_size, 1, (sequence_length + 4)/2, hidden_size/2)\n        \n        # GroupNorm\n        x = x.squeeze(1)  # Shape: (batch_size, (sequence_length + 4)/2, hidden_size/2)\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, hidden_size/2, (sequence_length + 4)/2)\n        x = self.group_norm(x)  # Shape: (batch_size, hidden_size/2, (sequence_length + 4)/2)\n        \n        # ZeroPad3d\n        x = x.unsqueeze(1)  # Shape: (batch_size, 1, hidden_size/2, (sequence_length + 4)/2)\n        x = self.zero_pad3d(x)  # Shape: (batch_size, 1, hidden_size/2 + 2, (sequence_length + 4)/2 + 2)\n        \n        # Transformer\n        x = x.squeeze(1)  # Shape: (batch_size, hidden_size/2 + 2, (sequence_length + 4)/2 + 2)\n        x = x.permute(2, 0, 1)  # Shape: ((sequence_length + 4)/2 + 2, batch_size, hidden_size/2 + 2)\n        x = self.transformer(x, x)  # Shape: ((sequence_length + 4)/2 + 2, batch_size, hidden_size/2 + 2)\n        \n        # Reshape to output\n        x = x.permute(1, 2, 0)  # Shape: (batch_size, hidden_size/2 + 2, (sequence_length + 4)/2 + 2)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 64).cuda()  # Example input shape: (batch_size, channels, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_tanh_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =(-2 )+x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *x1 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =libdevice .tanh (tmp6 )\n    tl .store (out_ptr0 +(x2 ),tmp7 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s1 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ),(4 *s0 +s0 *s1 ,4 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_tanh_0_xnumel =4 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_tanh_0 [grid (triton_poi_fused_constant_pad_nd_tanh_0_xnumel )](arg2_1 ,buf0 ,68 ,64 ,680 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n    return (reinterpret_tensor (buf0 ,(1 ,4 +s1 ,s0 ),(4 *s0 +s0 *s1 ,1 ,4 +s1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,10 ,64 ),(640 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "96ae1b85-9b60-44ff-9a12-1648e75b14eb",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['NLLLoss', 'ELU', 'RNN', 'Sequential', 'ConvTranspose2d', 'Bilinear', 'Fold', 'L1Loss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.elu = nn.ELU()\n        self.rnn = nn.RNN(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.sequential = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.ELU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n            nn.ELU()\n        )\n        self.bilinear = nn.Bilinear(32, 32, 16)\n        self.fold = nn.Fold(output_size=(32, 32), kernel_size=(4, 4), stride=(2, 2))\n        self.l1_loss = nn.L1Loss()\n        self.nll_loss = nn.NLLLoss()\n\n    def forward(self, x):\n        # Apply ELU activation\n        x = self.elu(x)\n        \n        # Reshape for RNN\n        batch_size, channels, height, width = x.size()\n        x = x.view(batch_size, -1, 64)  # Reshape to (batch_size, seq_len, input_size)\n        \n        # Pass through RNN\n        x, _ = self.rnn(x)\n        \n        # Reshape back to 4D tensor for ConvTranspose2d\n        x = x.view(batch_size, 128, height // 2, width // 2)\n        \n        # Pass through Sequential with ConvTranspose2d and ELU\n        x = self.sequential(x)\n        \n        # Reshape for Bilinear\n        x = x.view(batch_size, 32, -1)\n        x = self.bilinear(x, x)\n        \n        # Reshape for Fold\n        x = x.view(batch_size, -1, 32 * 32)\n        x = self.fold(x)\n        \n        # Compute L1 Loss with a dummy target\n        dummy_target = torch.zeros_like(x)\n        l1_loss_value = self.l1_loss(x, dummy_target)\n        \n        # Compute NLL Loss with a dummy target\n        log_probs = F.log_softmax(x.view(batch_size, -1), dim=1)\n        dummy_target_class = torch.zeros(batch_size, dtype=torch.long).to(x.device)\n        nll_loss_value = self.nll_loss(log_probs, dummy_target_class)\n        \n        # Return both losses\n        return l1_loss_value, nll_loss_value\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_elu_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =1.0 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .expm1 (tmp4 )\n    tmp6 =tmp5 *tmp3 \n    tmp7 =tl .where (tmp2 ,tmp4 ,tmp6 )\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ,64 ),(64 *s0 *s1 ,64 *s1 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 ,64 ),(64 *s0 *s1 ,64 *s1 ,64 ,1 ),torch .float32 )\n\n        triton_poi_fused_elu_0_xnumel =64 *s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_elu_0 [grid (triton_poi_fused_elu_0_xnumel )](arg2_1 ,buf0 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n    return (reinterpret_tensor (buf0 ,(1 ,s0 *s1 ,64 ),(64 *s0 *s1 ,64 ,1 ),0 ),s1 ,64 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9b167c78-e2c6-43c7-8c1a-5c233ed510c9",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxUnpool2d', 'InstanceNorm3d', 'TripletMarginWithDistanceLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.instance_norm = nn.InstanceNorm3d(num_features=10)\n        self.loss_fn = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n\n    def forward(self, x):\n        # Assuming x is a 4D tensor (batch, channels, height, width)\n        # First, apply MaxUnpool2d\n        # For MaxUnpool2d, we need indices from a previous MaxPool2d operation\n        # Since we don't have a MaxPool2d in the module list, we'll simulate it\n        pool_output, indices = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool(pool_output, indices)\n\n        # Reshape x to 5D tensor (batch, channels, depth, height, width) for InstanceNorm3d\n        # We'll add a dummy depth dimension\n        x = x.unsqueeze(2)  # Adding a depth dimension\n        x = self.instance_norm(x)\n\n        # For TripletMarginWithDistanceLoss, we need three inputs: anchor, positive, negative\n        # We'll split the output into three parts\n        batch_size = x.size(0)\n        anchor = x[:batch_size // 3]\n        positive = x[batch_size // 3:2 * batch_size // 3]\n        negative = x[2 * batch_size // 3:]\n\n        # Compute the triplet loss\n        loss = self.loss_fn(anchor, positive, negative)\n\n        # Return the loss as the output\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(12, 10, 32, 32).cuda()  # Batch size 12 to allow splitting into 3 parts\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_max_unpool2d_1 (in_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp35 =tl .load (in_ptr0 +(2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp36 =tl .load (in_ptr0 +(1 +2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp38 =tl .load (in_ptr0 +(ks4 +2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp40 =tl .load (in_ptr0 +(1 +ks4 +2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp8 =tmp7 >tmp6 \n    tmp9 =tl .full ([1 ],2 ,tl .int8 )\n    tmp10 =tl .where (tmp8 ,tmp9 ,tmp5 )\n    tmp11 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp13 =tmp12 >tmp11 \n    tmp14 =tl .full ([1 ],3 ,tl .int8 )\n    tmp15 =tl .where (tmp13 ,tmp14 ,tmp10 )\n    triton_helpers .maximum (tmp12 ,tmp11 )\n    tmp17 =tl .full ([1 ],2 ,tl .int32 )\n    tmp18 =tl .where ((tmp15 <0 )!=(tmp17 <0 ),tl .where (tmp15 %tmp17 !=0 ,tmp15 //tmp17 -1 ,tmp15 //tmp17 ),tmp15 //tmp17 )\n    tmp19 =tmp18 *tmp17 \n    tmp20 =tmp15 -tmp19 \n    tmp21 =2 *x1 \n    tmp22 =tmp21 +tmp18 \n    tmp23 =2 *x0 \n    tmp24 =tmp23 +tmp20 \n    tmp25 =ks4 \n    tmp26 =tmp22 *tmp25 \n    tmp27 =tmp26 +tmp24 \n    tmp28 =4 *ks0 *ks1 *x2 \n    tmp29 =tmp27 +tmp28 \n    tmp30 =40 *ks0 *ks1 *ks5 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =tmp29 <0 \n    tmp33 =tl .where (tmp32 ,tmp31 ,tmp29 )\n    tl .device_assert (((0 <=tmp33 )&(tmp33 <40 *ks5 *(ks3 //2 )*(ks4 //2 )))|~(xmask ),\"index out of bounds: 0 <= tmp33 < 40*ks5*(ks3 // 2)*(ks4 // 2)\")\n    tmp37 =triton_helpers .maximum (tmp36 ,tmp35 )\n    tmp39 =triton_helpers .maximum (tmp38 ,tmp37 )\n    tmp41 =triton_helpers .maximum (tmp40 ,tmp39 )\n    tl .store (out_ptr1 +(tl .broadcast_to ((tmp33 %(40 *ks0 *ks1 *ks5 )),[XBLOCK ])),tmp41 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_2 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp10_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp10_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp10_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =(r0_index %ks0 )\n        r0_2 =r0_index //ks0 \n        tmp0 =tl .load (in_ptr0 +(r0_1 +2 *ks1 *((((r0_1 +2 *ks1 *r0_2 )//(2 *ks1 ))%(2 *ks2 )))+4 *ks1 *ks2 *((((r0_1 +2 *ks1 *r0_2 +4 *ks1 *ks2 *((x0 %10 )))//(4 *ks1 *ks2 ))%10 ))+40 *ks1 *ks2 *((((r0_1 +2 *ks1 *r0_2 +4 *ks1 *ks2 *((x0 %10 ))+40 *ks1 *ks2 *(x0 //10 ))//(40 *ks1 *ks2 ))%ks3 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp8 =tl .load (in_ptr0 +(r0_1 +2 *ks1 *((((r0_1 +2 *ks1 *r0_2 )//ks0 )%(2 *ks2 )))+4 *ks1 *ks2 *((((r0_1 +2 *ks1 *r0_2 +4 *ks1 *ks2 *((x0 %10 )))//(4 *ks1 *ks2 ))%10 ))+40 *ks1 *ks2 *((((r0_1 +2 *ks1 *r0_2 +4 *ks1 *ks2 *((x0 %10 ))+40 *ks1 *ks2 *(x0 //10 ))//(40 *ks1 *ks2 ))%ks3 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n        tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n        tmp10_mean_next ,tmp10_m2_next ,tmp10_weight_next =triton_helpers .welford_reduce (\n        tmp9 ,tmp10_mean ,tmp10_m2 ,tmp10_weight ,roffset ==0 \n        )\n        tmp10_mean =tl .where (r0_mask &xmask ,tmp10_mean_next ,tmp10_mean )\n        tmp10_m2 =tl .where (r0_mask &xmask ,tmp10_m2_next ,tmp10_m2 )\n        tmp10_weight =tl .where (r0_mask &xmask ,tmp10_weight_next ,tmp10_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tmp13 ,tmp14 ,tmp15 =triton_helpers .welford (tmp10_mean ,tmp10_m2 ,tmp10_weight ,1 )\n    tmp10 =tmp13 [:,None ]\n    tmp11 =tmp14 [:,None ]\n    tmp12 =tmp15 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_norm_sub_3 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%10 )\n    x2 =xindex //ks1 \n    x4 =xindex //ks0 \n    tmp1 =tl .load (in_ptr1 +(x4 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x4 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr1 +(x4 +10 *(ks5 //3 )),xmask ,eviction_policy ='evict_last')\n    tmp14 =tl .load (in_ptr2 +(x4 +10 *(ks5 //3 )),xmask ,eviction_policy ='evict_last')\n    _tmp24 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    x5 =xindex \n    tmp30 =tl .load (in_ptr1 +(x4 +10 *((2 *ks5 )//3 )),xmask ,eviction_policy ='evict_last')\n    tmp32 =tl .load (in_ptr2 +(x4 +10 *((2 *ks5 )//3 )),xmask ,eviction_policy ='evict_last')\n    _tmp41 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_3 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_3 +2 *ks2 *((((r0_3 +2 *ks2 *x0 )//ks4 )%(2 *ks3 )))+4 *ks2 *ks3 *((((r0_3 +2 *ks2 *x0 +4 *ks2 *ks3 *x1 )//(4 *ks2 *ks3 ))%10 ))+40 *ks2 *ks3 *((((r0_3 +2 *ks2 *x0 +4 *ks2 *ks3 *x1 +40 *ks2 *ks3 *((x2 %ks5 )))//(40 *ks2 *ks3 ))%ks5 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp11 =tl .load (in_ptr0 +(r0_3 +2 *ks2 *((((r0_3 +2 *ks2 *x0 )//ks4 )%ks0 ))+4 *ks2 *ks3 *((((r0_3 +2 *ks2 *x0 +4 *ks2 *ks3 *x1 )//(4 *ks2 *ks3 ))%10 ))+40 *ks2 *ks3 *((((r0_3 +2 *ks2 *x0 +4 *ks2 *ks3 *x1 +40 *ks2 *ks3 *(((x2 +(ks5 //3 ))%ks5 )))//(40 *ks2 *ks3 ))%ks5 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp26 =tl .load (in_ptr0 +(r0_3 +2 *ks2 *((((r0_3 +2 *ks2 *x0 )//ks4 )%ks0 ))+4 *ks2 *ks3 *((((r0_3 +2 *ks2 *x0 +4 *ks2 *ks3 *x1 )//(4 *ks2 *ks3 ))%10 ))+40 *ks2 *ks3 *((((r0_3 +2 *ks2 *x0 +4 *ks2 *ks3 *x1 +40 *ks2 *ks3 *((x2 %ks5 )))//(40 *ks2 *ks3 ))%ks5 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp29 =tl .load (in_ptr0 +(r0_3 +2 *ks2 *((((r0_3 +2 *ks2 *x0 )//ks4 )%ks0 ))+4 *ks2 *ks3 *((((r0_3 +2 *ks2 *x0 +4 *ks2 *ks3 *x1 )//(4 *ks2 *ks3 ))%10 ))+40 *ks2 *ks3 *((((r0_3 +2 *ks2 *x0 +4 *ks2 *ks3 *x1 +40 *ks2 *ks3 *(((x2 +((2 *ks5 )//3 ))%ks5 )))//(40 *ks2 *ks3 ))%ks5 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp4 =4 *ks2 *ks3 \n        tmp5 =tmp4 .to (tl .float32 )\n        tmp6 =tmp3 /tmp5 \n        tmp7 =1e-05 \n        tmp8 =tmp6 +tmp7 \n        tmp9 =libdevice .rsqrt (tmp8 )\n        tmp10 =tmp2 *tmp9 \n        tmp13 =tmp11 -tmp12 \n        tmp15 =tmp14 /tmp5 \n        tmp16 =tmp15 +tmp7 \n        tmp17 =libdevice .rsqrt (tmp16 )\n        tmp18 =tmp13 *tmp17 \n        tmp19 =tmp10 -tmp18 \n        tmp20 =1e-06 \n        tmp21 =tmp19 +tmp20 \n        tmp22 =tmp21 *tmp21 \n        tmp23 =tl .broadcast_to (tmp22 ,[XBLOCK ,R0_BLOCK ])\n        tmp25 =_tmp24 +tmp23 \n        _tmp24 =tl .where (r0_mask &xmask ,tmp25 ,_tmp24 )\n        tmp27 =tmp26 -tmp1 \n        tmp28 =tmp27 *tmp9 \n        tmp31 =tmp29 -tmp30 \n        tmp33 =tmp32 /tmp5 \n        tmp34 =tmp33 +tmp7 \n        tmp35 =libdevice .rsqrt (tmp34 )\n        tmp36 =tmp31 *tmp35 \n        tmp37 =tmp28 -tmp36 \n        tmp38 =tmp37 +tmp20 \n        tmp39 =tmp38 *tmp38 \n        tmp40 =tl .broadcast_to (tmp39 ,[XBLOCK ,R0_BLOCK ])\n        tmp42 =_tmp41 +tmp40 \n        _tmp41 =tl .where (r0_mask &xmask ,tmp42 ,_tmp41 )\n    tmp24 =tl .sum (_tmp24 ,1 )[:,None ]\n    tmp41 =tl .sum (_tmp41 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x5 ),tmp24 ,xmask )\n    tl .store (out_ptr1 +(x5 ),tmp41 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_mean_norm_sub_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp10 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =libdevice .sqrt (tmp0 )\n        tmp2 =1.0 \n        tmp3 =tmp1 +tmp2 \n        tmp5 =libdevice .sqrt (tmp4 )\n        tmp6 =tmp3 -tmp5 \n        tmp7 =0.0 \n        tmp8 =triton_helpers .maximum (tmp6 ,tmp7 )\n        tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n        tmp11 =_tmp10 +tmp9 \n        _tmp10 =tl .where (r0_mask ,tmp11 ,_tmp10 )\n    tmp10 =tl .sum (_tmp10 ,1 )[:,None ]\n    tmp12 =20 *ks0 *(ks1 //3 )\n    tmp13 =tmp12 .to (tl .float32 )\n    tmp14 =tmp10 /tmp13 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp14 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s2 =arg1_1 \n    s3 =arg2_1 \n    assert_size_stride (arg3_1 ,(s0 ,10 ,s2 ,s3 ),(10 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((s0 ,10 ,2 *(s2 //2 ),2 *(s3 //2 )),(40 *(s2 //2 )*(s3 //2 ),4 *(s2 //2 )*(s3 //2 ),2 *(s3 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_0_xnumel =40 *s0 *(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_0 [grid (triton_poi_fused_max_unpool2d_0_xnumel )](buf1 ,122880 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        s3 //2 \n        s2 //2 \n        (s2 //2 )*(s3 //2 )\n\n        triton_poi_fused_max_pool2d_with_indices_max_unpool2d_1_xnumel =10 *s0 *(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_max_unpool2d_1 [grid (triton_poi_fused_max_pool2d_with_indices_max_unpool2d_1_xnumel )](arg3_1 ,buf1 ,16 ,16 ,256 ,32 ,32 ,12 ,30720 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        2 *(s3 //2 )\n        buf3 =empty_strided_cuda ((1 ,10 *s0 ,1 ,1 ,1 ),(10 *s0 ,1 ,10 *s0 ,10 *s0 ,10 *s0 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,10 *s0 ,1 ,1 ,1 ),(10 *s0 ,1 ,10 *s0 ,10 *s0 ,10 *s0 ),torch .float32 )\n\n        triton_red_fused__native_batch_norm_legit_2_xnumel =10 *s0 \n        4 *(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_2 [grid (triton_red_fused__native_batch_norm_legit_2_xnumel )](buf1 ,buf3 ,buf4 ,32 ,16 ,16 ,12 ,120 ,1024 ,XBLOCK =1 ,R0_BLOCK =1024 ,num_warps =8 ,num_stages =1 )\n        2 *(s2 //2 )\n        20 *(s2 //2 )\n        buf6 =empty_strided_cuda ((s0 //3 ,10 ,1 ,2 *(s2 //2 )),(20 *(s2 //2 ),2 *(s2 //2 ),20 *(s0 //3 )*(s2 //2 ),1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((s0 //3 ,10 ,1 ,2 *(s2 //2 )),(20 *(s2 //2 ),2 *(s2 //2 ),20 *(s0 //3 )*(s2 //2 ),1 ),torch .float32 )\n\n        triton_red_fused_add_norm_sub_3_xnumel =20 *(s0 //3 )*(s2 //2 )\n        2 *(s3 //2 )\n        get_raw_stream (0 )\n        triton_red_fused_add_norm_sub_3 [grid (triton_red_fused_add_norm_sub_3_xnumel )](buf1 ,buf3 ,buf4 ,buf6 ,buf7 ,32 ,320 ,16 ,16 ,32 ,12 ,1280 ,32 ,XBLOCK =8 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n        del buf3 \n        del buf4 \n        buf8 =empty_strided_cuda ((),(),torch .float32 )\n        buf9 =buf8 ;del buf8 \n\n        20 *(s0 //3 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_mean_norm_sub_4 [grid (1 )](buf9 ,buf6 ,buf7 ,16 ,12 ,1 ,1280 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf6 \n        del buf7 \n    return (buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =12 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((12 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9bbac7b3-5c9a-4095-861f-0d0776199a74",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MSELoss', 'BCEWithLogitsLoss', 'MaxUnpool1d', 'FractionalMaxPool2d', 'LSTM']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)\n        self.fractional_max_pool2d = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.lstm = nn.LSTM(input_size=14, hidden_size=32, num_layers=2, batch_first=True)\n        self.fc = nn.Linear(32, 1)\n        self.mseloss = nn.MSELoss()\n        self.bce_with_logits_loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        \n        # Apply FractionalMaxPool2d\n        x = self.fractional_max_pool2d(x)\n        \n        # Reshape for LSTM\n        x = x.view(batch_size, channels * x.size(2), x.size(3))  # (batch_size, seq_len, feature_size)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Take the last time step output\n        x = x[:, -1, :]\n        \n        # Fully connected layer\n        x = self.fc(x)\n        \n        # Generate a dummy target for loss calculation\n        dummy_target = torch.randn_like(x)\n        \n        # Calculate MSELoss\n        mse_loss = self.mseloss(x, dummy_target)\n        \n        # Calculate BCEWithLogitsLoss\n        bce_loss = self.bce_with_logits_loss(x, torch.sigmoid(dummy_target))\n        \n        # Return both losses for demonstration purposes\n        return mse_loss, bce_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 28, 28).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_fractional_max_pool2d_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex //196 \n    x1 =((xindex //14 )%14 )\n    x0 =(xindex %14 )\n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr0 +(1 +2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =((-2 )+ks0 )/13 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =x1 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =tmp4 +tmp0 \n    tmp6 =tmp5 *tmp2 \n    tmp7 =libdevice .floor (tmp6 )\n    tmp8 =tmp0 *tmp2 \n    tmp9 =libdevice .floor (tmp8 )\n    tmp10 =tmp7 -tmp9 \n    tmp11 =tmp10 .to (tl .int64 )\n    tmp12 =tl .full ([1 ],13 ,tl .int64 )\n    tmp13 =tmp4 <tmp12 \n    tmp14 =(-2 )+ks0 \n    tmp15 =tl .where (tmp13 ,tmp11 ,tmp14 )\n    tmp16 =ks0 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =tmp15 <0 \n    tmp19 =tl .where (tmp18 ,tmp17 ,tmp15 )\n    tl .device_assert (((0 <=tmp19 )&(tmp19 <ks0 ))|~(xmask ),\"index out of bounds: 0 <= tmp19 < ks0\")\n    tmp22 =((-2 )+ks1 )/13 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =x0 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 +tmp21 \n    tmp27 =tmp26 *tmp23 \n    tmp28 =libdevice .floor (tmp27 )\n    tmp29 =tmp21 *tmp23 \n    tmp30 =libdevice .floor (tmp29 )\n    tmp31 =tmp28 -tmp30 \n    tmp32 =tmp31 .to (tl .int64 )\n    tmp33 =tmp25 <tmp12 \n    tmp34 =(-2 )+ks1 \n    tmp35 =tl .where (tmp33 ,tmp32 ,tmp34 )\n    tmp36 =ks1 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =tmp35 <0 \n    tmp39 =tl .where (tmp38 ,tmp37 ,tmp35 )\n    tl .device_assert (((0 <=tmp39 )&(tmp39 <ks1 ))|~(xmask ),\"index out of bounds: 0 <= tmp39 < ks1\")\n    tmp41 =tl .load (in_ptr1 +(tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp42 =tl .load (in_ptr1 +(1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =triton_helpers .maximum (tmp42 ,tmp41 )\n    tmp44 =tl .load (in_ptr1 +(ks1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp45 =triton_helpers .maximum (tmp44 ,tmp43 )\n    tmp46 =tl .load (in_ptr1 +(1 +ks1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp47 =triton_helpers .maximum (tmp46 ,tmp45 )\n    tl .store (out_ptr0 +(x4 ),tmp47 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,2 ),(2 *s0 ,2 ,1 ),torch .float32 )\n\n        triton_poi_fused_rand_0_xnumel =2 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (triton_poi_fused_rand_0_xnumel )](buf0 ,buf1 ,0 ,6 ,XBLOCK =8 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,s0 ,14 ,14 ),(196 *s0 ,196 ,14 ,1 ),torch .float32 )\n\n        triton_poi_fused_fractional_max_pool2d_1_xnumel =196 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_fractional_max_pool2d_1 [grid (triton_poi_fused_fractional_max_pool2d_1_xnumel )](buf1 ,arg3_1 ,buf2 ,28 ,28 ,588 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n    return (reinterpret_tensor (buf2 ,(1 ,14 *s0 ,14 ),(196 *s0 ,14 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =28 \n    arg2_1 =28 \n    arg3_1 =rand_strided ((1 ,3 ,28 ,28 ),(2352 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9be299c3-3732-4ab9-b250-5ba6737c651c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['SELU', 'ModuleDict', 'CELU', 'Hardsigmoid']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.module_dict = nn.ModuleDict({\n            'selu1': nn.SELU(),\n            'celu1': nn.CELU(),\n            'hardsigmoid1': nn.Hardsigmoid(),\n            'selu2': nn.SELU(),\n            'celu2': nn.CELU(),\n        })\n\n    def forward(self, x):\n        # Apply SELU\n        x = self.module_dict['selu1'](x)\n        \n        # Apply CELU\n        x = self.module_dict['celu1'](x)\n        \n        # Apply Hardsigmoid\n        x = self.module_dict['hardsigmoid1'](x)\n        \n        # Apply SELU again\n        x = self.module_dict['selu2'](x)\n        \n        # Apply CELU again\n        x = self.module_dict['celu2'](x)\n        \n        # Flatten the output if necessary\n        x = x.view(x.size(0), -1)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_celu_elu_hardsigmoid_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =1.0507009873554805 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =1.0 \n    tmp6 =tmp0 *tmp5 \n    tmp7 =libdevice .expm1 (tmp6 )\n    tmp8 =1.7580993408473766 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =tl .where (tmp2 ,tmp4 ,tmp9 )\n    tmp11 =tmp10 >tmp1 \n    tmp12 =libdevice .expm1 (tmp10 )\n    tmp13 =tl .where (tmp11 ,tmp10 ,tmp12 )\n    tmp14 =3.0 \n    tmp15 =tmp13 +tmp14 \n    tmp16 =triton_helpers .maximum (tmp15 ,tmp1 )\n    tmp17 =6.0 \n    tmp18 =triton_helpers .minimum (tmp16 ,tmp17 )\n    tmp19 =0.16666666666666666 \n    tmp20 =tmp18 *tmp19 \n    tmp21 =tmp20 >tmp1 \n    tmp22 =tmp20 *tmp3 \n    tmp23 =tmp20 *tmp5 \n    tmp24 =libdevice .expm1 (tmp23 )\n    tmp25 =tmp24 *tmp8 \n    tmp26 =tl .where (tmp21 ,tmp22 ,tmp25 )\n    tmp27 =tmp26 >tmp1 \n    tmp28 =libdevice .expm1 (tmp26 )\n    tmp29 =tl .where (tmp27 ,tmp26 ,tmp28 )\n    tl .store (out_ptr0 +(x0 ),tmp29 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_celu_elu_hardsigmoid_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_celu_elu_hardsigmoid_0 [grid (triton_poi_fused_celu_elu_hardsigmoid_0_xnumel )](arg3_1 ,buf0 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,s0 *s1 *s2 ),(s0 *s1 *s2 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9c9e790b-2915-48c0-9acd-2e41b252c2b0",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AvgPool1d', 'RReLU', 'AdaptiveMaxPool3d', 'Unfold', 'Hardshrink', 'ConstantPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.avg_pool1d = nn.AvgPool1d(kernel_size=2)\n        self.rrelu = nn.RReLU()\n        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d(output_size=(4, 4, 4))\n        self.unfold = nn.Unfold(kernel_size=(2, 2))\n        self.hardshrink = nn.Hardshrink()\n        self.constant_pad1d = nn.ConstantPad1d(padding=2, value=0)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, length)\n        x = self.constant_pad1d(x)  # Apply padding to ensure compatibility with AvgPool1d\n        x = self.avg_pool1d(x)  # Apply AvgPool1d\n        x = self.rrelu(x)  # Apply RReLU\n        \n        # Reshape to 3D for AdaptiveMaxPool3d\n        x = x.unsqueeze(2).unsqueeze(3)  # Add two dimensions to make it 5D (batch_size, channels, 1, 1, length)\n        x = self.adaptive_max_pool3d(x)  # Apply AdaptiveMaxPool3d\n        \n        # Reshape back to 2D for Unfold\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten the last three dimensions\n        x = self.unfold(x)  # Apply Unfold\n        \n        x = self.hardshrink(x)  # Apply Hardshrink\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64).cuda()  # Example input shape (batch_size=1, channels=3, length=64)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =102 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %34 )\n    x1 =xindex //34 \n    x2 =xindex \n    tmp0 =(-2 )+2 *x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],64 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-2 )+2 *x0 +64 *x1 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =(-1 )+2 *x0 \n    tmp8 =tmp7 >=tmp1 \n    tmp9 =tmp7 <tmp3 \n    tmp10 =tmp8 &tmp9 \n    tmp11 =tl .load (in_ptr0 +((-1 )+2 *x0 +64 *x1 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tmp11 +tmp6 \n    tmp13 =0.5 \n    tmp14 =tmp12 *tmp13 \n    tl .store (out_ptr0 +(x2 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rrelu_with_noise_functional_1 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =102 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =tmp0 <=tmp1 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =tl .where (tmp2 ,tmp4 ,tmp0 )\n    tl .store (in_out_ptr0 +(x0 ),tmp5 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_le_scalar_tensor_where_2 (in_ptr0 ,out_ptr0 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    ynumel =4 \n    xnumel =126 \n    yoffset =tl .program_id (1 )*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x1 =xindex \n    y0 =yindex \n    tmp0 =tl .load (in_ptr0 +(64 *(x1 //63 )+64 *(y0 //2 )+((x1 %63 ))+((y0 %2 ))),xmask &ymask ,eviction_policy ='evict_last')\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =0.5 \n    tmp3 =tmp1 <=tmp2 \n    tmp4 =0.0 \n    tmp5 =tl .where (tmp3 ,tmp4 ,tmp0 )\n    tl .store (out_ptr0 +(x1 +126 *y0 ),tmp5 ,xmask &ymask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,3 ,64 ),(192 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,1 ,34 ),(102 ,34 ,34 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_0 [grid (102 )](arg0_1 ,buf0 ,102 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n\n        buf1 =torch .ops .aten .uniform .default (reinterpret_tensor (buf0 ,(1 ,3 ,34 ),(0 ,34 ,1 ),0 ),0.125 ,0.3333333333333333 )\n        buf2 =buf1 \n        del buf1 \n        buf3 =reinterpret_tensor (buf0 ,(1 ,3 ,34 ),(102 ,34 ,1 ),0 );del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_rrelu_with_noise_functional_1 [grid (102 )](buf3 ,buf2 ,102 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n\n        buf4 =torch .ops .aten .adaptive_max_pool3d .default (reinterpret_tensor (buf3 ,(1 ,3 ,1 ,1 ,34 ),(0 ,34 ,0 ,0 ,1 ),0 ),[4 ,4 ,4 ])\n        del buf3 \n        buf5 =buf4 [0 ]\n        del buf4 \n        buf7 =empty_strided_cuda ((4 ,126 ),(126 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_le_scalar_tensor_where_2 [grid (4 ,126 )](buf5 ,buf7 ,4 ,126 ,XBLOCK =32 ,YBLOCK =4 ,num_warps =4 ,num_stages =1 )\n        del buf5 \n    return (buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,3 ,64 ),(192 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9cace9cd-4b8c-41a3-94f2-b61c7c745750",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveAvgPool1d', 'ConstantPad1d', 'SoftMarginLoss', 'Dropout1d', 'LazyBatchNorm3d', 'Hardswish']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=64)\n        self.constant_pad1d = nn.ConstantPad1d(padding=2, value=0)\n        self.dropout1d = nn.Dropout1d(p=0.5)\n        self.lazy_batch_norm3d = nn.LazyBatchNorm3d()\n        self.hardswish = nn.Hardswish()\n        self.soft_margin_loss = nn.SoftMarginLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, length)\n        x = self.adaptive_avg_pool1d(x)  # Shape: (batch_size, channels, 64)\n        x = self.constant_pad1d(x)       # Shape: (batch_size, channels, 68)\n        x = self.dropout1d(x)            # Shape: (batch_size, channels, 68)\n        \n        # Reshape to 3D for LazyBatchNorm3d\n        x = x.unsqueeze(2).unsqueeze(3)  # Shape: (batch_size, channels, 1, 1, 68)\n        x = self.lazy_batch_norm3d(x)    # Shape: (batch_size, channels, 1, 1, 68)\n        \n        # Reshape back to 1D\n        x = x.squeeze(3).squeeze(2)      # Shape: (batch_size, channels, 68)\n        x = self.hardswish(x)            # Shape: (batch_size, channels, 68)\n        \n        # Dummy target for SoftMarginLoss\n        target = torch.ones_like(x)      # Shape: (batch_size, channels, 68)\n        loss = self.soft_margin_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 128).cuda()  # Example input shape: (batch_size=1, channels=3, length=128)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_functional_bernoulli_constant_pad_nd_0 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,out_ptr9 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    r0_numel =68 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n    tmp23_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp23_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp23_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp5 =(-2 )+r0_1 \n        tmp6 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp7 =tmp5 >=tmp6 \n        tmp8 =tl .full ([1 ,1 ],64 ,tl .int64 )\n        tmp9 =tmp5 <tmp8 \n        tmp10 =tmp7 &tmp9 \n        tmp11 =tl .load (in_ptr1 +((-4 )+2 *r0_1 +128 *x0 ),r0_mask &tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp12 =tl .load (in_ptr1 +((-3 )+2 *r0_1 +128 *x0 ),r0_mask &tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp13 =tmp12 +tmp11 \n        tmp14 =0.5 \n        tmp15 =tmp13 *tmp14 \n        tmp16 =tl .full (tmp15 .shape ,0.0 ,tmp15 .dtype )\n        tmp17 =tl .where (tmp10 ,tmp15 ,tmp16 )\n        tmp18 =tmp4 .to (tl .float32 )\n        tmp19 =2.0 \n        tmp20 =tmp18 *tmp19 \n        tmp21 =tmp17 *tmp20 \n        tmp22 =tl .broadcast_to (tmp21 ,[XBLOCK ,R0_BLOCK ])\n        tmp23_mean_next ,tmp23_m2_next ,tmp23_weight_next =triton_helpers .welford_reduce (\n        tmp22 ,tmp23_mean ,tmp23_m2 ,tmp23_weight ,roffset ==0 \n        )\n        tmp23_mean =tl .where (r0_mask &xmask ,tmp23_mean_next ,tmp23_mean )\n        tmp23_m2 =tl .where (r0_mask &xmask ,tmp23_m2_next ,tmp23_m2 )\n        tmp23_weight =tl .where (r0_mask &xmask ,tmp23_weight_next ,tmp23_weight )\n        tl .store (out_ptr2 +(r0_1 +68 *x0 ),tmp17 ,r0_mask &xmask )\n    tmp26 ,tmp27 ,tmp28 =triton_helpers .welford (tmp23_mean ,tmp23_m2 ,tmp23_weight ,1 )\n    tmp23 =tmp26 [:,None ]\n    tmp24 =tmp27 [:,None ]\n    tmp25 =tmp28 [:,None ]\n    tl .store (out_ptr3 +(x0 ),tmp23 ,xmask )\n    tmp38 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =68.0 \n    tmp30 =tmp24 /tmp29 \n    tmp31 =1e-05 \n    tmp32 =tmp30 +tmp31 \n    tmp33 =libdevice .rsqrt (tmp32 )\n    tmp34 =1.0149253731343284 \n    tmp35 =tmp30 *tmp34 \n    tmp36 =0.1 \n    tmp37 =tmp35 *tmp36 \n    tmp39 =0.9 \n    tmp40 =tmp38 *tmp39 \n    tmp41 =tmp37 +tmp40 \n    tmp42 =tmp23 *tmp36 \n    tmp44 =tmp43 *tmp39 \n    tmp45 =tmp42 +tmp44 \n    tl .store (out_ptr5 +(x0 ),tmp33 ,xmask )\n    tl .store (out_ptr7 +(x0 ),tmp41 ,xmask )\n    tl .store (out_ptr9 +(x0 ),tmp45 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_hardswish_soft_margin_loss_soft_margin_loss_backward_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =204 \n    R0_BLOCK :tl .constexpr =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_2 =r0_index \n    r0_1 =r0_index //68 \n    tmp0 =tl .load (in_ptr0 +(r0_2 ),r0_mask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 ).to (tl .int1 )\n    tmp6 =tl .load (in_ptr2 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =tl .load (in_ptr3 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp10 =tl .load (in_ptr4 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr5 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =2.0 \n    tmp4 =tmp2 *tmp3 \n    tmp5 =tmp0 *tmp4 \n    tmp7 =tmp5 -tmp6 \n    tmp9 =tmp7 *tmp8 \n    tmp11 =tmp9 *tmp10 \n    tmp13 =tmp11 +tmp12 \n    tmp14 =3.0 \n    tmp15 =tmp13 +tmp14 \n    tmp16 =0.0 \n    tmp17 =triton_helpers .maximum (tmp15 ,tmp16 )\n    tmp18 =6.0 \n    tmp19 =triton_helpers .minimum (tmp17 ,tmp18 )\n    tmp20 =tmp13 *tmp19 \n    tmp21 =0.16666666666666666 \n    tmp22 =tmp20 *tmp21 \n    tmp23 =-tmp22 \n    tmp24 =tl_math .exp (tmp23 )\n    tmp25 =libdevice .log1p (tmp24 )\n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (r0_mask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =204.0 \n    tmp31 =tmp29 /tmp30 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp31 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_2 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .full ([1 ],1 ,tl .int64 )\n    tmp3 =tmp1 +tmp2 \n    tl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 =args \n    args .clear ()\n    assert_size_stride (primals_2 ,(1 ,3 ,128 ),(384 ,128 ,1 ))\n    assert_size_stride (primals_3 ,(),())\n    assert_size_stride (primals_4 ,(3 ,),(1 ,))\n    assert_size_stride (primals_5 ,(3 ,),(1 ,))\n    assert_size_stride (primals_6 ,(3 ,),(1 ,))\n    assert_size_stride (primals_7 ,(3 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf1 )\n        buf3 =empty_strided_cuda ((1 ,3 ,1 ),(3 ,1 ,1 ),torch .bool )\n        buf0 =empty_strided_cuda ((1 ,3 ,68 ),(204 ,68 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,3 ,1 ,1 ,1 ),(3 ,1 ,1 ,1 ,1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,3 ,1 ,1 ,1 ),(3 ,1 ,1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_functional_bernoulli_constant_pad_nd_0 [grid (3 )](buf1 ,primals_2 ,primals_5 ,primals_4 ,buf3 ,buf0 ,buf4 ,buf7 ,primals_5 ,primals_4 ,0 ,3 ,68 ,XBLOCK =1 ,R0_BLOCK =128 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n        del primals_2 \n        del primals_4 \n        del primals_5 \n        buf9 =empty_strided_cuda ((),(),torch .float32 )\n        buf18 =buf9 ;del buf9 \n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_hardswish_soft_margin_loss_soft_margin_loss_backward_1 [grid (1 )](buf18 ,buf0 ,buf3 ,buf4 ,buf7 ,primals_6 ,primals_7 ,1 ,204 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_2 [grid (1 )](primals_3 ,primals_3 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_3 \n    return (buf18 ,primals_6 ,primals_7 ,buf0 ,buf3 ,buf4 ,buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =128 \n    primals_2 =rand_strided ((1 ,3 ,128 ),(384 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_4 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9d385012-a570-490c-a2a9-d6d473e09394",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['NLLLoss', 'TransformerEncoder', 'LazyLinear', 'ReLU', 'ZeroPad1d', 'InstanceNorm1d', 'HuberLoss', 'Unfold', 'AlphaDropout', 'MaxUnpool3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad = nn.ZeroPad1d(2)\n        self.instance_norm = nn.InstanceNorm1d(64)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=64, nhead=8), num_layers=3\n        )\n        self.unfold = nn.Unfold(kernel_size=(3, 3), padding=1)\n        self.max_unpool = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.lazy_linear1 = nn.LazyLinear(128)\n        self.lazy_linear2 = nn.LazyLinear(64)\n        self.relu = nn.ReLU()\n        self.alpha_dropout = nn.AlphaDropout(p=0.5)\n        self.nll_loss = nn.NLLLoss()\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Assume input x is of shape (batch_size, channels, height, width)\n        x = self.zero_pad(x)  # Apply ZeroPad1d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape for InstanceNorm1d\n        x = self.instance_norm(x)  # Apply InstanceNorm1d\n        x = x.view(x.size(0), x.size(1), int(x.size(2) ** 0.5), int(x.size(2) ** 0.5))  # Reshape back\n        x = self.unfold(x)  # Apply Unfold\n        x = x.view(x.size(0), x.size(1), int(x.size(2) ** 0.5), int(x.size(2) ** 0.5))  # Reshape for MaxUnpool3d\n        x = x.unsqueeze(2)  # Add a dimension for MaxUnpool3d\n        x, indices = F.max_pool3d(x, kernel_size=2, stride=2, return_indices=True)  # Prepare for MaxUnpool3d\n        x = self.max_unpool(x, indices)  # Apply MaxUnpool3d\n        x = x.squeeze(2)  # Remove the extra dimension\n        x = x.view(x.size(0), -1)  # Flatten for LazyLinear\n        x = self.lazy_linear1(x)  # Apply LazyLinear\n        x = self.relu(x)  # Apply ReLU\n        x = self.alpha_dropout(x)  # Apply AlphaDropout\n        x = self.lazy_linear2(x)  # Apply LazyLinear\n        x = x.view(x.size(0), -1, 64)  # Reshape for TransformerEncoder\n        x = self.transformer_encoder(x)  # Apply TransformerEncoder\n        x = x.mean(dim=1)  # Global average pooling\n        return x  # Output is ready for loss functions (NLLLoss, HuberLoss)\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n\n# Example usage:\n# model = Model().cuda()\n# inputs = get_inputs()\n# output = model(*inputs)\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3456 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %36 )\n    x1 =xindex //36 \n    x2 =xindex \n    tmp0 =(-2 )+x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],32 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-2 )+x0 +32 *x1 ),tmp5 &xmask ,other =0.0 )\n    tl .store (out_ptr0 +(x2 ),tmp6 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,32 ,36 ),(3456 ,1152 ,36 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (3456 )](arg0_1 ,buf0 ,3456 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n    return (reinterpret_tensor (buf0 ,(1 ,3 ,1152 ),(3456 ,1152 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9db290bc-e410-4ac0-b402-2840519c90c6",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softshrink', 'TransformerDecoderLayer', 'Softplus', 'ReflectionPad3d', 'LSTM', 'LPPool1d', 'TripletMarginLoss', 'AdaptiveLogSoftmaxWithLoss', 'CircularPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softshrink = nn.Softshrink(lambd=0.5)\n        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=64, nhead=8)\n        self.softplus = nn.Softplus()\n        self.reflection_pad3d = nn.ReflectionPad3d(padding=1)\n        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.lp_pool1d = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.triplet_margin_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(in_features=128, n_classes=10, cutoffs=[4, 8])\n        self.circular_pad3d = nn.CircularPad3d(padding=1)\n\n    def forward(self, x):\n        # Apply CircularPad3d\n        x = self.circular_pad3d(x)\n        \n        # Apply ReflectionPad3d\n        x = self.reflection_pad3d(x)\n        \n        # Reshape for LSTM\n        batch_size, *rest = x.shape\n        x = x.view(batch_size, -1, 64)  # Reshape to (batch_size, seq_len, 64)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Apply LPPool1d\n        x = x.permute(0, 2, 1)  # Reshape to (batch_size, 128, seq_len)\n        x = self.lp_pool1d(x)\n        \n        # Apply Softplus\n        x = self.softplus(x)\n        \n        # Apply TransformerDecoderLayer\n        x = x.permute(1, 0, 2)  # Reshape to (seq_len, batch_size, 128)\n        x = self.transformer_decoder_layer(x, x)\n        \n        # Apply Softshrink\n        x = self.softshrink(x)\n        \n        # Reshape for AdaptiveLogSoftmaxWithLoss\n        x = x.permute(1, 0, 2)  # Reshape to (batch_size, seq_len, 128)\n        x = x.mean(dim=1)  # Average over sequence length\n        \n        # Apply AdaptiveLogSoftmaxWithLoss\n        output = self.adaptive_log_softmax(x, torch.randint(0, 10, (batch_size,)).to(x.device))\n        \n        return output\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks2 )\n    x2 =((xindex //ks4 )%ks5 )\n    x3 =xindex //ks7 \n    x5 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =ks1 +x0 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .broadcast_to (1 +ks1 ,[XBLOCK ])\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =x1 \n    tmp11 =tl .full ([1 ],1 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp14 =tmp10 <tmp13 \n    tmp15 =tmp12 &tmp14 \n    tmp16 =tmp15 &tmp9 \n    tmp17 =x2 \n    tmp18 =tl .full ([1 ],1 ,tl .int64 )\n    tmp19 =tmp17 >=tmp18 \n    tmp20 =tl .broadcast_to (1 +ks6 ,[XBLOCK ])\n    tmp21 =tmp17 <tmp20 \n    tmp22 =tmp19 &tmp21 \n    tmp23 =tmp22 &tmp16 \n    tmp24 =tl .load (in_ptr0 +((-1 )+x0 +ks1 *x1 +((-1 )*ks1 *ks3 )+ks1 *ks3 *x2 +ks1 *ks3 *ks6 *x3 ),tmp23 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp25 =tl .load (in_ptr1 +(ks1 +x5 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tl .full (tmp26 .shape ,0.0 ,tmp26 .dtype )\n    tmp28 =tl .where (tmp16 ,tmp26 ,tmp27 )\n    tmp29 =tl .load (in_ptr1 +(ks1 +x5 ),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp30 =tl .where (tmp15 ,tmp28 ,tmp29 )\n    tmp31 =tl .full (tmp30 .shape ,0.0 ,tmp30 .dtype )\n    tmp32 =tl .where (tmp9 ,tmp30 ,tmp31 )\n    tmp33 =float (\"nan\")\n    tmp34 =tl .where (tmp8 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp2 ,tmp34 ,tmp35 )\n    tmp37 =tmp0 >=tmp1 \n    tmp38 =1 +ks1 \n    tmp39 =tmp0 <tmp38 \n    tmp40 =tmp37 &tmp39 \n    tmp41 =x1 \n    tmp42 =tl .full ([1 ],1 ,tl .int64 )\n    tmp43 =tmp41 >=tmp42 \n    tmp44 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp45 =tmp41 <tmp44 \n    tmp46 =tmp43 &tmp45 \n    tmp47 =tmp46 &tmp40 \n    tmp48 =x2 \n    tmp49 =tl .full ([1 ],1 ,tl .int64 )\n    tmp50 =tmp48 >=tmp49 \n    tmp51 =tl .broadcast_to (1 +ks6 ,[XBLOCK ])\n    tmp52 =tmp48 <tmp51 \n    tmp53 =tmp50 &tmp52 \n    tmp54 =tmp53 &tmp47 \n    tmp55 =tl .load (in_ptr0 +((-1 )+x0 +((-1 )*ks1 )+ks1 *x1 +((-1 )*ks1 *ks3 )+ks1 *ks3 *x2 +ks1 *ks3 *ks6 *x3 ),tmp54 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp56 =tl .load (in_ptr1 +(x5 ),tmp47 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp57 =tl .where (tmp53 ,tmp55 ,tmp56 )\n    tmp58 =tl .full (tmp57 .shape ,0.0 ,tmp57 .dtype )\n    tmp59 =tl .where (tmp47 ,tmp57 ,tmp58 )\n    tmp60 =tl .load (in_ptr1 +(x5 ),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp61 =tl .where (tmp46 ,tmp59 ,tmp60 )\n    tmp62 =tl .full (tmp61 .shape ,0.0 ,tmp61 .dtype )\n    tmp63 =tl .where (tmp40 ,tmp61 ,tmp62 )\n    tmp64 =float (\"nan\")\n    tmp65 =tl .where (tmp40 ,tmp63 ,tmp64 )\n    tmp66 =tl .where (tmp2 ,tmp36 ,tmp65 )\n    tl .store (out_ptr0 +(x5 ),tmp66 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x4 =xindex //ks0 \n    x3 =xindex \n    tmp41 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =1 +ks2 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =x1 +((-1 )*ks2 )\n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(1 +2 *x4 +ks3 *x4 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x3 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =x0 \n    tmp17 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp18 =tmp16 >=tmp17 \n    tmp19 =tmp18 &tmp2 \n    tmp20 =tl .load (in_ptr0 +(1 +((-2 )*ks2 )+2 *x4 +ks3 *x4 +((-1 )*ks2 *ks3 )),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr0 +(x3 +((-2 )*ks2 )+((-1 )*ks2 *ks3 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\n    tmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],1 ,tl .int64 )\n    tmp27 =tmp0 <tmp26 \n    tmp28 =x0 \n    tmp29 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp30 =tmp28 >=tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(1 +2 *ks2 +2 *x4 +ks2 *ks3 +ks3 *x4 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(x3 +2 *ks2 +ks2 *ks3 ),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =x0 \n    tmp38 =1 +ks3 \n    tmp39 =tmp37 >=tmp38 \n    tmp40 =tl .load (in_ptr0 +(1 +2 *x4 +ks3 *x4 ),tmp39 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tl .where (tmp39 ,tmp40 ,tmp41 )\n    tmp43 =tl .where (tmp27 ,tmp36 ,tmp42 )\n    tmp44 =tl .where (tmp2 ,tmp25 ,tmp43 )\n    tl .store (out_ptr0 +(x3 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_reflection_pad3d_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x6 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks3 )\n    x1 =((xindex //ks3 )%ks4 )\n    x3 =xindex //ks5 \n    x2 =((xindex //ks8 )%ks1 )\n    x8 =xindex \n    tmp15 =tl .load (in_ptr0 +(2 *(tl .where (1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))+2 *ks6 ,1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))))+4 *(tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))))+8 *x3 +ks7 *(tl .where (1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))+2 *ks6 ,1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))))+2 *ks6 *(tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))))+2 *ks7 *(tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))))+4 *ks2 *x3 +4 *ks6 *x3 +4 *ks7 *x3 +ks6 *ks7 *(tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))))+2 *ks2 *ks6 *x3 +2 *ks2 *ks7 *x3 +2 *ks6 *ks7 *x3 +ks2 *ks6 *ks7 *x3 +(tl .where (1 +ks7 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))+2 *ks7 ,1 +ks7 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))))),xmask ,eviction_policy ='evict_last')\n    tmp0 =tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x6 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x6 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x6 )))))\n    tmp1 =1 +ks2 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =((-1 )*ks2 )+(tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x6 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x6 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x6 ))))))\n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =tl .load (in_ptr0 +(2 *(tl .where (1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))+2 *ks6 ,1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))))+4 *ks2 +8 *x3 +ks7 *(tl .where (1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))+2 *ks6 ,1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))))+2 *ks2 *ks6 +2 *ks2 *ks7 +4 *ks2 *x3 +4 *ks6 *x3 +4 *ks7 *x3 +ks2 *ks6 *ks7 +2 *ks2 *ks6 *x3 +2 *ks2 *ks7 *x3 +2 *ks6 *ks7 *x3 +ks2 *ks6 *ks7 *x3 +(tl .where (1 +ks7 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))+2 *ks7 ,1 +ks7 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))))),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =tl .load (in_ptr0 +(((-4 )*ks2 )+2 *(tl .where (1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))+2 *ks6 ,1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))))+4 *(tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))))+8 *x3 +ks7 *(tl .where (1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))+2 *ks6 ,1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))))+((-2 )*ks2 *ks6 )+((-2 )*ks2 *ks7 )+2 *ks6 *(tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))))+2 *ks7 *(tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))))+4 *ks2 *x3 +4 *ks6 *x3 +4 *ks7 *x3 +ks6 *ks7 *(tl .where (1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))+2 *ks2 ,1 +ks2 +((-1 )*tl_math .abs (1 +ks2 +((-1 )*tl_math .abs ((-1 )+x2 ))))))+((-1 )*ks2 *ks6 *ks7 )+2 *ks2 *ks6 *x3 +2 *ks2 *ks7 *x3 +2 *ks6 *ks7 *x3 +ks2 *ks6 *ks7 *x3 +(tl .where (1 +ks7 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))+2 *ks7 ,1 +ks7 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))))),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tmp10 =tl .full (tmp9 .shape ,0.0 ,tmp9 .dtype )\n    tmp11 =tl .where (tmp2 ,tmp9 ,tmp10 )\n    tmp12 =tl .full ([1 ],1 ,tl .int64 )\n    tmp13 =tmp0 <tmp12 \n    tmp14 =tl .load (in_ptr0 +(2 *(tl .where (1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))+2 *ks6 ,1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))))+4 *ks2 +8 *x3 +ks7 *(tl .where (1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))+2 *ks6 ,1 +ks6 +((-1 )*tl_math .abs (1 +ks6 +((-1 )*tl_math .abs ((-1 )+x1 ))))))+2 *ks2 *ks6 +2 *ks2 *ks7 +4 *ks2 *x3 +4 *ks6 *x3 +4 *ks7 *x3 +ks2 *ks6 *ks7 +2 *ks2 *ks6 *x3 +2 *ks2 *ks7 *x3 +2 *ks6 *ks7 *x3 +ks2 *ks6 *ks7 *x3 +(tl .where (1 +ks7 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))+2 *ks7 ,1 +ks7 +((-1 )*tl_math .abs (1 +ks7 +((-1 )*tl_math .abs ((-1 )+x0 ))))))),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp16 =tl .where (tmp13 ,tmp14 ,tmp15 )\n    tmp17 =tl .where (tmp2 ,tmp11 ,tmp16 )\n    tl .store (out_ptr0 +(x8 ),tmp17 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n        2 +s3 \n        2 +s2 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        2 +s1 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf1 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_0_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (triton_poi_fused_copy_0_xnumel )](arg4_1 ,buf0 ,buf1 ,34 ,32 ,34 ,32 ,1156 ,34 ,32 ,39304 ,117912 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg4_1 \n        buf2 =buf0 ;del buf0 \n\n        triton_poi_fused_1_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_1 [grid (triton_poi_fused_1_xnumel )](buf1 ,buf2 ,34 ,34 ,32 ,32 ,117912 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf1 \n        16 +4 *s2 +4 *s3 +s2 *s3 \n        4 +s1 \n        4 +s3 \n        4 +s2 \n        64 +16 *s1 +16 *s2 +16 *s3 +4 *s1 *s2 +4 *s1 *s3 +4 *s2 *s3 +s1 *s2 *s3 \n        16 +4 *s2 +4 *s3 +s2 *s3 \n        buf3 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ,4 +s3 ),(64 *s0 +16 *s0 *s1 +16 *s0 *s2 +16 *s0 *s3 +4 *s0 *s1 *s2 +4 *s0 *s1 *s3 +4 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,64 +16 *s1 +16 *s2 +16 *s3 +4 *s1 *s2 +4 *s1 *s3 +4 *s2 *s3 +s1 *s2 *s3 ,16 +4 *s2 +4 *s3 +s2 *s3 ,4 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_reflection_pad3d_2_xnumel =64 *s0 +16 *s0 *s1 +16 *s0 *s2 +16 *s0 *s3 +4 *s0 *s1 *s2 +4 *s0 *s1 *s3 +4 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_reflection_pad3d_2 [grid (triton_poi_fused_reflection_pad3d_2_xnumel )](buf2 ,buf3 ,1296 ,36 ,32 ,36 ,36 ,46656 ,32 ,32 ,1296 ,139968 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf2 \n    return (reinterpret_tensor (buf3 ,(1 ,s0 +((16 *s0 *s1 +16 *s0 *s2 +16 *s0 *s3 +4 *s0 *s1 *s2 +4 *s0 *s1 *s3 +4 *s0 *s2 *s3 +s0 *s1 *s2 *s3 )//64 ),64 ),(64 *s0 +64 *((16 *s0 *s1 +16 *s0 *s2 +16 *s0 *s3 +4 *s0 *s1 *s2 +4 *s0 *s1 *s3 +4 *s0 *s2 *s3 +s0 *s1 *s2 *s3 )//64 ),64 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9db3addf-3493-4262-a47b-b03df13e51ad",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Container', 'KLDivLoss', 'ReLU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.container1 = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32)\n        )\n        self.container2 = nn.Sequential(\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Linear(16, 8)\n        )\n        self.kldivloss = nn.KLDivLoss(reduction='batchmean')\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # Flatten the input to fit into the linear layers\n        x = x.view(x.size(0), -1)\n        \n        # Pass through the first container\n        x = self.container1(x)\n        x = self.relu(x)\n        \n        # Pass through the second container\n        x = self.container2(x)\n        x = self.relu(x)\n        \n        # Compute KLDivLoss with a target tensor (randomly generated for demonstration)\n        target = torch.randn_like(x).softmax(dim=1)\n        loss = self.kldivloss(F.log_softmax(x, dim=1), target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 128).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_relu_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_relu_1 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_relu_2 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__log_softmax__softmax_div_mul_randn_like_relu_sub_sum_xlogy_3 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =8 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =triton_helpers .max2 (tmp3 ,1 )[:,None ]\n    tmp6 =tmp2 -tmp5 \n    tmp7 =tl_math .exp (tmp6 )\n    tmp8 =tl .broadcast_to (tmp7 ,[XBLOCK ,R0_BLOCK ])\n    tmp10 =tl .sum (tmp8 ,1 )[:,None ]\n    tmp11 =tl_math .log (tmp10 )\n    tmp12 =tl .load (in_ptr1 +load_seed_offset )\n    tmp13 =r0_0 \n    tmp14 =tl .randn (tmp12 ,(tmp13 ).to (tl .uint32 ))\n    tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp17 =triton_helpers .max2 (tmp15 ,1 )[:,None ]\n    tmp18 =tmp14 -tmp17 \n    tmp19 =tl_math .exp (tmp18 )\n    tmp20 =tl .broadcast_to (tmp19 ,[XBLOCK ,R0_BLOCK ])\n    tmp22 =tl .sum (tmp20 ,1 )[:,None ]\n    tmp23 =tmp19 /tmp22 \n    tmp24 =libdevice .isnan (tmp23 ).to (tl .int1 )\n    tmp25 =0.0 \n    tmp26 =tmp23 ==tmp25 \n    tmp27 =tl_math .log (tmp23 )\n    tmp28 =tmp23 *tmp27 \n    tmp29 =tl .where (tmp26 ,tmp25 ,tmp28 )\n    tmp30 =float (\"nan\")\n    tmp31 =tl .where (tmp24 ,tmp30 ,tmp29 )\n    tmp32 =tmp6 -tmp11 \n    tmp33 =tmp23 *tmp32 \n    tmp34 =tmp31 -tmp33 \n    tmp35 =tl .broadcast_to (tmp34 ,[XBLOCK ,R0_BLOCK ])\n    tmp37 =tl .sum (tmp35 ,1 )[:,None ]\n    tmp38 =1.0 \n    tmp39 =tmp37 *tmp38 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp11 ,None )\n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp14 ,None )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp39 ,None )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp5 ,None )\n    tl .store (out_ptr2 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp17 ,None )\n    tl .store (out_ptr3 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp22 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_2 ,(64 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_3 ,(64 ,),(1 ,))\n    assert_size_stride (primals_4 ,(32 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_5 ,(32 ,),(1 ,))\n    assert_size_stride (primals_6 ,(16 ,32 ),(32 ,1 ))\n    assert_size_stride (primals_7 ,(16 ,),(1 ,))\n    assert_size_stride (primals_8 ,(8 ,16 ),(16 ,1 ))\n    assert_size_stride (primals_9 ,(8 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (primals_1 ,reinterpret_tensor (primals_2 ,(128 ,64 ),(1 ,128 ),0 ),out =buf0 )\n        del primals_2 \n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_relu_0 [grid (64 )](buf1 ,primals_3 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del primals_3 \n        buf2 =empty_strided_cuda ((1 ,32 ),(32 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf1 ,reinterpret_tensor (primals_4 ,(64 ,32 ),(1 ,64 ),0 ),out =buf2 )\n        buf3 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_relu_1 [grid (32 )](buf3 ,primals_5 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del primals_5 \n        buf4 =empty_strided_cuda ((1 ,16 ),(16 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf3 ,reinterpret_tensor (primals_6 ,(32 ,16 ),(1 ,32 ),0 ),out =buf4 )\n        buf5 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_relu_2 [grid (16 )](buf5 ,primals_7 ,16 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del primals_7 \n        buf6 =empty_strided_cuda ((1 ,8 ),(8 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_9 ,buf5 ,reinterpret_tensor (primals_8 ,(16 ,8 ),(1 ,16 ),0 ),alpha =1 ,beta =1 ,out =buf6 )\n        del primals_9 \n        buf7 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf7 )\n        buf11 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf12 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf13 =buf12 ;del buf12 \n        buf8 =empty_strided_cuda ((1 ,8 ),(8 ,1 ),torch .float32 )\n        buf9 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf10 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf14 =empty_strided_cuda ((),(),torch .float32 )\n        buf15 =buf14 ;del buf14 \n\n        get_raw_stream (0 )\n        triton_per_fused__log_softmax__softmax_div_mul_randn_like_relu_sub_sum_xlogy_3 [grid (1 )](buf13 ,buf15 ,buf6 ,buf7 ,buf11 ,buf8 ,buf9 ,buf10 ,0 ,1 ,8 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf7 \n    return (buf15 ,primals_1 ,buf1 ,buf3 ,buf5 ,buf6 ,buf8 ,buf9 ,buf10 ,buf11 ,buf13 ,primals_8 ,primals_6 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((64 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((32 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((16 ,32 ),(32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((8 ,16 ),(16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((8 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9de54fec-ef92-4975-98e0-23635909110b",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FeatureAlphaDropout', 'InstanceNorm3d', 'MaxPool1d', 'Conv3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.instance_norm1 = nn.InstanceNorm3d(16)\n        self.max_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.instance_norm2 = nn.InstanceNorm3d(32)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.instance_norm3 = nn.InstanceNorm3d(64)\n        self.max_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.conv4 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.instance_norm4 = nn.InstanceNorm3d(128)\n        self.feature_alpha_dropout2 = nn.FeatureAlphaDropout(p=0.5)\n        self.conv5 = nn.Conv3d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.instance_norm5 = nn.InstanceNorm3d(256)\n\n    def forward(self, x):\n        # Initial 3D convolution and instance normalization\n        x = self.conv1(x)\n        x = self.instance_norm1(x)\n        \n        # Reshape for MaxPool1d\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size, channels * depth * height, width)\n        x = self.max_pool1(x)\n        x = x.view(batch_size, channels, depth, height, -1)\n        \n        # Second 3D convolution and instance normalization\n        x = self.conv2(x)\n        x = self.instance_norm2(x)\n        \n        # Apply feature alpha dropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Third 3D convolution and instance normalization\n        x = self.conv3(x)\n        x = self.instance_norm3(x)\n        \n        # Reshape for MaxPool1d\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size, channels * depth * height, width)\n        x = self.max_pool2(x)\n        x = x.view(batch_size, channels, depth, height, -1)\n        \n        # Fourth 3D convolution and instance normalization\n        x = self.conv4(x)\n        x = self.instance_norm4(x)\n        \n        # Apply feature alpha dropout\n        x = self.feature_alpha_dropout2(x)\n        \n        # Fifth 3D convolution and instance normalization\n        x = self.conv5(x)\n        x = self.instance_norm5(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 32, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_convolution_0 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \n    r0_numel =8192 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x3 =xindex \n    x1 =xindex //4 \n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index \n        tmp0 =tl .load (in_out_ptr0 +(r0_2 +8192 *x3 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp2 =tmp0 +tmp1 \n        tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n        tmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\n        tmp3 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n        )\n        tmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\n        tmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\n        tmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\n        tl .store (in_out_ptr0 +(r0_2 +8192 *x3 ),tmp2 ,r0_mask &xmask )\n    tmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\n    tmp4 =tmp7 [:,None ]\n    tmp5 =tmp8 [:,None ]\n    tmp6 =tmp9 [:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp4 ,xmask )\n    tl .store (out_ptr1 +(x3 ),tmp5 ,xmask )\n    tl .store (out_ptr2 +(x3 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    R0_BLOCK :tl .constexpr =4 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\n    tmp2 =tl .load (in_ptr2 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\n    tmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (xmask ,tmp3 ,0 )\n    tmp8 =tl .where (xmask ,tmp4 ,0 )\n    tmp9 =tl .where (xmask ,tmp5 ,0 )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\n    tmp13 =tmp10 [:,None ]\n    tmp14 =tmp11 [:,None ]\n    tmp15 =tmp12 [:,None ]\n    tmp16 =32768.0 \n    tmp17 =tmp14 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tl .store (out_ptr2 +(x0 ),tmp20 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_unsqueeze_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    yoffset =tl .program_id (1 )*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    tl .full ([XBLOCK ,YBLOCK ],True ,tl .int1 )\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x2 =xindex \n    y3 =yindex \n    y1 =yindex //1024 \n    tmp0 =tl .load (in_ptr0 +(x2 +32 *y3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(y1 ),None ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(y1 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp4 =32768.0 \n    tmp5 =tmp3 /tmp4 \n    tmp6 =1e-05 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =libdevice .rsqrt (tmp7 )\n    tmp9 =tmp2 *tmp8 \n    tl .store (out_ptr1 +(y3 +16384 *x2 ),tmp9 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_3 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    ynumel =16 \n    yoffset =tl .program_id (1 )*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,YBLOCK ],True ,tl .int1 )\n    x1 =xindex \n    y0 =yindex \n    tmp0 =tl .load (in_ptr0 +(x1 +32768 *y0 ),ymask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(16384 +x1 +32768 *y0 ),ymask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ,1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ,1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tl .store (out_ptr0 +(x1 +16384 *y0 ),tmp5 ,ymask )\n    tl .store (out_ptr1 +(y0 +16 *x1 ),tmp6 ,ymask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_convolution_4 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \n    r0_numel =8192 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x3 =xindex \n    x1 =xindex //2 \n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index \n        tmp0 =tl .load (in_out_ptr0 +(r0_2 +8192 *x3 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp2 =tmp0 +tmp1 \n        tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n        tmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\n        tmp3 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n        )\n        tmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\n        tmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\n        tmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\n        tl .store (in_out_ptr0 +(r0_2 +8192 *x3 ),tmp2 ,r0_mask &xmask )\n    tmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\n    tmp4 =tmp7 [:,None ]\n    tmp5 =tmp8 [:,None ]\n    tmp6 =tmp9 [:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp4 ,xmask )\n    tl .store (out_ptr1 +(x3 ),tmp5 ,xmask )\n    tl .store (out_ptr2 +(x3 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +2 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 +2 *x0 ),xmask ,other =0.0 )\n    tmp2 =tl .load (in_ptr2 +(r0_1 +2 *x0 ),xmask ,other =0.0 )\n    tmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (xmask ,tmp3 ,0 )\n    tmp8 =tl .where (xmask ,tmp4 ,0 )\n    tmp9 =tl .where (xmask ,tmp5 ,0 )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\n    tmp13 =tmp10 [:,None ]\n    tmp14 =tmp11 [:,None ]\n    tmp15 =tmp12 [:,None ]\n    tmp16 =16384.0 \n    tmp17 =tmp14 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tl .store (out_ptr2 +(x0 ),tmp20 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_6 (in_ptr0 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit__to_copy_add_mul_7 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =xindex \n    x1 =xindex //16384 \n    tmp0 =tl .load (in_ptr0 +(x2 ),None )\n    tmp1 =tl .load (in_ptr1 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr3 +(x1 ),None ,eviction_policy ='evict_last').to (tl .int1 )\n    tmp2 =tmp0 -tmp1 \n    tmp4 =16384.0 \n    tmp5 =tmp3 /tmp4 \n    tmp6 =1e-05 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =libdevice .rsqrt (tmp7 )\n    tmp9 =tmp2 *tmp8 \n    tmp11 =tmp10 .to (tl .float32 )\n    tmp12 =0.8864048946659319 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =tmp9 *tmp13 \n    tmp15 =-1.0 \n    tmp16 =tmp11 +tmp15 \n    tmp17 =1.558387861036063 \n    tmp18 =tmp16 *tmp17 \n    tmp19 =0.7791939305180315 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =tmp14 +tmp20 \n    tl .store (out_ptr0 +(x2 ),tmp21 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_convolution_8 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =128 \n    r0_numel =8192 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x3 =xindex \n    x1 =xindex //2 \n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index \n        tmp0 =tl .load (in_out_ptr0 +(r0_2 +8192 *x3 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp2 =tmp0 +tmp1 \n        tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n        tmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\n        tmp3 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n        )\n        tmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\n        tmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\n        tmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\n        tl .store (in_out_ptr0 +(r0_2 +8192 *x3 ),tmp2 ,r0_mask &xmask )\n    tmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\n    tmp4 =tmp7 [:,None ]\n    tmp5 =tmp8 [:,None ]\n    tmp6 =tmp9 [:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp4 ,xmask )\n    tl .store (out_ptr1 +(x3 ),tmp5 ,xmask )\n    tl .store (out_ptr2 +(x3 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_9 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +2 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 +2 *x0 ),xmask ,other =0.0 )\n    tmp2 =tl .load (in_ptr2 +(r0_1 +2 *x0 ),xmask ,other =0.0 )\n    tmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (xmask ,tmp3 ,0 )\n    tmp8 =tl .where (xmask ,tmp4 ,0 )\n    tmp9 =tl .where (xmask ,tmp5 ,0 )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\n    tmp13 =tmp10 [:,None ]\n    tmp14 =tmp11 [:,None ]\n    tmp15 =tmp12 [:,None ]\n    tmp16 =16384.0 \n    tmp17 =tmp14 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tl .store (out_ptr2 +(x0 ),tmp20 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_unsqueeze_10 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    ynumel =65536 \n    xnumel =16 \n    yoffset =(tl .program_id (1 )+tl .program_id (2 )*tl .num_programs (1 ))*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x2 =xindex \n    y3 =yindex \n    y1 =yindex //1024 \n    tmp0 =tl .load (in_ptr0 +(x2 +16 *y3 ),xmask &ymask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(y1 ),ymask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(y1 ),ymask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp4 =16384.0 \n    tmp5 =tmp3 /tmp4 \n    tmp6 =1e-05 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =libdevice .rsqrt (tmp7 )\n    tmp9 =tmp2 *tmp8 \n    tl .store (out_ptr1 +(y3 +65536 *x2 ),tmp9 ,xmask &ymask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_11 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    ynumel =8 \n    yoffset =tl .program_id (1 )*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,YBLOCK ],True ,tl .int1 )\n    x1 =xindex \n    y0 =yindex \n    tmp0 =tl .load (in_ptr0 +(x1 +131072 *y0 ),ymask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(65536 +x1 +131072 *y0 ),ymask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ,1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ,1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tl .store (out_ptr0 +(x1 +65536 *y0 ),tmp5 ,ymask )\n    tl .store (out_ptr1 +(y0 +8 *x1 ),tmp6 ,ymask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit__to_copy_add_bernoulli_convolution_mul_12 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr1 ,out_ptr2 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =128 \n    r0_numel =8192 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n    tmp6 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp9_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp9_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp9_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp5 =tl .load (in_out_ptr0 +(r0_1 +8192 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp7 =tmp5 +tmp6 \n        tmp8 =tl .broadcast_to (tmp7 ,[XBLOCK ,R0_BLOCK ])\n        tmp9_mean_next ,tmp9_m2_next ,tmp9_weight_next =triton_helpers .welford_reduce (\n        tmp8 ,tmp9_mean ,tmp9_m2 ,tmp9_weight ,roffset ==0 \n        )\n        tmp9_mean =tl .where (r0_mask &xmask ,tmp9_mean_next ,tmp9_mean )\n        tmp9_m2 =tl .where (r0_mask &xmask ,tmp9_m2_next ,tmp9_m2 )\n        tmp9_weight =tl .where (r0_mask &xmask ,tmp9_weight_next ,tmp9_weight )\n        tl .store (in_out_ptr0 +(r0_1 +8192 *x0 ),tmp7 ,r0_mask &xmask )\n    tmp12 ,tmp13 ,tmp14 =triton_helpers .welford (tmp9_mean ,tmp9_m2 ,tmp9_weight ,1 )\n    tmp9 =tmp12 [:,None ]\n    tmp10 =tmp13 [:,None ]\n    tmp11 =tmp14 [:,None ]\n    tl .store (out_ptr2 +(x0 ),tmp9 ,xmask )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp15 =tl .load (in_out_ptr0 +(r0_1 +8192 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp16 =tmp15 -tmp9 \n        tmp17 =8192.0 \n        tmp18 =tmp10 /tmp17 \n        tmp19 =1e-05 \n        tmp20 =tmp18 +tmp19 \n        tmp21 =libdevice .rsqrt (tmp20 )\n        tmp22 =tmp16 *tmp21 \n        tmp23 =tmp4 .to (tl .float32 )\n        tmp24 =0.8864048946659319 \n        tmp25 =tmp23 *tmp24 \n        tmp26 =tmp22 *tmp25 \n        tmp27 =-1.0 \n        tmp28 =tmp23 +tmp27 \n        tmp29 =1.558387861036063 \n        tmp30 =tmp28 *tmp29 \n        tmp31 =0.7791939305180315 \n        tmp32 =tmp30 +tmp31 \n        tmp33 =tmp26 +tmp32 \n        tl .store (out_ptr4 +(r0_1 +8192 *x0 ),tmp33 ,r0_mask &xmask )\n    tmp34 =8192.0 \n    tmp35 =tmp10 /tmp34 \n    tmp36 =1e-05 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =libdevice .rsqrt (tmp37 )\n    tl .store (out_ptr5 +(x0 ),tmp38 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_convolution_13 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,out_ptr2 ,out_ptr3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =256 \n    r0_numel =8192 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_out_ptr0 +(r0_1 +8192 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp2 =tmp0 +tmp1 \n        tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n        tmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\n        tmp3 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n        )\n        tmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\n        tmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\n        tmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\n        tl .store (in_out_ptr0 +(r0_1 +8192 *x0 ),tmp2 ,r0_mask &xmask )\n    tmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\n    tmp4 =tmp7 [:,None ]\n    tmp5 =tmp8 [:,None ]\n    tmp6 =tmp9 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp10 =tl .load (in_out_ptr0 +(r0_1 +8192 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp11 =tmp10 -tmp4 \n        tmp12 =8192.0 \n        tmp13 =tmp5 /tmp12 \n        tmp14 =1e-05 \n        tmp15 =tmp13 +tmp14 \n        tmp16 =libdevice .rsqrt (tmp15 )\n        tmp17 =tmp11 *tmp16 \n        tl .store (out_ptr2 +(r0_1 +8192 *x0 ),tmp17 ,r0_mask &xmask )\n    tmp18 =8192.0 \n    tmp19 =tmp5 /tmp18 \n    tmp20 =1e-05 \n    tmp21 =tmp19 +tmp20 \n    tmp22 =libdevice .rsqrt (tmp21 )\n    tl .store (out_ptr3 +(x0 ),tmp22 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(16 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_2 ,(16 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,1 ,32 ,32 ,32 ),(32768 ,32768 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_4 ,(32 ,16 ,3 ,3 ,3 ),(432 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_5 ,(32 ,),(1 ,))\n    assert_size_stride (primals_6 ,(64 ,32 ,3 ,3 ,3 ),(864 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_7 ,(64 ,),(1 ,))\n    assert_size_stride (primals_8 ,(128 ,64 ,3 ,3 ,3 ),(1728 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_9 ,(128 ,),(1 ,))\n    assert_size_stride (primals_10 ,(256 ,128 ,3 ,3 ,3 ),(3456 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_11 ,(256 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,1 ,1 ),padding =(1 ,1 ,1 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,16 ,32 ,32 ,32 ),(524288 ,32768 ,1024 ,32 ,1 ))\n        buf1 =buf0 ;del buf0 \n        buf2 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ,4 ),(64 ,4 ,64 ,64 ,64 ,1 ),torch .float32 )\n        buf3 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ,4 ),(64 ,4 ,64 ,64 ,64 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ,4 ),(64 ,4 ,64 ,64 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_convolution_0 [grid (64 )](buf1 ,primals_2 ,buf2 ,buf3 ,buf4 ,64 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del primals_2 \n        buf5 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ),(16 ,1 ,16 ,16 ,16 ),torch .float32 )\n        buf6 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ),(16 ,1 ,16 ,16 ,16 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,16 ,1 ,1 ,1 ),(16 ,1 ,16 ,16 ,16 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_1 [grid (16 )](buf2 ,buf3 ,buf4 ,buf5 ,buf6 ,buf8 ,16 ,4 ,XBLOCK =8 ,num_warps =2 ,num_stages =1 )\n        buf10 =empty_strided_cuda ((1 ,16384 ,1 ,32 ),(524288 ,1 ,524288 ,16384 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_unsqueeze_2 [grid (16384 ,32 )](buf1 ,buf5 ,buf6 ,buf10 ,16384 ,32 ,XBLOCK =16 ,YBLOCK =256 ,num_warps =8 ,num_stages =1 )\n        del buf6 \n        buf11 =empty_strided_cuda ((1 ,16384 ,1 ,16 ),(262144 ,1 ,262144 ,16384 ),torch .int8 )\n        buf12 =empty_strided_cuda ((1 ,16384 ,1 ,16 ),(262144 ,16 ,16 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_3 [grid (16 ,16384 )](buf10 ,buf11 ,buf12 ,16 ,16384 ,XBLOCK =64 ,YBLOCK =16 ,num_warps =4 ,num_stages =1 )\n\n        buf13 =extern_kernels .convolution (reinterpret_tensor (buf12 ,(1 ,16 ,32 ,32 ,16 ),(0 ,16384 ,512 ,16 ,1 ),0 ),primals_4 ,stride =(1 ,1 ,1 ),padding =(1 ,1 ,1 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf13 ,(1 ,32 ,32 ,32 ,16 ),(524288 ,16384 ,512 ,16 ,1 ))\n        buf14 =buf13 ;del buf13 \n        buf15 =reinterpret_tensor (buf4 ,(1 ,32 ,1 ,1 ,1 ,2 ),(64 ,2 ,64 ,64 ,64 ,1 ),0 );del buf4 \n        buf16 =reinterpret_tensor (buf3 ,(1 ,32 ,1 ,1 ,1 ,2 ),(64 ,2 ,64 ,64 ,64 ,1 ),0 );del buf3 \n        buf17 =reinterpret_tensor (buf2 ,(1 ,32 ,1 ,1 ,1 ,2 ),(64 ,2 ,64 ,64 ,64 ,1 ),0 );del buf2 \n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_convolution_4 [grid (64 )](buf14 ,primals_5 ,buf15 ,buf16 ,buf17 ,64 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del primals_5 \n        buf18 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,32 ,32 ,32 ),torch .float32 )\n        buf19 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,32 ,32 ,32 ),torch .float32 )\n        buf21 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,32 ,32 ,32 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_5 [grid (32 )](buf15 ,buf16 ,buf17 ,buf18 ,buf19 ,buf21 ,32 ,2 ,XBLOCK =8 ,num_warps =2 ,num_stages =1 )\n        buf22 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf22 )\n        buf24 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,1 ,1 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_6 [grid (32 )](buf22 ,buf24 ,0 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf25 =empty_strided_cuda ((1 ,32 ,32 ,32 ,16 ),(524288 ,16384 ,512 ,16 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit__to_copy_add_mul_7 [grid (524288 )](buf14 ,buf18 ,buf19 ,buf24 ,buf25 ,524288 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del buf19 \n\n        buf26 =extern_kernels .convolution (buf25 ,primals_6 ,stride =(1 ,1 ,1 ),padding =(1 ,1 ,1 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf26 ,(1 ,64 ,32 ,32 ,16 ),(1048576 ,16384 ,512 ,16 ,1 ))\n        buf27 =buf26 ;del buf26 \n        buf28 =empty_strided_cuda ((1 ,64 ,1 ,1 ,1 ,2 ),(128 ,2 ,128 ,128 ,128 ,1 ),torch .float32 )\n        buf29 =empty_strided_cuda ((1 ,64 ,1 ,1 ,1 ,2 ),(128 ,2 ,128 ,128 ,128 ,1 ),torch .float32 )\n        buf30 =empty_strided_cuda ((1 ,64 ,1 ,1 ,1 ,2 ),(128 ,2 ,128 ,128 ,128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_convolution_8 [grid (128 )](buf27 ,primals_7 ,buf28 ,buf29 ,buf30 ,128 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del primals_7 \n        buf31 =reinterpret_tensor (buf17 ,(1 ,64 ,1 ,1 ,1 ),(64 ,1 ,64 ,64 ,64 ),0 );del buf17 \n        buf32 =reinterpret_tensor (buf16 ,(1 ,64 ,1 ,1 ,1 ),(64 ,1 ,64 ,64 ,64 ),0 );del buf16 \n        buf34 =reinterpret_tensor (buf15 ,(1 ,64 ,1 ,1 ,1 ),(64 ,1 ,64 ,64 ,64 ),0 );del buf15 \n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_9 [grid (64 )](buf28 ,buf29 ,buf30 ,buf31 ,buf32 ,buf34 ,64 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf28 \n        buf36 =empty_strided_cuda ((1 ,65536 ,1 ,16 ),(1048576 ,1 ,1048576 ,65536 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_unsqueeze_10 [grid (65536 ,16 )](buf27 ,buf31 ,buf32 ,buf36 ,65536 ,16 ,XBLOCK =16 ,YBLOCK =256 ,num_warps =8 ,num_stages =1 )\n        del buf32 \n        buf37 =empty_strided_cuda ((1 ,65536 ,1 ,8 ),(524288 ,1 ,524288 ,65536 ),torch .int8 )\n        buf38 =empty_strided_cuda ((1 ,65536 ,1 ,8 ),(524288 ,8 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_11 [grid (8 ,65536 )](buf36 ,buf37 ,buf38 ,8 ,65536 ,XBLOCK =128 ,YBLOCK =8 ,num_warps =4 ,num_stages =1 )\n\n        buf39 =extern_kernels .convolution (reinterpret_tensor (buf38 ,(1 ,64 ,32 ,32 ,8 ),(0 ,8192 ,256 ,8 ,1 ),0 ),primals_8 ,stride =(1 ,1 ,1 ),padding =(1 ,1 ,1 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf39 ,(1 ,128 ,32 ,32 ,8 ),(1048576 ,8192 ,256 ,8 ,1 ))\n        buf46 =empty_strided_cuda ((1 ,128 ,1 ,1 ,1 ),(128 ,1 ,1 ,1 ,1 ),torch .bool )\n        buf40 =buf39 ;del buf39 \n        buf41 =reinterpret_tensor (buf30 ,(1 ,128 ,1 ,1 ,1 ),(128 ,1 ,128 ,128 ,128 ),0 );del buf30 \n        buf47 =empty_strided_cuda ((1 ,128 ,32 ,32 ,8 ),(1048576 ,8192 ,256 ,8 ,1 ),torch .float32 )\n        buf44 =reinterpret_tensor (buf29 ,(1 ,128 ,1 ,1 ,1 ),(128 ,1 ,128 ,128 ,128 ),0 );del buf29 \n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit__to_copy_add_bernoulli_convolution_mul_12 [grid (128 )](buf40 ,buf22 ,primals_9 ,buf46 ,buf41 ,buf47 ,buf44 ,1 ,128 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf22 \n        del primals_9 \n\n        buf48 =extern_kernels .convolution (buf47 ,primals_10 ,stride =(1 ,1 ,1 ),padding =(1 ,1 ,1 ),dilation =(1 ,1 ,1 ),transposed =False ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf48 ,(1 ,256 ,32 ,32 ,8 ),(2097152 ,8192 ,256 ,8 ,1 ))\n        buf49 =buf48 ;del buf48 \n        buf50 =empty_strided_cuda ((1 ,256 ,1 ,1 ,1 ),(256 ,1 ,256 ,256 ,256 ),torch .float32 )\n        buf53 =empty_strided_cuda ((1 ,256 ,32 ,32 ,8 ),(2097152 ,8192 ,256 ,8 ,1 ),torch .float32 )\n        buf54 =empty_strided_cuda ((1 ,256 ,1 ,1 ,1 ),(256 ,1 ,256 ,256 ,256 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_convolution_13 [grid (256 )](buf49 ,primals_11 ,buf50 ,buf53 ,buf54 ,256 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del primals_11 \n    return (buf53 ,primals_1 ,primals_3 ,primals_4 ,primals_6 ,primals_8 ,primals_10 ,buf1 ,reinterpret_tensor (buf8 ,(16 ,),(1 ,),0 ),buf10 ,buf11 ,reinterpret_tensor (buf12 ,(1 ,16 ,32 ,32 ,16 ),(262144 ,16384 ,512 ,16 ,1 ),0 ),buf14 ,reinterpret_tensor (buf21 ,(32 ,),(1 ,),0 ),buf24 ,buf25 ,buf27 ,reinterpret_tensor (buf34 ,(64 ,),(1 ,),0 ),buf36 ,buf37 ,reinterpret_tensor (buf38 ,(1 ,64 ,32 ,32 ,8 ),(524288 ,8192 ,256 ,8 ,1 ),0 ),buf40 ,reinterpret_tensor (buf44 ,(128 ,),(1 ,),0 ),buf46 ,buf47 ,buf49 ,reinterpret_tensor (buf54 ,(256 ,),(1 ,),0 ),reinterpret_tensor (buf50 ,(1 ,256 ,1 ,1 ,1 ),(256 ,1 ,1 ,1 ,1 ),0 ),reinterpret_tensor (buf41 ,(1 ,128 ,1 ,1 ,1 ),(128 ,1 ,1 ,1 ,1 ),0 ),reinterpret_tensor (buf31 ,(1 ,64 ,1 ,1 ,1 ),(64 ,1 ,1 ,1 ,1 ),0 ),reinterpret_tensor (buf18 ,(1 ,32 ,1 ,1 ,1 ),(32 ,1 ,1 ,1 ,1 ),0 ),reinterpret_tensor (buf5 ,(1 ,16 ,1 ,1 ,1 ),(16 ,1 ,1 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((16 ,1 ,3 ,3 ,3 ),(27 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,1 ,32 ,32 ,32 ),(32768 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((32 ,16 ,3 ,3 ,3 ),(432 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((64 ,32 ,3 ,3 ,3 ),(864 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((128 ,64 ,3 ,3 ,3 ),(1728 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((256 ,128 ,3 ,3 ,3 ),(3456 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9ded7b90-e33e-4703-9f35-d89d62a37644",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad3d', 'LPPool2d', 'GRU', 'Sequential']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad3d(padding=(1, 1, 1, 1, 1, 1), value=0)\n        self.pool = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)\n        self.gru = nn.GRU(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.sequential = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        # Apply ConstantPad3d\n        x = self.pad(x)\n        \n        # Reshape for LPPool2d (assuming input is 4D: batch, channels, height, width)\n        x = x.view(x.size(0), x.size(1), x.size(2), x.size(3) * x.size(4))\n        \n        # Apply LPPool2d\n        x = self.pool(x)\n        \n        # Reshape for GRU (assuming input is 3D: batch, sequence, features)\n        x = x.view(x.size(0), x.size(1), -1)\n        \n        # Apply GRU\n        x, _ = self.gru(x)\n        \n        # Take the last output of the GRU\n        x = x[:, -1, :]\n        \n        # Apply Sequential\n        x = self.sequential(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input: batch_size=1, channels=3, depth=32, height=32, width=32\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_pow_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks5 \n    x4 =xindex \n    tmp0 =(-1 )+2 *x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+((((2 *x0 )//(2 +ks4 ))%(2 +ks3 )))\n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-1 )+(((2 *x0 )%(2 +ks4 )))\n    tmp10 =tmp9 >=tmp1 \n    tmp11 =ks4 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =tl .load (in_ptr0 +((-1 )+((-1 )*ks4 )+ks4 *((((2 *x0 )//(2 +ks4 ))%(2 +ks3 )))+((-1 )*ks3 *ks4 )+2 *ks3 *ks4 *x1 +ks2 *ks3 *ks4 *x2 +(((2 *x0 )%(2 +ks4 )))),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =tmp18 *tmp18 \n    tmp20 =(-1 )+((((1 +2 *x0 )//(2 +ks4 ))%(2 +ks3 )))\n    tmp21 =tmp20 >=tmp1 \n    tmp22 =tmp20 <tmp7 \n    tmp23 =(-1 )+(((1 +2 *x0 )%(2 +ks4 )))\n    tmp24 =tmp23 >=tmp1 \n    tmp25 =tmp23 <tmp11 \n    tmp26 =tmp13 &tmp21 \n    tmp27 =tmp26 &tmp22 \n    tmp28 =tmp27 &tmp24 \n    tmp29 =tmp28 &tmp25 \n    tmp30 =tl .load (in_ptr0 +((-1 )+((-1 )*ks4 )+ks4 *((((1 +2 *x0 )//(2 +ks4 ))%(2 +ks3 )))+((-1 )*ks3 *ks4 )+2 *ks3 *ks4 *x1 +ks2 *ks3 *ks4 *x2 +(((1 +2 *x0 )%(2 +ks4 )))),tmp29 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp31 =tmp30 *tmp30 \n    tmp32 =tmp31 +tmp19 \n    tmp33 =2 *x1 \n    tmp34 =tmp33 >=tmp1 \n    tmp35 =tmp33 <tmp3 \n    tmp36 =tmp34 &tmp35 \n    tmp37 =tmp36 &tmp6 \n    tmp38 =tmp37 &tmp8 \n    tmp39 =tmp38 &tmp10 \n    tmp40 =tmp39 &tmp12 \n    tmp41 =tl .load (in_ptr0 +((-1 )+((-1 )*ks4 )+ks4 *((((2 *x0 )//(2 +ks4 ))%(2 +ks3 )))+2 *ks3 *ks4 *x1 +ks2 *ks3 *ks4 *x2 +(((2 *x0 )%(2 +ks4 )))),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tmp41 *tmp41 \n    tmp43 =tmp42 +tmp32 \n    tmp44 =tmp36 &tmp21 \n    tmp45 =tmp44 &tmp22 \n    tmp46 =tmp45 &tmp24 \n    tmp47 =tmp46 &tmp25 \n    tmp48 =tl .load (in_ptr0 +((-1 )+((-1 )*ks4 )+ks4 *((((1 +2 *x0 )//(2 +ks4 ))%(2 +ks3 )))+2 *ks3 *ks4 *x1 +ks2 *ks3 *ks4 *x2 +(((1 +2 *x0 )%(2 +ks4 )))),tmp47 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp49 =tmp48 *tmp48 \n    tmp50 =tmp49 +tmp43 \n    tmp51 =0.25 \n    tmp52 =tmp50 *tmp51 \n    tl .store (out_ptr0 +(x4 ),tmp52 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_mul_pow_relu_sign_1 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tmp1 <tmp0 \n    tmp3 =tmp2 .to (tl .int8 )\n    tmp4 =tmp0 <tmp1 \n    tmp5 =tmp4 .to (tl .int8 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =tmp6 .to (tmp0 .dtype )\n    tmp8 =tl_math .abs (tmp0 )\n    tmp9 =triton_helpers .maximum (tmp1 ,tmp8 )\n    tmp10 =tmp7 *tmp9 \n    tmp11 =4.0 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =libdevice .sqrt (tmp12 )\n    tl .store (in_out_ptr0 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_mul_pow_relu_sign_view_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x1 +2 *(x0 //ks1 )+ks3 *x1 +ks3 *(x0 //ks1 )+ks4 *x1 +ks4 *(x0 //ks1 )+x1 *((ks3 *ks4 )//2 )+(x0 //ks1 )*((ks3 *ks4 )//2 )+2 *x1 *(ks2 //2 )+ks3 *x1 *(ks2 //2 )+ks4 *x1 *(ks2 //2 )+x1 *(ks2 //2 )*((ks3 *ks4 )//2 )+((x0 %ks1 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +s2 +s3 +((s2 *s3 )//2 )\n        1 +(s1 //2 )\n        2 +s2 +s3 +2 *(s1 //2 )+s2 *(s1 //2 )+s3 *(s1 //2 )+(s1 //2 )*((s2 *s3 )//2 )+((s2 *s3 )//2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,1 +(s1 //2 ),2 +s2 +s3 +((s2 *s3 )//2 )),(2 *s0 +s0 *s2 +s0 *s3 +s0 *((s2 *s3 )//2 )+2 *s0 *(s1 //2 )+s0 *s2 *(s1 //2 )+s0 *s3 *(s1 //2 )+s0 *(s1 //2 )*((s2 *s3 )//2 ),2 +s2 +s3 +2 *(s1 //2 )+s2 *(s1 //2 )+s3 *(s1 //2 )+(s1 //2 )*((s2 *s3 )//2 )+((s2 *s3 )//2 ),2 +s2 +s3 +((s2 *s3 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_pow_0_xnumel =2 *s0 +s0 *s2 +s0 *s3 +s0 *((s2 *s3 )//2 )+2 *s0 *(s1 //2 )+s0 *s2 *(s1 //2 )+s0 *s3 *(s1 //2 )+s0 *(s1 //2 )*((s2 *s3 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_pow_0 [grid (triton_poi_fused_avg_pool2d_pow_0_xnumel )](arg4_1 ,buf0 ,578 ,17 ,32 ,32 ,32 ,9826 ,29478 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg4_1 \n        buf1 =buf0 ;del buf0 \n\n        triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel =2 *s0 +s0 *s2 +s0 *s3 +s0 *((s2 *s3 )//2 )+2 *s0 *(s1 //2 )+s0 *s2 *(s1 //2 )+s0 *s3 *(s1 //2 )+s0 *(s1 //2 )*((s2 *s3 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_mul_pow_relu_sign_1 [grid (triton_poi_fused_abs_mul_pow_relu_sign_1_xnumel )](buf1 ,29478 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        2 +s2 +s3 +2 *(s1 //2 )+s2 *(s1 //2 )+s3 *(s1 //2 )+(s1 //2 )*((s2 *s3 )//2 )+((s2 *s3 )//2 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,2 +s2 +s3 +2 *(s1 //2 )+s2 *(s1 //2 )+s3 *(s1 //2 )+(s1 //2 )*((s2 *s3 )//2 )+((s2 *s3 )//2 )),(2 *s0 +s0 *s2 +s0 *s3 +s0 *((s2 *s3 )//2 )+2 *s0 *(s1 //2 )+s0 *s2 *(s1 //2 )+s0 *s3 *(s1 //2 )+s0 *(s1 //2 )*((s2 *s3 )//2 ),2 +s2 +s3 +2 *(s1 //2 )+s2 *(s1 //2 )+s3 *(s1 //2 )+(s1 //2 )*((s2 *s3 )//2 )+((s2 *s3 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_abs_mul_pow_relu_sign_view_2_xnumel =2 *s0 +s0 *s2 +s0 *s3 +s0 *((s2 *s3 )//2 )+2 *s0 *(s1 //2 )+s0 *s2 *(s1 //2 )+s0 *s3 *(s1 //2 )+s0 *(s1 //2 )*((s2 *s3 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_mul_pow_relu_sign_view_2 [grid (triton_poi_fused_abs_mul_pow_relu_sign_view_2_xnumel )](buf1 ,buf2 ,9826 ,578 ,32 ,32 ,32 ,29478 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9df76539-4ad5-4309-b3de-d0415a6873bc",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softshrink', 'TransformerEncoder', 'RNNCell', 'LazyConv2d', 'CrossMapLRN2d', 'SELU', 'LPPool1d', 'MarginRankingLoss', 'ChannelShuffle', 'Softmax']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lazy_conv2d = nn.LazyConv2d(out_channels=32, kernel_size=3)\n        self.cross_map_lrn2d = nn.CrossMapLRN2d(size=5)\n        self.channel_shuffle = nn.ChannelShuffle(groups=4)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=32, nhead=4), num_layers=2\n        )\n        self.rnn_cell = nn.RNNCell(input_size=32, hidden_size=64)\n        self.lp_pool1d = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.selu = nn.SELU()\n        self.softshrink = nn.Softshrink(lambd=0.5)\n        self.softmax = nn.Softmax(dim=1)\n        self.margin_ranking_loss = nn.MarginRankingLoss()\n\n    def forward(self, x):\n        # Assume input x is of shape (batch_size, channels, height, width)\n        x = self.lazy_conv2d(x)  # Shape: (batch_size, 32, height-2, width-2)\n        x = self.cross_map_lrn2d(x)  # Shape: (batch_size, 32, height-2, width-2)\n        x = self.channel_shuffle(x)  # Shape: (batch_size, 32, height-2, width-2)\n        \n        # Reshape for TransformerEncoder\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1).permute(2, 0, 1)  # Shape: (seq_len, batch_size, channels)\n        x = self.transformer_encoder(x)  # Shape: (seq_len, batch_size, channels)\n        x = x.permute(1, 2, 0).view(batch_size, channels, height, width)  # Shape: (batch_size, channels, height, width)\n        \n        # Reshape for RNNCell\n        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # Shape: (batch_size, seq_len, channels)\n        hx = torch.zeros(batch_size, 64).to(x.device)  # Initialize hidden state for RNNCell\n        for i in range(x.size(1)):\n            hx = self.rnn_cell(x[:, i, :], hx)\n        x = hx  # Shape: (batch_size, 64)\n        \n        # Reshape for LPPool1d\n        x = x.unsqueeze(1)  # Shape: (batch_size, 1, 64)\n        x = self.lp_pool1d(x)  # Shape: (batch_size, 1, 31)\n        x = x.squeeze(1)  # Shape: (batch_size, 31)\n        \n        x = self.selu(x)  # Shape: (batch_size, 31)\n        x = self.softshrink(x)  # Shape: (batch_size, 31)\n        x = self.softmax(x)  # Shape: (batch_size, 31)\n        \n        # MarginRankingLoss requires two inputs and a target\n        # For simplicity, we'll create a dummy input and target\n        input1 = x\n        input2 = torch.rand_like(x)\n        target = torch.randint(0, 2, (batch_size,)).float().to(x.device)\n        loss = self.margin_ranking_loss(input1, input2, target)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =123008 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //3844 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(32 ,3 ,3 ,3 ),(27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_2 ,(32 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,1 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =False ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,32 ,62 ,62 ),(123008 ,3844 ,62 ,1 ))\n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_0 [grid (123008 )](buf1 ,primals_2 ,123008 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n    return (buf1 ,primals_1 ,primals_3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((32 ,3 ,3 ,3 ),(27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9efb7992-c776-45e1-8f0d-4626426d6d73",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Fold', 'L1Loss', 'LeakyReLU', 'MaxPool3d', 'Module', 'FractionalMaxPool2d', 'Hardshrink', 'Tanh', 'ZeroPad1d', 'ReflectionPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad1d = nn.ZeroPad1d(2)\n        self.reflection_pad1d = nn.ReflectionPad1d(2)\n        self.leaky_relu = nn.LeakyReLU(0.1)\n        self.max_pool3d = nn.MaxPool3d(kernel_size=2)\n        self.fractional_max_pool2d = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.hardshrink = nn.Hardshrink(0.5)\n        self.tanh = nn.Tanh()\n        self.fold = nn.Fold(output_size=(28, 28), kernel_size=2, stride=2)\n        self.l1_loss = nn.L1Loss()\n\n    def forward(self, x):\n        # Apply ZeroPad1d and ReflectionPad1d\n        x = self.zero_pad1d(x)\n        x = self.reflection_pad1d(x)\n        \n        # Reshape for MaxPool3d\n        x = x.view(1, 1, x.shape[1], x.shape[2], -1)\n        x = self.max_pool3d(x)\n        \n        # Reshape for FractionalMaxPool2d\n        x = x.view(1, 1, x.shape[2], x.shape[3])\n        x = self.fractional_max_pool2d(x)\n        \n        # Apply LeakyReLU, Hardshrink, and Tanh\n        x = self.leaky_relu(x)\n        x = self.hardshrink(x)\n        x = self.tanh(x)\n        \n        # Reshape for Fold\n        x = x.view(1, -1)\n        x = self.fold(x)\n        \n        # Compute L1Loss with a dummy target\n        dummy_target = torch.zeros_like(x)\n        loss = self.l1_loss(x, dummy_target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 28, 28).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =(-2 )+x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *x1 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tl .store (out_ptr0 +(x2 ),tmp6 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,1 ,s0 ,s1 ),(s0 *s1 ,s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s1 \n        buf0 =empty_strided_cuda ((1 ,1 ,s0 ,4 +s1 ),(4 *s0 +s0 *s1 ,4 *s0 +s0 *s1 ,4 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =4 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg2_1 ,buf0 ,32 ,28 ,896 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =28 \n    arg1_1 =28 \n    arg2_1 =rand_strided ((1 ,1 ,28 ,28 ),(784 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9f083ae7-15bb-402a-896a-d3fa4207aba8",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ZeroPad3d', 'ReplicationPad1d', 'SELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad3d = nn.ZeroPad3d(padding=(1, 1, 1, 1, 1, 1))\n        self.replication_pad1d = nn.ReplicationPad1d(padding=(2, 2))\n        self.selu = nn.SELU()\n\n    def forward(self, x):\n        # Apply ZeroPad3d to the input\n        x = self.zero_pad3d(x)\n        \n        # Reshape the tensor to fit ReplicationPad1d\n        # Assuming the input is 5D (batch, channels, depth, height, width)\n        # We reshape it to 3D (batch * depth * height, channels, width)\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size * depth * height, channels, width)\n        \n        # Apply ReplicationPad1d\n        x = self.replication_pad1d(x)\n        \n        # Reshape back to the original 5D shape\n        x = x.view(batch_size, depth, height, channels, width + 4)  # +4 due to padding\n        \n        # Apply SELU activation\n        x = self.selu(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 10, 10, 10).cuda()  # Example input: (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_elu_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =6912 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %16 )\n    x1 =((xindex //16 )%36 )\n    x2 =xindex //576 \n    x4 =xindex \n    tmp0 =(-1 )+((((12 *x1 +432 *x2 +((11 )*((11 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<(11 ))))//144 )%12 ))\n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],10 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+((((12 *x1 +((11 )*((11 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<(11 ))))//12 )%12 ))\n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =(-1 )+((11 )*((11 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<(11 )))\n    tmp9 =tmp8 >=tmp1 \n    tmp10 =tmp8 <tmp3 \n    tmp11 =tmp2 &tmp4 \n    tmp12 =tmp11 &tmp6 \n    tmp13 =tmp12 &tmp7 \n    tmp14 =tmp13 &tmp9 \n    tmp15 =tmp14 &tmp10 \n    tmp16 =tl .load (in_ptr0 +((-111 )+10 *((((12 *x1 +((11 )*((11 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<(11 ))))//12 )%12 ))+100 *((((12 *x1 +432 *x2 +((11 )*((11 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<(11 ))))//144 )%12 ))+1000 *((12 *x1 +432 *x2 +((11 )*((11 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<(11 ))))//1728 )+((11 )*((11 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<(11 )))),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp17 =0.0 \n    tmp18 =tmp16 >tmp17 \n    tmp19 =1.0507009873554805 \n    tmp20 =tmp16 *tmp19 \n    tmp21 =1.0 \n    tmp22 =tmp16 *tmp21 \n    tmp23 =libdevice .expm1 (tmp22 )\n    tmp24 =1.7580993408473766 \n    tmp25 =tmp23 *tmp24 \n    tmp26 =tl .where (tmp18 ,tmp20 ,tmp25 )\n    tl .store (out_ptr0 +(x4 ),tmp26 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,3 ,10 ,10 ,10 ),(3000 ,1000 ,100 ,10 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,12 ,12 ,3 ,16 ),(6912 ,576 ,48 ,16 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_elu_0 [grid (6912 )](arg0_1 ,buf0 ,6912 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,3 ,10 ,10 ,10 ),(3000 ,1000 ,100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "9f6a3a7b-9708-49b7-a6cd-5bf962ed415d",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReplicationPad1d', 'ParameterDict', 'CircularPad2d', 'LSTM', 'LazyConv1d', 'LazyLinear', 'TransformerEncoder', 'Conv1d', 'CosineSimilarity', 'AvgPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.replication_pad1d = nn.ReplicationPad1d(2)\n        self.circular_pad2d = nn.CircularPad2d(1)\n        self.lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.lazy_conv1d = nn.LazyConv1d(out_channels=32, kernel_size=3)\n        self.lazy_linear = nn.LazyLinear(out_features=50)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=64, nhead=8), num_layers=3\n        )\n        self.conv1d = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n        self.avg_pool1d = nn.AvgPool1d(kernel_size=2)\n        self.parameter_dict = nn.ParameterDict({\n            'param1': nn.Parameter(torch.randn(10)),\n            'param2': nn.Parameter(torch.randn(20))\n        })\n\n    def forward(self, x):\n        # Apply ReplicationPad1d\n        x = self.replication_pad1d(x)\n        \n        # Reshape for CircularPad2d (assuming 2D padding is needed)\n        x = x.unsqueeze(1)  # Add a dummy channel dimension\n        x = self.circular_pad2d(x)\n        x = x.squeeze(1)  # Remove the dummy channel dimension\n        \n        # Reshape for LSTM (assuming sequence data)\n        x = x.permute(0, 2, 1)  # Swap dimensions for LSTM\n        x, _ = self.lstm(x)\n        \n        # Reshape for LazyConv1d\n        x = x.permute(0, 2, 1)\n        x = self.lazy_conv1d(x)\n        \n        # Apply AvgPool1d\n        x = self.avg_pool1d(x)\n        \n        # Reshape for TransformerEncoder\n        x = x.permute(2, 0, 1)  # Transformer expects (seq_len, batch_size, features)\n        x = self.transformer_encoder(x)\n        x = x.permute(1, 2, 0)  # Reshape back to (batch_size, features, seq_len)\n        \n        # Apply Conv1d\n        x = self.conv1d(x)\n        \n        # Reshape for LazyLinear\n        x = x.permute(0, 2, 1)\n        x = self.lazy_linear(x)\n        \n        # Apply CosineSimilarity (assuming two tensors are needed)\n        x2 = torch.randn_like(x)  # Random tensor for cosine similarity\n        x = self.cosine_similarity(x, x2)\n        \n        # Use ParameterDict parameters (just for demonstration)\n        x = x + self.parameter_dict['param1'].unsqueeze(0)  # Add parameter to the output\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 64).cuda()  # Arbitrary shape (batch_size, channels, sequence_length)\n    return [x]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =840 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %70 )\n    x1 =xindex //70 \n    x2 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =68 +x0 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .full ([1 ],69 ,tl .int64 )\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =x1 \n    tmp11 =tl .full ([1 ],1 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .full ([1 ],11 ,tl .int64 )\n    tmp14 =tmp10 <tmp13 \n    tmp15 =tmp12 &tmp14 \n    tmp16 =tmp15 &tmp9 \n    tmp17 =tl .load (in_ptr0 +((-1 )+64 *x1 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp18 =tl .load (in_ptr1 +(68 +x2 ),tmp9 &xmask ,other =0.0 )\n    tmp19 =tl .where (tmp15 ,tmp17 ,tmp18 )\n    tmp20 =tl .full (tmp19 .shape ,0.0 ,tmp19 .dtype )\n    tmp21 =tl .where (tmp9 ,tmp19 ,tmp20 )\n    tmp22 =float (\"nan\")\n    tmp23 =tl .where (tmp8 ,tmp21 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tmp0 >=tmp1 \n    tmp27 =tl .full ([1 ],69 ,tl .int64 )\n    tmp28 =tmp0 <tmp27 \n    tmp29 =tmp26 &tmp28 \n    tmp30 =x1 \n    tmp31 =tl .full ([1 ],1 ,tl .int64 )\n    tmp32 =tmp30 >=tmp31 \n    tmp33 =tl .full ([1 ],11 ,tl .int64 )\n    tmp34 =tmp30 <tmp33 \n    tmp35 =tmp32 &tmp34 \n    tmp36 =tmp35 &tmp29 \n    tmp37 =tl .load (in_ptr0 +((-64 )+64 *x1 +((63 )*((63 )<=(((0 )*((0 )>=((-3 )+x0 ))+((-3 )+x0 )*(((-3 )+x0 )>(0 )))))+(((0 )*((0 )>=((-3 )+x0 ))+((-3 )+x0 )*(((-3 )+x0 )>(0 ))))*((((0 )*((0 )>=((-3 )+x0 ))+((-3 )+x0 )*(((-3 )+x0 )>(0 ))))<(63 )))),tmp36 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp38 =tl .load (in_ptr1 +(x2 ),tmp29 &xmask ,other =0.0 )\n    tmp39 =tl .where (tmp35 ,tmp37 ,tmp38 )\n    tmp40 =tl .full (tmp39 .shape ,0.0 ,tmp39 .dtype )\n    tmp41 =tl .where (tmp29 ,tmp39 ,tmp40 )\n    tmp42 =float (\"nan\")\n    tmp43 =tl .where (tmp29 ,tmp41 ,tmp42 )\n    tmp44 =tl .where (tmp2 ,tmp25 ,tmp43 )\n    tl .store (out_ptr0 +(x2 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =840 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //70 \n    x0 =(xindex %70 )\n    x2 =xindex \n    tmp41 =tl .load (in_ptr0 +(x2 ),xmask )\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],11 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(-10 )+x1 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .full ([1 ],69 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(1 +70 *x1 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x2 ),tmp6 &xmask ,other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =x0 \n    tmp17 =tl .full ([1 ],69 ,tl .int64 )\n    tmp18 =tmp16 >=tmp17 \n    tmp19 =tmp18 &tmp2 \n    tmp20 =tl .load (in_ptr0 +((-699 )+70 *x1 ),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr0 +((-700 )+x2 ),tmp2 &xmask ,other =0.0 )\n    tmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\n    tmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],1 ,tl .int64 )\n    tmp27 =tmp0 <tmp26 \n    tmp28 =x0 \n    tmp29 =tl .full ([1 ],69 ,tl .int64 )\n    tmp30 =tmp28 >=tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(701 +70 *x1 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(700 +x2 ),tmp27 &xmask ,other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =x0 \n    tmp38 =tl .full ([1 ],69 ,tl .int64 )\n    tmp39 =tmp37 >=tmp38 \n    tmp40 =tl .load (in_ptr0 +(1 +70 *x1 ),tmp39 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tl .where (tmp39 ,tmp40 ,tmp41 )\n    tmp43 =tl .where (tmp27 ,tmp36 ,tmp42 )\n    tmp44 =tl .where (tmp2 ,tmp25 ,tmp43 )\n    tl .store (out_ptr0 +(x2 ),tmp44 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,10 ,64 ),(640 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,12 ,70 ),(840 ,840 ,70 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,1 ,12 ,70 ),(840 ,840 ,70 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (840 )](arg0_1 ,buf0 ,buf1 ,840 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n        buf2 =reinterpret_tensor (buf0 ,(1 ,1 ,12 ,70 ),(840 ,1 ,70 ,1 ),0 );del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_1 [grid (840 )](buf1 ,buf2 ,840 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n    return (reinterpret_tensor (buf2 ,(1 ,70 ,12 ),(840 ,1 ,70 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,10 ,64 ),(640 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a0ab1ded-eb0a-42e2-b116-5fe0f5dfe8c0",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxUnpool1d', 'GRU', 'Mish', 'UpsamplingBilinear2d', 'GELU', 'SELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)\n        self.gru = nn.GRU(input_size=128, hidden_size=256, num_layers=2, batch_first=True)\n        self.mish = nn.Mish()\n        self.upsampling_bilinear2d = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.gelu = nn.GELU()\n        self.selu = nn.SELU()\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, channels, length)\n        batch_size, channels, length = x.shape\n        \n        # MaxUnpool1d requires indices from a previous MaxPool1d operation\n        # For simplicity, we'll create dummy indices\n        pool_output, indices = F.max_pool1d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool1d(pool_output, indices)\n        \n        # Reshape for GRU\n        x = x.permute(0, 2, 1)  # (batch_size, length, channels)\n        x, _ = self.gru(x)\n        \n        # Apply Mish activation\n        x = self.mish(x)\n        \n        # Reshape for UpsamplingBilinear2d\n        x = x.permute(0, 2, 1)  # (batch_size, channels, length)\n        x = x.unsqueeze(-1)  # (batch_size, channels, length, 1)\n        x = self.upsampling_bilinear2d(x)\n        \n        # Apply GELU activation\n        x = self.gelu(x)\n        \n        # Apply SELU activation\n        x = self.selu(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 128, 64).cuda()  # (batch_size, channels, length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *((x0 %(ks0 //2 )))+ks0 *(triton_helpers .div_floor_integer (x0 ,ks0 //2 ))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *((x0 %(ks0 //2 )))+ks0 *(triton_helpers .div_floor_integer (x0 ,ks0 //2 ))),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp7 =tl .full ([1 ],2 ,tl .int32 )\n    tmp8 =tl .where ((tmp5 <0 )!=(tmp7 <0 ),tl .where (tmp5 %tmp7 !=0 ,tmp5 //tmp7 -1 ,tmp5 //tmp7 ),tmp5 //tmp7 )\n    tmp9 =tmp8 *tmp7 \n    tmp10 =tmp5 -tmp9 \n    tmp11 =tl .full ([1 ],0 ,tl .int64 )\n    tmp12 =tmp11 +tmp8 \n    tmp13 =2 *((x0 %(ks0 //2 )))\n    tmp14 =tmp13 +tmp10 \n    tmp15 =ks0 \n    tmp16 =tmp12 *tmp15 \n    tmp17 =tmp16 +tmp14 \n    tmp18 =2 *(ks0 //2 )*(triton_helpers .div_floor_integer (x0 ,ks0 //2 ))\n    tmp19 =tmp17 +tmp18 \n    tmp20 =2 *ks1 *(ks0 //2 )\n    tmp21 =tmp19 +tmp20 \n    tmp22 =tmp19 <0 \n    tmp23 =tl .where (tmp22 ,tmp21 ,tmp19 )\n    tl .device_assert (((0 <=tmp23 )&(tmp23 <2 *ks1 *(ks0 //2 )))|~(xmask ),\"index out of bounds: 0 <= tmp23 < 2*ks1*(ks0 // 2)\")\n    tl .store (out_ptr0 +(tl .broadcast_to ((tmp23 %(2 *ks1 *(ks0 //2 ))),[XBLOCK ])),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_permute_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +2 *(ks2 //2 )*((((x0 +2 *x1 *(ks2 //2 ))//(2 *(ks2 //2 )))%ks1 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,2 *(s1 //2 ),1 ),(2 *s0 *(s1 //2 ),2 *(s1 //2 ),1 ,1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_0_xnumel =2 *s0 *(s1 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_0 [grid (triton_poi_fused_max_unpool2d_0_xnumel )](buf0 ,8192 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool2d_1_xnumel =s0 *(s1 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_1 [grid (triton_poi_fused_max_unpool2d_1_xnumel )](arg2_1 ,buf0 ,64 ,128 ,4096 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        2 *(s1 //2 )\n        buf2 =empty_strided_cuda ((1 ,2 *(s1 //2 ),s0 ),(2 *s0 *(s1 //2 ),1 ,2 *(s1 //2 )),torch .float32 )\n\n        triton_poi_fused_permute_2_xnumel =2 *s0 *(s1 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_permute_2 [grid (triton_poi_fused_permute_2_xnumel )](buf0 ,buf2 ,64 ,128 ,64 ,8192 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =128 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,128 ,64 ),(8192 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a284ca42-58b3-4b9e-b2f6-d05d1189600d",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LazyBatchNorm3d', 'Softshrink', 'MaxUnpool3d', 'Tanh', 'ConvTranspose1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.bn1 = nn.LazyBatchNorm3d()\n        self.softshrink1 = nn.Softshrink()\n        self.maxunpool1 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.tanh1 = nn.Tanh()\n        self.convtranspose1 = nn.ConvTranspose1d(10, 20, kernel_size=5, stride=2)\n        self.bn2 = nn.LazyBatchNorm3d()\n        self.softshrink2 = nn.Softshrink()\n        self.maxunpool2 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.tanh2 = nn.Tanh()\n        self.convtranspose2 = nn.ConvTranspose1d(20, 30, kernel_size=5, stride=2)\n        self.bn3 = nn.LazyBatchNorm3d()\n        self.softshrink3 = nn.Softshrink()\n        self.maxunpool3 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.tanh3 = nn.Tanh()\n        self.convtranspose3 = nn.ConvTranspose1d(30, 40, kernel_size=5, stride=2)\n        self.bn4 = nn.LazyBatchNorm3d()\n        self.softshrink4 = nn.Softshrink()\n        self.maxunpool4 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.tanh4 = nn.Tanh()\n        self.convtranspose4 = nn.ConvTranspose1d(40, 50, kernel_size=5, stride=2)\n        self.bn5 = nn.LazyBatchNorm3d()\n        self.softshrink5 = nn.Softshrink()\n        self.maxunpool5 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.tanh5 = nn.Tanh()\n        self.convtranspose5 = nn.ConvTranspose1d(50, 60, kernel_size=5, stride=2)\n\n    def forward(self, x):\n        # Assuming input is 5D (batch, channels, depth, height, width)\n        x = self.bn1(x)\n        x = self.softshrink1(x)\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten to 3D for ConvTranspose1d\n        x = self.convtranspose1(x)\n        x = x.view(x.size(0), x.size(1), x.size(2), 1, 1)  # Reshape back to 5D\n        x = self.maxunpool1(x)\n        x = self.tanh1(x)\n        \n        x = self.bn2(x)\n        x = self.softshrink2(x)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.convtranspose2(x)\n        x = x.view(x.size(0), x.size(1), x.size(2), 1, 1)\n        x = self.maxunpool2(x)\n        x = self.tanh2(x)\n        \n        x = self.bn3(x)\n        x = self.softshrink3(x)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.convtranspose3(x)\n        x = x.view(x.size(0), x.size(1), x.size(2), 1, 1)\n        x = self.maxunpool3(x)\n        x = self.tanh3(x)\n        \n        x = self.bn4(x)\n        x = self.softshrink4(x)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.convtranspose4(x)\n        x = x.view(x.size(0), x.size(1), x.size(2), 1, 1)\n        x = self.maxunpool4(x)\n        x = self.tanh4(x)\n        \n        x = self.bn5(x)\n        x = self.softshrink5(x)\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.convtranspose5(x)\n        x = x.view(x.size(0), x.size(1), x.size(2), 1, 1)\n        x = self.maxunpool5(x)\n        x = self.tanh5(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 32, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_functional_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =40 \n    r0_numel =8192 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +8192 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp3 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr4 ,out_ptr6 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    R0_BLOCK :tl .constexpr =4 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\n    tmp2 =tl .load (in_ptr2 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\n    tmp25 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp30 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (xmask ,tmp3 ,0 )\n    tmp8 =tl .where (xmask ,tmp4 ,0 )\n    tmp9 =tl .where (xmask ,tmp5 ,0 )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\n    tmp13 =tmp10 [:,None ]\n    tmp14 =tmp11 [:,None ]\n    tmp15 =tmp12 [:,None ]\n    tmp16 =32768.0 \n    tmp17 =tmp14 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tmp21 =1.000030518509476 \n    tmp22 =tmp17 *tmp21 \n    tmp23 =0.1 \n    tmp24 =tmp22 *tmp23 \n    tmp26 =0.9 \n    tmp27 =tmp25 *tmp26 \n    tmp28 =tmp24 +tmp27 \n    tmp29 =tmp13 *tmp23 \n    tmp31 =tmp30 *tmp26 \n    tmp32 =tmp29 +tmp31 \n    tl .store (out_ptr2 +(x0 ),tmp20 ,xmask )\n    tl .store (out_ptr4 +(x0 ),tmp28 ,xmask )\n    tl .store (out_ptr6 +(x0 ),tmp32 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_functional_abs_gt_mul_sign_sub_where_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =xindex \n    x1 =xindex //32768 \n    tmp0 =tl .load (in_ptr0 +(x2 ),None )\n    tmp1 =tl .load (in_ptr1 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr3 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr4 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp4 =tmp2 *tmp3 \n    tmp6 =tmp4 *tmp5 \n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl_math .abs (tmp8 )\n    tmp10 =0.5 \n    tmp11 =tmp9 >tmp10 \n    tmp12 =tl .full ([1 ],0 ,tl .int32 )\n    tmp13 =tmp12 <tmp8 \n    tmp14 =tmp13 .to (tl .int8 )\n    tmp15 =tmp8 <tmp12 \n    tmp16 =tmp15 .to (tl .int8 )\n    tmp17 =tmp14 -tmp16 \n    tmp18 =tmp17 .to (tmp8 .dtype )\n    tmp19 =tmp18 *tmp10 \n    tmp20 =tmp8 -tmp19 \n    tmp21 =0.0 \n    tmp22 =tmp8 *tmp21 \n    tmp23 =tl .where (tmp11 ,tmp20 ,tmp22 )\n    tl .store (in_out_ptr0 +(x2 ),tmp23 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_3 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1310780 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //65539 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_4 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .full ([1 ],1 ,tl .int64 )\n    tmp3 =tmp1 +tmp2 \n    tl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ,32 ,32 ,32 ),(327680 ,32768 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_2 ,(),())\n    assert_size_stride (primals_3 ,(10 ,),(1 ,))\n    assert_size_stride (primals_4 ,(10 ,),(1 ,))\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    assert_size_stride (primals_6 ,(10 ,),(1 ,))\n    assert_size_stride (primals_7 ,(10 ,20 ,5 ),(100 ,5 ,1 ))\n    assert_size_stride (primals_8 ,(20 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ,4 ),(40 ,4 ,40 ,40 ,40 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ,4 ),(40 ,4 ,40 ,40 ,40 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ,4 ),(40 ,4 ,40 ,40 ,40 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_functional_0 [grid (40 )](primals_1 ,buf0 ,buf1 ,buf2 ,40 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,1 ,1 ,1 ),torch .float32 )\n        buf6 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_1 [grid (10 )](buf0 ,buf1 ,buf2 ,primals_4 ,primals_3 ,buf3 ,buf6 ,primals_4 ,primals_3 ,10 ,4 ,XBLOCK =8 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n        del buf1 \n        del buf2 \n        del primals_3 \n        del primals_4 \n        buf7 =empty_strided_cuda ((1 ,10 ,32 ,32 ,32 ),(327680 ,32768 ,1024 ,32 ,1 ),torch .float32 )\n        buf8 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_functional_abs_gt_mul_sign_sub_where_2 [grid (327680 )](buf8 ,primals_1 ,buf3 ,buf6 ,primals_5 ,primals_6 ,327680 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n\n        buf9 =extern_kernels .convolution (reinterpret_tensor (buf8 ,(1 ,10 ,32768 ),(0 ,32768 ,1 ),0 ),primals_7 ,stride =(2 ,),padding =(0 ,),dilation =(1 ,),transposed =True ,output_padding =(0 ,),groups =1 ,bias =None )\n        assert_size_stride (buf9 ,(1 ,20 ,65539 ),(1310780 ,65539 ,1 ))\n        buf10 =buf9 ;del buf9 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_3 [grid (1310780 )](buf10 ,primals_8 ,1310780 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_8 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_4 [grid (1 )](primals_2 ,primals_2 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_2 \n    return (reinterpret_tensor (buf10 ,(1 ,20 ,65539 ,1 ,1 ),(1310780 ,65539 ,1 ,1 ,1 ),0 ),primals_1 ,primals_5 ,primals_6 ,primals_7 ,buf3 ,buf6 ,reinterpret_tensor (buf8 ,(1 ,10 ,32768 ),(327680 ,32768 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ,32 ,32 ,32 ),(327680 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_3 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((10 ,20 ,5 ),(100 ,5 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a3998dcc-fabe-4fa7-927a-0d4a0af7ac7a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softmin', 'ReplicationPad1d', 'SELU', 'Mish']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.replication_pad1d = nn.ReplicationPad1d(2)\n        self.softmin = nn.Softmin(dim=1)\n        self.selu = nn.SELU()\n        self.mish = nn.Mish()\n\n    def forward(self, x):\n        # Ensure the input is at least 3D (batch, channels, length)\n        if x.dim() == 2:\n            x = x.unsqueeze(1)  # Add channel dimension if necessary\n        \n        # Apply ReplicationPad1d\n        x = self.replication_pad1d(x)\n        \n        # Apply SELU\n        x = self.selu(x)\n        \n        # Apply Mish\n        x = self.mish(x)\n        \n        # Apply Softmin\n        x = self.softmin(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 32).cuda()  # Example input: (batch_size, channels, length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_elu_mish_neg_replication_pad1d_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp20 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(ks0 *r0_1 +(((-1 )+ks0 )*(((-1 )+ks0 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks0 )))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =0.0 \n        tmp2 =tmp0 >tmp1 \n        tmp3 =1.0507009873554805 \n        tmp4 =tmp0 *tmp3 \n        tmp5 =1.0 \n        tmp6 =tmp0 *tmp5 \n        tmp7 =libdevice .expm1 (tmp6 )\n        tmp8 =1.7580993408473766 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tl .where (tmp2 ,tmp4 ,tmp9 )\n        tmp11 =20.0 \n        tmp12 =tmp10 >tmp11 \n        tmp13 =tl_math .exp (tmp10 )\n        tmp14 =libdevice .log1p (tmp13 )\n        tmp15 =tl .where (tmp12 ,tmp10 ,tmp14 )\n        tmp16 =libdevice .tanh (tmp15 )\n        tmp17 =tmp10 *tmp16 \n        tmp18 =-tmp17 \n        tmp19 =tl .broadcast_to (tmp18 ,[XBLOCK ,R0_BLOCK ])\n        tmp21 =triton_helpers .maximum (_tmp20 ,tmp19 )\n        _tmp20 =tl .where (r0_mask &xmask ,tmp21 ,_tmp20 )\n    tmp20 =triton_helpers .max2 (_tmp20 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp20 ,xmask )\n    _tmp44 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp22 =tl .load (in_ptr0 +(ks0 *r0_1 +(((-1 )+ks0 )*(((-1 )+ks0 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks0 )))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp23 =0.0 \n        tmp24 =tmp22 >tmp23 \n        tmp25 =1.0507009873554805 \n        tmp26 =tmp22 *tmp25 \n        tmp27 =1.0 \n        tmp28 =tmp22 *tmp27 \n        tmp29 =libdevice .expm1 (tmp28 )\n        tmp30 =1.7580993408473766 \n        tmp31 =tmp29 *tmp30 \n        tmp32 =tl .where (tmp24 ,tmp26 ,tmp31 )\n        tmp33 =20.0 \n        tmp34 =tmp32 >tmp33 \n        tmp35 =tl_math .exp (tmp32 )\n        tmp36 =libdevice .log1p (tmp35 )\n        tmp37 =tl .where (tmp34 ,tmp32 ,tmp36 )\n        tmp38 =libdevice .tanh (tmp37 )\n        tmp39 =tmp32 *tmp38 \n        tmp40 =-tmp39 \n        tmp41 =tmp40 -tmp20 \n        tmp42 =tl_math .exp (tmp41 )\n        tmp43 =tl .broadcast_to (tmp42 ,[XBLOCK ,R0_BLOCK ])\n        tmp45 =_tmp44 +tmp43 \n        _tmp44 =tl .where (r0_mask &xmask ,tmp45 ,_tmp44 )\n    tmp44 =tl .sum (_tmp44 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_elu_mish_neg_replication_pad1d_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks1 *x1 +(((-1 )+ks1 )*(((-1 )+ks1 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks1 )))),xmask ,eviction_policy ='evict_last')\n    tmp19 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp22 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =1.0507009873554805 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =1.0 \n    tmp6 =tmp0 *tmp5 \n    tmp7 =libdevice .expm1 (tmp6 )\n    tmp8 =1.7580993408473766 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =tl .where (tmp2 ,tmp4 ,tmp9 )\n    tmp11 =20.0 \n    tmp12 =tmp10 >tmp11 \n    tmp13 =tl_math .exp (tmp10 )\n    tmp14 =libdevice .log1p (tmp13 )\n    tmp15 =tl .where (tmp12 ,tmp10 ,tmp14 )\n    tmp16 =libdevice .tanh (tmp15 )\n    tmp17 =tmp10 *tmp16 \n    tmp18 =-tmp17 \n    tmp20 =tmp18 -tmp19 \n    tmp21 =tl_math .exp (tmp20 )\n    tmp23 =tmp21 /tmp22 \n    tl .store (out_ptr0 +(x2 ),tmp23 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,4 +s1 ),(4 +s1 ,4 +s1 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,1 ,4 +s1 ),(4 +s1 ,4 +s1 ,1 ),torch .float32 )\n\n        triton_red_fused__softmax_elu_mish_neg_replication_pad1d_0_xnumel =4 +s1 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_elu_mish_neg_replication_pad1d_0 [grid (triton_red_fused__softmax_elu_mish_neg_replication_pad1d_0_xnumel )](arg2_1 ,buf0 ,buf1 ,32 ,36 ,10 ,XBLOCK =1 ,R0_BLOCK =16 ,num_warps =2 ,num_stages =1 )\n        4 +s1 \n        buf2 =empty_strided_cuda ((1 ,s0 ,4 +s1 ),(4 *s0 +s0 *s1 ,4 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused__softmax_elu_mish_neg_replication_pad1d_1_xnumel =4 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused__softmax_elu_mish_neg_replication_pad1d_1 [grid (triton_poi_fused__softmax_elu_mish_neg_replication_pad1d_1_xnumel )](arg2_1 ,buf0 ,buf1 ,buf2 ,36 ,32 ,360 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        del buf0 \n        del buf1 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =32 \n    arg2_1 =rand_strided ((1 ,10 ,32 ),(320 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a3c4c9f9-8be7-4bdf-9570-6e9a648012ea",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Identity', 'LeakyReLU', 'ReLU', 'CircularPad3d', 'ReplicationPad3d', 'Mish', 'MSELoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.identity = nn.Identity()\n        self.leaky_relu = nn.LeakyReLU()\n        self.relu = nn.ReLU()\n        self.circular_pad3d = nn.CircularPad3d(1)\n        self.replication_pad3d = nn.ReplicationPad3d(1)\n        self.mish = nn.Mish()\n        self.mse_loss = nn.MSELoss()\n\n    def forward(self, x):\n        # Apply CircularPad3d\n        x = self.circular_pad3d(x)\n        \n        # Apply ReplicationPad3d\n        x = self.replication_pad3d(x)\n        \n        # Apply Identity\n        x = self.identity(x)\n        \n        # Apply LeakyReLU\n        x = self.leaky_relu(x)\n        \n        # Apply ReLU\n        x = self.relu(x)\n        \n        # Apply Mish\n        x = self.mish(x)\n        \n        # Apply MSELoss (assuming a target tensor of the same shape as x)\n        target = torch.zeros_like(x)\n        loss = self.mse_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape for 3D operations\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks2 )\n    x2 =((xindex //ks4 )%ks5 )\n    x3 =xindex //ks7 \n    x5 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =ks1 +x0 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .broadcast_to (1 +ks1 ,[XBLOCK ])\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =x1 \n    tmp11 =tl .full ([1 ],1 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp14 =tmp10 <tmp13 \n    tmp15 =tmp12 &tmp14 \n    tmp16 =tmp15 &tmp9 \n    tmp17 =x2 \n    tmp18 =tl .full ([1 ],1 ,tl .int64 )\n    tmp19 =tmp17 >=tmp18 \n    tmp20 =tl .broadcast_to (1 +ks6 ,[XBLOCK ])\n    tmp21 =tmp17 <tmp20 \n    tmp22 =tmp19 &tmp21 \n    tmp23 =tmp22 &tmp16 \n    tmp24 =tl .load (in_ptr0 +((-1 )+x0 +ks1 *x1 +((-1 )*ks1 *ks3 )+ks1 *ks3 *x2 +ks1 *ks3 *ks6 *x3 ),tmp23 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp25 =tl .load (in_ptr1 +(ks1 +x5 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tl .full (tmp26 .shape ,0.0 ,tmp26 .dtype )\n    tmp28 =tl .where (tmp16 ,tmp26 ,tmp27 )\n    tmp29 =tl .load (in_ptr1 +(ks1 +x5 ),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp30 =tl .where (tmp15 ,tmp28 ,tmp29 )\n    tmp31 =tl .full (tmp30 .shape ,0.0 ,tmp30 .dtype )\n    tmp32 =tl .where (tmp9 ,tmp30 ,tmp31 )\n    tmp33 =float (\"nan\")\n    tmp34 =tl .where (tmp8 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp2 ,tmp34 ,tmp35 )\n    tmp37 =tmp0 >=tmp1 \n    tmp38 =1 +ks1 \n    tmp39 =tmp0 <tmp38 \n    tmp40 =tmp37 &tmp39 \n    tmp41 =x1 \n    tmp42 =tl .full ([1 ],1 ,tl .int64 )\n    tmp43 =tmp41 >=tmp42 \n    tmp44 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp45 =tmp41 <tmp44 \n    tmp46 =tmp43 &tmp45 \n    tmp47 =tmp46 &tmp40 \n    tmp48 =x2 \n    tmp49 =tl .full ([1 ],1 ,tl .int64 )\n    tmp50 =tmp48 >=tmp49 \n    tmp51 =tl .broadcast_to (1 +ks6 ,[XBLOCK ])\n    tmp52 =tmp48 <tmp51 \n    tmp53 =tmp50 &tmp52 \n    tmp54 =tmp53 &tmp47 \n    tmp55 =tl .load (in_ptr0 +((-1 )+x0 +((-1 )*ks1 )+ks1 *x1 +((-1 )*ks1 *ks3 )+ks1 *ks3 *x2 +ks1 *ks3 *ks6 *x3 ),tmp54 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp56 =tl .load (in_ptr1 +(x5 ),tmp47 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp57 =tl .where (tmp53 ,tmp55 ,tmp56 )\n    tmp58 =tl .full (tmp57 .shape ,0.0 ,tmp57 .dtype )\n    tmp59 =tl .where (tmp47 ,tmp57 ,tmp58 )\n    tmp60 =tl .load (in_ptr1 +(x5 ),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp61 =tl .where (tmp46 ,tmp59 ,tmp60 )\n    tmp62 =tl .full (tmp61 .shape ,0.0 ,tmp61 .dtype )\n    tmp63 =tl .where (tmp40 ,tmp61 ,tmp62 )\n    tmp64 =float (\"nan\")\n    tmp65 =tl .where (tmp40 ,tmp63 ,tmp64 )\n    tmp66 =tl .where (tmp2 ,tmp36 ,tmp65 )\n    tl .store (out_ptr0 +(x5 ),tmp66 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x4 =xindex //ks0 \n    x3 =xindex \n    tmp41 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =1 +ks2 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =x1 +((-1 )*ks2 )\n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(1 +2 *x4 +ks3 *x4 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x3 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =x0 \n    tmp17 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp18 =tmp16 >=tmp17 \n    tmp19 =tmp18 &tmp2 \n    tmp20 =tl .load (in_ptr0 +(1 +((-2 )*ks2 )+2 *x4 +ks3 *x4 +((-1 )*ks2 *ks3 )),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr0 +(x3 +((-2 )*ks2 )+((-1 )*ks2 *ks3 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\n    tmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],1 ,tl .int64 )\n    tmp27 =tmp0 <tmp26 \n    tmp28 =x0 \n    tmp29 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp30 =tmp28 >=tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(1 +2 *ks2 +2 *x4 +ks2 *ks3 +ks3 *x4 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(x3 +2 *ks2 +ks2 *ks3 ),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =x0 \n    tmp38 =1 +ks3 \n    tmp39 =tmp37 >=tmp38 \n    tmp40 =tl .load (in_ptr0 +(1 +2 *x4 +ks3 *x4 ),tmp39 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tl .where (tmp39 ,tmp40 ,tmp41 )\n    tmp43 =tl .where (tmp27 ,tmp36 ,tmp42 )\n    tmp44 =tl .where (tmp2 ,tmp25 ,tmp43 )\n    tl .store (out_ptr0 +(x3 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_leaky_relu_replication_pad3d_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x6 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks3 )\n    x1 =((xindex //ks3 )%ks4 )\n    x3 =xindex //ks5 \n    x2 =((xindex //ks8 )%ks1 )\n    x8 =xindex \n    tmp15 =tl .load (in_ptr0 +(2 *((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<(1 +ks6 )))+4 *((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))<(1 +ks2 )))+8 *x3 +ks7 *((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<(1 +ks6 )))+2 *ks6 *((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))<(1 +ks2 )))+2 *ks7 *((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))<(1 +ks2 )))+4 *ks2 *x3 +4 *ks6 *x3 +4 *ks7 *x3 +ks6 *ks7 *((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))<(1 +ks2 )))+2 *ks2 *ks6 *x3 +2 *ks2 *ks7 *x3 +2 *ks6 *ks7 *x3 +ks2 *ks6 *ks7 *x3 +((1 +ks7 )*((1 +ks7 )<=(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))<(1 +ks7 )))),xmask ,eviction_policy ='evict_last')\n    tmp0 =((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x6 ))+((-1 )+x6 )*(((-1 )+x6 )>(0 )))))+(((0 )*((0 )>=((-1 )+x6 ))+((-1 )+x6 )*(((-1 )+x6 )>(0 ))))*((((0 )*((0 )>=((-1 )+x6 ))+((-1 )+x6 )*(((-1 )+x6 )>(0 ))))<(1 +ks2 )))\n    tmp1 =1 +ks2 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =((-1 )*ks2 )+((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x6 ))+((-1 )+x6 )*(((-1 )+x6 )>(0 )))))+(((0 )*((0 )>=((-1 )+x6 ))+((-1 )+x6 )*(((-1 )+x6 )>(0 ))))*((((0 )*((0 )>=((-1 )+x6 ))+((-1 )+x6 )*(((-1 )+x6 )>(0 ))))<(1 +ks2 )))\n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =tl .load (in_ptr0 +(2 *((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<(1 +ks6 )))+4 *ks2 +8 *x3 +ks7 *((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<(1 +ks6 )))+2 *ks2 *ks6 +2 *ks2 *ks7 +4 *ks2 *x3 +4 *ks6 *x3 +4 *ks7 *x3 +ks2 *ks6 *ks7 +2 *ks2 *ks6 *x3 +2 *ks2 *ks7 *x3 +2 *ks6 *ks7 *x3 +ks2 *ks6 *ks7 *x3 +((1 +ks7 )*((1 +ks7 )<=(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))<(1 +ks7 )))),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =tl .load (in_ptr0 +(((-4 )*ks2 )+2 *((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<(1 +ks6 )))+4 *((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))<(1 +ks2 )))+8 *x3 +ks7 *((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<(1 +ks6 )))+((-2 )*ks2 *ks6 )+((-2 )*ks2 *ks7 )+2 *ks6 *((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))<(1 +ks2 )))+2 *ks7 *((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))<(1 +ks2 )))+4 *ks2 *x3 +4 *ks6 *x3 +4 *ks7 *x3 +ks6 *ks7 *((1 +ks2 )*((1 +ks2 )<=(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))<(1 +ks2 )))+((-1 )*ks2 *ks6 *ks7 )+2 *ks2 *ks6 *x3 +2 *ks2 *ks7 *x3 +2 *ks6 *ks7 *x3 +ks2 *ks6 *ks7 *x3 +((1 +ks7 )*((1 +ks7 )<=(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))<(1 +ks7 )))),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tmp10 =tl .full (tmp9 .shape ,0.0 ,tmp9 .dtype )\n    tmp11 =tl .where (tmp2 ,tmp9 ,tmp10 )\n    tmp12 =tl .full ([1 ],1 ,tl .int64 )\n    tmp13 =tmp0 <tmp12 \n    tmp14 =tl .load (in_ptr0 +(2 *((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<(1 +ks6 )))+4 *ks2 +8 *x3 +ks7 *((1 +ks6 )*((1 +ks6 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<(1 +ks6 )))+2 *ks2 *ks6 +2 *ks2 *ks7 +4 *ks2 *x3 +4 *ks6 *x3 +4 *ks7 *x3 +ks2 *ks6 *ks7 +2 *ks2 *ks6 *x3 +2 *ks2 *ks7 *x3 +2 *ks6 *ks7 *x3 +ks2 *ks6 *ks7 *x3 +((1 +ks7 )*((1 +ks7 )<=(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))<(1 +ks7 )))),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp16 =tl .where (tmp13 ,tmp14 ,tmp15 )\n    tmp17 =tl .where (tmp2 ,tmp11 ,tmp16 )\n    tmp18 =0.0 \n    tmp19 =tmp17 >tmp18 \n    tmp20 =0.01 \n    tmp21 =tmp17 *tmp20 \n    tmp22 =tl .where (tmp19 ,tmp17 ,tmp21 )\n    tl .store (out_ptr0 +(x8 ),tmp22 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_mish_mse_loss_relu_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =18 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp17 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 )\n        tmp1 =64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(4 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//ks6 )%ks7 ))+16 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//ks4 )%ks5 ))+64 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//(64 +16 *ks1 +16 *ks2 +16 *ks3 +4 *ks1 *ks2 +4 *ks1 *ks3 +4 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+ks3 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//ks6 )%ks7 ))+4 *ks2 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//ks4 )%ks5 ))+4 *ks3 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//ks4 )%ks5 ))+16 *ks1 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//(64 +16 *ks1 +16 *ks2 +16 *ks3 +4 *ks1 *ks2 +4 *ks1 *ks3 +4 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+16 *ks2 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//(64 +16 *ks1 +16 *ks2 +16 *ks3 +4 *ks1 *ks2 +4 *ks1 *ks3 +4 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+16 *ks3 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//(64 +16 *ks1 +16 *ks2 +16 *ks3 +4 *ks1 *ks2 +4 *ks1 *ks3 +4 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+ks2 *ks3 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//ks4 )%ks5 ))+4 *ks1 *ks2 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//(64 +16 *ks1 +16 *ks2 +16 *ks3 +4 *ks1 *ks2 +4 *ks1 *ks3 +4 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+4 *ks1 *ks3 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//(64 +16 *ks1 +16 *ks2 +16 *ks3 +4 *ks1 *ks2 +4 *ks1 *ks3 +4 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+4 *ks2 *ks3 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//(64 +16 *ks1 +16 *ks2 +16 *ks3 +4 *ks1 *ks2 +4 *ks1 *ks3 +4 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+ks1 *ks2 *ks3 *((((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))//(64 +16 *ks1 +16 *ks2 +16 *ks3 +4 *ks1 *ks2 +4 *ks1 *ks3 +4 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+(((r0_1 +x0 *((17 +64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//18 ))%ks6 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp5 =triton_helpers .maximum (tmp4 ,tmp3 )\n        tmp6 =20.0 \n        tmp7 =tmp5 >tmp6 \n        tmp8 =tl_math .exp (tmp5 )\n        tmp9 =libdevice .log1p (tmp8 )\n        tmp10 =tl .where (tmp7 ,tmp5 ,tmp9 )\n        tmp11 =libdevice .tanh (tmp10 )\n        tmp12 =tmp5 *tmp11 \n        tmp13 =tmp12 *tmp12 \n        tmp14 =tl .full (tmp13 .shape ,0 ,tmp13 .dtype )\n        tmp15 =tl .where (tmp2 ,tmp13 ,tmp14 )\n        tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ,R0_BLOCK ])\n        tmp18 =_tmp17 +tmp16 \n        _tmp17 =tl .where (r0_mask &xmask ,tmp18 ,_tmp17 )\n    tmp17 =tl .sum (_tmp17 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_mish_mse_loss_relu_4 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =18 \n    R0_BLOCK :tl .constexpr =32 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =64 *ks0 +16 *ks0 *ks1 +16 *ks0 *ks2 +16 *ks0 *ks3 +4 *ks0 *ks1 *ks2 +4 *ks0 *ks1 *ks3 +4 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tmp4 /tmp6 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp7 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n        2 +s3 \n        2 +s2 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        2 +s1 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf1 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_0_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (triton_poi_fused_copy_0_xnumel )](arg4_1 ,buf0 ,buf1 ,34 ,32 ,34 ,32 ,1156 ,34 ,32 ,39304 ,117912 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg4_1 \n        buf2 =buf0 ;del buf0 \n\n        triton_poi_fused_1_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_1 [grid (triton_poi_fused_1_xnumel )](buf1 ,buf2 ,34 ,34 ,32 ,32 ,117912 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf1 \n        16 +4 *s2 +4 *s3 +s2 *s3 \n        4 +s1 \n        4 +s3 \n        4 +s2 \n        64 +16 *s1 +16 *s2 +16 *s3 +4 *s1 *s2 +4 *s1 *s3 +4 *s2 *s3 +s1 *s2 *s3 \n        16 +4 *s2 +4 *s3 +s2 *s3 \n        buf3 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ,4 +s3 ),(64 *s0 +16 *s0 *s1 +16 *s0 *s2 +16 *s0 *s3 +4 *s0 *s1 *s2 +4 *s0 *s1 *s3 +4 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,64 +16 *s1 +16 *s2 +16 *s3 +4 *s1 *s2 +4 *s1 *s3 +4 *s2 *s3 +s1 *s2 *s3 ,16 +4 *s2 +4 *s3 +s2 *s3 ,4 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_leaky_relu_replication_pad3d_2_xnumel =64 *s0 +16 *s0 *s1 +16 *s0 *s2 +16 *s0 *s3 +4 *s0 *s1 *s2 +4 *s0 *s1 *s3 +4 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_leaky_relu_replication_pad3d_2 [grid (triton_poi_fused_leaky_relu_replication_pad3d_2_xnumel )](buf2 ,buf3 ,1296 ,36 ,32 ,36 ,36 ,46656 ,32 ,32 ,1296 ,139968 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf2 \n        buf4 =empty_strided_cuda ((18 ,),(1 ,),torch .float32 )\n\n        (17 +64 *s0 +16 *s0 *s1 +16 *s0 *s2 +16 *s0 *s3 +4 *s0 *s1 *s2 +4 *s0 *s1 *s3 +4 *s0 *s2 *s3 +s0 *s1 *s2 *s3 )//18 \n        get_raw_stream (0 )\n        triton_red_fused_mish_mse_loss_relu_3 [grid (18 )](buf3 ,buf4 ,3 ,32 ,32 ,32 ,1296 ,36 ,36 ,36 ,18 ,7776 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf3 \n        buf5 =empty_strided_cuda ((),(),torch .float32 )\n        buf6 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_per_fused_mish_mse_loss_relu_4 [grid (1 )](buf6 ,buf4 ,3 ,32 ,32 ,32 ,1 ,18 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf4 \n    return (buf6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a436b2f0-f4d2-4327-a502-94228050c5c1",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReLU', 'HuberLoss', 'Softmax2d', 'SmoothL1Loss', 'ModuleList', 'Upsample']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.module_list = nn.ModuleList([\n            nn.ReLU(),\n            nn.Softmax2d(),\n            nn.ReLU(),\n            nn.Softmax2d(),\n            nn.ReLU()\n        ])\n        self.huber_loss = nn.HuberLoss()\n        self.smooth_l1_loss = nn.SmoothL1Loss()\n\n    def forward(self, x):\n        # Upsample the input\n        x = self.upsample(x)\n        \n        # Apply the modules in the ModuleList\n        for module in self.module_list:\n            x = module(x)\n        \n        # Compute a dummy target for loss calculation\n        target = torch.zeros_like(x)\n        \n        # Compute Huber loss\n        huber_loss = self.huber_loss(x, target)\n        \n        # Compute Smooth L1 loss\n        smooth_l1_loss = self.smooth_l1_loss(x, target)\n        \n        # Return the sum of the losses as the output\n        return huber_loss + smooth_l1_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0 (in_out_ptr1 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks4 \n    x5 =xindex \n    tmp0 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float64 )\n    tmp3 =tmp0 +tmp2 \n    tmp4 =2.0 \n    tmp5 =tmp1 .to (tl .float32 )\n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp6 .to (tl .float64 )\n    tmp8 =tmp0 +tmp7 \n    tmp9 =tmp3 /tmp8 \n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =x1 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =tmp12 *tmp10 \n    tmp14 =0.0 \n    tmp15 =triton_helpers .maximum (tmp13 ,tmp14 )\n    tmp16 =tmp15 .to (tl .int64 )\n    tmp17 =ks3 \n    tmp18 =tmp17 .to (tl .float64 )\n    tmp19 =tmp0 +tmp18 \n    tmp20 =tmp17 .to (tl .float32 )\n    tmp21 =tmp4 *tmp20 \n    tmp22 =tmp21 .to (tl .float64 )\n    tmp23 =tmp0 +tmp22 \n    tmp24 =tmp19 /tmp23 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =x0 \n    tmp27 =tmp26 .to (tl .float32 )\n    tmp28 =tmp27 *tmp25 \n    tmp29 =triton_helpers .maximum (tmp28 ,tmp14 )\n    tmp30 =tmp29 .to (tl .int64 )\n    tmp31 =tl .load (in_ptr0 +(tmp30 +ks3 *tmp16 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp32 =tl .full ([1 ],1 ,tl .int64 )\n    tmp33 =tmp16 +tmp32 \n    tmp34 =(-1 )+ks0 \n    tmp35 =triton_helpers .minimum (tmp33 ,tmp34 )\n    tmp36 =tl .load (in_ptr0 +(tmp30 +ks3 *tmp35 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp37 =tmp30 +tmp32 \n    tmp38 =(-1 )+ks3 \n    tmp39 =triton_helpers .minimum (tmp37 ,tmp38 )\n    tmp40 =tl .load (in_ptr0 +(tmp39 +ks3 *tmp35 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp41 =tmp40 -tmp36 \n    tmp42 =tl .load (in_ptr0 +(tmp39 +ks3 *tmp16 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =tmp42 -tmp31 \n    tmp44 =tmp30 .to (tl .float32 )\n    tmp45 =tmp29 -tmp44 \n    tmp46 =triton_helpers .maximum (tmp45 ,tmp14 )\n    tmp47 =1.0 \n    tmp48 =triton_helpers .minimum (tmp46 ,tmp47 )\n    tmp49 =tmp41 *tmp48 \n    tmp50 =tmp36 +tmp49 \n    tmp51 =tmp43 *tmp48 \n    tmp52 =tmp31 +tmp51 \n    tmp53 =tmp50 -tmp52 \n    tmp54 =tmp16 .to (tl .float32 )\n    tmp55 =tmp15 -tmp54 \n    tmp56 =triton_helpers .maximum (tmp55 ,tmp14 )\n    tmp57 =triton_helpers .minimum (tmp56 ,tmp47 )\n    tmp58 =tmp53 *tmp57 \n    tmp59 =tmp52 +tmp58 \n    tl .store (in_out_ptr1 +(x5 ),tmp59 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_relu_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +4 *ks0 *ks1 *r0_1 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .where (r0_mask &xmask ,tmp3 ,float (\"-inf\"))\n    tmp6 =triton_helpers .max2 (tmp5 ,1 )[:,None ]\n    tmp7 =tmp2 -tmp6 \n    tmp8 =tl_math .exp (tmp7 )\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n    tmp11 =tl .where (r0_mask &xmask ,tmp9 ,0 )\n    tmp12 =tl .sum (tmp11 ,1 )[:,None ]\n    tmp13 =tmp8 /tmp12 \n    tmp14 =triton_helpers .maximum (tmp1 ,tmp13 )\n    tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp17 =tl .where (r0_mask &xmask ,tmp15 ,float (\"-inf\"))\n    tmp18 =triton_helpers .max2 (tmp17 ,1 )[:,None ]\n    tmp19 =tmp14 -tmp18 \n    tmp20 =tl_math .exp (tmp19 )\n    tmp21 =tl .broadcast_to (tmp20 ,[XBLOCK ,R0_BLOCK ])\n    tmp23 =tl .where (r0_mask &xmask ,tmp21 ,0 )\n    tmp24 =tl .sum (tmp23 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp6 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp12 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp18 ,xmask )\n    tl .store (out_ptr3 +(x0 ),tmp24 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_relu_smooth_l1_loss_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %ks0 )\n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp6 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =tmp2 -tmp3 \n    tmp5 =tl_math .exp (tmp4 )\n    tmp7 =tmp5 /tmp6 \n    tmp8 =triton_helpers .maximum (tmp1 ,tmp7 )\n    tmp10 =tmp8 -tmp9 \n    tmp11 =tl_math .exp (tmp10 )\n    tmp13 =tmp11 /tmp12 \n    tmp14 =triton_helpers .maximum (tmp1 ,tmp13 )\n    tl .store (in_out_ptr0 +(x2 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_huber_loss_smooth_l1_loss_3 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_2 =r0_index \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *ks3 *((((r0_2 +ks1 *x0 +2 *ks1 *ks3 *x1 )//ks0 )%(2 *ks1 *ks2 )))+(((r0_2 +ks1 *x0 )%ks0 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =1.0 \n    tmp3 =tmp1 <tmp2 \n    tmp4 =0.5 \n    tmp5 =tmp1 *tmp4 \n    tmp6 =tmp5 *tmp1 \n    tmp7 =tmp1 -tmp4 \n    tmp8 =tmp7 *tmp2 \n    tmp9 =tl .where (tmp3 ,tmp6 ,tmp8 )\n    tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n    tmp12 =tl .where (r0_mask &xmask ,tmp10 ,0 )\n    tmp13 =tl .sum (tmp12 ,1 )[:,None ]\n    tmp14 =tmp1 *tmp1 \n    tmp15 =tmp14 *tmp4 \n    tmp16 =tmp15 *tmp2 \n    tmp17 =tl .where (tmp3 ,tmp16 ,tmp7 )\n    tmp18 =tl .broadcast_to (tmp17 ,[XBLOCK ,R0_BLOCK ])\n    tmp20 =tl .where (r0_mask &xmask ,tmp18 ,0 )\n    tmp21 =tl .sum (tmp20 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp13 ,xmask )\n    tl .store (out_ptr1 +(x3 ),tmp21 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_huber_loss_smooth_l1_loss_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =_tmp2 +tmp1 \n        _tmp2 =tl .where (r0_mask ,tmp3 ,_tmp2 )\n    tmp2 =tl .sum (_tmp2 ,1 )[:,None ]\n    _tmp6 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp4 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n        tmp7 =_tmp6 +tmp5 \n        _tmp6 =tl .where (r0_mask ,tmp7 ,_tmp6 )\n    tmp6 =tl .sum (_tmp6 ,1 )[:,None ]\n    tmp8 =4 *ks0 *ks1 *ks2 \n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp2 /tmp9 \n    tmp11 =tmp6 /tmp9 \n    tmp12 =tmp10 +tmp11 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp12 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 *s2 \n        2 *s1 \n        4 *s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 ,2 *s1 ,2 *s2 ),(4 *s0 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n        buf5 =buf2 ;del buf2 \n\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0_xnumel =4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0 [grid (triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_view_0_xnumel )](buf5 ,arg3_1 ,32 ,64 ,64 ,32 ,4096 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf6 =empty_strided_cuda ((1 ,1 ,2 *s1 ,2 *s2 ),(4 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,1 ,2 *s1 ,2 *s2 ),(4 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,1 ,2 *s1 ,2 *s2 ),(4 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n        buf9 =empty_strided_cuda ((1 ,1 ,2 *s1 ,2 *s2 ),(4 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n\n        triton_per_fused__softmax_relu_1_xnumel =4 *s1 *s2 \n        get_raw_stream (0 )\n        triton_per_fused__softmax_relu_1 [grid (triton_per_fused__softmax_relu_1_xnumel )](buf5 ,buf6 ,buf7 ,buf8 ,buf9 ,32 ,32 ,4096 ,3 ,XBLOCK =8 ,num_warps =2 ,num_stages =1 )\n        buf10 =buf5 ;del buf5 \n\n        triton_poi_fused__softmax_relu_smooth_l1_loss_2_xnumel =4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__softmax_relu_smooth_l1_loss_2 [grid (triton_poi_fused__softmax_relu_smooth_l1_loss_2_xnumel )](buf10 ,buf6 ,buf7 ,buf8 ,buf9 ,4096 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf6 \n        del buf7 \n        buf11 =buf9 ;del buf9 \n        buf13 =buf8 ;del buf8 \n\n        triton_per_fused_huber_loss_smooth_l1_loss_3_xnumel =4 *s1 *s2 \n        get_raw_stream (0 )\n        triton_per_fused_huber_loss_smooth_l1_loss_3 [grid (triton_per_fused_huber_loss_smooth_l1_loss_3_xnumel )](buf10 ,buf11 ,buf13 ,64 ,3 ,32 ,32 ,4096 ,3 ,XBLOCK =8 ,num_warps =2 ,num_stages =1 )\n        del buf10 \n        buf12 =empty_strided_cuda ((),(),torch .float32 )\n        buf15 =buf12 ;del buf12 \n\n        4 *s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused_add_huber_loss_smooth_l1_loss_4 [grid (1 )](buf15 ,buf11 ,buf13 ,3 ,32 ,32 ,1 ,4096 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf11 \n        del buf13 \n    return (buf15 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a547db76-32d5-4d08-9006-5a4ba41be1a3",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['SiLU', 'ZeroPad2d', 'Dropout', 'MaxUnpool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.silu1 = nn.SiLU()\n        self.zero_pad1 = nn.ZeroPad2d(2)\n        self.dropout1 = nn.Dropout(0.5)\n        self.max_unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.silu2 = nn.SiLU()\n        self.zero_pad2 = nn.ZeroPad2d(1)\n        self.dropout2 = nn.Dropout(0.3)\n        self.max_unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.silu3 = nn.SiLU()\n\n    def forward(self, x):\n        # Apply SiLU activation\n        x = self.silu1(x)\n        \n        # Zero padding\n        x = self.zero_pad1(x)\n        \n        # Dropout\n        x = self.dropout1(x)\n        \n        # MaxUnpool2d requires indices from a previous MaxPool2d operation\n        # For simplicity, we assume the input has been through a MaxPool2d layer before\n        # Here, we simulate the indices by performing a MaxPool2d operation\n        pool_size = 2\n        x, indices = F.max_pool2d(x, pool_size, return_indices=True)\n        \n        # MaxUnpool2d\n        x = self.max_unpool1(x, indices)\n        \n        # Apply SiLU activation again\n        x = self.silu2(x)\n        \n        # Zero padding\n        x = self.zero_pad2(x)\n        \n        # Dropout\n        x = self.dropout2(x)\n        \n        # Another MaxUnpool2d operation\n        x, indices = F.max_pool2d(x, pool_size, return_indices=True)\n        x = self.max_unpool2(x, indices)\n        \n        # Final SiLU activation\n        x = self.silu3(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_native_dropout_silu_1 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x3 =xindex \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    tmp0 =tl .load (in_out_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.5 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =tmp2 .to (tl .float32 )\n    tmp4 =(-2 )+x1 \n    tmp5 =tl .full ([1 ],0 ,tl .int64 )\n    tmp6 =tmp4 >=tmp5 \n    tmp7 =ks2 \n    tmp8 =tmp4 <tmp7 \n    tmp9 =(-2 )+x0 \n    tmp10 =tmp9 >=tmp5 \n    tmp11 =ks3 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp6 &tmp8 \n    tmp14 =tmp13 &tmp10 \n    tmp15 =tmp14 &tmp12 \n    tmp16 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp17 =tl .sigmoid (tmp16 )\n    tmp18 =tmp16 *tmp17 \n    tmp19 =tl .full (tmp18 .shape ,0.0 ,tmp18 .dtype )\n    tmp20 =tl .where (tmp15 ,tmp18 ,tmp19 )\n    tmp21 =tmp3 *tmp20 \n    tmp22 =2.0 \n    tmp23 =tmp21 *tmp22 \n    tl .store (in_out_ptr0 +(x3 ),tmp23 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_max_unpool2d_native_dropout_silu_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x7 =xindex //ks6 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +8 *x1 +16 *x2 +2 *ks4 *x1 +4 *ks3 *x2 +4 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +8 *x1 +16 *x2 +2 *ks4 *x1 +4 *ks3 *x2 +4 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(4 +ks4 +2 *x0 +8 *x1 +16 *x2 +2 *ks4 *x1 +4 *ks3 *x2 +4 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr0 +(5 +ks4 +2 *x0 +8 *x1 +16 *x2 +2 *ks4 *x1 +4 *ks3 *x2 +4 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp8 =tmp7 >tmp6 \n    tmp9 =tl .full ([1 ],2 ,tl .int8 )\n    tmp10 =tl .where (tmp8 ,tmp9 ,tmp5 )\n    tmp11 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp13 =tmp12 >tmp11 \n    tmp14 =tl .full ([1 ],3 ,tl .int8 )\n    tmp15 =tl .where (tmp13 ,tmp14 ,tmp10 )\n    triton_helpers .maximum (tmp12 ,tmp11 )\n    tmp17 =tl .full ([1 ],2 ,tl .int32 )\n    tmp18 =tl .where ((tmp15 <0 )!=(tmp17 <0 ),tl .where (tmp15 %tmp17 !=0 ,tmp15 //tmp17 -1 ,tmp15 //tmp17 ),tmp15 //tmp17 )\n    tmp19 =tmp18 *tmp17 \n    tmp20 =tmp15 -tmp19 \n    tmp21 =2 *x1 \n    tmp22 =tmp21 +tmp18 \n    tmp23 =2 *x0 \n    tmp24 =tmp23 +tmp20 \n    tmp25 =ks5 \n    tmp26 =tmp22 *tmp25 \n    tmp27 =tmp26 +tmp24 \n    tmp28 =16 *x7 +8 *x7 *(ks3 //2 )+8 *x7 *(ks4 //2 )+4 *x7 *(ks3 //2 )*(ks4 //2 )\n    tmp29 =tmp27 +tmp28 \n    tl .store (out_ptr0 +(x4 ),tmp29 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_3 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_4 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp6 =tl .load (in_ptr1 +(2 *((x0 %ks3 ))+8 *(((x0 //ks3 )%ks4 ))+16 *(x0 //ks5 )+2 *ks2 *(((x0 //ks3 )%ks4 ))+4 *ks1 *(x0 //ks5 )+4 *ks2 *(x0 //ks5 )+ks1 *ks2 *(x0 //ks5 )),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr1 +(1 +2 *((x0 %ks3 ))+8 *(((x0 //ks3 )%ks4 ))+16 *(x0 //ks5 )+2 *ks2 *(((x0 //ks3 )%ks4 ))+4 *ks1 *(x0 //ks5 )+4 *ks2 *(x0 //ks5 )+ks1 *ks2 *(x0 //ks5 )),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr1 +(4 +ks2 +2 *((x0 %ks3 ))+8 *(((x0 //ks3 )%ks4 ))+16 *(x0 //ks5 )+2 *ks2 *(((x0 //ks3 )%ks4 ))+4 *ks1 *(x0 //ks5 )+4 *ks2 *(x0 //ks5 )+ks1 *ks2 *(x0 //ks5 )),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr1 +(5 +ks2 +2 *((x0 %ks3 ))+8 *(((x0 //ks3 )%ks4 ))+16 *(x0 //ks5 )+2 *ks2 *(((x0 //ks3 )%ks4 ))+4 *ks1 *(x0 //ks5 )+4 *ks2 *(x0 //ks5 )+ks1 *ks2 *(x0 //ks5 )),xmask ,eviction_policy ='evict_last')\n    tmp1 =16 *ks0 +8 *ks0 *(ks1 //2 )+8 *ks0 *(ks2 //2 )+4 *ks0 *(ks1 //2 )*(ks2 //2 )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tmp0 <0 \n    tmp4 =tl .where (tmp3 ,tmp2 ,tmp0 )\n    tl .device_assert (((0 <=tmp4 )&(tmp4 <16 *ks0 +8 *ks0 *(ks1 //2 )+8 *ks0 *(ks2 //2 )+4 *ks0 *(ks1 //2 )*(ks2 //2 )))|~(xmask ),\"index out of bounds: 0 <= tmp4 < 16*ks0 + 8*ks0*(ks1 // 2) + 8*ks0*(ks2 // 2) + 4*ks0*(ks1 // 2)*(ks2 // 2)\")\n    tmp8 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\n    tl .store (out_ptr0 +(tl .broadcast_to (4 *(((tmp4 //(4 +2 *(ks2 //2 )))%(4 +2 *(ks1 //2 ))))+16 *(((tmp4 //(16 +8 *(ks1 //2 )+8 *(ks2 //2 )+4 *(ks1 //2 )*(ks2 //2 )))%ks0 ))+2 *(ks2 //2 )*(((tmp4 //(4 +2 *(ks2 //2 )))%(4 +2 *(ks1 //2 ))))+8 *(ks1 //2 )*(((tmp4 //(16 +8 *(ks1 //2 )+8 *(ks2 //2 )+4 *(ks1 //2 )*(ks2 //2 )))%ks0 ))+8 *(ks2 //2 )*(((tmp4 //(16 +8 *(ks1 //2 )+8 *(ks2 //2 )+4 *(ks1 //2 )*(ks2 //2 )))%ks0 ))+4 *(ks1 //2 )*(ks2 //2 )*(((tmp4 //(16 +8 *(ks1 //2 )+8 *(ks2 //2 )+4 *(ks1 //2 )*(ks2 //2 )))%ks0 ))+((tmp4 %(4 +2 *(ks2 //2 )))),[XBLOCK ])),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_5 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_native_dropout_silu_6 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x3 =xindex \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    tmp0 =tl .load (in_out_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.3 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =tmp2 .to (tl .float32 )\n    tmp4 =(-1 )+x1 \n    tmp5 =tl .full ([1 ],0 ,tl .int64 )\n    tmp6 =tmp4 >=tmp5 \n    tmp7 =4 +2 *(ks2 //2 )\n    tmp8 =tmp4 <tmp7 \n    tmp9 =(-1 )+x0 \n    tmp10 =tmp9 >=tmp5 \n    tmp11 =4 +2 *(ks3 //2 )\n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp6 &tmp8 \n    tmp14 =tmp13 &tmp10 \n    tmp15 =tmp14 &tmp12 \n    tmp16 =tl .load (in_ptr0 +(4 *(((((-5 )+x0 +((-2 )*(ks3 //2 ))+4 *x1 +16 *x2 +2 *x1 *(ks3 //2 )+8 *x2 *(ks2 //2 )+8 *x2 *(ks3 //2 )+4 *x2 *(ks2 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks2 //2 ))))+16 *(((((-5 )+x0 +((-2 )*(ks3 //2 ))+4 *x1 +16 *x2 +2 *x1 *(ks3 //2 )+8 *x2 *(ks2 //2 )+8 *x2 *(ks3 //2 )+4 *x2 *(ks2 //2 )*(ks3 //2 ))//(16 +8 *(ks2 //2 )+8 *(ks3 //2 )+4 *(ks2 //2 )*(ks3 //2 )))%ks5 ))+2 *(ks3 //2 )*(((((-5 )+x0 +((-2 )*(ks3 //2 ))+4 *x1 +16 *x2 +2 *x1 *(ks3 //2 )+8 *x2 *(ks2 //2 )+8 *x2 *(ks3 //2 )+4 *x2 *(ks2 //2 )*(ks3 //2 ))//(4 +2 *(ks3 //2 )))%(4 +2 *(ks2 //2 ))))+8 *(ks2 //2 )*(((((-5 )+x0 +((-2 )*(ks3 //2 ))+4 *x1 +16 *x2 +2 *x1 *(ks3 //2 )+8 *x2 *(ks2 //2 )+8 *x2 *(ks3 //2 )+4 *x2 *(ks2 //2 )*(ks3 //2 ))//(16 +8 *(ks2 //2 )+8 *(ks3 //2 )+4 *(ks2 //2 )*(ks3 //2 )))%ks5 ))+8 *(ks3 //2 )*(((((-5 )+x0 +((-2 )*(ks3 //2 ))+4 *x1 +16 *x2 +2 *x1 *(ks3 //2 )+8 *x2 *(ks2 //2 )+8 *x2 *(ks3 //2 )+4 *x2 *(ks2 //2 )*(ks3 //2 ))//(16 +8 *(ks2 //2 )+8 *(ks3 //2 )+4 *(ks2 //2 )*(ks3 //2 )))%ks5 ))+4 *(ks2 //2 )*(ks3 //2 )*(((((-5 )+x0 +((-2 )*(ks3 //2 ))+4 *x1 +16 *x2 +2 *x1 *(ks3 //2 )+8 *x2 *(ks2 //2 )+8 *x2 *(ks3 //2 )+4 *x2 *(ks2 //2 )*(ks3 //2 ))//(16 +8 *(ks2 //2 )+8 *(ks3 //2 )+4 *(ks2 //2 )*(ks3 //2 )))%ks5 ))+((((-5 )+x0 +((-2 )*(ks3 //2 ))+4 *x1 +16 *x2 +2 *x1 *(ks3 //2 )+8 *x2 *(ks2 //2 )+8 *x2 *(ks3 //2 )+4 *x2 *(ks2 //2 )*(ks3 //2 ))%(4 +2 *(ks3 //2 ))))),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp17 =tl .sigmoid (tmp16 )\n    tmp18 =tmp16 *tmp17 \n    tmp19 =tl .full (tmp18 .shape ,0.0 ,tmp18 .dtype )\n    tmp20 =tl .where (tmp15 ,tmp18 ,tmp19 )\n    tmp21 =tmp3 *tmp20 \n    tmp22 =1.4285714285714286 \n    tmp23 =tmp21 *tmp22 \n    tl .store (in_out_ptr0 +(x3 ),tmp23 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_max_unpool2d_native_dropout_silu_7 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x3 =xindex //ks0 \n    x1 =((xindex //ks0 )%ks2 )\n    x7 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +12 *x3 +4 *x3 *(ks1 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +12 *x3 +4 *x3 *(ks1 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(6 +2 *x0 +2 *(ks1 //2 )+12 *x3 +4 *x3 *(ks1 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr0 +(7 +2 *x0 +2 *(ks1 //2 )+12 *x3 +4 *x3 *(ks1 //2 )),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp8 =tmp7 >tmp6 \n    tmp9 =tl .full ([1 ],2 ,tl .int8 )\n    tmp10 =tl .where (tmp8 ,tmp9 ,tmp5 )\n    tmp11 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp13 =tmp12 >tmp11 \n    tmp14 =tl .full ([1 ],3 ,tl .int8 )\n    tmp15 =tl .where (tmp13 ,tmp14 ,tmp10 )\n    triton_helpers .maximum (tmp12 ,tmp11 )\n    tmp17 =tl .full ([1 ],2 ,tl .int32 )\n    tmp18 =tl .where ((tmp15 <0 )!=(tmp17 <0 ),tl .where (tmp15 %tmp17 !=0 ,tmp15 //tmp17 -1 ,tmp15 //tmp17 ),tmp15 //tmp17 )\n    tmp19 =tmp18 *tmp17 \n    tmp20 =tmp15 -tmp19 \n    tmp21 =2 *x1 \n    tmp22 =tmp21 +tmp18 \n    tmp23 =2 *x0 \n    tmp24 =tmp23 +tmp20 \n    tmp25 =ks3 \n    tmp26 =tmp22 *tmp25 \n    tmp27 =tmp26 +tmp24 \n    tmp28 =36 *x7 +12 *x7 *(ks1 //2 )+12 *x7 *(ks5 //2 )+4 *x7 *(ks1 //2 )*(ks5 //2 )\n    tmp29 =tmp27 +tmp28 \n    tl .store (out_ptr0 +(x4 ),tmp29 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_8 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp6 =tl .load (in_ptr1 +(2 *((x0 %ks3 ))+12 *(((x0 //ks3 )%ks4 ))+36 *(x0 //ks5 )+4 *(ks2 //2 )*(((x0 //ks3 )%ks4 ))+12 *(ks1 //2 )*(x0 //ks5 )+12 *(ks2 //2 )*(x0 //ks5 )+4 *(ks1 //2 )*(ks2 //2 )*(x0 //ks5 )),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr1 +(1 +2 *((x0 %ks3 ))+12 *(((x0 //ks3 )%ks4 ))+36 *(x0 //ks5 )+4 *(ks2 //2 )*(((x0 //ks3 )%ks4 ))+12 *(ks1 //2 )*(x0 //ks5 )+12 *(ks2 //2 )*(x0 //ks5 )+4 *(ks1 //2 )*(ks2 //2 )*(x0 //ks5 )),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr1 +(6 +2 *(ks2 //2 )+2 *((x0 %ks3 ))+12 *(((x0 //ks3 )%ks4 ))+36 *(x0 //ks5 )+4 *(ks2 //2 )*(((x0 //ks3 )%ks4 ))+12 *(ks1 //2 )*(x0 //ks5 )+12 *(ks2 //2 )*(x0 //ks5 )+4 *(ks1 //2 )*(ks2 //2 )*(x0 //ks5 )),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr1 +(7 +2 *(ks2 //2 )+2 *((x0 %ks3 ))+12 *(((x0 //ks3 )%ks4 ))+36 *(x0 //ks5 )+4 *(ks2 //2 )*(((x0 //ks3 )%ks4 ))+12 *(ks1 //2 )*(x0 //ks5 )+12 *(ks2 //2 )*(x0 //ks5 )+4 *(ks1 //2 )*(ks2 //2 )*(x0 //ks5 )),xmask ,eviction_policy ='evict_last')\n    tmp1 =36 *ks0 +12 *ks0 *(ks1 //2 )+12 *ks0 *(ks2 //2 )+4 *ks0 *(ks1 //2 )*(ks2 //2 )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tmp0 <0 \n    tmp4 =tl .where (tmp3 ,tmp2 ,tmp0 )\n    tl .device_assert (((0 <=tmp4 )&(tmp4 <36 *ks0 +12 *ks0 *(ks1 //2 )+12 *ks0 *(ks2 //2 )+4 *ks0 *(ks1 //2 )*(ks2 //2 )))|~(xmask ),\"index out of bounds: 0 <= tmp4 < 36*ks0 + 12*ks0*(ks1 // 2) + 12*ks0*(ks2 // 2) + 4*ks0*(ks1 // 2)*(ks2 // 2)\")\n    tmp8 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp12 =triton_helpers .maximum (tmp11 ,tmp10 )\n    tl .store (out_ptr0 +(tl .broadcast_to (6 *(((tmp4 //ks6 )%ks7 ))+36 *(((tmp4 //(36 +12 *(ks1 //2 )+12 *(ks2 //2 )+4 *(ks1 //2 )*(ks2 //2 )))%ks0 ))+2 *(ks2 //2 )*(((tmp4 //ks6 )%ks7 ))+12 *(ks1 //2 )*(((tmp4 //(36 +12 *(ks1 //2 )+12 *(ks2 //2 )+4 *(ks1 //2 )*(ks2 //2 )))%ks0 ))+12 *(ks2 //2 )*(((tmp4 //(36 +12 *(ks1 //2 )+12 *(ks2 //2 )+4 *(ks1 //2 )*(ks2 //2 )))%ks0 ))+4 *(ks1 //2 )*(ks2 //2 )*(((tmp4 //(36 +12 *(ks1 //2 )+12 *(ks2 //2 )+4 *(ks1 //2 )*(ks2 //2 )))%ks0 ))+((tmp4 %ks6 )),[XBLOCK ])),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_silu_9 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +6 *((((x0 +6 *x1 +36 *x2 +2 *x1 *(ks5 //2 )+12 *x2 *(ks4 //2 )+12 *x2 *(ks5 //2 )+4 *x2 *(ks4 //2 )*(ks5 //2 ))//ks0 )%ks1 ))+36 *((((x0 +6 *x1 +36 *x2 +2 *x1 *(ks5 //2 )+12 *x2 *(ks4 //2 )+12 *x2 *(ks5 //2 )+4 *x2 *(ks4 //2 )*(ks5 //2 ))//(36 +12 *(ks4 //2 )+12 *(ks5 //2 )+4 *(ks4 //2 )*(ks5 //2 )))%ks3 ))+2 *(ks5 //2 )*((((x0 +6 *x1 +36 *x2 +2 *x1 *(ks5 //2 )+12 *x2 *(ks4 //2 )+12 *x2 *(ks5 //2 )+4 *x2 *(ks4 //2 )*(ks5 //2 ))//ks0 )%ks1 ))+12 *(ks4 //2 )*((((x0 +6 *x1 +36 *x2 +2 *x1 *(ks5 //2 )+12 *x2 *(ks4 //2 )+12 *x2 *(ks5 //2 )+4 *x2 *(ks4 //2 )*(ks5 //2 ))//(36 +12 *(ks4 //2 )+12 *(ks5 //2 )+4 *(ks4 //2 )*(ks5 //2 )))%ks3 ))+12 *(ks5 //2 )*((((x0 +6 *x1 +36 *x2 +2 *x1 *(ks5 //2 )+12 *x2 *(ks4 //2 )+12 *x2 *(ks5 //2 )+4 *x2 *(ks4 //2 )*(ks5 //2 ))//(36 +12 *(ks4 //2 )+12 *(ks5 //2 )+4 *(ks4 //2 )*(ks5 //2 )))%ks3 ))+4 *(ks4 //2 )*(ks5 //2 )*((((x0 +6 *x1 +36 *x2 +2 *x1 *(ks5 //2 )+12 *x2 *(ks4 //2 )+12 *x2 *(ks5 //2 )+4 *x2 *(ks4 //2 )*(ks5 //2 ))//(36 +12 *(ks4 //2 )+12 *(ks5 //2 )+4 *(ks4 //2 )*(ks5 //2 )))%ks3 ))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .sigmoid (tmp0 )\n    tmp2 =tmp0 *tmp1 \n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_native_dropout_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_0 [grid (triton_poi_fused_native_dropout_0_xnumel )](buf0 ,buf1 ,0 ,13872 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf2 =buf1 ;del buf1 \n\n        triton_poi_fused_constant_pad_nd_native_dropout_silu_1_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_native_dropout_silu_1 [grid (triton_poi_fused_constant_pad_nd_native_dropout_silu_1_xnumel )](buf2 ,arg3_1 ,68 ,68 ,64 ,64 ,4624 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        2 +(s2 //2 )\n        2 +(s1 //2 )\n        4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        buf3 =empty_strided_cuda ((1 ,s0 ,2 +(s1 //2 ),2 +(s2 //2 )),(4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 ),2 +(s2 //2 ),1 ),torch .int64 )\n\n        triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_max_unpool2d_native_dropout_silu_2_xnumel =4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_max_unpool2d_native_dropout_silu_2 [grid (triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_max_unpool2d_native_dropout_silu_2_xnumel )](buf2 ,buf3 ,34 ,34 ,1156 ,64 ,64 ,68 ,1156 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((1 ,s0 ,4 +2 *(s1 //2 ),4 +2 *(s2 //2 )),(16 *s0 +8 *s0 *(s1 //2 )+8 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 ),16 +8 *(s1 //2 )+8 *(s2 //2 )+4 *(s1 //2 )*(s2 //2 ),4 +2 *(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_3_xnumel =16 *s0 +8 *s0 *(s1 //2 )+8 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_3 [grid (triton_poi_fused_max_unpool2d_3_xnumel )](buf4 ,14700 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool2d_4_xnumel =4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_4 [grid (triton_poi_fused_max_unpool2d_4_xnumel )](buf3 ,buf2 ,buf4 ,3 ,64 ,64 ,34 ,34 ,1156 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        del buf3 \n        buf6 =empty_strided_cuda ((1 ,s0 ,6 +2 *(s1 //2 ),6 +2 *(s2 //2 )),(36 *s0 +12 *s0 *(s1 //2 )+12 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 ),36 +12 *(s1 //2 )+12 *(s2 //2 )+4 *(s1 //2 )*(s2 //2 ),6 +2 *(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_native_dropout_5_xnumel =36 *s0 +12 *s0 *(s1 //2 )+12 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_5 [grid (triton_poi_fused_native_dropout_5_xnumel )](buf0 ,buf6 ,1 ,14700 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        6 +2 *(s2 //2 )\n        6 +2 *(s1 //2 )\n        36 +12 *(s1 //2 )+12 *(s2 //2 )+4 *(s1 //2 )*(s2 //2 )\n        buf7 =buf6 ;del buf6 \n\n        triton_poi_fused_constant_pad_nd_native_dropout_silu_6_xnumel =36 *s0 +12 *s0 *(s1 //2 )+12 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_native_dropout_silu_6 [grid (triton_poi_fused_constant_pad_nd_native_dropout_silu_6_xnumel )](buf7 ,buf4 ,70 ,70 ,64 ,64 ,4900 ,3 ,14700 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n        3 +(s2 //2 )\n        3 +(s1 //2 )\n        9 +3 *(s1 //2 )+3 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        buf8 =empty_strided_cuda ((1 ,s0 ,3 +(s1 //2 ),3 +(s2 //2 )),(9 *s0 +3 *s0 *(s1 //2 )+3 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),9 +3 *(s1 //2 )+3 *(s2 //2 )+(s1 //2 )*(s2 //2 ),3 +(s2 //2 ),1 ),torch .int64 )\n\n        triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_max_unpool2d_native_dropout_silu_7_xnumel =9 *s0 +3 *s0 *(s1 //2 )+3 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_max_unpool2d_native_dropout_silu_7 [grid (triton_poi_fused_constant_pad_nd_max_pool2d_with_indices_max_unpool2d_native_dropout_silu_7_xnumel )](buf7 ,buf8 ,35 ,64 ,35 ,70 ,1225 ,64 ,3675 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf9 =empty_strided_cuda ((1 ,s0 ,6 +2 *(s1 //2 ),6 +2 *(s2 //2 )),(36 *s0 +12 *s0 *(s1 //2 )+12 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 ),36 +12 *(s1 //2 )+12 *(s2 //2 )+4 *(s1 //2 )*(s2 //2 ),6 +2 *(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_3_xnumel =36 *s0 +12 *s0 *(s1 //2 )+12 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_3 [grid (triton_poi_fused_max_unpool2d_3_xnumel )](buf9 ,14700 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool2d_8_xnumel =9 *s0 +3 *s0 *(s1 //2 )+3 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_8 [grid (triton_poi_fused_max_unpool2d_8_xnumel )](buf8 ,buf7 ,buf9 ,3 ,64 ,64 ,35 ,35 ,1225 ,70 ,70 ,3675 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf8 \n        buf11 =buf7 ;del buf7 \n\n        triton_poi_fused_silu_9_xnumel =36 *s0 +12 *s0 *(s1 //2 )+12 *s0 *(s2 //2 )+4 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_silu_9 [grid (triton_poi_fused_silu_9_xnumel )](buf9 ,buf11 ,70 ,70 ,4900 ,3 ,64 ,64 ,14700 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf9 \n    return (buf11 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a5937e60-a896-4e03-95ee-de20cc1e7021",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveMaxPool1d', 'Flatten', 'Softmin', 'LeakyReLU', 'Dropout1d', 'FeatureAlphaDropout', 'Hardshrink', 'InstanceNorm3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool1d = nn.AdaptiveMaxPool1d(output_size=128)\n        self.flatten = nn.Flatten()\n        self.softmin = nn.Softmin(dim=1)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n        self.dropout1d = nn.Dropout1d(p=0.5)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.hardshrink = nn.Hardshrink(lambd=0.5)\n        self.instance_norm3d = nn.InstanceNorm3d(num_features=64)\n\n    def forward(self, x):\n        # Reshape input to 3D for InstanceNorm3d\n        x = x.view(-1, 64, 32, 32, 32)\n        x = self.instance_norm3d(x)\n        \n        # Reshape back to 1D for AdaptiveMaxPool1d\n        x = x.view(-1, 64, 32 * 32 * 32)\n        x = self.adaptive_max_pool1d(x)\n        \n        # Apply Dropout1d\n        x = self.dropout1d(x)\n        \n        # Flatten the tensor\n        x = self.flatten(x)\n        \n        # Apply LeakyReLU\n        x = self.leaky_relu(x)\n        \n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Apply Hardshrink\n        x = self.hardshrink(x)\n        \n        # Apply Softmin\n        x = self.softmin(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 32, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =256 \n    r0_numel =8192 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +8192 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp3 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    R0_BLOCK :tl .constexpr =4 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\n    tmp2 =tl .load (in_ptr2 +(r0_1 +4 *x0 ),xmask ,other =0.0 )\n    tmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (xmask ,tmp3 ,0 )\n    tmp8 =tl .where (xmask ,tmp4 ,0 )\n    tmp9 =tl .where (xmask ,tmp5 ,0 )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\n    tmp13 =tmp10 [:,None ]\n    tmp14 =tmp11 [:,None ]\n    tmp15 =tmp12 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =xindex \n    x1 =xindex //32768 \n    tmp0 =tl .load (in_ptr0 +(x2 ),None )\n    tmp1 =tl .load (in_ptr1 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp4 =32768.0 \n    tmp5 =tmp3 /tmp4 \n    tmp6 =1e-05 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =libdevice .rsqrt (tmp7 )\n    tmp9 =tmp2 *tmp8 \n    tl .store (out_ptr0 +(x2 ),tmp9 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_3 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax__to_copy_abs_add_bernoulli_le_leaky_relu_mul_neg_scalar_tensor_where_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    r0_numel =8192 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp33 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp3 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +(r0_0 //128 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =r0_0 \n        tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n        tmp5 =0.5 \n        tmp6 =tmp4 <tmp5 \n        tmp7 =tmp6 .to (tl .float32 )\n        tmp8 =2.0 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tmp3 *tmp9 \n        tmp11 =0.0 \n        tmp12 =tmp10 >tmp11 \n        tmp13 =0.01 \n        tmp14 =tmp10 *tmp13 \n        tmp15 =tl .where (tmp12 ,tmp10 ,tmp14 )\n        tmp16 =tmp2 <tmp5 \n        tmp17 =tmp16 .to (tl .float32 )\n        tmp18 =0.8864048946659319 \n        tmp19 =tmp17 *tmp18 \n        tmp20 =tmp15 *tmp19 \n        tmp21 =-1.0 \n        tmp22 =tmp17 +tmp21 \n        tmp23 =1.558387861036063 \n        tmp24 =tmp22 *tmp23 \n        tmp25 =0.7791939305180315 \n        tmp26 =tmp24 +tmp25 \n        tmp27 =tmp20 +tmp26 \n        tmp28 =tl_math .abs (tmp27 )\n        tmp29 =tmp28 <=tmp5 \n        tmp30 =tl .where (tmp29 ,tmp11 ,tmp27 )\n        tmp31 =-tmp30 \n        tmp32 =tl .broadcast_to (tmp31 ,[XBLOCK ,R0_BLOCK ])\n        tmp34 =triton_helpers .maximum (_tmp33 ,tmp32 )\n        _tmp33 =tl .where (r0_mask ,tmp34 ,_tmp33 )\n        tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp31 ,r0_mask )\n    tmp33 =triton_helpers .max2 (_tmp33 ,1 )[:,None ]\n    _tmp39 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp35 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp36 =tmp35 -tmp33 \n        tmp37 =tl_math .exp (tmp36 )\n        tmp38 =tl .broadcast_to (tmp37 ,[XBLOCK ,R0_BLOCK ])\n        tmp40 =_tmp39 +tmp38 \n        _tmp39 =tl .where (r0_mask ,tmp40 ,_tmp39 )\n    tmp39 =tl .sum (_tmp39 ,1 )[:,None ]\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp41 =tl .load (in_out_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp42 =tmp41 -tmp33 \n        tmp43 =tl_math .exp (tmp42 )\n        tmp44 =tmp43 /tmp39 \n        tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp44 ,r0_mask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,64 ,32 ,32 ,32 ),(2097152 ,32768 ,1024 ,32 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,64 ,1 ,1 ,1 ,4 ),(256 ,4 ,256 ,256 ,256 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,64 ,1 ,1 ,1 ,4 ),(256 ,4 ,256 ,256 ,256 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,64 ,1 ,1 ,1 ,4 ),(256 ,4 ,256 ,256 ,256 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_0 [grid (256 )](arg0_1 ,buf0 ,buf1 ,buf2 ,256 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,64 ,1 ,1 ,1 ),(64 ,1 ,64 ,64 ,64 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,64 ,1 ,1 ,1 ),(64 ,1 ,64 ,64 ,64 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_1 [grid (64 )](buf0 ,buf1 ,buf2 ,buf3 ,buf4 ,64 ,4 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n        del buf1 \n        del buf2 \n        buf6 =empty_strided_cuda ((1 ,64 ,32 ,32 ,32 ),(2097152 ,32768 ,1024 ,32 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_2 [grid (2097152 )](arg0_1 ,buf3 ,buf4 ,buf6 ,2097152 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n        del buf3 \n\n        buf7 =torch .ops .aten .max_pool2d_with_indices .default (reinterpret_tensor (buf6 ,(1 ,64 ,1 ,32768 ),(0 ,32768 ,0 ,1 ),0 ),[1 ,256 ])\n        del buf6 \n        buf8 =buf7 [0 ]\n        del buf7 \n        buf10 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf10 )\n        buf11 =reinterpret_tensor (buf4 ,(1 ,64 ,1 ),(64 ,1 ,64 ),0 );del buf4 \n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_3 [grid (64 )](buf10 ,buf11 ,0 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf13 =reinterpret_tensor (buf8 ,(1 ,8192 ),(8192 ,1 ),0 );del buf8 \n        buf16 =buf13 ;del buf13 \n\n        get_raw_stream (0 )\n        triton_red_fused__softmax__to_copy_abs_add_bernoulli_le_leaky_relu_mul_neg_scalar_tensor_where_4 [grid (1 )](buf16 ,buf10 ,buf11 ,1 ,1 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf10 \n        del buf11 \n    return (buf16 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,64 ,32 ,32 ,32 ),(2097152 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a5e5438c-1be9-426d-8944-4f4c40d0e10d",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softmax', 'Upsample', 'GaussianNLLLoss', 'Dropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout1 = nn.Dropout(p=0.5)\n        self.dropout2 = nn.Dropout(p=0.5)\n        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.softmax = nn.Softmax(dim=1)\n        self.gaussian_nll_loss = nn.GaussianNLLLoss()\n\n    def forward(self, x):\n        # Apply dropout to the input\n        x = self.dropout1(x)\n        \n        # Upsample the input\n        x = self.upsample1(x)\n        \n        # Apply another dropout\n        x = self.dropout2(x)\n        \n        # Upsample again\n        x = self.upsample2(x)\n        \n        # Apply softmax to the output\n        x = self.softmax(x)\n        \n        # Assuming we have a target tensor for GaussianNLLLoss\n        # For demonstration, we'll create a dummy target tensor with the same shape as x\n        target = torch.randn_like(x)\n        var = torch.ones_like(x)  # Variance tensor\n        \n        # Compute GaussianNLLLoss (this is typically used in loss computation, not in forward pass)\n        loss = self.gaussian_nll_loss(x, target, var)\n        \n        # Return both the output and the loss\n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__unsafe_index_native_dropout_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .full ([1 ],2.0 ,tl .float64 )\n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float64 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tmp2 /tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =x1 \n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp7 *tmp5 \n    tmp9 =tmp8 .to (tl .int64 )\n    tmp10 =tmp9 +tmp1 \n    tmp11 =tmp9 <0 \n    tmp12 =tl .where (tmp11 ,tmp10 ,tmp9 )\n    tmp13 =ks3 \n    tmp14 =tmp13 .to (tl .float64 )\n    tmp15 =tmp0 *tmp14 \n    tmp16 =tmp14 /tmp15 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =x0 \n    tmp19 =tmp18 .to (tl .float32 )\n    tmp20 =tmp19 *tmp17 \n    tmp21 =tmp20 .to (tl .int64 )\n    tmp22 =tmp21 +tmp13 \n    tmp23 =tmp21 <0 \n    tmp24 =tl .where (tmp23 ,tmp22 ,tmp21 )\n    tmp25 =tl .load (in_ptr0 +(tmp24 +ks3 *tmp12 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp26 =0.5 \n    tmp27 =tmp25 >tmp26 \n    tmp28 =tmp27 .to (tl .float32 )\n    tmp29 =tl .load (in_ptr1 +(tmp24 +ks3 *tmp12 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp30 =tmp28 *tmp29 \n    tmp31 =2.0 \n    tmp32 =tmp30 *tmp31 \n    tl .store (out_ptr0 +(x4 ),tmp32 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__unsafe_index_native_dropout_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks6 \n    x3 =xindex \n    tmp0 =2.0 \n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tmp3 .to (tl .float64 )\n    tmp5 =tl .full ([1 ],2.0 ,tl .float64 )\n    tmp6 =tmp5 *tmp4 \n    tmp7 =tmp4 /tmp6 \n    tmp8 =tmp7 .to (tl .float32 )\n    tmp9 =x1 \n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp10 *tmp8 \n    tmp12 =tmp11 .to (tl .int64 )\n    tmp13 =ks3 \n    tmp14 =tmp12 +tmp13 \n    tmp15 =tmp12 <0 \n    tmp16 =tl .where (tmp15 ,tmp14 ,tmp12 )\n    tmp17 =ks4 \n    tmp18 =tmp17 .to (tl .float32 )\n    tmp19 =tmp0 *tmp18 \n    tmp20 =tmp19 .to (tl .float64 )\n    tmp21 =tmp5 *tmp20 \n    tmp22 =tmp20 /tmp21 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =x0 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 *tmp23 \n    tmp27 =tmp26 .to (tl .int64 )\n    tmp28 =ks5 \n    tmp29 =tmp27 +tmp28 \n    tmp30 =tmp27 <0 \n    tmp31 =tl .where (tmp30 ,tmp29 ,tmp27 )\n    tmp32 =tl .load (in_ptr0 +(tmp31 +2 *ks4 *tmp16 +4 *ks0 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp33 =0.5 \n    tmp34 =tmp32 >tmp33 \n    tmp35 =tmp34 .to (tl .float32 )\n    tmp36 =tl .load (in_ptr1 +(tmp31 +2 *ks4 *tmp16 +4 *ks0 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp37 =tmp35 *tmp36 \n    tmp38 =tmp37 *tmp0 \n    tl .store (out_ptr0 +(x3 ),tmp38 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_4 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +16 *ks0 *ks1 *r0_1 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask &xmask ,tmp1 ,float (\"-inf\"))\n    tmp4 =triton_helpers .max2 (tmp3 ,1 )[:,None ]\n    tmp5 =tmp0 -tmp4 \n    tmp6 =tl_math .exp (tmp5 )\n    tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n    tmp9 =tl .where (r0_mask &xmask ,tmp7 ,0 )\n    tmp10 =tl .sum (tmp9 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_5 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %ks0 )\n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp4 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp3 =tl_math .exp (tmp2 )\n    tmp5 =tmp3 /tmp4 \n    tl .store (in_out_ptr0 +(x2 ),tmp5 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_randn_like_6 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_ones_like_7 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =1.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((3 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[3 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,2 *s1 ,2 *s2 ),(4 *s0 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_native_dropout_0_xnumel =4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_0 [grid (triton_poi_fused_native_dropout_0_xnumel )](buf0 ,buf1 ,1 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_native_dropout_1_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_1 [grid (triton_poi_fused_native_dropout_1_xnumel )](buf0 ,buf2 ,0 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        2 *s2 \n        2 *s1 \n        4 *s1 *s2 \n        buf3 =empty_strided_cuda ((1 ,s0 ,2 *s1 ,2 *s2 ),(4 *s0 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__unsafe_index_native_dropout_2_xnumel =4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__unsafe_index_native_dropout_2 [grid (triton_poi_fused__unsafe_index_native_dropout_2_xnumel )](buf2 ,arg3_1 ,buf3 ,32 ,64 ,64 ,32 ,4096 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf2 \n        4 *s2 \n        4 *s1 \n        16 *s1 *s2 \n        buf4 =empty_strided_cuda ((1 ,s0 ,4 *s1 ,4 *s2 ),(16 *s0 *s1 *s2 ,16 *s1 *s2 ,4 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__unsafe_index_native_dropout_3_xnumel =16 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__unsafe_index_native_dropout_3 [grid (triton_poi_fused__unsafe_index_native_dropout_3_xnumel )](buf1 ,buf3 ,buf4 ,32 ,128 ,128 ,64 ,32 ,64 ,16384 ,49152 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        del buf3 \n        buf5 =empty_strided_cuda ((1 ,1 ,4 *s1 ,4 *s2 ),(16 *s1 *s2 ,16 *s1 *s2 ,4 *s2 ,1 ),torch .float32 )\n        buf6 =empty_strided_cuda ((1 ,1 ,4 *s1 ,4 *s2 ),(16 *s1 *s2 ,16 *s1 *s2 ,4 *s2 ,1 ),torch .float32 )\n\n        triton_per_fused__softmax_4_xnumel =16 *s1 *s2 \n        get_raw_stream (0 )\n        triton_per_fused__softmax_4 [grid (triton_per_fused__softmax_4_xnumel )](buf4 ,buf5 ,buf6 ,32 ,32 ,16384 ,3 ,XBLOCK =8 ,num_warps =2 ,num_stages =1 )\n        buf7 =buf4 ;del buf4 \n\n        triton_poi_fused__softmax_5_xnumel =16 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__softmax_5 [grid (triton_poi_fused__softmax_5_xnumel )](buf7 ,buf5 ,buf6 ,16384 ,49152 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del buf5 \n        del buf6 \n        buf8 =empty_strided_cuda ((1 ,s0 ,4 *s1 ,4 *s2 ),(16 *s0 *s1 *s2 ,16 *s1 *s2 ,4 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_randn_like_6_xnumel =16 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_randn_like_6 [grid (triton_poi_fused_randn_like_6_xnumel )](buf0 ,buf8 ,2 ,49152 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        buf9 =empty_strided_cuda ((1 ,s0 ,4 *s1 ,4 *s2 ),(16 *s0 *s1 *s2 ,16 *s1 *s2 ,4 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_ones_like_7_xnumel =16 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_ones_like_7 [grid (triton_poi_fused_ones_like_7_xnumel )](buf9 ,49152 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n    return (buf7 ,buf8 ,buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a5f6aa45-5859-4d66-b3f0-bdf33a528021",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveMaxPool2d', 'AvgPool1d', 'InstanceNorm1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool2d = nn.AdaptiveMaxPool2d((5, 5))\n        self.avg_pool1d = nn.AvgPool1d(kernel_size=2)\n        self.instance_norm1d = nn.InstanceNorm1d(5)\n        self.adaptive_max_pool2d_2 = nn.AdaptiveMaxPool2d((3, 3))\n        self.avg_pool1d_2 = nn.AvgPool1d(kernel_size=3)\n        self.instance_norm1d_2 = nn.InstanceNorm1d(3)\n\n    def forward(self, x):\n        # Apply AdaptiveMaxPool2d to the input\n        x = self.adaptive_max_pool2d(x)\n        \n        # Reshape the tensor to fit AvgPool1d\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten the last two dimensions\n        x = self.avg_pool1d(x)\n        \n        # Apply InstanceNorm1d\n        x = self.instance_norm1d(x)\n        \n        # Reshape the tensor to fit AdaptiveMaxPool2d again\n        x = x.view(x.size(0), x.size(1), 5, 5)  # Reshape back to 2D\n        x = self.adaptive_max_pool2d_2(x)\n        \n        # Reshape the tensor to fit AvgPool1d again\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten the last two dimensions\n        x = self.avg_pool1d_2(x)\n        \n        # Apply InstanceNorm1d again\n        x = self.instance_norm1d_2(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels and 64x64 spatial dimensions\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %12 )\n    x1 =xindex //12 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +25 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +25 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp3 =0.5 \n    tmp4 =tmp2 *tmp3 \n    tl .store (out_ptr0 +(x2 ),tmp4 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,64 ,64 ),(4096 *s0 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .adaptive_max_pool2d .default (arg3_1 ,[5 ,5 ])\n        del arg3_1 \n        buf1 =buf0 [0 ]\n        del buf0 \n        buf3 =empty_strided_cuda ((1 ,s0 ,1 ,12 ),(12 *s0 ,12 ,12 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_0_xnumel =12 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_0 [grid (triton_poi_fused_avg_pool2d_0_xnumel )](buf1 ,buf3 ,36 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del buf1 \n    return (reinterpret_tensor (buf3 ,(1 ,s0 ,12 ),(12 *s0 ,12 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a8fc7a42-eb67-4e44-92ac-784ff29ea268",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['FractionalMaxPool2d', 'Conv2d', 'ParameterList', 'LSTM', 'TransformerEncoder', 'KLDivLoss', 'LazyInstanceNorm2d', 'Softshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.fractional_max_pool = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.lazy_instance_norm = nn.LazyInstanceNorm2d()\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.softshrink = nn.Softshrink(lambd=0.5)\n        self.parameter_list = nn.ParameterList([nn.Parameter(torch.randn(32, 32)) for _ in range(3)])\n        self.lstm = nn.LSTM(input_size=32, hidden_size=64, num_layers=2, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=64, nhead=8), num_layers=2\n        )\n        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, x):\n        # Apply Conv2d and FractionalMaxPool2d\n        x = self.conv1(x)\n        x = self.fractional_max_pool(x)\n        \n        # Apply LazyInstanceNorm2d and Conv2d\n        x = self.lazy_instance_norm(x)\n        x = self.conv2(x)\n        \n        # Apply Softshrink\n        x = self.softshrink(x)\n        \n        # Reshape for LSTM\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Apply TransformerEncoder\n        x = self.transformer_encoder(x)\n        \n        # Apply ParameterList\n        for param in self.parameter_list:\n            x = torch.matmul(x, param)\n        \n        # Reshape for KLDivLoss\n        x = x.view(batch_size, -1)\n        target = torch.randn_like(x)\n        \n        # Apply KLDivLoss\n        loss = self.kl_div_loss(F.log_softmax(x, dim=1), F.softmax(target, dim=1))\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 28, 28).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =12544 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //784 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_functional_fractional_max_pool2d_mean_native_batch_norm_backward_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,out_ptr0 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,out_ptr6 ,out_ptr8 ,out_ptr10 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =16 \n    r0_numel =196 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp20 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp64_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp64_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp64_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index //14 \n        r0_1 =(r0_index %14 )\n        r0_3 =r0_index \n        tmp1 =r0_2 \n        tmp2 =tmp1 .to (tl .float32 )\n        tmp3 =tmp2 +tmp0 \n        tmp4 =2.0 \n        tmp5 =tmp3 *tmp4 \n        tmp6 =libdevice .floor (tmp5 )\n        tmp7 =tmp0 *tmp4 \n        tmp8 =libdevice .floor (tmp7 )\n        tmp9 =tmp6 -tmp8 \n        tmp10 =tmp9 .to (tl .int64 )\n        tmp11 =tl .full ([1 ,1 ],13 ,tl .int64 )\n        tmp12 =tmp2 <tmp11 \n        tmp13 =tl .full ([1 ,1 ],26 ,tl .int64 )\n        tmp14 =tl .where (tmp12 ,tmp10 ,tmp13 )\n        tmp15 =tl .full ([XBLOCK ,R0_BLOCK ],28 ,tl .int32 )\n        tmp16 =tmp14 +tmp15 \n        tmp17 =tmp14 <0 \n        tmp18 =tl .where (tmp17 ,tmp16 ,tmp14 )\n        tl .device_assert (((0 <=tmp18 )&(tmp18 <28 ))|~(r0_mask &xmask ),\"index out of bounds: 0 <= tmp18 < 28\")\n        tmp21 =r0_1 \n        tmp22 =tmp21 .to (tl .float32 )\n        tmp23 =tmp22 +tmp20 \n        tmp24 =tmp23 *tmp4 \n        tmp25 =libdevice .floor (tmp24 )\n        tmp26 =tmp20 *tmp4 \n        tmp27 =libdevice .floor (tmp26 )\n        tmp28 =tmp25 -tmp27 \n        tmp29 =tmp28 .to (tl .int64 )\n        tmp30 =tmp22 <tmp11 \n        tmp31 =tl .where (tmp30 ,tmp29 ,tmp13 )\n        tmp32 =tmp31 +tmp15 \n        tmp33 =tmp31 <0 \n        tmp34 =tl .where (tmp33 ,tmp32 ,tmp31 )\n        tl .device_assert (((0 <=tmp34 )&(tmp34 <28 ))|~(r0_mask &xmask ),\"index out of bounds: 0 <= tmp34 < 28\")\n        tmp36 =tl .load (in_ptr1 +(tmp34 +28 *tmp18 +784 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp37 =tl .load (in_ptr1 +(1 +tmp34 +28 *tmp18 +784 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp38 =triton_helpers .maximum (tmp37 ,tmp36 )\n        tmp39 =tl .load (in_ptr1 +(28 +tmp34 +28 *tmp18 +784 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp40 =triton_helpers .maximum (tmp39 ,tmp38 )\n        tmp41 =tl .load (in_ptr1 +(29 +tmp34 +28 *tmp18 +784 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp42 =triton_helpers .maximum (tmp41 ,tmp40 )\n        tmp43 =tmp37 >tmp36 \n        tmp44 =libdevice .isnan (tmp37 ).to (tl .int1 )\n        tmp45 =tmp43 |tmp44 \n        tmp46 =1 +tmp34 +28 *tmp18 \n        tmp47 =tmp46 .to (tl .int32 )\n        tmp48 =tmp34 +28 *tmp18 \n        tmp49 =tmp48 .to (tl .int32 )\n        tmp50 =tl .where (tmp45 ,tmp47 ,tmp49 )\n        tmp51 =tmp39 >tmp38 \n        tmp52 =libdevice .isnan (tmp39 ).to (tl .int1 )\n        tmp53 =tmp51 |tmp52 \n        tmp54 =28 +tmp34 +28 *tmp18 \n        tmp55 =tmp54 .to (tl .int32 )\n        tmp56 =tl .where (tmp53 ,tmp55 ,tmp50 )\n        tmp57 =tmp41 >tmp40 \n        tmp58 =libdevice .isnan (tmp41 ).to (tl .int1 )\n        tmp59 =tmp57 |tmp58 \n        tmp60 =29 +tmp34 +28 *tmp18 \n        tmp61 =tmp60 .to (tl .int32 )\n        tmp62 =tl .where (tmp59 ,tmp61 ,tmp56 )\n        tmp63 =tl .broadcast_to (tmp42 ,[XBLOCK ,R0_BLOCK ])\n        tmp64_mean_next ,tmp64_m2_next ,tmp64_weight_next =triton_helpers .welford_reduce (\n        tmp63 ,tmp64_mean ,tmp64_m2 ,tmp64_weight ,roffset ==0 \n        )\n        tmp64_mean =tl .where (r0_mask &xmask ,tmp64_mean_next ,tmp64_mean )\n        tmp64_m2 =tl .where (r0_mask &xmask ,tmp64_m2_next ,tmp64_m2 )\n        tmp64_weight =tl .where (r0_mask &xmask ,tmp64_weight_next ,tmp64_weight )\n        tl .store (out_ptr0 +(r0_3 +196 *x0 ),tmp42 ,r0_mask &xmask )\n        tl .store (out_ptr1 +(r0_3 +196 *x0 ),tmp62 ,r0_mask &xmask )\n    tmp67 ,tmp68 ,tmp69 =triton_helpers .welford (tmp64_mean ,tmp64_m2 ,tmp64_weight ,1 )\n    tmp64 =tmp67 [:,None ]\n    tmp65 =tmp68 [:,None ]\n    tmp66 =tmp69 [:,None ]\n    tmp78 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp80 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_3 =r0_index \n        tmp70 =tl .load (out_ptr0 +(r0_3 +196 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp71 =tmp70 -tmp64 \n        tmp72 =196.0 \n        tmp73 =tmp65 /tmp72 \n        tmp74 =1e-05 \n        tmp75 =tmp73 +tmp74 \n        tmp76 =libdevice .rsqrt (tmp75 )\n        tmp77 =tmp71 *tmp76 \n        tmp79 =tmp77 *tmp78 \n        tmp81 =tmp79 +tmp80 \n        tl .store (out_ptr4 +(r0_3 +196 *x0 ),tmp81 ,r0_mask &xmask )\n        tl .store (out_ptr5 +(r0_3 +196 *x0 ),tmp71 ,r0_mask &xmask )\n    tmp91 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp98 =tl .load (in_ptr5 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp82 =196.0 \n    tmp83 =tmp65 /tmp82 \n    tmp84 =1e-05 \n    tmp85 =tmp83 +tmp84 \n    tmp86 =libdevice .rsqrt (tmp85 )\n    tmp87 =1.005128205128205 \n    tmp88 =tmp83 *tmp87 \n    tmp89 =0.1 \n    tmp90 =tmp88 *tmp89 \n    tmp92 =0.9 \n    tmp93 =tmp91 *tmp92 \n    tmp94 =tmp90 +tmp93 \n    tmp95 =1.0 \n    tmp96 =tmp94 /tmp95 \n    tmp97 =tmp64 *tmp89 \n    tmp99 =tmp98 *tmp92 \n    tmp100 =tmp97 +tmp99 \n    tmp101 =tmp100 /tmp95 \n    tl .store (out_ptr6 +(x0 ),tmp86 ,xmask )\n    tl .store (out_ptr8 +(x0 ),tmp96 ,xmask )\n    tl .store (out_ptr10 +(x0 ),tmp101 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_convolution_gt_mul_sign_sub_where_3 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =6272 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //196 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl_math .abs (tmp2 )\n    tmp4 =0.5 \n    tmp5 =tmp3 >tmp4 \n    tmp6 =tl .full ([1 ],0 ,tl .int32 )\n    tmp7 =tmp6 <tmp2 \n    tmp8 =tmp7 .to (tl .int8 )\n    tmp9 =tmp2 <tmp6 \n    tmp10 =tmp9 .to (tl .int8 )\n    tmp11 =tmp8 -tmp10 \n    tmp12 =tmp11 .to (tmp2 .dtype )\n    tmp13 =tmp12 *tmp4 \n    tmp14 =tmp2 -tmp13 \n    tmp15 =0.0 \n    tmp16 =tmp2 *tmp15 \n    tmp17 =tl .where (tmp5 ,tmp14 ,tmp16 )\n    tl .store (out_ptr0 +(x2 ),tmp5 ,xmask )\n    tl .store (in_out_ptr0 +(x2 ),tmp17 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(16 ,3 ,3 ,3 ),(27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_2 ,(16 ,),(1 ,))\n    assert_size_stride (primals_5 ,(1 ,3 ,28 ,28 ),(2352 ,784 ,28 ,1 ))\n    assert_size_stride (primals_6 ,(16 ,),(1 ,))\n    assert_size_stride (primals_7 ,(16 ,),(1 ,))\n    assert_size_stride (primals_8 ,(16 ,),(1 ,))\n    assert_size_stride (primals_9 ,(16 ,),(1 ,))\n    assert_size_stride (primals_10 ,(32 ,16 ,3 ,3 ),(144 ,9 ,3 ,1 ))\n    assert_size_stride (primals_11 ,(32 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_5 ,primals_1 ,stride =(1 ,1 ),padding =(1 ,1 ),dilation =(1 ,1 ),transposed =False ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,16 ,28 ,28 ),(12544 ,784 ,28 ,1 ))\n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_0 [grid (12544 )](buf1 ,primals_2 ,12544 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,16 ,2 ),(32 ,2 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_1 [grid (32 )](buf2 ,buf3 ,0 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del buf2 \n        buf4 =empty_strided_cuda ((1 ,16 ,14 ,14 ),(3136 ,196 ,14 ,1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,16 ,14 ,14 ),(3136 ,196 ,14 ,1 ),torch .int64 )\n        buf10 =empty_strided_cuda ((1 ,16 ,14 ,14 ),(3136 ,196 ,14 ,1 ),torch .float32 )\n        buf14 =empty_strided_cuda ((1 ,16 ,14 ,14 ),(3136 ,196 ,14 ,1 ),torch .float32 )\n        buf9 =empty_strided_cuda ((1 ,16 ,1 ,1 ),(16 ,1 ,16 ,16 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_functional_fractional_max_pool2d_mean_native_batch_norm_backward_2 [grid (16 )](buf3 ,buf1 ,primals_8 ,primals_9 ,primals_7 ,primals_6 ,buf4 ,buf5 ,buf10 ,buf14 ,buf9 ,primals_7 ,primals_6 ,16 ,196 ,XBLOCK =1 ,R0_BLOCK =256 ,num_warps =2 ,num_stages =1 )\n        del buf3 \n        del buf4 \n        del primals_6 \n        del primals_7 \n        del primals_9 \n\n        buf11 =extern_kernels .convolution (buf10 ,primals_10 ,stride =(1 ,1 ),padding =(1 ,1 ),dilation =(1 ,1 ),transposed =False ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf11 ,(1 ,32 ,14 ,14 ),(6272 ,196 ,14 ,1 ))\n        buf12 =empty_strided_cuda ((1 ,32 ,14 ,14 ),(6272 ,196 ,14 ,1 ),torch .bool )\n        buf13 =buf11 ;del buf11 \n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_convolution_gt_mul_sign_sub_where_3 [grid (6272 )](buf13 ,primals_11 ,buf12 ,6272 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_11 \n    return (reinterpret_tensor (buf13 ,(1 ,196 ,32 ),(6272 ,1 ,196 ),0 ),primals_1 ,primals_5 ,primals_8 ,primals_10 ,buf1 ,buf5 ,reinterpret_tensor (buf9 ,(16 ,),(1 ,),0 ),buf10 ,buf12 ,buf14 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((16 ,3 ,3 ,3 ),(27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =28 \n    primals_4 =28 \n    primals_5 =rand_strided ((1 ,3 ,28 ,28 ),(2352 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((32 ,16 ,3 ,3 ),(144 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "a90ede27-2c5c-4d5e-b450-6f0cef392e10",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad2d', 'ChannelShuffle', 'Tanh', 'FeatureAlphaDropout']\nimport torch\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ConstantPad2d(2, 3.0)  # Padding with value 3.0\n        self.shuffle1 = nn.ChannelShuffle(4)  # Shuffling channels in groups of 4\n        self.tanh1 = nn.Tanh()  # Applying Tanh activation\n        self.dropout1 = nn.FeatureAlphaDropout(0.5)  # Applying FeatureAlphaDropout with 50% probability\n        self.pad2 = nn.ConstantPad2d(1, 2.0)  # Padding with value 2.0\n        self.shuffle2 = nn.ChannelShuffle(2)  # Shuffling channels in groups of 2\n        self.tanh2 = nn.Tanh()  # Applying Tanh activation\n        self.dropout2 = nn.FeatureAlphaDropout(0.3)  # Applying FeatureAlphaDropout with 30% probability\n\n    def forward(self, x):\n        x = self.pad1(x)  # Apply padding\n        x = self.shuffle1(x)  # Shuffle channels\n        x = self.tanh1(x)  # Apply Tanh activation\n        x = self.dropout1(x)  # Apply FeatureAlphaDropout\n        x = self.pad2(x)  # Apply padding\n        x = self.shuffle2(x)  # Shuffle channels\n        x = self.tanh2(x)  # Apply Tanh activation\n        x = self.dropout2(x)  # Apply FeatureAlphaDropout\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 4, 32, 32).cuda()  # Example input with 4 channels and 32x32 spatial dimensions\n    return [x]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =4 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_channel_shuffle_constant_pad_nd_mul_tanh_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x3 =xindex \n    tmp0 =(-1 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =4 +ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =4 +ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =(-3 )+x1 \n    tmp13 =tl .full ([1 ],0 ,tl .int64 )\n    tmp14 =tmp12 >=tmp13 \n    tmp15 =tl .broadcast_to (ks2 ,[XBLOCK ])\n    tmp16 =tmp12 <tmp15 \n    tmp17 =(-3 )+x0 \n    tmp18 =tmp17 >=tmp13 \n    tmp19 =tl .broadcast_to (ks3 ,[XBLOCK ])\n    tmp20 =tmp17 <tmp19 \n    tmp21 =tmp14 &tmp16 \n    tmp22 =tmp21 &tmp18 \n    tmp23 =tmp22 &tmp20 \n    tmp24 =tmp23 &tmp11 \n    tmp25 =tl .load (in_ptr0 +((-3 )+x0 +((-3 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp24 &xmask ,eviction_policy ='evict_last',other =3.0 )\n    tmp26 =libdevice .tanh (tmp25 )\n    tmp27 =tl .load (in_ptr1 +(x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp28 =0.5 \n    tmp29 =tmp27 <tmp28 \n    tmp30 =tmp29 .to (tl .float32 )\n    tmp31 =0.8864048946659319 \n    tmp32 =tmp30 *tmp31 \n    tmp33 =tmp26 *tmp32 \n    tmp34 =-1.0 \n    tmp35 =tmp30 +tmp34 \n    tmp36 =1.558387861036063 \n    tmp37 =tmp35 *tmp36 \n    tmp38 =0.7791939305180315 \n    tmp39 =tmp37 +tmp38 \n    tmp40 =tmp33 +tmp39 \n    tmp41 =tl .full (tmp40 .shape ,2.0 ,tmp40 .dtype )\n    tmp42 =tl .where (tmp11 ,tmp40 ,tmp41 )\n    tl .store (out_ptr0 +(x3 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =4 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_mul_tanh_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =(xindex %ks0 )\n    x3 =xindex //ks0 \n    x1 =xindex //ks3 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(x2 +36 *(x3 //2 )+72 *((x3 %2 ))+6 *ks1 *(x3 //2 )+6 *ks2 *(x3 //2 )+12 *ks1 *((x3 %2 ))+12 *ks2 *((x3 %2 ))+ks1 *ks2 *(x3 //2 )+2 *ks1 *ks2 *((x3 %2 ))),xmask ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =libdevice .tanh (tmp0 )\n    tmp3 =0.7 \n    tmp4 =tmp2 <tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =0.8609526162463561 \n    tmp7 =tmp5 *tmp6 \n    tmp8 =tmp1 *tmp7 \n    tmp9 =-1.0 \n    tmp10 =tmp5 +tmp9 \n    tmp11 =1.513640227123543 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =0.4540920681370629 \n    tmp14 =tmp12 +tmp13 \n    tmp15 =tmp8 +tmp14 \n    tl .store (out_ptr0 +(x4 ),tmp15 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    s2 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,4 ,s1 ,s2 ),(4 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,4 ,4 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (4 )](buf0 ,buf1 ,0 ,4 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        6 +s2 \n        6 +s1 \n        36 +6 *s1 +6 *s2 +s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,4 ,6 +s1 ,6 +s2 ),(144 +24 *s1 +24 *s2 +4 *s1 *s2 ,36 +6 *s1 +6 *s2 +s1 *s2 ,6 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_add_bernoulli_channel_shuffle_constant_pad_nd_mul_tanh_1_xnumel =144 +24 *s1 +24 *s2 +4 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_channel_shuffle_constant_pad_nd_mul_tanh_1 [grid (triton_poi_fused__to_copy_add_bernoulli_channel_shuffle_constant_pad_nd_mul_tanh_1_xnumel )](arg2_1 ,buf1 ,buf2 ,38 ,38 ,32 ,32 ,1444 ,5776 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        buf3 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_2 [grid (4 )](buf0 ,buf3 ,1 ,4 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        36 +6 *s1 +6 *s2 +s1 *s2 \n        buf4 =empty_strided_cuda ((1 ,4 ,6 +s1 ,6 +s2 ),(144 +24 *s1 +24 *s2 +4 *s1 *s2 ,36 +6 *s1 +6 *s2 +s1 *s2 ,6 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_add_bernoulli_mul_tanh_3_xnumel =144 +24 *s1 +24 *s2 +4 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_mul_tanh_3 [grid (triton_poi_fused__to_copy_add_bernoulli_mul_tanh_3_xnumel )](buf2 ,buf3 ,buf4 ,1444 ,32 ,32 ,1444 ,5776 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        del buf3 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =32 \n    arg1_1 =32 \n    arg2_1 =rand_strided ((1 ,4 ,32 ,32 ),(4096 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "aa5884ad-b246-4450-a529-b19525631b1b",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PixelUnshuffle', 'PixelShuffle', 'LogSigmoid', 'CTCLoss', 'GELU', 'MarginRankingLoss', 'TripletMarginLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pixel_unshuffle = nn.PixelUnshuffle(downscale_factor=2)\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=2)\n        self.log_sigmoid = nn.LogSigmoid()\n        self.gelu = nn.GELU()\n        self.ctc_loss = nn.CTCLoss()\n        self.margin_ranking_loss = nn.MarginRankingLoss()\n        self.triplet_margin_loss = nn.TripletMarginLoss()\n\n    def forward(self, x):\n        # Apply PixelUnshuffle\n        x = self.pixel_unshuffle(x)\n        \n        # Apply GELU activation\n        x = self.gelu(x)\n        \n        # Apply PixelShuffle\n        x = self.pixel_shuffle(x)\n        \n        # Apply LogSigmoid\n        x = self.log_sigmoid(x)\n        \n        # Compute CTC Loss (dummy target and input lengths for demonstration)\n        input_lengths = torch.full((x.size(0),), x.size(1), dtype=torch.long)\n        target_lengths = torch.randint(1, x.size(1), (x.size(0),), dtype=torch.long)\n        ctc_loss = self.ctc_loss(x, torch.randint(0, 10, (x.size(0), x.size(1)), dtype=torch.long), input_lengths, target_lengths)\n        \n        # Compute MarginRankingLoss (dummy inputs for demonstration)\n        input1 = torch.randn_like(x)\n        input2 = torch.randn_like(x)\n        target = torch.randint(0, 2, (x.size(0),), dtype=torch.float)\n        margin_ranking_loss = self.margin_ranking_loss(input1, input2, target)\n        \n        # Compute TripletMarginLoss (dummy inputs for demonstration)\n        anchor = torch.randn_like(x)\n        positive = torch.randn_like(x)\n        negative = torch.randn_like(x)\n        triplet_margin_loss = self.triplet_margin_loss(anchor, positive, negative)\n        \n        # Return the final output and the computed losses\n        return x, ctc_loss, margin_ranking_loss, triplet_margin_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nfrom ctypes import c_void_p ,c_long ,c_int \nimport torch \nimport math \nimport random \nimport os \nimport tempfile \nfrom math import inf ,nan \nfrom cmath import nanj \nfrom torch ._inductor .hooks import run_intermediate_hooks \nfrom torch ._inductor .utils import maybe_profile \nfrom torch ._inductor .codegen .memory_planning import _align as align \nfrom torch import device ,empty_strided \nfrom torch ._inductor .async_compile import AsyncCompile \nfrom torch ._inductor .select_algorithm import extern_kernels \nfrom torch ._inductor .codegen .multi_kernel import MultiKernelCall \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\nsplit_scan_grid ,\ngrid_combo_kernels ,\nstart_graph ,\nend_graph ,\ncooperative_reduction_grid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nextern \"C\"void kernel (int64_t *out_ptr0 ,\nconst int64_t ks0 ,\nconst int64_t ks1 ,\nconst int64_t ks2 )\n{\n{\n{\n{\nauto tmp0 =c10 ::div_floor_integer (static_cast <int64_t >(ks0 *(c10 ::div_floor_integer (static_cast <int64_t >(ks1 ),static_cast <int64_t >(c10 ::div_floor_integer (static_cast <int64_t >(ks1 ),static_cast <int64_t >(2 L )))))*(c10 ::div_floor_integer (static_cast <int64_t >(ks2 ),static_cast <int64_t >(c10 ::div_floor_integer (static_cast <int64_t >(ks2 ),static_cast <int64_t >(2 L )))))),static_cast <int64_t >(4 L ));\nauto tmp1 =c10 ::convert <int64_t >(tmp0 );\nout_ptr0 [static_cast <int64_t >(0 L )]=tmp1 ;\n}\n}\n}\n}\n''')\n\n#include \"/tmp/torchinductor_sahanp/3b/c3bi5gk6mslf6u4iaqafhxm64z6u65e3eain4xlary5blqnvv6xx.h\"\nextern \"C\"  void kernel(const int64_t* in_ptr0,\n                       int64_t* out_ptr0,\n                       int64_t* out_ptr1,\n                       const int64_t ks0,\n                       const int64_t ks1,\n                       const int64_t ks2)\n{\n    {\n        {\n            {\n                auto tmp0 = in_ptr0[static_cast<int64_t>(0L)];\n                auto tmp1 = static_cast<int32_t>(0);\n                auto tmp2 = static_cast<int64_t>(1);\n                auto tmp3 = c10::div_floor_integer(static_cast<int64_t>(ks0*(c10::div_floor_integer(static_cast<int64_t>(ks1), static_cast<int64_t>(c10::div_floor_integer(static_cast<int64_t>(ks1), static_cast<int64_t>(2L)))))*(c10::div_floor_integer(static_cast<int64_t>(ks2), static_cast<int64_t>(c10::div_floor_integer(static_cast<int64_t>(ks2), static_cast<int64_t>(2L)))))), static_cast<int64_t>(4L));\n                auto tmp4 = c10::convert<int64_t>(tmp3);\n                auto tmp5 = randint64_cpu(tmp0, tmp1, tmp2, tmp4);\n                out_ptr0[static_cast<int64_t>(0L)] = tmp5;\n            }\n        }\n    }\n    {\n        #pragma GCC ivdep\n        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(c10::div_floor_integer(static_cast<int64_t>(ks0*(c10::div_floor_integer(static_cast<int64_t>(ks1), static_cast<int64_t>(c10::div_floor_integer(static_cast<int64_t>(ks1), static_cast<int64_t>(2L)))))*(c10::div_floor_integer(static_cast<int64_t>(ks2), static_cast<int64_t>(c10::div_floor_integer(static_cast<int64_t>(ks2), static_cast<int64_t>(2L)))))), static_cast<int64_t>(4L))); x0+=static_cast<int64_t>(1L))\n        {\n            {\n                {\n                    auto tmp0 = in_ptr0[static_cast<int64_t>(1L)];\n                    auto tmp1 = x0;\n                    auto tmp2 = c10::convert<int32_t>(tmp1);\n                    auto tmp3 = static_cast<int64_t>(0);\n                    auto tmp4 = static_cast<int64_t>(10);\n                    auto tmp5 = randint64_cpu(tmp0, tmp2, tmp3, tmp4);\n                    out_ptr1[static_cast<int64_t>(x0)] = tmp5;\n                }\n            }\n        }\n    }\n}\n''')\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_log_sigmoid_forward_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(x0 +2 *x1 *(ks5 //2 )+2 *(ks5 //2 )*(((x0 %2 ))//2 )+4 *(ks4 //2 )*(ks5 //2 )*((((2 *((x1 %2 ))+4 *x2 +((x0 %2 )))//4 )%ks3 ))),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.5 \n    tmp2 =tmp0 *tmp1 \n    tmp3 =0.7071067811865476 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .erf (tmp4 )\n    tmp6 =1.0 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =tmp2 *tmp7 \n    tmp9 =0.0 \n    tmp10 =triton_helpers .minimum (tmp9 ,tmp8 )\n    tmp11 =tl_math .abs (tmp8 )\n    tmp12 =-tmp11 \n    tmp13 =tl_math .exp (tmp12 )\n    tmp14 =libdevice .log1p (tmp13 )\n    tmp15 =tmp10 -tmp14 \n    tl .store (out_ptr0 +(x0 +x3 *(ks5 //2 )*(triton_helpers .div_floor_integer (ks3 *(triton_helpers .div_floor_integer (ks4 ,ks4 //2 ))*(triton_helpers .div_floor_integer (ks5 ,ks5 //2 )),2 *(triton_helpers .div_floor_integer (ks3 *(triton_helpers .div_floor_integer (ks4 ,ks4 //2 ))*(triton_helpers .div_floor_integer (ks5 ,ks5 //2 )),4 ))))),tmp15 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    buf3 =empty_strided_cpu ((1 ,),(1 ,),torch .int64 )\n    cpp_fused_full_0 (buf3 ,s0 ,s1 ,s2 )\n    buf1 =empty_strided_cpu ((2 ,),(1 ,),torch .int64 )\n\n    aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf1 )\n    buf4 =empty_strided_cpu ((1 ,),(1 ,),torch .int64 )\n    buf2 =empty_strided_cpu ((1 ,(s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//4 ),((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//4 ,1 ),torch .int64 )\n    cpp_fused_randint_1 (buf1 ,buf4 ,buf2 ,s0 ,s1 ,s2 )\n    del buf1 \n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        ps0 =2 *(s2 //2 )\n        ps1 =2 *(s1 //2 )\n        ps2 =4 *(s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,(s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//4 ,2 *(s1 //2 ),2 *(s2 //2 )),(2 *(s1 //2 )*(s2 //2 )*((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//4 )*((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//(2 *((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//4 ))),2 *(s1 //2 )*(s2 //2 )*((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//(2 *((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//4 ))),(s2 //2 )*((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//(2 *((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//4 ))),1 ),torch .float32 )\n\n        triton_poi_fused_log_sigmoid_forward_2_xnumel =4 *(s1 //2 )*(s2 //2 )*((s0 *(s1 //(s1 //2 ))*(s2 //(s2 //2 )))//4 )\n        stream0 =get_raw_stream (0 )\n        triton_poi_fused_log_sigmoid_forward_2 [grid (triton_poi_fused_log_sigmoid_forward_2_xnumel )](arg3_1 ,buf0 ,64 ,64 ,4096 ,3 ,64 ,64 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,buf2 ,buf3 ,buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ac4de9a3-5030-4ceb-a5b7-2f7b0bddbf1c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MultiMarginLoss', 'FeatureAlphaDropout', 'Dropout2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout2d = nn.Dropout2d(p=0.5)\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.multi_margin_loss = nn.MultiMarginLoss()\n\n    def forward(self, x):\n        # Apply Dropout2d\n        x = self.dropout2d(x)\n        \n        # Reshape to accommodate FeatureAlphaDropout\n        x = x.view(x.size(0), -1)  # Flatten the input\n        x = x.unsqueeze(1)  # Add a channel dimension\n        \n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Reshape back to original shape (or any shape suitable for MultiMarginLoss)\n        x = x.squeeze(1)  # Remove the channel dimension\n        x = x.view(x.size(0), -1)  # Flatten again\n        \n        # Apply MultiMarginLoss (assuming target is provided externally)\n        # Since MultiMarginLoss is a loss function, it is typically used during training.\n        # Here, we just return the output for demonstration purposes.\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_bernoulli_mul_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 //(ks0 *ks1 )),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr2 +(0 ))\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ])\n    tmp2 =0.5 \n    tmp3 =tmp1 <tmp2 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =2.0 \n    tmp6 =tmp4 *tmp5 \n    tmp7 =tmp0 *tmp6 \n    tmp10 =tmp9 <tmp2 \n    tmp11 =tmp10 .to (tl .float32 )\n    tmp12 =0.8864048946659319 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =tmp7 *tmp13 \n    tmp15 =-1.0 \n    tmp16 =tmp11 +tmp15 \n    tmp17 =1.558387861036063 \n    tmp18 =tmp16 *tmp17 \n    tmp19 =0.7791939305180315 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =tmp14 +tmp20 \n    tl .store (out_ptr0 +(x0 ),tmp21 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_1 [grid (1 )](buf0 ,buf2 ,1 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf3 =empty_strided_cuda ((1 ,1 ,s0 *s1 *s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_add_bernoulli_mul_2_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_bernoulli_mul_2 [grid (triton_poi_fused__to_copy_add_bernoulli_mul_2_xnumel )](arg3_1 ,buf1 ,buf2 ,buf3 ,64 ,64 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        del buf2 \n    return (reinterpret_tensor (buf3 ,(1 ,s0 *s1 *s2 ),(s0 *s1 *s2 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ac817af7-d326-4894-9a1d-63d385daa392",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReflectionPad3d', 'Softmax2d', 'AdaptiveMaxPool3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reflection_pad = nn.ReflectionPad3d(1)\n        self.softmax2d = nn.Softmax2d()\n        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((8, 8, 8))\n        self.reflection_pad2 = nn.ReflectionPad3d(2)\n        self.adaptive_max_pool3d2 = nn.AdaptiveMaxPool3d((4, 4, 4))\n\n    def forward(self, x):\n        # Apply ReflectionPad3d\n        x = self.reflection_pad(x)\n        \n        # Reshape to 4D for Softmax2d\n        x = x.view(x.size(0), x.size(1), x.size(2), -1)\n        x = self.softmax2d(x)\n        \n        # Reshape back to 5D for AdaptiveMaxPool3d\n        x = x.view(x.size(0), x.size(1), x.size(2), x.size(3), -1)\n        x = self.adaptive_max_pool3d(x)\n        \n        # Apply ReflectionPad3d again\n        x = self.reflection_pad2(x)\n        \n        # Apply AdaptiveMaxPool3d again\n        x = self.adaptive_max_pool3d2(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 16, 16, 16).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =tl .load (in_ptr0 +(ks3 *(tl .where ((-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+(x0 //(2 +ks3 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+(x0 //(2 +ks3 )))))+2 *ks2 ,(-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+(x0 //(2 +ks3 )))))))+ks2 *ks3 *(tl .where ((-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+x1 )))+2 *ks1 ,(-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+x1 )))))+ks1 *ks2 *ks3 *r0_2 +(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+((x0 %(2 +ks3 ))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+((x0 %(2 +ks3 ))))))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+((x0 %(2 +ks3 ))))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =triton_helpers .maximum (_tmp2 ,tmp1 )\n        _tmp2 =tl .where (r0_mask &xmask ,tmp3 ,_tmp2 )\n    tmp2 =triton_helpers .max2 (_tmp2 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp2 ,xmask )\n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp4 =tl .load (in_ptr0 +(ks3 *(tl .where ((-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+(x0 //(2 +ks3 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+(x0 //(2 +ks3 )))))+2 *ks2 ,(-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+(x0 //(2 +ks3 )))))))+ks2 *ks3 *(tl .where ((-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+x1 )))+2 *ks1 ,(-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+x1 )))))+ks1 *ks2 *ks3 *r0_2 +(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+((x0 %(2 +ks3 ))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+((x0 %(2 +ks3 ))))))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+((x0 %(2 +ks3 ))))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tmp4 -tmp2 \n        tmp6 =tl_math .exp (tmp5 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =_tmp8 +tmp7 \n        _tmp8 =tl .where (r0_mask &xmask ,tmp9 ,_tmp8 )\n    tmp8 =tl .sum (_tmp8 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x3 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =(xindex %ks2 )\n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks5 *(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+(x0 //(2 +ks5 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+(x0 //(2 +ks5 )))))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+(x0 //(2 +ks5 )))))))+ks4 *ks5 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+x1 )))))+ks3 *ks4 *ks5 *x2 +(tl .where ((-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+((x0 %(2 +ks5 ))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+((x0 %(2 +ks5 ))))))+2 *ks5 ,(-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+((x0 %(2 +ks5 ))))))))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp4 =tl .load (in_ptr2 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp3 =tl_math .exp (tmp2 )\n    tmp5 =tmp3 /tmp4 \n    tl .store (out_ptr0 +(x4 ),tmp5 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_reflection_pad3d_2 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %12 )\n    x1 =((xindex //12 )%12 )\n    x2 =((xindex //144 )%12 )\n    x3 =xindex //1728 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(511 +((-1 )*tl_math .abs ((-7 )+tl_math .abs ((-2 )+x0 )))+((-64 )*tl_math .abs ((-7 )+tl_math .abs ((-2 )+x2 )))+((-8 )*tl_math .abs ((-7 )+tl_math .abs ((-2 )+x1 )))+512 *x3 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x4 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +2 *s2 +2 *s3 +s2 *s3 \n        buf0 =empty_strided_cuda ((1 ,1 ,2 +s1 ,4 +2 *s2 +2 *s3 +s2 *s3 ),(8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,1 ,2 +s1 ,4 +2 *s2 +2 *s3 +s2 *s3 ),(8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,1 ),torch .float32 )\n\n        triton_red_fused__softmax_0_xnumel =8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_0 [grid (triton_red_fused__softmax_0_xnumel )](arg4_1 ,buf0 ,buf1 ,324 ,16 ,16 ,16 ,5832 ,3 ,XBLOCK =128 ,R0_BLOCK =4 ,num_warps =4 ,num_stages =1 )\n        2 +s1 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf2 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,4 +2 *s2 +2 *s3 +s2 *s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,1 ),torch .float32 )\n\n        triton_poi_fused__softmax_1_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused__softmax_1 [grid (triton_poi_fused__softmax_1_xnumel )](arg4_1 ,buf0 ,buf1 ,buf2 ,324 ,18 ,5832 ,16 ,16 ,16 ,17496 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg4_1 \n        del buf0 \n        del buf1 \n\n        buf3 =torch .ops .aten .adaptive_max_pool3d .default (reinterpret_tensor (buf2 ,(1 ,s0 ,2 +s1 ,4 +2 *s2 +2 *s3 +s2 *s3 ,1 ),(0 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,1 ,0 ),0 ),[8 ,8 ,8 ])\n        del buf2 \n        buf4 =buf3 [0 ]\n        del buf3 \n        buf6 =empty_strided_cuda ((1 ,s0 ,12 ,12 ,12 ),(1728 *s0 ,1728 ,144 ,12 ,1 ),torch .float32 )\n\n        triton_poi_fused_reflection_pad3d_2_xnumel =1728 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_reflection_pad3d_2 [grid (triton_poi_fused_reflection_pad3d_2_xnumel )](buf4 ,buf6 ,5184 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n\n        buf7 =torch .ops .aten .adaptive_max_pool3d .default (buf6 ,[4 ,4 ,4 ])\n        del buf6 \n        buf8 =buf7 [0 ]\n        del buf7 \n    return (buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =16 \n    arg2_1 =16 \n    arg3_1 =16 \n    arg4_1 =rand_strided ((1 ,3 ,16 ,16 ,16 ),(12288 ,4096 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ad7c1510-ec6a-4865-8399-8f2914c57677",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LogSoftmax', 'ConstantPad3d', 'GRU', 'Softmin', 'Dropout', 'MaxUnpool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad3d(padding=(1, 1, 1, 1, 1, 1), value=0)\n        self.gru1 = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.dropout = nn.Dropout(p=0.5)\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.gru2 = nn.GRU(input_size=20, hidden_size=10, num_layers=1, batch_first=True)\n        self.softmin = nn.Softmin(dim=1)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        # Assume input x is of shape (batch_size, channels, depth, height, width)\n        x = self.pad(x)  # Apply ConstantPad3d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape for GRU\n        x, _ = self.gru1(x)  # Apply first GRU\n        x = self.dropout(x)  # Apply Dropout\n        x = x.view(x.size(0), x.size(1), int(x.size(2)**0.5), int(x.size(2)**0.5))  # Reshape for MaxUnpool2d\n        x = self.max_unpool(x, torch.zeros_like(x))  # Apply MaxUnpool2d (dummy indices)\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape for second GRU\n        x, _ = self.gru2(x)  # Apply second GRU\n        x = self.softmin(x)  # Apply Softmin\n        x = x.view(x.size(0), -1)  # Reshape for LogSoftmax\n        x = self.log_softmax(x)  # Apply LogSoftmax\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 10, 10, 10).cuda()  # Example input shape (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x6 =((xindex //ks0 )%ks1 )\n    x1 =((xindex //ks3 )%ks4 )\n    x0 =(xindex %ks3 )\n    x2 =((xindex //ks7 )%ks1 )\n    x3 =xindex //ks8 \n    x10 =xindex \n    tmp0 =(-1 )+x6 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks5 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-1 )+x0 \n    tmp10 =tmp9 >=tmp1 \n    tmp11 =ks6 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =tl .load (in_ptr0 +((-1 )+x0 +((-1 )*ks6 )+ks6 *x1 +((-1 )*ks5 *ks6 )+ks5 *ks6 *x2 +ks2 *ks5 *ks6 *x3 ),tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tl .store (out_ptr0 +(x10 ),tmp18 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_view_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *(((x0 //ks2 )%ks3 ))+4 *(x0 //ks1 )+8 *x1 +ks6 *(((x0 //ks2 )%ks3 ))+2 *ks5 *(x0 //ks1 )+2 *ks6 *(x0 //ks1 )+4 *ks4 *x1 +4 *ks5 *x1 +4 *ks6 *x1 +ks5 *ks6 *(x0 //ks1 )+2 *ks4 *ks5 *x1 +2 *ks4 *ks6 *x1 +2 *ks5 *ks6 *x1 +ks4 *ks5 *ks6 *x1 +((x0 %ks2 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +2 *s2 +2 *s3 +s2 *s3 \n        2 +s1 \n        2 +s3 \n        2 +s2 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg4_1 ,buf0 ,144 ,12 ,10 ,12 ,12 ,10 ,10 ,144 ,1728 ,5184 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg4_1 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf1 =empty_strided_cuda ((1 ,s0 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_view_1_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_view_1 [grid (triton_poi_fused_constant_pad_nd_view_1_xnumel )](buf0 ,buf1 ,1728 ,144 ,12 ,12 ,10 ,10 ,10 ,5184 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =10 \n    arg2_1 =10 \n    arg3_1 =10 \n    arg4_1 =rand_strided ((1 ,3 ,10 ,10 ,10 ),(3000 ,1000 ,100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ada2886c-1029-497c-b537-5171b2b33896",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CircularPad3d', 'Softplus', 'Sigmoid', 'CircularPad2d', 'NLLLoss2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.circular_pad3d = nn.CircularPad3d(1)\n        self.softplus = nn.Softplus()\n        self.sigmoid = nn.Sigmoid()\n        self.circular_pad2d = nn.CircularPad2d(1)\n        self.nll_loss2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Assuming input is 3D, pad it using CircularPad3d\n        x = self.circular_pad3d(x)\n        \n        # Apply Softplus activation\n        x = self.softplus(x)\n        \n        # Reshape to 2D for CircularPad2d\n        x = x.view(x.size(0), x.size(1), -1, x.size(-1))\n        \n        # Pad the reshaped tensor using CircularPad2d\n        x = self.circular_pad2d(x)\n        \n        # Apply Sigmoid activation\n        x = self.sigmoid(x)\n        \n        # Reshape back to original 3D shape\n        x = x.view(x.size(0), x.size(1), -1, x.size(-1))\n        \n        # Apply NLLLoss2d (assuming target is provided externally)\n        # Note: NLLLoss2d requires a target, so this is just a placeholder\n        # In practice, you would need to pass the target to the forward method\n        # and compute the loss separately.\n        # Here, we just return the output before the loss computation.\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64, 64).cuda()  # Example input with shape (batch, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks2 )\n    x2 =((xindex //ks4 )%ks5 )\n    x3 =xindex //ks7 \n    x5 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =ks1 +x0 \n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .broadcast_to (1 +ks1 ,[XBLOCK ])\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =x1 \n    tmp11 =tl .full ([1 ],1 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp14 =tmp10 <tmp13 \n    tmp15 =tmp12 &tmp14 \n    tmp16 =tmp15 &tmp9 \n    tmp17 =x2 \n    tmp18 =tl .full ([1 ],1 ,tl .int64 )\n    tmp19 =tmp17 >=tmp18 \n    tmp20 =tl .broadcast_to (1 +ks6 ,[XBLOCK ])\n    tmp21 =tmp17 <tmp20 \n    tmp22 =tmp19 &tmp21 \n    tmp23 =tmp22 &tmp16 \n    tmp24 =tl .load (in_ptr0 +((-1 )+x0 +ks1 *x1 +((-1 )*ks1 *ks3 )+ks1 *ks3 *x2 +ks1 *ks3 *ks6 *x3 ),tmp23 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp25 =tl .load (in_ptr1 +(ks1 +x5 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tl .full (tmp26 .shape ,0.0 ,tmp26 .dtype )\n    tmp28 =tl .where (tmp16 ,tmp26 ,tmp27 )\n    tmp29 =tl .load (in_ptr1 +(ks1 +x5 ),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp30 =tl .where (tmp15 ,tmp28 ,tmp29 )\n    tmp31 =tl .full (tmp30 .shape ,0.0 ,tmp30 .dtype )\n    tmp32 =tl .where (tmp9 ,tmp30 ,tmp31 )\n    tmp33 =float (\"nan\")\n    tmp34 =tl .where (tmp8 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp2 ,tmp34 ,tmp35 )\n    tmp37 =tmp0 >=tmp1 \n    tmp38 =1 +ks1 \n    tmp39 =tmp0 <tmp38 \n    tmp40 =tmp37 &tmp39 \n    tmp41 =x1 \n    tmp42 =tl .full ([1 ],1 ,tl .int64 )\n    tmp43 =tmp41 >=tmp42 \n    tmp44 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp45 =tmp41 <tmp44 \n    tmp46 =tmp43 &tmp45 \n    tmp47 =tmp46 &tmp40 \n    tmp48 =x2 \n    tmp49 =tl .full ([1 ],1 ,tl .int64 )\n    tmp50 =tmp48 >=tmp49 \n    tmp51 =tl .broadcast_to (1 +ks6 ,[XBLOCK ])\n    tmp52 =tmp48 <tmp51 \n    tmp53 =tmp50 &tmp52 \n    tmp54 =tmp53 &tmp47 \n    tmp55 =tl .load (in_ptr0 +((-1 )+x0 +((-1 )*ks1 )+ks1 *x1 +((-1 )*ks1 *ks3 )+ks1 *ks3 *x2 +ks1 *ks3 *ks6 *x3 ),tmp54 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp56 =tl .load (in_ptr1 +(x5 ),tmp47 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp57 =tl .where (tmp53 ,tmp55 ,tmp56 )\n    tmp58 =tl .full (tmp57 .shape ,0.0 ,tmp57 .dtype )\n    tmp59 =tl .where (tmp47 ,tmp57 ,tmp58 )\n    tmp60 =tl .load (in_ptr1 +(x5 ),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp61 =tl .where (tmp46 ,tmp59 ,tmp60 )\n    tmp62 =tl .full (tmp61 .shape ,0.0 ,tmp61 .dtype )\n    tmp63 =tl .where (tmp40 ,tmp61 ,tmp62 )\n    tmp64 =float (\"nan\")\n    tmp65 =tl .where (tmp40 ,tmp63 ,tmp64 )\n    tmp66 =tl .where (tmp2 ,tmp36 ,tmp65 )\n    tl .store (out_ptr0 +(x5 ),tmp66 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x4 =xindex //ks0 \n    x3 =xindex \n    tmp41 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =1 +ks2 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =x1 +((-1 )*ks2 )\n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(1 +2 *x4 +ks3 *x4 ),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x3 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =x0 \n    tmp17 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp18 =tmp16 >=tmp17 \n    tmp19 =tmp18 &tmp2 \n    tmp20 =tl .load (in_ptr0 +(1 +((-2 )*ks2 )+2 *x4 +ks3 *x4 +((-1 )*ks2 *ks3 )),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr0 +(x3 +((-2 )*ks2 )+((-1 )*ks2 *ks3 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\n    tmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],1 ,tl .int64 )\n    tmp27 =tmp0 <tmp26 \n    tmp28 =x0 \n    tmp29 =tl .broadcast_to (1 +ks3 ,[XBLOCK ])\n    tmp30 =tmp28 >=tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(1 +2 *ks2 +2 *x4 +ks2 *ks3 +ks3 *x4 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(x3 +2 *ks2 +ks2 *ks3 ),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =x0 \n    tmp38 =1 +ks3 \n    tmp39 =tmp37 >=tmp38 \n    tmp40 =tl .load (in_ptr0 +(1 +2 *x4 +ks3 *x4 ),tmp39 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tl .where (tmp39 ,tmp40 ,tmp41 )\n    tmp43 =tl .where (tmp27 ,tmp36 ,tmp42 )\n    tmp44 =tl .where (tmp2 ,tmp25 ,tmp43 )\n    tl .store (out_ptr0 +(x3 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks6 \n    x4 =xindex //ks0 \n    x3 =xindex \n    tmp36 =tl .load (in_ptr1 +(1 +x0 +4 *x4 +ks7 *x4 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =5 +2 *ks2 +2 *ks3 +ks2 *ks3 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =((((-1 )+x1 )//ks4 )%ks5 )\n    tmp7 =tl .broadcast_to (1 +ks2 ,[XBLOCK ])\n    tmp8 =tmp6 >=tmp7 \n    tmp9 =tmp8 &tmp5 \n    tmp10 =((-1 )*ks2 )+(((((-1 )+x1 )//ks4 )%ks5 ))\n    tmp11 =tl .full ([1 ],1 ,tl .int64 )\n    tmp12 =tmp10 <tmp11 \n    tmp13 =tmp12 &tmp9 \n    tmp14 =tl .load (in_ptr0 +(x0 +2 *((((-1 )+x1 )%ks4 ))+4 *ks2 +8 *x2 +ks7 *((((-1 )+x1 )%ks4 ))+2 *ks2 *ks3 +2 *ks2 *ks7 +4 *ks2 *x2 +4 *ks3 *x2 +4 *ks7 *x2 +ks2 *ks3 *ks7 +2 *ks2 *ks3 *x2 +2 *ks2 *ks7 *x2 +2 *ks3 *ks7 *x2 +ks2 *ks3 *ks7 *x2 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =tl .load (in_ptr0 +(x0 +((-4 )*ks2 )+2 *((((-1 )+x1 )%ks4 ))+4 *(((((-1 )+x1 )//ks4 )%ks5 ))+8 *x2 +ks7 *((((-1 )+x1 )%ks4 ))+((-2 )*ks2 *ks3 )+((-2 )*ks2 *ks7 )+2 *ks3 *(((((-1 )+x1 )//ks4 )%ks5 ))+2 *ks7 *(((((-1 )+x1 )//ks4 )%ks5 ))+4 *ks2 *x2 +4 *ks3 *x2 +4 *ks7 *x2 +ks3 *ks7 *(((((-1 )+x1 )//ks4 )%ks5 ))+((-1 )*ks2 *ks3 *ks7 )+2 *ks2 *ks3 *x2 +2 *ks2 *ks7 *x2 +2 *ks3 *ks7 *x2 +ks2 *ks3 *ks7 *x2 ),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp16 =tl .where (tmp12 ,tmp14 ,tmp15 )\n    tmp17 =tl .full (tmp16 .shape ,0.0 ,tmp16 .dtype )\n    tmp18 =tl .where (tmp9 ,tmp16 ,tmp17 )\n    tmp19 =tl .full ([1 ],1 ,tl .int64 )\n    tmp20 =tmp6 <tmp19 \n    tmp21 =tmp20 &tmp5 \n    tmp22 =tl .load (in_ptr0 +(x0 +2 *((((-1 )+x1 )%ks4 ))+4 *ks2 +8 *x2 +ks7 *((((-1 )+x1 )%ks4 ))+2 *ks2 *ks3 +2 *ks2 *ks7 +4 *ks2 *x2 +4 *ks3 *x2 +4 *ks7 *x2 +ks2 *ks3 *ks7 +2 *ks2 *ks3 *x2 +2 *ks2 *ks7 *x2 +2 *ks3 *ks7 *x2 +ks2 *ks3 *ks7 *x2 ),tmp21 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp23 =tl .load (in_ptr0 +(x0 +2 *((((-1 )+x1 )%ks4 ))+4 *(((((-1 )+x1 )//ks4 )%ks5 ))+8 *x2 +ks7 *((((-1 )+x1 )%ks4 ))+2 *ks3 *(((((-1 )+x1 )//ks4 )%ks5 ))+2 *ks7 *(((((-1 )+x1 )//ks4 )%ks5 ))+4 *ks2 *x2 +4 *ks3 *x2 +4 *ks7 *x2 +ks3 *ks7 *(((((-1 )+x1 )//ks4 )%ks5 ))+2 *ks2 *ks3 *x2 +2 *ks2 *ks7 *x2 +2 *ks3 *ks7 *x2 +ks2 *ks3 *ks7 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp24 =tl .where (tmp20 ,tmp22 ,tmp23 )\n    tmp25 =tl .where (tmp8 ,tmp18 ,tmp24 )\n    tmp26 =1.0 \n    tmp27 =tmp25 *tmp26 \n    tmp28 =20.0 \n    tmp29 =tmp27 >tmp28 \n    tmp30 =tl_math .exp (tmp27 )\n    tmp31 =libdevice .log1p (tmp30 )\n    tmp32 =tmp31 *tmp26 \n    tmp33 =tl .where (tmp29 ,tmp25 ,tmp32 )\n    tmp34 =tl .full (tmp33 .shape ,0.0 ,tmp33 .dtype )\n    tmp35 =tl .where (tmp5 ,tmp33 ,tmp34 )\n    tmp37 =tl .where (tmp5 ,tmp35 ,tmp36 )\n    tl .store (out_ptr0 +(x3 ),tmp37 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =x0 \n    tmp1 =3 +ks1 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(-2 )+x0 +((-1 )*ks1 )\n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .full ([1 ],1 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tl .broadcast_to (3 +ks1 ,[XBLOCK ])\n    tmp11 =tmp7 <tmp10 \n    tmp12 =tmp9 &tmp11 \n    tmp13 =tmp12 &tmp6 \n    tmp14 =tl .load (in_ptr0 +((-1 )+x0 +2 *x1 +ks1 *x1 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =float (\"nan\")\n    tmp16 =tl .where (tmp12 ,tmp14 ,tmp15 )\n    tmp17 =tl .full (tmp16 .shape ,0.0 ,tmp16 .dtype )\n    tmp18 =tl .where (tmp6 ,tmp16 ,tmp17 )\n    tmp19 =tmp3 >=tmp4 \n    tmp20 =tl .broadcast_to (3 +ks1 ,[XBLOCK ])\n    tmp21 =tmp3 <tmp20 \n    tmp22 =tmp19 &tmp21 \n    tmp23 =tmp22 &tmp2 \n    tmp24 =tl .load (in_ptr0 +((-3 )+x0 +((-1 )*ks1 )+2 *x1 +ks1 *x1 ),tmp23 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp25 =float (\"nan\")\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tl .where (tmp5 ,tmp18 ,tmp26 )\n    tmp28 =tl .full (tmp27 .shape ,0.0 ,tmp27 .dtype )\n    tmp29 =tl .where (tmp2 ,tmp27 ,tmp28 )\n    tmp30 =tl .full ([1 ],1 ,tl .int64 )\n    tmp31 =tmp0 <tmp30 \n    tmp32 =2 +ks1 +x0 \n    tmp33 =tl .full ([1 ],1 ,tl .int64 )\n    tmp34 =tmp32 >=tmp33 \n    tmp35 =tl .broadcast_to (3 +ks1 ,[XBLOCK ])\n    tmp36 =tmp32 <tmp35 \n    tmp37 =tmp34 &tmp36 \n    tmp38 =tmp37 &tmp31 \n    tmp39 =tl .load (in_ptr0 +(1 +ks1 +x0 +2 *x1 +ks1 *x1 ),tmp38 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp40 =float (\"nan\")\n    tmp41 =tl .where (tmp37 ,tmp39 ,tmp40 )\n    tmp42 =tl .full (tmp41 .shape ,0.0 ,tmp41 .dtype )\n    tmp43 =tl .where (tmp31 ,tmp41 ,tmp42 )\n    tmp44 =tmp0 >=tmp30 \n    tmp45 =tmp0 <tmp1 \n    tmp46 =tmp44 &tmp45 \n    tmp47 =tl .load (in_ptr0 +((-1 )+x0 +2 *x1 +ks1 *x1 ),tmp46 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp48 =float (\"nan\")\n    tmp49 =tl .where (tmp46 ,tmp47 ,tmp48 )\n    tmp50 =tl .where (tmp31 ,tmp43 ,tmp49 )\n    tmp51 =tl .where (tmp2 ,tmp29 ,tmp50 )\n    tl .store (out_ptr0 +(x2 ),tmp51 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_sigmoid_view_4 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks0 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks4 \n    x3 =xindex \n    tmp15 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =5 +2 *ks2 +2 *ks3 +ks2 *ks3 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =(-4 )+x1 +((-2 )*ks2 )+((-2 )*ks3 )+((-1 )*ks2 *ks3 )\n    tmp4 =tl .full ([1 ],1 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =tl .load (in_ptr0 +(16 +x0 +4 *ks5 +8 *ks2 +8 *ks3 +24 *x2 +2 *ks2 *ks5 +2 *ks3 *ks5 +4 *ks2 *ks3 +6 *ks5 *x2 +8 *ks2 *x2 +8 *ks3 *x2 +ks2 *ks3 *ks5 +2 *ks2 *ks5 *x2 +2 *ks3 *ks5 *x2 +4 *ks2 *ks3 *x2 +ks2 *ks3 *ks5 *x2 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =tl .load (in_ptr0 +((-16 )+x3 +((-8 )*ks2 )+((-8 )*ks3 )+((-4 )*ks5 )+((-4 )*ks2 *ks3 )+((-2 )*ks2 *ks5 )+((-2 )*ks3 *ks5 )+((-1 )*ks2 *ks3 *ks5 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tmp10 =tl .full (tmp9 .shape ,0.0 ,tmp9 .dtype )\n    tmp11 =tl .where (tmp2 ,tmp9 ,tmp10 )\n    tmp12 =tl .full ([1 ],1 ,tl .int64 )\n    tmp13 =tmp0 <tmp12 \n    tmp14 =tl .load (in_ptr0 +(16 +x0 +4 *ks5 +8 *ks2 +8 *ks3 +24 *x2 +2 *ks2 *ks5 +2 *ks3 *ks5 +4 *ks2 *ks3 +6 *ks5 *x2 +8 *ks2 *x2 +8 *ks3 *x2 +ks2 *ks3 *ks5 +2 *ks2 *ks5 *x2 +2 *ks3 *ks5 *x2 +4 *ks2 *ks3 *x2 +ks2 *ks3 *ks5 *x2 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp16 =tl .where (tmp13 ,tmp14 ,tmp15 )\n    tmp17 =tl .where (tmp2 ,tmp11 ,tmp16 )\n    tmp18 =tl .sigmoid (tmp17 )\n    tl .store (out_ptr0 +(x3 ),tmp18 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n        2 +s3 \n        2 +s2 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        2 +s1 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf1 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_0_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (triton_poi_fused_copy_0_xnumel )](arg4_1 ,buf0 ,buf1 ,66 ,64 ,66 ,64 ,4356 ,66 ,64 ,287496 ,862488 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg4_1 \n        buf2 =buf0 ;del buf0 \n\n        triton_poi_fused_1_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_1 [grid (triton_poi_fused_1_xnumel )](buf1 ,buf2 ,66 ,66 ,64 ,64 ,862488 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf1 \n        buf3 =empty_strided_cuda ((1 ,s0 ,6 +2 *s1 +2 *s2 +s1 *s2 ,4 +s3 ),(24 *s0 +6 *s0 *s3 +8 *s0 *s1 +8 *s0 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +4 *s0 *s1 *s2 +s0 *s1 *s2 *s3 ,24 +6 *s3 +8 *s1 +8 *s2 +2 *s1 *s3 +2 *s2 *s3 +4 *s1 *s2 +s1 *s2 *s3 ,4 +s3 ,1 ),torch .float32 )\n        6 +2 *s1 +2 *s2 +s1 *s2 \n        12 +4 *s1 +4 *s2 +6 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf4 =empty_strided_cuda ((1 ,s0 ,6 +2 *s1 +2 *s2 +s1 *s2 ,2 +s3 ),(12 *s0 +4 *s0 *s1 +4 *s0 *s2 +6 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,12 +4 *s1 +4 *s2 +6 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_2_xnumel =12 *s0 +4 *s0 *s1 +4 *s0 *s2 +6 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_2 [grid (triton_poi_fused_copy_2_xnumel )](buf2 ,buf3 ,buf4 ,66 ,4358 ,64 ,64 ,66 ,66 ,287628 ,64 ,862884 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf2 \n        4 +s3 \n        buf5 =buf3 ;del buf3 \n\n        triton_poi_fused_3_xnumel =24 *s0 +6 *s0 *s3 +8 *s0 *s1 +8 *s0 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +4 *s0 *s1 *s2 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_3 [grid (triton_poi_fused_3_xnumel )](buf4 ,buf5 ,68 ,64 ,889032 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf4 \n        24 +6 *s3 +8 *s1 +8 *s2 +2 *s1 *s3 +2 *s2 *s3 +4 *s1 *s2 +s1 *s2 *s3 \n        buf6 =empty_strided_cuda ((1 ,s0 ,6 +2 *s1 +2 *s2 +s1 *s2 ,4 +s3 ),(24 *s0 +6 *s0 *s3 +8 *s0 *s1 +8 *s0 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +4 *s0 *s1 *s2 +s0 *s1 *s2 *s3 ,24 +6 *s3 +8 *s1 +8 *s2 +2 *s1 *s3 +2 *s2 *s3 +4 *s1 *s2 +s1 *s2 *s3 ,4 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_sigmoid_view_4_xnumel =24 *s0 +6 *s0 *s3 +8 *s0 *s1 +8 *s0 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +4 *s0 *s1 *s2 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_sigmoid_view_4 [grid (triton_poi_fused_sigmoid_view_4_xnumel )](buf5 ,buf6 ,4358 ,68 ,64 ,64 ,296344 ,64 ,889032 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf5 \n    return (buf6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =64 \n    arg4_1 =rand_strided ((1 ,3 ,64 ,64 ,64 ),(786432 ,262144 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ae26c287-36a8-4ffd-b00f-ca981b053d9c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxUnpool3d', 'LSTM', 'LazyConvTranspose3d', 'HingeEmbeddingLoss', 'ModuleList']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.max_unpool3d = nn.MaxUnpool3d(kernel_size=2, stride=2)\n        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.lazy_conv_transpose3d = nn.LazyConvTranspose3d(out_channels=32, kernel_size=3, stride=1)\n        self.hinge_embedding_loss = nn.HingeEmbeddingLoss()\n        self.module_list = nn.ModuleList([\n            nn.LazyConvTranspose3d(out_channels=16, kernel_size=3, stride=1),\n            nn.LazyConvTranspose3d(out_channels=8, kernel_size=3, stride=1)\n        ])\n\n    def forward(self, x):\n        # Assuming x is a 5D tensor (batch, channels, depth, height, width)\n        # Apply MaxUnpool3d\n        x, indices = F.max_pool3d_with_indices(x, kernel_size=2, stride=2)\n        x = self.max_unpool3d(x, indices)\n        \n        # Reshape for LSTM\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size, channels * depth, height * width).transpose(1, 2)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Reshape back to 5D tensor\n        x = x.transpose(1, 2).view(batch_size, -1, depth, height, width)\n        \n        # Apply LazyConvTranspose3d\n        x = self.lazy_conv_transpose3d(x)\n        \n        # Apply ModuleList layers\n        for layer in self.module_list:\n            x = layer(x)\n        \n        # Compute HingeEmbeddingLoss (assuming a dummy target for demonstration)\n        target = torch.ones_like(x)\n        loss = self.hinge_embedding_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool3d_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp8 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp1 =8 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )*(triton_helpers .div_floor_integer (x0 ,(ks0 //2 )*(ks1 //2 )*(ks2 //2 )))\n    tmp2 =tmp0 +tmp1 \n    tmp3 =8 *ks3 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )\n    tmp4 =tmp2 +tmp3 \n    tmp5 =tmp2 <0 \n    tmp6 =tl .where (tmp5 ,tmp4 ,tmp2 )\n    tl .device_assert (((0 <=tmp6 )&(tmp6 <8 *ks3 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 )))|~(xmask ),\"index out of bounds: 0 <= tmp6 < 8*ks3*(ks0 // 2)*(ks1 // 2)*(ks2 // 2)\")\n    tl .store (out_ptr0 +(tl .broadcast_to ((tmp6 %(8 *ks3 *(ks0 //2 )*(ks1 //2 )*(ks2 //2 ))),[XBLOCK ])),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_transpose_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *(ks4 //2 )*((((2 *(ks4 //2 )*(triton_helpers .div_floor_integer (x0 ,2 *(ks4 //2 )))+((x0 %(2 *(ks4 //2 )))))//(2 *(ks4 //2 )))%(2 *(ks3 //2 ))))+4 *(ks3 //2 )*(ks4 //2 )*((((2 *(ks4 //2 )*(triton_helpers .div_floor_integer (x0 ,2 *(ks4 //2 )))+4 *(ks3 //2 )*(ks4 //2 )*((x1 %(2 *(ks2 //2 ))))+((x0 %(2 *(ks4 //2 )))))//(4 *(ks3 //2 )*(ks4 //2 )))%(2 *(ks2 //2 ))))+8 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 )*((((2 *(ks4 //2 )*(triton_helpers .div_floor_integer (x0 ,2 *(ks4 //2 )))+4 *(ks3 //2 )*(ks4 //2 )*((x1 %(2 *(ks2 //2 ))))+8 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 )*(triton_helpers .div_floor_integer (x1 ,2 *(ks2 //2 )))+((x0 %(2 *(ks4 //2 )))))//(8 *(ks2 //2 )*(ks3 //2 )*(ks4 //2 )))%ks1 ))+((((x0 %(2 *(ks4 //2 ))))%(2 *(ks4 //2 ))))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .max_pool3d_with_indices .default (arg4_1 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del arg4_1 \n        buf1 =buf0 [0 ]\n        buf2 =buf0 [1 ]\n        del buf0 \n        buf3 =empty_strided_cuda ((1 ,s0 ,2 *(s1 //2 ),2 *(s2 //2 ),2 *(s3 //2 )),(8 *s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),8 *(s1 //2 )*(s2 //2 )*(s3 //2 ),4 *(s2 //2 )*(s3 //2 ),2 *(s3 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool3d_0_xnumel =8 *s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_0 [grid (triton_poi_fused_max_unpool3d_0_xnumel )](buf3 ,98304 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool3d_1_xnumel =s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool3d_1 [grid (triton_poi_fused_max_unpool3d_1_xnumel )](buf2 ,buf1 ,buf3 ,32 ,32 ,32 ,3 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        del buf2 \n        4 *(s2 //2 )*(s3 //2 )\n        buf5 =empty_strided_cuda ((1 ,4 *(s2 //2 )*(s3 //2 ),2 *s0 *(s1 //2 )),(8 *s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),1 ,4 *(s2 //2 )*(s3 //2 )),torch .float32 )\n\n        triton_poi_fused_transpose_2_xnumel =8 *s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_transpose_2 [grid (triton_poi_fused_transpose_2_xnumel )](buf3 ,buf5 ,1024 ,3 ,32 ,32 ,32 ,98304 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf3 \n    return (buf5 ,2 *(s1 //2 ),2 *(s2 //2 ),2 *(s3 //2 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "aff4f491-e0a2-40e9-ae08-39ba7f8be84d",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Tanhshrink', 'ReLU', 'SiLU', 'AdaptiveAvgPool3d', 'NLLLoss2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.tanhshrink = nn.Tanhshrink()\n        self.relu = nn.ReLU()\n        self.silu = nn.SiLU()\n        self.adaptive_avg_pool3d = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.nll_loss2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Apply Tanhshrink\n        x = self.tanhshrink(x)\n        \n        # Apply ReLU\n        x = self.relu(x)\n        \n        # Apply SiLU\n        x = self.silu(x)\n        \n        # Reshape to 5D tensor for AdaptiveAvgPool3d\n        x = x.view(x.size(0), 1, x.size(1), x.size(2), x.size(3))\n        \n        # Apply AdaptiveAvgPool3d\n        x = self.adaptive_avg_pool3d(x)\n        \n        # Reshape back to 4D tensor for NLLLoss2d\n        x = x.view(x.size(0), x.size(2), x.size(3), x.size(4))\n        \n        # Apply NLLLoss2d (assuming target is provided externally)\n        # Note: NLLLoss2d is typically used in the loss function, not in the forward pass.\n        # For the sake of this example, we will just return the output before NLLLoss2d.\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_mean_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp13 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )\n        tmp1 =ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 ))%(ks0 *ks1 *ks2 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =libdevice .tanh (tmp3 )\n        tmp5 =tmp3 -tmp4 \n        tmp6 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp7 =triton_helpers .maximum (tmp6 ,tmp5 )\n        tmp8 =tl .sigmoid (tmp7 )\n        tmp9 =tmp7 *tmp8 \n        tmp10 =tl .full (tmp9 .shape ,0 ,tmp9 .dtype )\n        tmp11 =tl .where (tmp2 ,tmp9 ,tmp10 )\n        tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n        tmp14 =_tmp13 +tmp12 \n        _tmp13 =tl .where (r0_mask &xmask ,tmp14 ,_tmp13 )\n    tmp13 =tl .sum (_tmp13 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_mean_1 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =ks0 *ks1 *ks2 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp6 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,1 ,1 ,1 ,2 ),(2 ,2 ,2 ,2 ,2 ,1 ),torch .float32 )\n\n        (1 +s0 *s1 *s2 )//2 \n        get_raw_stream (0 )\n        triton_red_fused_mean_0 [grid (2 )](arg3_1 ,buf0 ,3 ,64 ,64 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((1 ,1 ,1 ,1 ,1 ),(1 ,1 ,1 ,1 ,1 ),torch .float32 )\n        buf2 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_per_fused_mean_1 [grid (1 )](buf2 ,buf0 ,3 ,64 ,64 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n    return (reinterpret_tensor (buf2 ,(1 ,1 ,1 ,1 ),(1 ,1 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "b0de45a3-b768-4ef8-9169-d6f8d4440d6a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['L1Loss', 'GELU', 'ParameterDict', 'Unfold', 'GRU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.unfold = nn.Unfold(kernel_size=(3, 3), padding=1)\n        self.gru = nn.GRU(input_size=9, hidden_size=16, num_layers=2, batch_first=True)\n        self.gelu = nn.GELU()\n        self.parameter_dict = nn.ParameterDict({\n            'weight': nn.Parameter(torch.randn(16, 10)),\n            'bias': nn.Parameter(torch.randn(10))\n        })\n        self.l1_loss = nn.L1Loss()\n\n    def forward(self, x):\n        # Unfold the input to create patches\n        batch_size, channels, height, width = x.shape\n        x = self.unfold(x)  # Shape: (batch_size, channels * kernel_size * kernel_size, num_patches)\n        x = x.view(batch_size, -1, 9)  # Reshape to (batch_size, num_patches, 9)\n        \n        # Pass through GRU\n        x, _ = self.gru(x)  # Shape: (batch_size, num_patches, hidden_size)\n        \n        # Apply GELU activation\n        x = self.gelu(x)  # Shape: (batch_size, num_patches, hidden_size)\n        \n        # Linear transformation using ParameterDict\n        weight = self.parameter_dict['weight']\n        bias = self.parameter_dict['bias']\n        x = torch.matmul(x, weight) + bias  # Shape: (batch_size, num_patches, 10)\n        \n        # Compute L1 loss with a dummy target (for demonstration purposes)\n        dummy_target = torch.zeros_like(x)\n        loss = self.l1_loss(x, dummy_target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_im2col_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    yoffset =(tl .program_id (1 )+tl .program_id (2 )*tl .num_programs (1 ))*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x4 =xindex //ks0 \n    y1 =((yindex //3 )%3 )\n    x3 =(xindex %ks0 )\n    y0 =(yindex %3 )\n    x6 =xindex \n    y2 =yindex //9 \n    y7 =yindex \n    tl .device_assert ((x4 +y1 <2 +ks1 )|~(xmask &ymask ),\"index out of bounds: x4 + y1 < 2 + ks1\")\n    tl .device_assert ((x3 +y0 <2 +ks0 )|~(xmask &ymask ),\"index out of bounds: x3 + y0 < 2 + ks0\")\n    tmp2 =(-1 )+x4 +y1 \n    tmp3 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp4 =tmp2 >=tmp3 \n    tmp5 =ks1 \n    tmp6 =tmp2 <tmp5 \n    tmp7 =(-1 )+x3 +y0 \n    tmp8 =tmp7 >=tmp3 \n    tmp9 =ks0 \n    tmp10 =tmp7 <tmp9 \n    tmp11 =tmp4 &tmp6 \n    tmp12 =tmp11 &tmp8 \n    tmp13 =tmp12 &tmp10 \n    tmp14 =tl .load (in_ptr0 +((-1 )+x6 +y0 +((-1 )*ks0 )+ks0 *y1 +ks0 *ks1 *y2 ),tmp13 &xmask &ymask ,eviction_policy ='evict_last',other =0.0 )\n    tl .store (out_ptr0 +(x6 +ks0 *ks1 *y7 ),tmp14 ,xmask &ymask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %9 )\n    x1 =xindex //9 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks1 *ks2 *((((((x0 +9 *x1 )//(ks1 *ks2 ))%(9 *ks0 )))%(9 *ks0 )))+(((((x0 +9 *x1 )%(ks1 *ks2 )))%(ks1 *ks2 )))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,3 ,3 ,s1 ,s2 ),(9 *s0 *s1 *s2 ,9 *s1 *s2 ,3 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_im2col_0_ynumel =9 *s0 \n        triton_poi_fused_im2col_0_xnumel =s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_im2col_0 [grid (triton_poi_fused_im2col_0_ynumel ,triton_poi_fused_im2col_0_xnumel )](arg3_1 ,buf0 ,32 ,32 ,27 ,1024 ,XBLOCK =256 ,YBLOCK =1 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((1 ,s0 *s1 *s2 ,9 ),(9 *s0 *s1 *s2 ,9 ,1 ),torch .float32 )\n\n        triton_poi_fused_view_1_xnumel =9 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_view_1 [grid (triton_poi_fused_view_1_xnumel )](buf0 ,buf1 ,3 ,32 ,32 ,27648 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "b10e6607-9483-4ef1-a135-c9e9c0b1cf8a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CELU', 'NLLLoss', 'Softplus', 'Hardshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.celu1 = nn.CELU()\n        self.celu2 = nn.CELU()\n        self.softplus1 = nn.Softplus()\n        self.softplus2 = nn.Softplus()\n        self.hardshrink1 = nn.Hardshrink()\n        self.hardshrink2 = nn.Hardshrink()\n        self.nllloss = nn.NLLLoss()\n\n    def forward(self, x):\n        # Apply CELU activation\n        x = self.celu1(x)\n        x = self.celu2(x)\n        \n        # Apply Softplus activation\n        x = self.softplus1(x)\n        x = self.softplus2(x)\n        \n        # Apply Hardshrink activation\n        x = self.hardshrink1(x)\n        x = self.hardshrink2(x)\n        \n        # Flatten the tensor for NLLLoss\n        x = x.view(x.size(0), -1)\n        \n        # Apply log_softmax for NLLLoss compatibility\n        x = F.log_softmax(x, dim=1)\n        \n        # Assuming a target tensor for NLLLoss (this is just a placeholder)\n        target = torch.zeros(x.size(0), dtype=torch.long).to(x.device)\n        \n        # Compute NLLLoss\n        loss = self.nllloss(x, target)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp35 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )\n        tmp1 =ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 ))%(ks0 *ks1 *ks2 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =0.0 \n        tmp5 =tmp3 >tmp4 \n        tmp6 =libdevice .expm1 (tmp3 )\n        tmp7 =tl .where (tmp5 ,tmp3 ,tmp6 )\n        tmp8 =tmp7 >tmp4 \n        tmp9 =libdevice .expm1 (tmp7 )\n        tmp10 =tl .where (tmp8 ,tmp7 ,tmp9 )\n        tmp11 =1.0 \n        tmp12 =tmp10 *tmp11 \n        tmp13 =20.0 \n        tmp14 =tmp12 >tmp13 \n        tmp15 =tl_math .exp (tmp12 )\n        tmp16 =libdevice .log1p (tmp15 )\n        tmp17 =tmp16 *tmp11 \n        tmp18 =tl .where (tmp14 ,tmp10 ,tmp17 )\n        tmp19 =tmp18 *tmp11 \n        tmp20 =tmp19 >tmp13 \n        tmp21 =tl_math .exp (tmp19 )\n        tmp22 =libdevice .log1p (tmp21 )\n        tmp23 =tmp22 *tmp11 \n        tmp24 =tl .where (tmp20 ,tmp18 ,tmp23 )\n        tmp25 =tl_math .abs (tmp24 )\n        tmp26 =0.5 \n        tmp27 =tmp25 <=tmp26 \n        tmp28 =tl .where (tmp27 ,tmp4 ,tmp24 )\n        tmp29 =tl_math .abs (tmp28 )\n        tmp30 =tmp29 <=tmp26 \n        tmp31 =tl .where (tmp30 ,tmp4 ,tmp28 )\n        tmp32 =tl .full (tmp31 .shape ,float (\"-inf\"),tmp31 .dtype )\n        tmp33 =tl .where (tmp2 ,tmp31 ,tmp32 )\n        tmp34 =tl .broadcast_to (tmp33 ,[XBLOCK ,R0_BLOCK ])\n        tmp36 =triton_helpers .maximum (_tmp35 ,tmp34 )\n        _tmp35 =tl .where (r0_mask &xmask ,tmp36 ,_tmp35 )\n    tmp35 =triton_helpers .max2 (_tmp35 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp35 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__log_softmax_1 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =triton_helpers .max2 (tmp1 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp3 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__log_softmax_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp29 =tl .load (in_ptr1 +(0 ))\n    tmp30 =tl .broadcast_to (tmp29 ,[XBLOCK ])\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =libdevice .expm1 (tmp0 )\n    tmp4 =tl .where (tmp2 ,tmp0 ,tmp3 )\n    tmp5 =tmp4 >tmp1 \n    tmp6 =libdevice .expm1 (tmp4 )\n    tmp7 =tl .where (tmp5 ,tmp4 ,tmp6 )\n    tmp8 =1.0 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =20.0 \n    tmp11 =tmp9 >tmp10 \n    tmp12 =tl_math .exp (tmp9 )\n    tmp13 =libdevice .log1p (tmp12 )\n    tmp14 =tmp13 *tmp8 \n    tmp15 =tl .where (tmp11 ,tmp7 ,tmp14 )\n    tmp16 =tmp15 *tmp8 \n    tmp17 =tmp16 >tmp10 \n    tmp18 =tl_math .exp (tmp16 )\n    tmp19 =libdevice .log1p (tmp18 )\n    tmp20 =tmp19 *tmp8 \n    tmp21 =tl .where (tmp17 ,tmp15 ,tmp20 )\n    tmp22 =tl_math .abs (tmp21 )\n    tmp23 =0.5 \n    tmp24 =tmp22 <=tmp23 \n    tmp25 =tl .where (tmp24 ,tmp1 ,tmp21 )\n    tmp26 =tl_math .abs (tmp25 )\n    tmp27 =tmp26 <=tmp23 \n    tmp28 =tl .where (tmp27 ,tmp1 ,tmp25 )\n    tmp31 =tmp28 -tmp30 \n    tl .store (out_ptr0 +(x0 ),tmp31 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )\n        tmp1 =ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(r0_1 +x0 *((1 +ks0 *ks1 *ks2 )//2 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl_math .exp (tmp3 )\n        tmp5 =tl .full (tmp4 .shape ,0 ,tmp4 .dtype )\n        tmp6 =tl .where (tmp2 ,tmp4 ,tmp5 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =_tmp8 +tmp7 \n        _tmp8 =tl .where (r0_mask &xmask ,tmp9 ,_tmp8 )\n    tmp8 =tl .sum (_tmp8 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__log_softmax_nll_loss_forward_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp4 =tl .load (in_ptr1 +(0 ))\n    tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,1 ])\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp6 =tl_math .log (tmp3 )\n    tmp7 =tmp5 -tmp6 \n    tmp8 =-tmp7 \n    tmp9 =tl .full ([1 ,1 ],True ,tl .int1 )\n    tmp10 =0.0 \n    tmp11 =tl .where (tmp9 ,tmp8 ,tmp10 )\n    tmp12 =1.0 \n    tmp13 =tmp11 /tmp12 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp13 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,2 ),(2 ,2 ,1 ),torch .float32 )\n\n        (1 +s0 *s1 *s2 )//2 \n        get_raw_stream (0 )\n        triton_red_fused__log_softmax_0 [grid (2 )](arg3_1 ,buf0 ,3 ,64 ,64 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__log_softmax_1 [grid (1 )](buf0 ,buf1 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,s0 *s1 *s2 ),(s0 *s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__log_softmax_2_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__log_softmax_2 [grid (triton_poi_fused__log_softmax_2_xnumel )](arg3_1 ,buf1 ,buf2 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf3 =buf0 ;del buf0 \n\n        (1 +s0 *s1 *s2 )//2 \n        get_raw_stream (0 )\n        triton_red_fused__log_softmax_3 [grid (2 )](buf2 ,buf3 ,3 ,64 ,64 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf4 =buf1 ;del buf1 \n        buf5 =reinterpret_tensor (buf4 ,(),(),0 );del buf4 \n\n        get_raw_stream (0 )\n        triton_per_fused__log_softmax_nll_loss_forward_4 [grid (1 )](buf5 ,buf3 ,buf2 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf2 \n        del buf3 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "b1b4429f-8d3d-40b8-8bcd-92a57ac9d0b6",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveAvgPool1d', 'LSTM', 'MultiLabelSoftMarginLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool1d(output_size=128)\n        self.lstm1 = nn.LSTM(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n        self.lstm2 = nn.LSTM(input_size=64, hidden_size=32, num_layers=2, batch_first=True)\n        self.lstm3 = nn.LSTM(input_size=32, hidden_size=16, num_layers=2, batch_first=True)\n        self.lstm4 = nn.LSTM(input_size=16, hidden_size=8, num_layers=2, batch_first=True)\n        self.lstm5 = nn.LSTM(input_size=8, hidden_size=4, num_layers=2, batch_first=True)\n        self.loss = nn.MultiLabelSoftMarginLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, sequence_length, features)\n        x = x.permute(0, 2, 1)  # Reshape to (batch_size, features, sequence_length)\n        x = self.adaptive_avg_pool(x)\n        x = x.permute(0, 2, 1)  # Reshape back to (batch_size, sequence_length, features)\n        \n        x, _ = self.lstm1(x)\n        x, _ = self.lstm2(x)\n        x, _ = self.lstm3(x)\n        x, _ = self.lstm4(x)\n        x, _ = self.lstm5(x)\n        \n        # Assuming the output is of shape (batch_size, sequence_length, 4)\n        # We can apply the loss function if we have target labels\n        # For demonstration, let's assume the target is a random tensor of the same shape\n        target = torch.randint(0, 2, x.shape).float()  # Random binary target\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 100, 256).cuda()  # Example input shape (batch_size=1, sequence_length=100, features=256)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %128 )\n    x1 =xindex //128 \n    x2 =xindex \n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =(25 *x0 )//32 \n    tmp4 =(227 +100 *x0 )//128 \n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp2 &tmp5 \n    tmp7 =tl .load (in_ptr0 +(x1 +ks0 *((25 *x0 )//32 )),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =1 +((25 *x0 )//32 )\n    tmp9 =tmp8 <tmp4 \n    tmp10 =tmp2 &tmp9 \n    tmp11 =tl .load (in_ptr0 +(ks0 +x1 +ks0 *((25 *x0 )//32 )),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tmp11 +tmp7 \n    tmp13 =1.0 \n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =1.0 \n    tmp17 =tl .full (tmp16 .shape ,0.0 ,tmp16 .dtype )\n    tmp18 =tl .where (tmp10 ,tmp16 ,tmp17 )\n    tmp19 =tmp18 +tmp15 \n    tmp20 =tmp12 /tmp19 \n    tl .store (out_ptr0 +(x2 ),tmp20 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_permute_1 (in_ptr0 ,out_ptr0 ,ks0 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    ynumel =128 \n    yoffset =tl .program_id (1 )*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x1 =xindex \n    y0 =yindex \n    tmp0 =tl .load (in_ptr0 +(y0 +128 *x1 ),xmask &ymask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x1 +ks0 *y0 ),tmp0 ,xmask &ymask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,100 ,s1 ),(100 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s1 ,1 ,128 ),(128 *s1 ,128 ,128 ,1 ),torch .float32 )\n\n        triton_poi_fused__adaptive_avg_pool2d_0_xnumel =128 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_0 [grid (triton_poi_fused__adaptive_avg_pool2d_0_xnumel )](arg2_1 ,buf0 ,256 ,32768 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        buf1 =empty_strided_cuda ((1 ,128 ,s1 ),(128 *s1 ,s1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_permute_1 [grid (128 ,s1 )](buf0 ,buf1 ,256 ,128 ,256 ,XBLOCK =256 ,YBLOCK =1 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n    return (buf1 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =100 \n    arg1_1 =256 \n    arg2_1 =rand_strided ((1 ,100 ,256 ),(25600 ,256 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "b1b8775a-7c50-4a9a-8479-df76cefbcdca",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardshrink', 'FeatureAlphaDropout', 'ReplicationPad3d', 'CTCLoss', 'InstanceNorm1d', 'L1Loss', 'AvgPool3d', 'Softsign']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardshrink = nn.Hardshrink()\n        self.feature_alpha_dropout = nn.FeatureAlphaDropout()\n        self.replication_pad3d = nn.ReplicationPad3d(padding=1)\n        self.instance_norm1d = nn.InstanceNorm1d(num_features=10)\n        self.avg_pool3d = nn.AvgPool3d(kernel_size=2)\n        self.softsign = nn.Softsign()\n        self.ctc_loss = nn.CTCLoss()\n        self.l1_loss = nn.L1Loss()\n\n    def forward(self, x):\n        # Apply Hardshrink\n        x = self.hardshrink(x)\n        \n        # Apply FeatureAlphaDropout\n        x = self.feature_alpha_dropout(x)\n        \n        # Reshape for ReplicationPad3d\n        x = x.view(-1, 1, x.shape[1], x.shape[2], x.shape[3])\n        x = self.replication_pad3d(x)\n        \n        # Reshape for InstanceNorm1d\n        x = x.view(x.shape[0], x.shape[1], -1)\n        x = self.instance_norm1d(x)\n        \n        # Reshape for AvgPool3d\n        x = x.view(x.shape[0], x.shape[1], x.shape[2], 1, 1)\n        x = self.avg_pool3d(x)\n        \n        # Apply Softsign\n        x = self.softsign(x)\n        \n        # Compute CTC Loss (dummy target for demonstration)\n        input_lengths = torch.full((x.shape[0],), x.shape[1], dtype=torch.long)\n        target_lengths = torch.randint(1, x.shape[1], (x.shape[0],), dtype=torch.long)\n        target = torch.randint(0, 10, (x.shape[0], target_lengths.max()), dtype=torch.long)\n        ctc_loss = self.ctc_loss(x, target, input_lengths, target_lengths)\n        \n        # Compute L1 Loss (dummy target for demonstration)\n        l1_loss = self.l1_loss(x, torch.zeros_like(x))\n        \n        # Return the sum of losses for demonstration purposes\n        return ctc_loss + l1_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_replication_pad3d_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x5 =xindex //ks6 \n    x6 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks5 *(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+x1 ))+((-1 )+x1 )*(((-1 )+x1 )>(0 ))))<((-1 )+ks4 )))+ks4 *ks5 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 )))))+(((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))*((((0 )*((0 )>=((-1 )+x2 ))+((-1 )+x2 )*(((-1 )+x2 )>(0 ))))<((-1 )+ks3 )))+(((-1 )+ks5 )*(((-1 )+ks5 )<=(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))<((-1 )+ks5 )))),xmask ,eviction_policy ='evict_last')\n    tmp6 =tl .load (in_ptr1 +((((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+x5 ))+((-1 )+x5 )*(((-1 )+x5 )>(0 )))))+(((0 )*((0 )>=((-1 )+x5 ))+((-1 )+x5 )*(((-1 )+x5 )>(0 ))))*((((0 )*((0 )>=((-1 )+x5 ))+((-1 )+x5 )*(((-1 )+x5 )>(0 ))))<((-1 )+ks3 )))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =0.5 \n    tmp3 =tmp1 <=tmp2 \n    tmp4 =0.0 \n    tmp5 =tl .where (tmp3 ,tmp4 ,tmp0 )\n    tmp7 =tmp6 <tmp2 \n    tmp8 =tmp7 .to (tl .float32 )\n    tmp9 =0.8864048946659319 \n    tmp10 =tmp8 *tmp9 \n    tmp11 =tmp5 *tmp10 \n    tmp12 =-1.0 \n    tmp13 =tmp8 +tmp12 \n    tmp14 =1.558387861036063 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =0.7791939305180315 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =tmp11 +tmp17 \n    tl .store (out_ptr0 +(x6 ),tmp18 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,0 ,10 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        2 +s2 \n        2 +s1 \n        4 +2 *s1 +2 *s2 +s1 *s2 \n        4 +2 *s1 +2 *s2 +s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,1 ,2 +s0 ,2 +s1 ,2 +s2 ),(8 +4 *s0 +4 *s1 +4 *s2 +2 *s0 *s1 +2 *s0 *s2 +2 *s1 *s2 +s0 *s1 *s2 ,8 +4 *s0 +4 *s1 +4 *s2 +2 *s0 *s1 +2 *s0 *s2 +2 *s1 *s2 +s0 *s1 *s2 ,4 +2 *s1 +2 *s2 +s1 *s2 ,2 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_replication_pad3d_1_xnumel =8 +4 *s0 +4 *s1 +4 *s2 +2 *s0 *s1 +2 *s0 *s2 +2 *s1 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_replication_pad3d_1 [grid (triton_poi_fused_replication_pad3d_1_xnumel )](arg3_1 ,buf1 ,buf2 ,34 ,34 ,1156 ,10 ,32 ,32 ,1156 ,13872 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n    return (reinterpret_tensor (buf2 ,(1 ,1 ,8 +4 *s0 +4 *s1 +4 *s2 +2 *s0 *s1 +2 *s0 *s2 +2 *s1 *s2 +s0 *s1 *s2 ),(8 +4 *s0 +4 *s1 +4 *s2 +2 *s0 *s1 +2 *s0 *s2 +2 *s1 *s2 +s0 *s1 *s2 ,8 +4 *s0 +4 *s1 +4 *s2 +2 *s0 *s1 +2 *s0 *s2 +2 *s1 *s2 +s0 *s1 *s2 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "b2454c5d-bdef-42bb-8a1a-840c8eb59e71",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GaussianNLLLoss', 'MaxPool1d', 'ConvTranspose2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool1d = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.convtranspose2d1 = nn.ConvTranspose2d(1, 10, kernel_size=5, stride=2)\n        self.convtranspose2d2 = nn.ConvTranspose2d(10, 20, kernel_size=5, stride=2)\n        self.gaussian_nll_loss = nn.GaussianNLLLoss()\n\n    def forward(self, x):\n        # Assuming input is 1D, reshape to 2D for ConvTranspose2d\n        x = x.view(x.size(0), 1, -1, 1)  # Reshape to (batch_size, 1, sequence_length, 1)\n        \n        # Apply ConvTranspose2d layers\n        x = F.relu(self.convtranspose2d1(x))\n        x = F.relu(self.convtranspose2d2(x))\n        \n        # Reshape back to 1D for MaxPool1d\n        x = x.view(x.size(0), -1)  # Reshape to (batch_size, sequence_length)\n        x = x.unsqueeze(1)  # Add channel dimension for MaxPool1d\n        \n        # Apply MaxPool1d\n        x = self.maxpool1d(x)\n        \n        # Reshape for GaussianNLLLoss (assuming target is the same shape as output)\n        x = x.view(x.size(0), -1)  # Flatten for loss calculation\n        \n        # Dummy target for GaussianNLLLoss (same shape as x)\n        target = torch.randn_like(x)\n        var = torch.ones_like(x)  # Variance for GaussianNLLLoss\n        \n        # Apply GaussianNLLLoss\n        loss = self.gaussian_nll_loss(x, target, var)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 100).cuda()  # Arbitrary 1D input\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_relu_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10150 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //1015 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x2 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_relu_threshold_backward_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =106340 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //5317 \n    x0 =(xindex %5317 )\n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp5 =0.0 \n    tmp6 =tmp4 <=tmp5 \n    tl .store (out_ptr0 +(x0 +5344 *x1 ),tmp4 ,xmask )\n    tl .store (out_ptr1 +(x0 +5376 *x1 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_unsqueeze_2 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =106340 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(5344 *(x0 //5317 )+((x0 %5317 ))),xmask )\n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_3 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =53170 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tl .store (out_ptr0 +(x0 ),tmp5 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_randn_like_4 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =53170 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_ones_like_5 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =53170 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =1.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,100 ),(100 ,1 ))\n    assert_size_stride (primals_2 ,(1 ,10 ,5 ,5 ),(250 ,25 ,5 ,1 ))\n    assert_size_stride (primals_3 ,(10 ,),(1 ,))\n    assert_size_stride (primals_4 ,(10 ,20 ,5 ,5 ),(500 ,25 ,5 ,1 ))\n    assert_size_stride (primals_5 ,(20 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (reinterpret_tensor (primals_1 ,(1 ,1 ,100 ,1 ),(100 ,100 ,1 ,1 ),0 ),primals_2 ,stride =(2 ,2 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =True ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,10 ,203 ,5 ),(10150 ,1015 ,5 ,1 ))\n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_relu_0 [grid (10150 )](buf1 ,primals_3 ,10150 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_3 \n\n        buf2 =extern_kernels .convolution (buf1 ,primals_4 ,stride =(2 ,2 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =True ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf2 ,(1 ,20 ,409 ,13 ),(106340 ,5317 ,13 ,1 ))\n        buf3 =empty_strided_cuda ((1 ,20 ,409 ,13 ),(106880 ,5344 ,13 ,1 ),torch .float32 )\n        buf10 =empty_strided_cuda ((1 ,20 ,409 ,13 ),(107520 ,5376 ,13 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_relu_threshold_backward_1 [grid (106340 )](buf2 ,primals_5 ,buf3 ,buf10 ,106340 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_5 \n        buf4 =reinterpret_tensor (buf2 ,(1 ,1 ,1 ,106340 ),(106368 ,106368 ,106368 ,1 ),0 );del buf2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_unsqueeze_2 [grid (106340 )](buf3 ,buf4 ,106340 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        buf5 =empty_strided_cuda ((1 ,1 ,1 ,53170 ),(53248 ,53248 ,53248 ,1 ),torch .int8 )\n        buf6 =empty_strided_cuda ((1 ,1 ,1 ,53170 ),(53170 ,1 ,53170 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_3 [grid (53170 )](buf4 ,buf5 ,buf6 ,53170 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf7 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf7 )\n        buf8 =empty_strided_cuda ((1 ,53170 ),(53170 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_randn_like_4 [grid (53170 )](buf7 ,buf8 ,0 ,53170 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf7 \n        buf9 =empty_strided_cuda ((1 ,53170 ),(53170 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_ones_like_5 [grid (53170 )](buf9 ,53170 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n    return (reinterpret_tensor (buf6 ,(1 ,53170 ),(53170 ,1 ),0 ),buf8 ,buf9 ,primals_2 ,primals_4 ,reinterpret_tensor (primals_1 ,(1 ,1 ,100 ,1 ),(100 ,100 ,1 ,1 ),0 ),buf1 ,buf4 ,buf5 ,buf10 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,100 ),(100 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((1 ,10 ,5 ,5 ),(250 ,25 ,5 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,20 ,5 ,5 ),(500 ,25 ,5 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "b3a9dc84-e611-4947-aa85-ded63eff1e8d",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LPPool2d', 'TripletMarginLoss', 'PixelShuffle', 'HingeEmbeddingLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lp_pool = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=2)\n        self.triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n        self.hinge_loss = nn.HingeEmbeddingLoss(margin=1.0)\n\n    def forward(self, x):\n        # Apply LPPool2d\n        x = self.lp_pool(x)\n        \n        # Apply PixelShuffle\n        x = self.pixel_shuffle(x)\n        \n        # Generate anchor, positive, and negative samples for TripletMarginLoss\n        anchor = x[:, :, :x.size(2)//2, :x.size(3)//2]\n        positive = x[:, :, :x.size(2)//2, x.size(3)//2:]\n        negative = x[:, :, x.size(2)//2:, :x.size(3)//2]\n        \n        # Compute TripletMarginLoss\n        triplet_loss = self.triplet_loss(anchor, positive, negative)\n        \n        # Generate a target tensor for HingeEmbeddingLoss\n        target = torch.ones_like(anchor.mean(dim=(1, 2, 3)))\n        \n        # Compute HingeEmbeddingLoss\n        hinge_loss = self.hinge_loss(anchor.mean(dim=(1, 2, 3)), target)\n        \n        # Return the sum of the losses as the output\n        return triplet_loss + hinge_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 4, 64, 64).cuda()  # Input shape adjusted for PixelShuffle\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_mean_sub_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_0 =(r0_index %32 )\n    r0_1 =r0_index //32 \n    r0_2 =r0_index \n    tmp0 =tl .load (in_ptr0 +(2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(1 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(64 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr0 +(65 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp26 =tl .load (in_ptr0 +(32 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp28 =tl .load (in_ptr0 +(33 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp31 =tl .load (in_ptr0 +(96 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp34 =tl .load (in_ptr0 +(97 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp50 =tl .load (in_ptr0 +(2048 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp52 =tl .load (in_ptr0 +(2049 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp55 =tl .load (in_ptr0 +(2112 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp58 =tl .load (in_ptr0 +(2113 +2 *(r0_0 //2 )+128 *(r0_1 //2 )+4096 *((r0_0 %2 ))+8192 *((r0_1 %2 ))),None ,eviction_policy ='evict_last')\n    tmp1 =tmp0 *tmp0 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tmp3 +tmp1 \n    tmp6 =tmp5 *tmp5 \n    tmp7 =tmp6 +tmp4 \n    tmp9 =tmp8 *tmp8 \n    tmp10 =tmp9 +tmp7 \n    tmp11 =0.25 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tl .full ([1 ],0 ,tl .int32 )\n    tmp14 =tmp13 <tmp12 \n    tmp15 =tmp14 .to (tl .int8 )\n    tmp16 =tmp12 <tmp13 \n    tmp17 =tmp16 .to (tl .int8 )\n    tmp18 =tmp15 -tmp17 \n    tmp19 =tmp18 .to (tmp12 .dtype )\n    tmp20 =tl_math .abs (tmp12 )\n    tmp21 =triton_helpers .maximum (tmp13 ,tmp20 )\n    tmp22 =tmp19 *tmp21 \n    tmp23 =4.0 \n    tmp24 =tmp22 *tmp23 \n    tmp25 =libdevice .sqrt (tmp24 )\n    tmp27 =tmp26 *tmp26 \n    tmp29 =tmp28 *tmp28 \n    tmp30 =tmp29 +tmp27 \n    tmp32 =tmp31 *tmp31 \n    tmp33 =tmp32 +tmp30 \n    tmp35 =tmp34 *tmp34 \n    tmp36 =tmp35 +tmp33 \n    tmp37 =tmp36 *tmp11 \n    tmp38 =tmp13 <tmp37 \n    tmp39 =tmp38 .to (tl .int8 )\n    tmp40 =tmp37 <tmp13 \n    tmp41 =tmp40 .to (tl .int8 )\n    tmp42 =tmp39 -tmp41 \n    tmp43 =tmp42 .to (tmp37 .dtype )\n    tmp44 =tl_math .abs (tmp37 )\n    tmp45 =triton_helpers .maximum (tmp13 ,tmp44 )\n    tmp46 =tmp43 *tmp45 \n    tmp47 =tmp46 *tmp23 \n    tmp48 =libdevice .sqrt (tmp47 )\n    tmp49 =tmp25 -tmp48 \n    tmp51 =tmp50 *tmp50 \n    tmp53 =tmp52 *tmp52 \n    tmp54 =tmp53 +tmp51 \n    tmp56 =tmp55 *tmp55 \n    tmp57 =tmp56 +tmp54 \n    tmp59 =tmp58 *tmp58 \n    tmp60 =tmp59 +tmp57 \n    tmp61 =tmp60 *tmp11 \n    tmp62 =tmp13 <tmp61 \n    tmp63 =tmp62 .to (tl .int8 )\n    tmp64 =tmp61 <tmp13 \n    tmp65 =tmp64 .to (tl .int8 )\n    tmp66 =tmp63 -tmp65 \n    tmp67 =tmp66 .to (tmp61 .dtype )\n    tmp68 =tl_math .abs (tmp61 )\n    tmp69 =triton_helpers .maximum (tmp13 ,tmp68 )\n    tmp70 =tmp67 *tmp69 \n    tmp71 =tmp70 *tmp23 \n    tmp72 =libdevice .sqrt (tmp71 )\n    tmp73 =tmp25 -tmp72 \n    tmp74 =tl .broadcast_to (tmp25 ,[R0_BLOCK ])\n    tmp76 =triton_helpers .promote_to_tensor (tl .sum (tmp74 ,0 ))\n    tl .store (out_ptr0 +(tl .broadcast_to (r0_2 ,[R0_BLOCK ])),tmp49 ,None )\n    tl .store (out_ptr1 +(tl .broadcast_to (r0_2 ,[R0_BLOCK ])),tmp73 ,None )\n    tl .store (out_ptr2 +(tl .full ([1 ],0 ,tl .int32 )),tmp76 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_norm_1 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    R0_BLOCK :tl .constexpr =32 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +32 *x0 ),xmask ,other =0.0 )\n    tmp1 =1e-06 \n    tmp2 =tmp0 +tmp1 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n    tmp6 =tl .where (xmask ,tmp4 ,0 )\n    tmp7 =tl .sum (tmp6 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_clamp_min_fill_mean_ne_norm_sub_where_zeros_like_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =32 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp4 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp14 =tl .load (in_ptr2 +(0 ))\n    tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,1 ])\n    tmp1 =libdevice .sqrt (tmp0 )\n    tmp2 =1.0 \n    tmp3 =tmp1 +tmp2 \n    tmp5 =libdevice .sqrt (tmp4 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =0.0 \n    tmp8 =triton_helpers .maximum (tmp6 ,tmp7 )\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n    tmp11 =tl .sum (tmp9 ,1 )[:,None ]\n    tmp12 =32.0 \n    tmp13 =tmp11 /tmp12 \n    tmp16 =1024.0 \n    tmp17 =tmp15 /tmp16 \n    tmp18 =tmp2 -tmp17 \n    tmp19 =triton_helpers .maximum (tmp18 ,tmp7 )\n    tmp20 =tl .full ([1 ,1 ],False ,tl .int1 )\n    tmp21 =tl .where (tmp20 ,tmp19 ,tmp7 )\n    tmp22 =tl .full ([1 ,1 ],True ,tl .int1 )\n    tmp23 =tl .where (tmp22 ,tmp17 ,tmp7 )\n    tmp24 =tmp21 +tmp23 \n    tmp25 =tmp24 /tmp2 \n    tmp26 =tmp13 +tmp25 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp26 ,None )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,4 ,64 ,64 ),(16384 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,32 ,32 ),(1024 ,1024 ,32 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,1 ,32 ,32 ),(1024 ,1024 ,32 ,1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_mean_sub_0 [grid (1 )](arg0_1 ,buf0 ,buf2 ,buf5 ,1 ,1024 ,num_warps =8 ,num_stages =1 )\n        del arg0_1 \n        buf1 =empty_strided_cuda ((1 ,1 ,32 ),(32 ,32 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_norm_1 [grid (32 )](buf0 ,buf1 ,32 ,32 ,XBLOCK =32 ,num_warps =8 ,num_stages =1 )\n        del buf0 \n        buf3 =empty_strided_cuda ((1 ,1 ,32 ),(32 ,32 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_norm_1 [grid (32 )](buf2 ,buf3 ,32 ,32 ,XBLOCK =32 ,num_warps =8 ,num_stages =1 )\n        del buf2 \n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf6 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_clamp_min_fill_mean_ne_norm_sub_where_zeros_like_2 [grid (1 )](buf6 ,buf1 ,buf3 ,buf5 ,1 ,32 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n        del buf3 \n        del buf5 \n    return (buf6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,4 ,64 ,64 ),(16384 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "b59a8809-7b64-4022-b34e-6fef245277a4",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LocalResponseNorm', 'AvgPool1d', 'ZeroPad1d', 'RNNBase', 'GLU', 'ReplicationPad3d', 'CrossMapLRN2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.local_response_norm = nn.LocalResponseNorm(size=5)\n        self.avg_pool1d = nn.AvgPool1d(kernel_size=2, stride=2)\n        self.zero_pad1d = nn.ZeroPad1d(padding=2)\n        self.rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.glu = nn.GLU(dim=1)\n        self.replication_pad3d = nn.ReplicationPad3d(padding=1)\n        self.cross_map_lrn2d = nn.CrossMapLRN2d(size=5)\n\n    def forward(self, x):\n        # Apply LocalResponseNorm\n        x = self.local_response_norm(x)\n        \n        # Reshape for AvgPool1d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, sequence_length)\n        x = self.avg_pool1d(x)\n        \n        # Apply ZeroPad1d\n        x = self.zero_pad1d(x)\n        \n        # Reshape for RNN\n        x = x.permute(0, 2, 1)  # Reshape to (batch_size, sequence_length, features)\n        x, _ = self.rnn(x)\n        \n        # Apply GLU\n        x = x.permute(0, 2, 1)  # Reshape back to (batch_size, features, sequence_length)\n        x = self.glu(x)\n        \n        # Reshape for ReplicationPad3d\n        x = x.unsqueeze(1)  # Add a dummy dimension for 3D padding\n        x = self.replication_pad3d(x)\n        \n        # Reshape for CrossMapLRN2d\n        x = x.squeeze(1)  # Remove the dummy dimension\n        x = x.unsqueeze(1)  # Add a channel dimension for 2D LRN\n        x = self.cross_map_lrn2d(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 64).cuda()  # Example input shape: (batch_size, channels, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_avg_pool2d_constant_pad_nd_div_mul_pow_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp48 =tl .load (in_ptr0 +(x2 ),xmask )\n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +(x2 +((-2 )*ks0 )),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tmp6 *tmp6 \n    tmp8 =tl .full (tmp7 .shape ,0.0 ,tmp7 .dtype )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tmp10 =(-1 )+x1 \n    tmp11 =tmp10 >=tmp1 \n    tmp12 =tmp10 <tmp3 \n    tmp13 =tmp11 &tmp12 \n    tmp14 =tl .load (in_ptr0 +(x2 +((-1 )*ks0 )),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =tmp14 *tmp14 \n    tmp16 =tl .full (tmp15 .shape ,0.0 ,tmp15 .dtype )\n    tmp17 =tl .where (tmp13 ,tmp15 ,tmp16 )\n    tmp18 =tmp17 +tmp9 \n    tmp19 =x1 \n    tmp20 =tmp19 >=tmp1 \n    tmp21 =tmp19 <tmp3 \n    tmp22 =tmp20 &tmp21 \n    tmp23 =tl .load (in_ptr0 +(x2 ),tmp22 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp24 =tmp23 *tmp23 \n    tmp25 =tl .full (tmp24 .shape ,0.0 ,tmp24 .dtype )\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tmp26 +tmp18 \n    tmp28 =1 +x1 \n    tmp29 =tmp28 >=tmp1 \n    tmp30 =tmp28 <tmp3 \n    tmp31 =tmp29 &tmp30 \n    tmp32 =tl .load (in_ptr0 +(ks0 +x2 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tmp32 *tmp32 \n    tmp34 =tl .full (tmp33 .shape ,0.0 ,tmp33 .dtype )\n    tmp35 =tl .where (tmp31 ,tmp33 ,tmp34 )\n    tmp36 =tmp35 +tmp27 \n    tmp37 =2 +x1 \n    tmp38 =tmp37 >=tmp1 \n    tmp39 =tmp37 <tmp3 \n    tmp40 =tmp38 &tmp39 \n    tmp41 =tl .load (in_ptr0 +(x2 +2 *ks0 ),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tmp41 *tmp41 \n    tmp43 =tl .full (tmp42 .shape ,0.0 ,tmp42 .dtype )\n    tmp44 =tl .where (tmp40 ,tmp42 ,tmp43 )\n    tmp45 =tmp44 +tmp36 \n    tmp46 =0.2 \n    tmp47 =tmp45 *tmp46 \n    tmp49 =0.0001 \n    tmp50 =tmp47 *tmp49 \n    tmp51 =1.0 \n    tmp52 =tmp50 +tmp51 \n    tmp53 =0.75 \n    tmp54 =libdevice .pow (tmp52 ,tmp53 )\n    tmp55 =tmp48 /tmp54 \n    tl .store (in_out_ptr0 +(x2 ),tmp55 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =(-2 )+x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 //2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-4 )+2 *x0 +ks1 *x1 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tl .load (in_ptr0 +((-3 )+2 *x0 +ks1 *x1 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =tmp7 +tmp6 \n    tmp9 =0.5 \n    tmp10 =tmp8 *tmp9 \n    tmp11 =tl .full (tmp10 .shape ,0.0 ,tmp10 .dtype )\n    tmp12 =tl .where (tmp5 ,tmp10 ,tmp11 )\n    tl .store (out_ptr0 +(x2 ),tmp12 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,s0 ,s1 ),(s0 *s1 ,s0 *s1 ,s1 ,1 ),torch .float32 )\n        buf1 =reinterpret_tensor (buf0 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ),0 );del buf0 \n\n        triton_poi_fused_add_avg_pool2d_constant_pad_nd_div_mul_pow_0_xnumel =s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_add_avg_pool2d_constant_pad_nd_div_mul_pow_0 [grid (triton_poi_fused_add_avg_pool2d_constant_pad_nd_div_mul_pow_0_xnumel )](buf1 ,arg2_1 ,64 ,10 ,640 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        4 +(s1 //2 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,4 +(s1 //2 )),(4 *s0 +s0 *(s1 //2 ),4 +(s1 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_1_xnumel =4 *s0 +s0 *(s1 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_1 [grid (triton_poi_fused_constant_pad_nd_1_xnumel )](buf1 ,buf2 ,36 ,64 ,360 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n    return (reinterpret_tensor (buf2 ,(1 ,4 +(s1 //2 ),s0 ),(4 *s0 +s0 *(s1 //2 ),1 ,4 +(s1 //2 )),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,10 ,64 ),(640 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "b6296670-b668-4c6e-bf46-4bea6dfa05af",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PoissonNLLLoss', 'LogSoftmax', 'TripletMarginWithDistanceLoss', 'InstanceNorm3d', 'PairwiseDistance']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.instance_norm = nn.InstanceNorm3d(10)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n        self.pairwise_distance = nn.PairwiseDistance()\n        self.triplet_loss = nn.TripletMarginWithDistanceLoss(distance_function=self.pairwise_distance)\n        self.poisson_loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Assuming x is a 5D tensor (batch, channels, depth, height, width)\n        x = self.instance_norm(x)\n        \n        # Flatten the tensor for LogSoftmax\n        x = x.view(x.size(0), -1)\n        x = self.log_softmax(x)\n        \n        # Reshape back to original shape for triplet loss\n        x = x.view(x.size(0), 10, -1)\n        \n        # Generate anchor, positive, and negative samples for triplet loss\n        anchor = x[:, 0, :]\n        positive = x[:, 1, :]\n        negative = x[:, 2, :]\n        \n        # Compute triplet loss\n        triplet_loss = self.triplet_loss(anchor, positive, negative)\n        \n        # Compute PoissonNLLLoss\n        poisson_loss = self.poisson_loss(x, torch.exp(x))\n        \n        # Return the sum of losses as the output\n        return triplet_loss + poisson_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 5, 5, 5).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    r0_numel =125 \n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +125 *x0 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (r0_mask &xmask ,tmp1 ,0 )\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp6 =tl .where (r0_mask &xmask ,tmp4 ,0 )\n    tmp7 =tl .sum (tmp6 ,1 )[:,None ]\n    tmp8 =tl .full ([XBLOCK ,1 ],125 ,tl .int32 )\n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp7 /tmp9 \n    tmp11 =tmp1 -tmp10 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n    tmp15 =tl .where (r0_mask &xmask ,tmp13 ,0 )\n    tmp16 =tl .sum (tmp15 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp16 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax_exp_mean_mul_sub_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr2 ,out_ptr3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    r0_numel =1250 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr1 +(r0_0 //125 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tl .load (in_ptr2 +(r0_0 //125 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp4 =125.0 \n        tmp5 =tmp3 /tmp4 \n        tmp6 =1e-05 \n        tmp7 =tmp5 +tmp6 \n        tmp8 =libdevice .rsqrt (tmp7 )\n        tmp9 =tmp2 *tmp8 \n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =triton_helpers .maximum (_tmp11 ,tmp10 )\n        _tmp11 =tl .where (r0_mask ,tmp12 ,_tmp11 )\n    tmp11 =triton_helpers .max2 (_tmp11 ,1 )[:,None ]\n    _tmp26 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp13 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp14 =tl .load (in_ptr1 +(r0_0 //125 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp16 =tl .load (in_ptr2 +(r0_0 //125 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp15 =tmp13 -tmp14 \n        tmp17 =125.0 \n        tmp18 =tmp16 /tmp17 \n        tmp19 =1e-05 \n        tmp20 =tmp18 +tmp19 \n        tmp21 =libdevice .rsqrt (tmp20 )\n        tmp22 =tmp15 *tmp21 \n        tmp23 =tmp22 -tmp11 \n        tmp24 =tl_math .exp (tmp23 )\n        tmp25 =tl .broadcast_to (tmp24 ,[XBLOCK ,R0_BLOCK ])\n        tmp27 =_tmp26 +tmp25 \n        _tmp26 =tl .where (r0_mask ,tmp27 ,_tmp26 )\n    tmp26 =tl .sum (_tmp26 ,1 )[:,None ]\n    _tmp45 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp28 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp29 =tl .load (in_ptr1 +(r0_0 //125 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp31 =tl .load (in_ptr2 +(r0_0 //125 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp30 =tmp28 -tmp29 \n        tmp32 =125.0 \n        tmp33 =tmp31 /tmp32 \n        tmp34 =1e-05 \n        tmp35 =tmp33 +tmp34 \n        tmp36 =libdevice .rsqrt (tmp35 )\n        tmp37 =tmp30 *tmp36 \n        tmp38 =tmp37 -tmp11 \n        tmp39 =tl_math .log (tmp26 )\n        tmp40 =tmp38 -tmp39 \n        tmp41 =tl_math .exp (tmp40 )\n        tmp42 =tmp41 *tmp40 \n        tmp43 =tmp41 -tmp42 \n        tmp44 =tl .broadcast_to (tmp43 ,[XBLOCK ,R0_BLOCK ])\n        tmp46 =_tmp45 +tmp44 \n        _tmp45 =tl .where (r0_mask ,tmp46 ,_tmp45 )\n        tl .store (out_ptr2 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp40 ,r0_mask )\n    tmp45 =tl .sum (_tmp45 ,1 )[:,None ]\n    tl .store (out_ptr3 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp45 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_clamp_min_exp_mean_mul_norm_sub_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =125 \n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .load (in_ptr0 +(125 +r0_0 ),r0_mask ,other =0.0 )\n    tmp10 =tl .load (in_ptr0 +(250 +r0_0 ),r0_mask ,other =0.0 )\n    tmp26 =tl .load (in_ptr1 +(0 ))\n    tmp27 =tl .broadcast_to (tmp26 ,[XBLOCK ,1 ])\n    tmp2 =tmp0 -tmp1 \n    tmp3 =1e-06 \n    tmp4 =tmp2 +tmp3 \n    tmp5 =tmp4 *tmp4 \n    tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n    tmp8 =tl .where (r0_mask ,tmp6 ,0 )\n    tmp9 =tl .sum (tmp8 ,1 )[:,None ]\n    tmp11 =tmp0 -tmp10 \n    tmp12 =tmp11 +tmp3 \n    tmp13 =tmp12 *tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tmp16 =tl .where (r0_mask ,tmp14 ,0 )\n    tmp17 =tl .sum (tmp16 ,1 )[:,None ]\n    tmp18 =libdevice .sqrt (tmp9 )\n    tmp19 =1.0 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =libdevice .sqrt (tmp17 )\n    tmp22 =tmp20 -tmp21 \n    tmp23 =0.0 \n    tmp24 =triton_helpers .maximum (tmp22 ,tmp23 )\n    tmp25 =tmp24 /tmp19 \n    tmp28 =1250.0 \n    tmp29 =tmp27 /tmp28 \n    tmp30 =tmp25 +tmp29 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp30 ,None )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,10 ,5 ,5 ,5 ),(1250 ,125 ,25 ,5 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,10 ,10 ,10 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,10 ,10 ,10 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_0 [grid (10 )](arg0_1 ,buf0 ,buf1 ,10 ,125 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((1 ,1250 ),(1280 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((),(),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__log_softmax_exp_mean_mul_sub_1 [grid (1 )](arg0_1 ,buf0 ,buf1 ,buf5 ,buf8 ,1 ,1250 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg0_1 \n        del buf0 \n        del buf1 \n        buf6 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf9 =reinterpret_tensor (buf6 ,(),(),0 );del buf6 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_clamp_min_exp_mean_mul_norm_sub_2 [grid (1 )](buf9 ,buf5 ,buf8 ,1 ,125 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf5 \n        del buf8 \n    return (buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,10 ,5 ,5 ,5 ),(1250 ,125 ,25 ,5 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "b90bedca-9b69-4f98-8629-3e6903fa0510",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['BCEWithLogitsLoss', 'ReplicationPad2d', 'MaxPool2d', 'Softmin', 'AdaptiveMaxPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.replication_pad = nn.ReplicationPad2d(2)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.adaptive_max_pool = nn.AdaptiveMaxPool2d((5, 5))\n        self.softmin = nn.Softmin(dim=1)\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        # Apply ReplicationPad2d\n        x = self.replication_pad(x)\n        \n        # Apply MaxPool2d\n        x = self.max_pool(x)\n        \n        # Apply AdaptiveMaxPool2d\n        x = self.adaptive_max_pool(x)\n        \n        # Reshape for Softmin\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x = x.unsqueeze(1)  # Add a dimension for Softmin\n        \n        # Apply Softmin\n        x = self.softmin(x)\n        \n        # Reshape back to a 2D tensor for BCEWithLogitsLoss\n        x = x.squeeze(1)\n        \n        # Generate a dummy target for BCEWithLogitsLoss\n        target = torch.zeros_like(x)\n        \n        # Apply BCEWithLogitsLoss\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels and 64x64 spatial dimensions\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_replication_pad2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks4 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-2 )+2 *x1 ))+((-2 )+2 *x1 )*(((-2 )+2 *x1 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x1 ))+((-2 )+2 *x1 )*(((-2 )+2 *x1 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x1 ))+((-2 )+2 *x1 )*(((-2 )+2 *x1 )>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *x2 +(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))<((-1 )+ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(ks4 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-2 )+2 *x1 ))+((-2 )+2 *x1 )*(((-2 )+2 *x1 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x1 ))+((-2 )+2 *x1 )*(((-2 )+2 *x1 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x1 ))+((-2 )+2 *x1 )*(((-2 )+2 *x1 )>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *x2 +(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))<((-1 )+ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks4 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *x2 +(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+2 *x0 ))+((-2 )+2 *x0 )*(((-2 )+2 *x0 )>(0 ))))<((-1 )+ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(ks4 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x1 ))+((-1 )+2 *x1 )*(((-1 )+2 *x1 )>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *x2 +(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 )))))+(((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+2 *x0 ))+((-1 )+2 *x0 )*(((-1 )+2 *x0 )>(0 ))))<((-1 )+ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\n    tl .store (out_ptr0 +(x3 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_binary_cross_entropy_with_logits_zeros_like_1 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp16 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =-tmp0 \n        tmp2 =tmp1 -tmp1 \n        tmp3 =tl_math .exp (tmp2 )\n        tmp4 =tmp3 /tmp3 \n        tmp5 =1.0 \n        tmp6 =tmp5 *tmp4 \n        tmp7 =0.0 \n        tmp8 =triton_helpers .minimum (tmp7 ,tmp4 )\n        tmp9 =tl_math .abs (tmp4 )\n        tmp10 =-tmp9 \n        tmp11 =tl_math .exp (tmp10 )\n        tmp12 =libdevice .log1p (tmp11 )\n        tmp13 =tmp8 -tmp12 \n        tmp14 =tmp6 -tmp13 \n        tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n        tmp17 =_tmp16 +tmp15 \n        _tmp16 =tl .where (r0_mask ,tmp17 ,_tmp16 )\n    tmp16 =tl .sum (_tmp16 ,1 )[:,None ]\n    tmp18 =25 *ks0 \n    tmp19 =tmp18 .to (tl .float32 )\n    tmp20 =tmp16 /tmp19 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp20 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +(s2 //2 )\n        2 +(s1 //2 )\n        4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +(s1 //2 ),2 +(s2 //2 )),(4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 ),4 +2 *(s1 //2 )+2 *(s2 //2 )+(s1 //2 )*(s2 //2 ),2 +(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_pool2d_with_indices_replication_pad2d_0_xnumel =4 *s0 +2 *s0 *(s1 //2 )+2 *s0 *(s2 //2 )+s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_replication_pad2d_0 [grid (triton_poi_fused_max_pool2d_with_indices_replication_pad2d_0_xnumel )](arg3_1 ,buf0 ,34 ,34 ,1156 ,64 ,64 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n\n        buf1 =torch .ops .aten .adaptive_max_pool2d .default (buf0 ,[5 ,5 ])\n        del buf0 \n        buf2 =buf1 [0 ]\n        del buf1 \n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf5 =buf4 ;del buf4 \n\n        25 *s0 \n        get_raw_stream (0 )\n        triton_red_fused_binary_cross_entropy_with_logits_zeros_like_1 [grid (1 )](buf5 ,buf2 ,3 ,1 ,75 ,XBLOCK =1 ,R0_BLOCK =128 ,num_warps =2 ,num_stages =1 )\n        del buf2 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "bb779914-7040-449a-aa21-33255cd7bb3e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['BCELoss', 'Upsample', 'Softmax2d', 'ParameterDict']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.softmax2d = nn.Softmax2d()\n        self.parameter_dict = nn.ParameterDict({\n            'param1': nn.Parameter(torch.randn(1, 3, 64, 64)),\n            'param2': nn.Parameter(torch.randn(1, 3, 128, 128)),\n        })\n        self.bce_loss = nn.BCELoss()\n\n    def forward(self, x):\n        # Upsample the input\n        x = self.upsample(x)\n        \n        # Apply Softmax2d\n        x = self.softmax2d(x)\n        \n        # Add a parameter from ParameterDict\n        if x.shape == self.parameter_dict['param1'].shape:\n            x = x + self.parameter_dict['param1']\n        else:\n            x = x + self.parameter_dict['param2']\n        \n        # Compute BCE loss with a dummy target\n        target = torch.zeros_like(x)\n        loss = self.bce_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x1 =((xindex //64 )%64 )\n    x0 =(xindex %64 )\n    x2 =xindex //4096 \n    x4 =xindex \n    tmp0 =x1 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.49206349206349204 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],31 ,tl .int64 )\n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tmp11 =x0 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =tmp12 *tmp2 \n    tmp14 =triton_helpers .maximum (tmp13 ,tmp4 )\n    tmp15 =tmp14 .to (tl .int32 )\n    tmp16 =tl .load (in_ptr0 +(tmp15 +32 *tmp10 +1024 *x2 ),None ,eviction_policy ='evict_last')\n    tmp17 =tmp15 +tmp7 \n    tmp18 =triton_helpers .minimum (tmp17 ,tmp9 )\n    tmp19 =tl .load (in_ptr0 +(tmp18 +32 *tmp10 +1024 *x2 ),None ,eviction_policy ='evict_last')\n    tmp20 =tmp19 -tmp16 \n    tmp21 =tmp15 .to (tl .float32 )\n    tmp22 =tmp14 -tmp21 \n    tmp23 =triton_helpers .maximum (tmp22 ,tmp4 )\n    tmp24 =1.0 \n    tmp25 =triton_helpers .minimum (tmp23 ,tmp24 )\n    tmp26 =tmp20 *tmp25 \n    tmp27 =tmp16 +tmp26 \n    tmp28 =tl .load (in_ptr0 +(tmp15 +32 *tmp6 +1024 *x2 ),None ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr0 +(tmp18 +32 *tmp6 +1024 *x2 ),None ,eviction_policy ='evict_last')\n    tmp30 =tmp29 -tmp28 \n    tmp31 =tmp30 *tmp25 \n    tmp32 =tmp28 +tmp31 \n    tmp33 =tmp27 -tmp32 \n    tmp34 =tmp6 .to (tl .float32 )\n    tmp35 =tmp5 -tmp34 \n    tmp36 =triton_helpers .maximum (tmp35 ,tmp4 )\n    tmp37 =triton_helpers .minimum (tmp36 ,tmp24 )\n    tmp38 =tmp33 *tmp37 \n    tmp39 =tmp32 +tmp38 \n    tl .store (in_out_ptr0 +(x4 ),tmp39 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_binary_cross_entropy_backward_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =xindex \n    x0 =(xindex %4096 )\n    tmp0 =tl .load (in_ptr0 +(x2 ),None )\n    tmp1 =tl .load (in_ptr0 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(4096 +x0 ),None ,eviction_policy ='evict_last')\n    tmp4 =tl .load (in_ptr0 +(8192 +x0 ),None ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr1 +(x2 ),None )\n    tmp3 =triton_helpers .maximum (tmp1 ,tmp2 )\n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp0 -tmp5 \n    tmp7 =tl_math .exp (tmp6 )\n    tmp8 =tmp1 -tmp5 \n    tmp9 =tl_math .exp (tmp8 )\n    tmp10 =tmp2 -tmp5 \n    tmp11 =tl_math .exp (tmp10 )\n    tmp12 =tmp9 +tmp11 \n    tmp13 =tmp4 -tmp5 \n    tmp14 =tl_math .exp (tmp13 )\n    tmp15 =tmp12 +tmp14 \n    tmp16 =tmp7 /tmp15 \n    tmp18 =tmp16 +tmp17 \n    tl .store (out_ptr0 +(x2 ),tmp18 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_binary_cross_entropy_zeros_like_2 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =6144 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp13 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +6144 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =-tmp0 \n        tmp2 =libdevice .log1p (tmp1 )\n        tmp3 =-100.0 \n        tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n        tmp5 =-1.0 \n        tmp6 =tmp5 *tmp4 \n        tmp7 =tl_math .log (tmp0 )\n        tmp8 =triton_helpers .maximum (tmp7 ,tmp3 )\n        tmp9 =0.0 \n        tmp10 =tmp9 *tmp8 \n        tmp11 =tmp6 -tmp10 \n        tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n        tmp14 =_tmp13 +tmp12 \n        _tmp13 =tl .where (r0_mask &xmask ,tmp14 ,_tmp13 )\n    tmp13 =tl .sum (_tmp13 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_binary_cross_entropy_zeros_like_3 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =12288.0 \n    tmp5 =tmp3 /tmp4 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp5 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_2 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),torch .float32 )\n        buf1 =buf0 ;del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy__unsafe_index_add_arange_clamp_mul_sub_0 [grid (12288 )](buf1 ,primals_1 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_1 \n        buf2 =empty_strided_cuda ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__softmax_binary_cross_entropy_backward_1 [grid (12288 )](buf1 ,primals_2 ,buf2 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        del primals_2 \n        buf3 =empty_strided_cuda ((2 ,),(1 ,),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused_binary_cross_entropy_zeros_like_2 [grid (2 )](buf2 ,buf3 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf5 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_per_fused_binary_cross_entropy_zeros_like_3 [grid (1 )](buf5 ,buf3 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf3 \n    return (buf5 ,buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "bc330d4c-33e7-48e9-82c5-68c12a129227",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PixelUnshuffle', 'Mish', 'Dropout1d', 'AdaptiveAvgPool2d', 'UpsamplingBilinear2d', 'ModuleList', 'CELU', 'LPPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pixel_unshuffle = nn.PixelUnshuffle(downscale_factor=2)\n        self.mish = nn.Mish()\n        self.dropout1d = nn.Dropout1d(p=0.5)\n        self.adaptive_avg_pool2d = nn.AdaptiveAvgPool2d((16, 16))\n        self.upsampling_bilinear2d = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.module_list = nn.ModuleList([\n            nn.CELU(),\n            nn.LPPool2d(norm_type=2, kernel_size=2, stride=2),\n            nn.CELU(),\n            nn.LPPool2d(norm_type=2, kernel_size=2, stride=2),\n            nn.CELU()\n        ])\n        self.lp_pool2d = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.pixel_unshuffle(x)\n        x = self.mish(x)\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape for Dropout1d\n        x = self.dropout1d(x)\n        x = x.view(x.size(0), x.size(1), int(x.size(2)**0.5), int(x.size(2)**0.5))  # Reshape back to 4D\n        x = self.adaptive_avg_pool2d(x)\n        x = self.upsampling_bilinear2d(x)\n        for module in self.module_list:\n            x = module(x)\n        x = self.lp_pool2d(x)\n        return x\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =12 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =(xindex %12 )\n    x1 =xindex //12 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *((x1 %32 ))+64 *(((x0 //2 )%2 ))+128 *(x1 //32 )+4096 *(x0 //4 )+((x0 %2 ))),None )\n    tmp8 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp1 =20.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =tl_math .exp (tmp0 )\n    tmp4 =libdevice .log1p (tmp3 )\n    tmp5 =tl .where (tmp2 ,tmp0 ,tmp4 )\n    tmp6 =libdevice .tanh (tmp5 )\n    tmp7 =tmp0 *tmp6 \n    tmp9 =0.5 \n    tmp10 =tmp8 <tmp9 \n    tmp11 =tmp10 .to (tl .float32 )\n    tmp12 =2.0 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =tmp7 *tmp13 \n    tl .store (out_ptr0 +(x2 ),tmp14 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d__to_copy__unsafe_index_add_arange_celu_clamp_mul_pow_sub_2 (in_out_ptr1 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x1 =((xindex //32 )%32 )\n    x0 =(xindex %32 )\n    x2 =xindex //1024 \n    x4 =xindex \n    tmp0 =x1 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.4838709677419355 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],15 ,tl .int64 )\n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tmp11 =x0 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =tmp12 *tmp2 \n    tmp14 =triton_helpers .maximum (tmp13 ,tmp4 )\n    tmp15 =tmp14 .to (tl .int32 )\n    tmp16 =tmp15 +tmp7 \n    tmp17 =triton_helpers .minimum (tmp16 ,tmp9 )\n    tmp18 =tl .load (in_ptr0 +(x2 +24 *tmp17 +768 *tmp10 ),None ,eviction_policy ='evict_last')\n    tmp19 =tl .load (in_ptr0 +(12 +x2 +24 *tmp17 +768 *tmp10 ),None ,eviction_policy ='evict_last')\n    tmp20 =tmp19 +tmp18 \n    tmp21 =tl .load (in_ptr0 +(384 +x2 +24 *tmp17 +768 *tmp10 ),None ,eviction_policy ='evict_last')\n    tmp22 =tmp21 +tmp20 \n    tmp23 =tl .load (in_ptr0 +(396 +x2 +24 *tmp17 +768 *tmp10 ),None ,eviction_policy ='evict_last')\n    tmp24 =tmp23 +tmp22 \n    tmp25 =0.25 \n    tmp26 =tmp24 *tmp25 \n    tmp27 =tl .load (in_ptr0 +(x2 +24 *tmp15 +768 *tmp10 ),None ,eviction_policy ='evict_last')\n    tmp28 =tl .load (in_ptr0 +(12 +x2 +24 *tmp15 +768 *tmp10 ),None ,eviction_policy ='evict_last')\n    tmp29 =tmp28 +tmp27 \n    tmp30 =tl .load (in_ptr0 +(384 +x2 +24 *tmp15 +768 *tmp10 ),None ,eviction_policy ='evict_last')\n    tmp31 =tmp30 +tmp29 \n    tmp32 =tl .load (in_ptr0 +(396 +x2 +24 *tmp15 +768 *tmp10 ),None ,eviction_policy ='evict_last')\n    tmp33 =tmp32 +tmp31 \n    tmp34 =tmp33 *tmp25 \n    tmp35 =tmp26 -tmp34 \n    tmp36 =tl .load (in_ptr0 +(x2 +24 *tmp17 +768 *tmp6 ),None ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr0 +(12 +x2 +24 *tmp17 +768 *tmp6 ),None ,eviction_policy ='evict_last')\n    tmp38 =tmp37 +tmp36 \n    tmp39 =tl .load (in_ptr0 +(384 +x2 +24 *tmp17 +768 *tmp6 ),None ,eviction_policy ='evict_last')\n    tmp40 =tmp39 +tmp38 \n    tmp41 =tl .load (in_ptr0 +(396 +x2 +24 *tmp17 +768 *tmp6 ),None ,eviction_policy ='evict_last')\n    tmp42 =tmp41 +tmp40 \n    tmp43 =tmp42 *tmp25 \n    tmp44 =tl .load (in_ptr0 +(x2 +24 *tmp15 +768 *tmp6 ),None ,eviction_policy ='evict_last')\n    tmp45 =tl .load (in_ptr0 +(12 +x2 +24 *tmp15 +768 *tmp6 ),None ,eviction_policy ='evict_last')\n    tmp46 =tmp45 +tmp44 \n    tmp47 =tl .load (in_ptr0 +(384 +x2 +24 *tmp15 +768 *tmp6 ),None ,eviction_policy ='evict_last')\n    tmp48 =tmp47 +tmp46 \n    tmp49 =tl .load (in_ptr0 +(396 +x2 +24 *tmp15 +768 *tmp6 ),None ,eviction_policy ='evict_last')\n    tmp50 =tmp49 +tmp48 \n    tmp51 =tmp50 *tmp25 \n    tmp52 =tmp43 -tmp51 \n    tmp53 =tmp15 .to (tl .float32 )\n    tmp54 =tmp14 -tmp53 \n    tmp55 =triton_helpers .maximum (tmp54 ,tmp4 )\n    tmp56 =1.0 \n    tmp57 =triton_helpers .minimum (tmp55 ,tmp56 )\n    tmp58 =tmp35 *tmp57 \n    tmp59 =tmp34 +tmp58 \n    tmp60 =tmp52 *tmp57 \n    tmp61 =tmp51 +tmp60 \n    tmp62 =tmp59 -tmp61 \n    tmp63 =tmp6 .to (tl .float32 )\n    tmp64 =tmp5 -tmp63 \n    tmp65 =triton_helpers .maximum (tmp64 ,tmp4 )\n    tmp66 =triton_helpers .minimum (tmp65 ,tmp56 )\n    tmp67 =tmp62 *tmp66 \n    tmp68 =tmp61 +tmp67 \n    tmp69 =tmp68 >tmp4 \n    tmp70 =libdevice .expm1 (tmp68 )\n    tmp71 =tl .where (tmp69 ,tmp68 ,tmp70 )\n    tmp72 =tmp71 *tmp71 \n    tl .store (in_out_ptr1 +(x4 ),tmp72 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_abs_add_avg_pool2d_celu_clamp_mul_pow_relu_sign_sub_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3072 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %16 )\n    x1 =xindex //16 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +64 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +64 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(32 +2 *x0 +64 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(33 +2 *x0 +64 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =tmp9 <tmp8 \n    tmp11 =tmp10 .to (tl .int8 )\n    tmp12 =tmp8 <tmp9 \n    tmp13 =tmp12 .to (tl .int8 )\n    tmp14 =tmp11 -tmp13 \n    tmp15 =tmp14 .to (tmp8 .dtype )\n    tmp16 =tl_math .abs (tmp8 )\n    tmp17 =triton_helpers .maximum (tmp9 ,tmp16 )\n    tmp18 =tmp15 *tmp17 \n    tmp19 =4.0 \n    tmp20 =tmp18 *tmp19 \n    tmp21 =libdevice .sqrt (tmp20 )\n    tmp22 =0.0 \n    tmp23 =tmp21 >tmp22 \n    tmp24 =libdevice .expm1 (tmp21 )\n    tmp25 =tl .where (tmp23 ,tmp21 ,tmp24 )\n    tmp26 =tmp25 *tmp25 \n    tl .store (out_ptr0 +(x2 ),tmp26 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_abs_add_avg_pool2d_celu_clamp_mul_pow_relu_sign_sub_4 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =768 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %8 )\n    x1 =xindex //8 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +32 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +32 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(16 +2 *x0 +32 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(17 +2 *x0 +32 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =tmp9 <tmp8 \n    tmp11 =tmp10 .to (tl .int8 )\n    tmp12 =tmp8 <tmp9 \n    tmp13 =tmp12 .to (tl .int8 )\n    tmp14 =tmp11 -tmp13 \n    tmp15 =tmp14 .to (tmp8 .dtype )\n    tmp16 =tl_math .abs (tmp8 )\n    tmp17 =triton_helpers .maximum (tmp9 ,tmp16 )\n    tmp18 =tmp15 *tmp17 \n    tmp19 =4.0 \n    tmp20 =tmp18 *tmp19 \n    tmp21 =libdevice .sqrt (tmp20 )\n    tmp22 =0.0 \n    tmp23 =tmp21 >tmp22 \n    tmp24 =libdevice .expm1 (tmp21 )\n    tmp25 =tl .where (tmp23 ,tmp21 ,tmp24 )\n    tmp26 =tmp25 *tmp25 \n    tl .store (out_ptr0 +(x2 ),tmp26 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_abs_add_avg_pool2d_celu_clamp_mul_pow_relu_sign_sub_5 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =192 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %4 )\n    x1 =xindex //4 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +16 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +16 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(8 +2 *x0 +16 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(9 +2 *x0 +16 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =tmp9 <tmp8 \n    tmp11 =tmp10 .to (tl .int8 )\n    tmp12 =tmp8 <tmp9 \n    tmp13 =tmp12 .to (tl .int8 )\n    tmp14 =tmp11 -tmp13 \n    tmp15 =tmp14 .to (tmp8 .dtype )\n    tmp16 =tl_math .abs (tmp8 )\n    tmp17 =triton_helpers .maximum (tmp9 ,tmp16 )\n    tmp18 =tmp15 *tmp17 \n    tmp19 =4.0 \n    tmp20 =tmp18 *tmp19 \n    tmp21 =libdevice .sqrt (tmp20 )\n    tl .store (out_ptr0 +(x2 ),tmp21 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,12 ,1 ),(12 ,1 ,12 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (12 )](buf0 ,buf1 ,0 ,12 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,12 ,1024 ),(12288 ,1 ,12 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_1 [grid (12288 )](arg0_1 ,buf1 ,buf2 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg0_1 \n        del buf1 \n        buf5 =empty_strided_cuda ((1 ,12 ,32 ,32 ),(12288 ,1024 ,32 ,1 ),torch .float32 )\n        buf6 =buf5 ;del buf5 \n        buf7 =buf6 ;del buf6 \n\n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d__to_copy__unsafe_index_add_arange_celu_clamp_mul_pow_sub_2 [grid (12288 )](buf7 ,buf2 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        buf8 =empty_strided_cuda ((1 ,12 ,16 ,16 ),(3072 ,256 ,16 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_abs_add_avg_pool2d_celu_clamp_mul_pow_relu_sign_sub_3 [grid (3072 )](buf7 ,buf8 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf7 \n        buf9 =empty_strided_cuda ((1 ,12 ,8 ,8 ),(768 ,64 ,8 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_abs_add_avg_pool2d_celu_clamp_mul_pow_relu_sign_sub_4 [grid (768 )](buf8 ,buf9 ,768 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf8 \n        buf10 =empty_strided_cuda ((1 ,12 ,4 ,4 ),(192 ,16 ,4 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_abs_add_avg_pool2d_celu_clamp_mul_pow_relu_sign_sub_5 [grid (192 )](buf9 ,buf10 ,192 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf9 \n    return (buf10 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "bd873067-39c9-41d6-b9ea-d82c48e3c34e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softshrink', 'MSELoss', 'Hardtanh']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softshrink = nn.Softshrink()\n        self.hardtanh = nn.Hardtanh()\n        self.mseloss = nn.MSELoss()\n\n    def forward(self, x):\n        # Apply Softshrink\n        x = self.softshrink(x)\n        \n        # Apply Hardtanh\n        x = self.hardtanh(x)\n        \n        # Reshape the tensor to match the expected input shape for MSELoss\n        # Assuming the target is a tensor of the same shape as x\n        target = torch.zeros_like(x)\n        \n        # Compute MSELoss\n        loss = self.mseloss(x, target)\n        \n        # Return the loss as the output\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_abs_gt_hardtanh_mse_loss_mul_sign_sub_where_0 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =6144 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp22 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +6144 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl_math .abs (tmp0 )\n        tmp2 =0.5 \n        tmp3 =tmp1 >tmp2 \n        tmp4 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp5 =tmp4 <tmp0 \n        tmp6 =tmp5 .to (tl .int8 )\n        tmp7 =tmp0 <tmp4 \n        tmp8 =tmp7 .to (tl .int8 )\n        tmp9 =tmp6 -tmp8 \n        tmp10 =tmp9 .to (tmp0 .dtype )\n        tmp11 =tmp10 *tmp2 \n        tmp12 =tmp0 -tmp11 \n        tmp13 =0.0 \n        tmp14 =tmp0 *tmp13 \n        tmp15 =tl .where (tmp3 ,tmp12 ,tmp14 )\n        tmp16 =-1.0 \n        tmp17 =triton_helpers .maximum (tmp15 ,tmp16 )\n        tmp18 =1.0 \n        tmp19 =triton_helpers .minimum (tmp17 ,tmp18 )\n        tmp20 =tmp19 *tmp19 \n        tmp21 =tl .broadcast_to (tmp20 ,[XBLOCK ,R0_BLOCK ])\n        tmp23 =_tmp22 +tmp21 \n        _tmp22 =tl .where (r0_mask &xmask ,tmp23 ,_tmp22 )\n    tmp22 =tl .sum (_tmp22 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp22 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_abs_gt_hardtanh_mse_loss_mul_sign_sub_where_1 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =12288.0 \n    tmp5 =tmp3 /tmp4 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp5 ,None )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused_abs_gt_hardtanh_mse_loss_mul_sign_sub_where_0 [grid (2 )](arg0_1 ,buf0 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg0_1 \n        buf1 =empty_strided_cuda ((),(),torch .float32 )\n        buf2 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_per_fused_abs_gt_hardtanh_mse_loss_mul_sign_sub_where_1 [grid (1 )](buf2 ,buf0 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "be3818e4-4c2b-47e5-a9e8-b6115b627c40",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MultiMarginLoss', 'CircularPad1d', 'PoissonNLLLoss', 'CTCLoss', 'Softplus', 'GRU', 'PixelUnshuffle']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.circular_pad = nn.CircularPad1d(2)\n        self.softplus = nn.Softplus()\n        self.gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.pixel_unshuffle = nn.PixelUnshuffle(downscale_factor=2)\n        self.multi_margin_loss = nn.MultiMarginLoss()\n        self.poisson_nll_loss = nn.PoissonNLLLoss()\n        self.ctc_loss = nn.CTCLoss()\n\n    def forward(self, x):\n        # Apply CircularPad1d\n        x = self.circular_pad(x)\n        \n        # Apply Softplus\n        x = self.softplus(x)\n        \n        # Reshape for GRU\n        x = x.view(x.size(0), -1, 10)  # Reshape to (batch_size, seq_len, input_size)\n        \n        # Apply GRU\n        x, _ = self.gru(x)\n        \n        # Reshape for PixelUnshuffle\n        x = x.view(x.size(0), 20, 8, 8)  # Reshape to (batch_size, channels, height, width)\n        \n        # Apply PixelUnshuffle\n        x = self.pixel_unshuffle(x)\n        \n        # Compute losses (dummy targets for demonstration)\n        target_multi_margin = torch.randint(0, 10, (x.size(0),), dtype=torch.long, device=x.device)\n        target_poisson = torch.randn_like(x)\n        target_ctc = torch.randint(0, 10, (x.size(0), 10), dtype=torch.long, device=x.device)\n        input_lengths = torch.full((x.size(0),), 10, dtype=torch.long, device=x.device)\n        target_lengths = torch.randint(1, 10, (x.size(0),), dtype=torch.long, device=x.device)\n        \n        # Apply MultiMarginLoss\n        loss_multi_margin = self.multi_margin_loss(x.view(x.size(0), -1), target_multi_margin)\n        \n        # Apply PoissonNLLLoss\n        loss_poisson = self.poisson_nll_loss(x, target_poisson)\n        \n        # Apply CTCLoss\n        loss_ctc = self.ctc_loss(x.view(x.size(0), 10, -1), target_ctc, input_lengths, target_lengths)\n        \n        # Return the sum of losses (for demonstration purposes)\n        return loss_multi_margin + loss_poisson + loss_ctc\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 16).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =x0 \n    tmp1 =2 +ks1 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =x0 +((-1 )*ks1 )\n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .full ([1 ],2 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tl .broadcast_to (2 +ks1 ,[XBLOCK ])\n    tmp11 =tmp7 <tmp10 \n    tmp12 =tmp9 &tmp11 \n    tmp13 =tmp12 &tmp6 \n    tmp14 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *x1 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =float (\"nan\")\n    tmp16 =tl .where (tmp12 ,tmp14 ,tmp15 )\n    tmp17 =tl .full (tmp16 .shape ,0.0 ,tmp16 .dtype )\n    tmp18 =tl .where (tmp6 ,tmp16 ,tmp17 )\n    tmp19 =tmp3 >=tmp4 \n    tmp20 =tl .broadcast_to (2 +ks1 ,[XBLOCK ])\n    tmp21 =tmp3 <tmp20 \n    tmp22 =tmp19 &tmp21 \n    tmp23 =tmp22 &tmp2 \n    tmp24 =tl .load (in_ptr0 +((-2 )+x0 +((-1 )*ks1 )+ks1 *x1 ),tmp23 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp25 =float (\"nan\")\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tl .where (tmp5 ,tmp18 ,tmp26 )\n    tmp28 =tl .full (tmp27 .shape ,0.0 ,tmp27 .dtype )\n    tmp29 =tl .where (tmp2 ,tmp27 ,tmp28 )\n    tmp30 =tl .full ([1 ],2 ,tl .int64 )\n    tmp31 =tmp0 <tmp30 \n    tmp32 =ks1 +x0 \n    tmp33 =tl .full ([1 ],2 ,tl .int64 )\n    tmp34 =tmp32 >=tmp33 \n    tmp35 =tl .broadcast_to (2 +ks1 ,[XBLOCK ])\n    tmp36 =tmp32 <tmp35 \n    tmp37 =tmp34 &tmp36 \n    tmp38 =tmp37 &tmp31 \n    tmp39 =tl .load (in_ptr0 +((-2 )+ks1 +x0 +ks1 *x1 ),tmp38 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp40 =float (\"nan\")\n    tmp41 =tl .where (tmp37 ,tmp39 ,tmp40 )\n    tmp42 =tl .full (tmp41 .shape ,0.0 ,tmp41 .dtype )\n    tmp43 =tl .where (tmp31 ,tmp41 ,tmp42 )\n    tmp44 =tmp0 >=tmp30 \n    tmp45 =tmp0 <tmp1 \n    tmp46 =tmp44 &tmp45 \n    tmp47 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *x1 ),tmp46 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp48 =float (\"nan\")\n    tmp49 =tl .where (tmp46 ,tmp47 ,tmp48 )\n    tmp50 =tl .where (tmp31 ,tmp43 ,tmp49 )\n    tmp51 =tl .where (tmp2 ,tmp29 ,tmp50 )\n    tl .store (out_ptr0 +(x2 ),tmp51 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_softplus_1 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =1.0 \n    tmp2 =tmp0 *tmp1 \n    tmp3 =20.0 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tl_math .exp (tmp2 )\n    tmp6 =libdevice .log1p (tmp5 )\n    tmp7 =tmp6 *tmp1 \n    tmp8 =tl .where (tmp4 ,tmp0 ,tmp7 )\n    tl .store (in_out_ptr0 +(x0 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_softplus_view_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %10 )\n    x1 =xindex //10 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *((((x0 +10 *x1 )//ks0 )%ks1 ))+ks2 *((((x0 +10 *x1 )//ks0 )%ks1 ))+(((x0 +10 *x1 )%ks0 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s1 \n        buf1 =empty_strided_cuda ((1 ,s0 ,4 +s1 ),(4 *s0 +s0 *s1 ,4 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_0_xnumel =4 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (triton_poi_fused_copy_0_xnumel )](arg2_1 ,buf1 ,20 ,16 ,200 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        buf2 =buf1 ;del buf1 \n\n        triton_poi_fused_softplus_1_xnumel =4 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_softplus_1 [grid (triton_poi_fused_softplus_1_xnumel )](buf2 ,200 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,(4 *s0 +s0 *s1 )//10 ,10 ),(10 *((4 *s0 +s0 *s1 )//10 ),10 ,1 ),torch .float32 )\n\n        triton_poi_fused_softplus_view_2_xnumel =10 *((4 *s0 +s0 *s1 )//10 )\n        get_raw_stream (0 )\n        triton_poi_fused_softplus_view_2 [grid (triton_poi_fused_softplus_view_2_xnumel )](buf2 ,buf3 ,20 ,10 ,16 ,200 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =16 \n    arg2_1 =rand_strided ((1 ,10 ,16 ),(160 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c070b163-b6da-4a4c-a880-b45aa9f25199",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LocalResponseNorm', 'Identity', 'FractionalMaxPool2d', 'CTCLoss', 'L1Loss', 'ZeroPad2d', 'Hardshrink', 'ReflectionPad3d', 'CrossEntropyLoss', 'CircularPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.local_response_norm = nn.LocalResponseNorm(size=5)\n        self.identity = nn.Identity()\n        self.fractional_max_pool = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.zero_pad = nn.ZeroPad2d(2)\n        self.hardshrink = nn.Hardshrink()\n        self.reflection_pad = nn.ReflectionPad3d(1)\n        self.circular_pad = nn.CircularPad3d(1)\n        self.ctc_loss = nn.CTCLoss()\n        self.l1_loss = nn.L1Loss()\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        # Apply LocalResponseNorm\n        x = self.local_response_norm(x)\n        \n        # Apply Identity\n        x = self.identity(x)\n        \n        # Apply ZeroPad2d\n        x = self.zero_pad(x)\n        \n        # Apply FractionalMaxPool2d\n        x = self.fractional_max_pool(x)\n        \n        # Apply Hardshrink\n        x = self.hardshrink(x)\n        \n        # Reshape for 3D operations\n        x = x.unsqueeze(1)  # Add a channel dimension\n        x = x.unsqueeze(2)  # Add a depth dimension\n        \n        # Apply ReflectionPad3d\n        x = self.reflection_pad(x)\n        \n        # Apply CircularPad3d\n        x = self.circular_pad(x)\n        \n        # Reshape back to 2D for loss computation\n        x = x.squeeze(2).squeeze(1)\n        \n        # Compute CTC Loss (dummy target and input length)\n        target = torch.randint(1, 10, (10,), dtype=torch.long)\n        input_lengths = torch.full((1,), x.size(1), dtype=torch.long)\n        target_lengths = torch.randint(1, 10, (1,), dtype=torch.long)\n        ctc_loss = self.ctc_loss(x, target, input_lengths, target_lengths)\n        \n        # Compute L1 Loss (dummy target)\n        dummy_target = torch.randn_like(x)\n        l1_loss = self.l1_loss(x, dummy_target)\n        \n        # Compute CrossEntropyLoss (dummy target)\n        dummy_class_target = torch.randint(0, 10, (x.size(0),), dtype=torch.long)\n        cross_entropy_loss = self.cross_entropy_loss(x, dummy_class_target)\n        \n        # Return a combination of losses (for demonstration purposes)\n        return ctc_loss + l1_loss + cross_entropy_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool3d_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +(x2 +((-2 )*ks2 *ks3 )),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tmp6 *tmp6 \n    tmp8 =tl .full (tmp7 .shape ,0.0 ,tmp7 .dtype )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tmp10 =(-1 )+x1 \n    tmp11 =tmp10 >=tmp1 \n    tmp12 =tmp10 <tmp3 \n    tmp13 =tmp11 &tmp12 \n    tmp14 =tl .load (in_ptr0 +(x2 +((-1 )*ks2 *ks3 )),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =tmp14 *tmp14 \n    tmp16 =tl .full (tmp15 .shape ,0.0 ,tmp15 .dtype )\n    tmp17 =tl .where (tmp13 ,tmp15 ,tmp16 )\n    tmp18 =tmp17 +tmp9 \n    tmp19 =x1 \n    tmp20 =tmp19 >=tmp1 \n    tmp21 =tmp19 <tmp3 \n    tmp22 =tmp20 &tmp21 \n    tmp23 =tl .load (in_ptr0 +(x2 ),tmp22 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp24 =tmp23 *tmp23 \n    tmp25 =tl .full (tmp24 .shape ,0.0 ,tmp24 .dtype )\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tmp26 +tmp18 \n    tmp28 =1 +x1 \n    tmp29 =tmp28 >=tmp1 \n    tmp30 =tmp28 <tmp3 \n    tmp31 =tmp29 &tmp30 \n    tmp32 =tl .load (in_ptr0 +(ks0 +x2 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tmp32 *tmp32 \n    tmp34 =tl .full (tmp33 .shape ,0.0 ,tmp33 .dtype )\n    tmp35 =tl .where (tmp31 ,tmp33 ,tmp34 )\n    tmp36 =tmp35 +tmp27 \n    tmp37 =2 +x1 \n    tmp38 =tmp37 >=tmp1 \n    tmp39 =tmp37 <tmp3 \n    tmp40 =tmp38 &tmp39 \n    tmp41 =tl .load (in_ptr0 +(x2 +2 *ks2 *ks3 ),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tmp41 *tmp41 \n    tmp43 =tl .full (tmp42 .shape ,0.0 ,tmp42 .dtype )\n    tmp44 =tl .where (tmp40 ,tmp42 ,tmp43 )\n    tmp45 =tmp44 +tmp36 \n    tmp46 =0.2 \n    tmp47 =tmp45 *tmp46 \n    tl .store (out_ptr0 +(x2 ),tmp47 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_constant_pad_nd_div_mul_pow_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .load (in_ptr1 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp14 =0.0001 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =1.0 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =0.75 \n    tmp19 =libdevice .pow (tmp17 ,tmp18 )\n    tmp20 =tmp12 /tmp19 \n    tmp21 =tl .full (tmp20 .shape ,0.0 ,tmp20 .dtype )\n    tmp22 =tl .where (tmp11 ,tmp20 ,tmp21 )\n    tl .store (out_ptr0 +(x4 ),tmp22 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_constant_pad_nd_div_fractional_max_pool2d_le_mul_pow_scalar_tensor_where_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex //196 \n    x1 =((xindex //14 )%14 )\n    x0 =(xindex %14 )\n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr0 +(1 +2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =(2 +ks0 )/13 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =x1 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =tmp4 +tmp0 \n    tmp6 =tmp5 *tmp2 \n    tmp7 =libdevice .floor (tmp6 )\n    tmp8 =tmp0 *tmp2 \n    tmp9 =libdevice .floor (tmp8 )\n    tmp10 =tmp7 -tmp9 \n    tmp11 =tmp10 .to (tl .int64 )\n    tmp12 =tl .full ([1 ],13 ,tl .int64 )\n    tmp13 =tmp4 <tmp12 \n    tmp14 =2 +ks0 \n    tmp15 =tl .where (tmp13 ,tmp11 ,tmp14 )\n    tmp16 =ks1 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =tmp15 <0 \n    tmp19 =tl .where (tmp18 ,tmp17 ,tmp15 )\n    tl .device_assert (((0 <=tmp19 )&(tmp19 <4 +ks0 ))|~(xmask ),\"index out of bounds: 0 <= tmp19 < 4 + ks0\")\n    tmp22 =(2 +ks2 )/13 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =x0 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 +tmp21 \n    tmp27 =tmp26 *tmp23 \n    tmp28 =libdevice .floor (tmp27 )\n    tmp29 =tmp21 *tmp23 \n    tmp30 =libdevice .floor (tmp29 )\n    tmp31 =tmp28 -tmp30 \n    tmp32 =tmp31 .to (tl .int64 )\n    tmp33 =tmp25 <tmp12 \n    tmp34 =2 +ks2 \n    tmp35 =tl .where (tmp33 ,tmp32 ,tmp34 )\n    tmp36 =ks3 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =tmp35 <0 \n    tmp39 =tl .where (tmp38 ,tmp37 ,tmp35 )\n    tl .device_assert (((0 <=tmp39 )&(tmp39 <4 +ks2 ))|~(xmask ),\"index out of bounds: 0 <= tmp39 < 4 + ks2\")\n    tmp41 =tl .load (in_ptr1 +(tmp39 +4 *tmp19 +16 *x2 +ks2 *tmp19 +4 *ks0 *x2 +4 *ks2 *x2 +ks0 *ks2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp42 =tl .load (in_ptr1 +(1 +tmp39 +4 *tmp19 +16 *x2 +ks2 *tmp19 +4 *ks0 *x2 +4 *ks2 *x2 +ks0 *ks2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =triton_helpers .maximum (tmp42 ,tmp41 )\n    tmp44 =tl .load (in_ptr1 +(4 +ks2 +tmp39 +4 *tmp19 +16 *x2 +ks2 *tmp19 +4 *ks0 *x2 +4 *ks2 *x2 +ks0 *ks2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp45 =triton_helpers .maximum (tmp44 ,tmp43 )\n    tmp46 =tl .load (in_ptr1 +(5 +ks2 +tmp39 +4 *tmp19 +16 *x2 +ks2 *tmp19 +4 *ks0 *x2 +4 *ks2 *x2 +ks0 *ks2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp47 =triton_helpers .maximum (tmp46 ,tmp45 )\n    tmp48 =tl_math .abs (tmp47 )\n    tmp49 =0.5 \n    tmp50 =tmp48 <=tmp49 \n    tmp51 =0.0 \n    tmp52 =tl .where (tmp50 ,tmp51 ,tmp47 )\n    tl .store (in_out_ptr0 +(x3 ),tmp52 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool3d_constant_pad_nd_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool3d_constant_pad_nd_0 [grid (triton_poi_fused_avg_pool3d_constant_pad_nd_0_xnumel )](arg3_1 ,buf0 ,1024 ,3 ,32 ,32 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf1 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf3 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_add_constant_pad_nd_div_mul_pow_1_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_add_constant_pad_nd_div_mul_pow_1 [grid (triton_poi_fused_add_constant_pad_nd_div_mul_pow_1_xnumel )](arg3_1 ,buf0 ,buf3 ,36 ,36 ,32 ,32 ,1296 ,3888 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,s0 ,2 ),(2 *s0 ,2 ,1 ),torch .float32 )\n\n        triton_poi_fused_rand_2_xnumel =2 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_rand_2 [grid (triton_poi_fused_rand_2_xnumel )](buf1 ,buf2 ,0 ,6 ,XBLOCK =8 ,num_warps =1 ,num_stages =1 )\n        del buf1 \n        buf4 =empty_strided_cuda ((1 ,s0 ,14 ,14 ),(196 *s0 ,196 ,14 ,1 ),torch .float32 )\n        buf5 =buf4 ;del buf4 \n\n        triton_poi_fused_abs_add_constant_pad_nd_div_fractional_max_pool2d_le_mul_pow_scalar_tensor_where_3_xnumel =196 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_constant_pad_nd_div_fractional_max_pool2d_le_mul_pow_scalar_tensor_where_3 [grid (triton_poi_fused_abs_add_constant_pad_nd_div_fractional_max_pool2d_le_mul_pow_scalar_tensor_where_3_xnumel )](buf5 ,buf2 ,buf3 ,32 ,36 ,32 ,36 ,588 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        del buf3 \n    return (reinterpret_tensor (buf5 ,(1 ,1 ,1 ,s0 ,14 ,14 ),(196 *s0 ,196 *s0 ,196 *s0 ,196 ,14 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c1097028-469f-4295-b928-f4f339cc0c0f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['HingeEmbeddingLoss', 'Tanhshrink', 'ReplicationPad2d', 'FractionalMaxPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.replication_pad = nn.ReplicationPad2d(2)\n        self.fractional_max_pool = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.tanhshrink = nn.Tanhshrink()\n        self.hinge_loss = nn.HingeEmbeddingLoss()\n\n    def forward(self, x):\n        # Apply ReplicationPad2d\n        x = self.replication_pad(x)\n        \n        # Apply FractionalMaxPool2d\n        x = self.fractional_max_pool(x)\n        \n        # Apply Tanhshrink\n        x = self.tanhshrink(x)\n        \n        # Flatten the tensor for HingeEmbeddingLoss\n        x = x.view(x.size(0), -1)\n        \n        # Create a dummy target tensor for HingeEmbeddingLoss\n        target = torch.ones(x.size(0), dtype=torch.float32, device=x.device)\n        \n        # Apply HingeEmbeddingLoss\n        loss = self.hinge_loss(x, target)\n        \n        # Return the loss as the output\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 28, 28).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_fill_fractional_max_pool2d_mean_ne_replication_pad2d_sub_where_zeros_like_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp60 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index //196 \n        r0_1 =((r0_index //14 )%14 )\n        r0_0 =(r0_index %14 )\n        tmp0 =tl .load (in_ptr0 +(2 *r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp21 =tl .load (in_ptr0 +(1 +2 *r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =(2 +ks0 )/13 \n        tmp2 =tmp1 .to (tl .float32 )\n        tmp3 =r0_1 \n        tmp4 =tmp3 .to (tl .float32 )\n        tmp5 =tmp4 +tmp0 \n        tmp6 =tmp5 *tmp2 \n        tmp7 =libdevice .floor (tmp6 )\n        tmp8 =tmp0 *tmp2 \n        tmp9 =libdevice .floor (tmp8 )\n        tmp10 =tmp7 -tmp9 \n        tmp11 =tmp10 .to (tl .int64 )\n        tmp12 =tl .full ([1 ,1 ],13 ,tl .int64 )\n        tmp13 =tmp4 <tmp12 \n        tmp14 =2 +ks0 \n        tmp15 =tl .where (tmp13 ,tmp11 ,tmp14 )\n        tmp16 =4 +ks0 \n        tmp17 =tmp15 +tmp16 \n        tmp18 =tmp15 <0 \n        tmp19 =tl .where (tmp18 ,tmp17 ,tmp15 )\n        tl .device_assert (((0 <=tmp19 )&(tmp19 <4 +ks0 ))|~(r0_mask ),\"index out of bounds: 0 <= tmp19 < 4 + ks0\")\n        tmp22 =(2 +ks1 )/13 \n        tmp23 =tmp22 .to (tl .float32 )\n        tmp24 =r0_0 \n        tmp25 =tmp24 .to (tl .float32 )\n        tmp26 =tmp25 +tmp21 \n        tmp27 =tmp26 *tmp23 \n        tmp28 =libdevice .floor (tmp27 )\n        tmp29 =tmp21 *tmp23 \n        tmp30 =libdevice .floor (tmp29 )\n        tmp31 =tmp28 -tmp30 \n        tmp32 =tmp31 .to (tl .int64 )\n        tmp33 =tmp25 <tmp12 \n        tmp34 =2 +ks1 \n        tmp35 =tl .where (tmp33 ,tmp32 ,tmp34 )\n        tmp36 =4 +ks1 \n        tmp37 =tmp35 +tmp36 \n        tmp38 =tmp35 <0 \n        tmp39 =tl .where (tmp38 ,tmp37 ,tmp35 )\n        tl .device_assert (((0 <=tmp39 )&(tmp39 <4 +ks1 ))|~(r0_mask ),\"index out of bounds: 0 <= tmp39 < 4 + ks1\")\n        tmp41 =tl .load (in_ptr1 +(ks1 *(((-1 )+ks0 )*(((-1 )+ks0 )<=(((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 ))))<((-1 )+ks0 )))+ks0 *ks1 *r0_2 +(((-1 )+ks1 )*(((-1 )+ks1 )<=(((0 )*((0 )>=((-2 )+tmp39 ))+((-2 )+tmp39 )*(((-2 )+tmp39 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp39 ))+((-2 )+tmp39 )*(((-2 )+tmp39 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp39 ))+((-2 )+tmp39 )*(((-2 )+tmp39 )>(0 ))))<((-1 )+ks1 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp42 =tl .load (in_ptr1 +(ks1 *(((-1 )+ks0 )*(((-1 )+ks0 )<=(((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp19 ))+((-2 )+tmp19 )*(((-2 )+tmp19 )>(0 ))))<((-1 )+ks0 )))+ks0 *ks1 *r0_2 +(((-1 )+ks1 )*(((-1 )+ks1 )<=(((0 )*((0 )>=((-1 )+tmp39 ))+((-1 )+tmp39 )*(((-1 )+tmp39 )>(0 )))))+(((0 )*((0 )>=((-1 )+tmp39 ))+((-1 )+tmp39 )*(((-1 )+tmp39 )>(0 ))))*((((0 )*((0 )>=((-1 )+tmp39 ))+((-1 )+tmp39 )*(((-1 )+tmp39 )>(0 ))))<((-1 )+ks1 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp43 =triton_helpers .maximum (tmp42 ,tmp41 )\n        tmp44 =tl .load (in_ptr1 +(ks1 *(((-1 )+ks0 )*(((-1 )+ks0 )<=(((0 )*((0 )>=((-1 )+tmp19 ))+((-1 )+tmp19 )*(((-1 )+tmp19 )>(0 )))))+(((0 )*((0 )>=((-1 )+tmp19 ))+((-1 )+tmp19 )*(((-1 )+tmp19 )>(0 ))))*((((0 )*((0 )>=((-1 )+tmp19 ))+((-1 )+tmp19 )*(((-1 )+tmp19 )>(0 ))))<((-1 )+ks0 )))+ks0 *ks1 *r0_2 +(((-1 )+ks1 )*(((-1 )+ks1 )<=(((0 )*((0 )>=((-2 )+tmp39 ))+((-2 )+tmp39 )*(((-2 )+tmp39 )>(0 )))))+(((0 )*((0 )>=((-2 )+tmp39 ))+((-2 )+tmp39 )*(((-2 )+tmp39 )>(0 ))))*((((0 )*((0 )>=((-2 )+tmp39 ))+((-2 )+tmp39 )*(((-2 )+tmp39 )>(0 ))))<((-1 )+ks1 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp45 =triton_helpers .maximum (tmp44 ,tmp43 )\n        tmp46 =tl .load (in_ptr1 +(ks1 *(((-1 )+ks0 )*(((-1 )+ks0 )<=(((0 )*((0 )>=((-1 )+tmp19 ))+((-1 )+tmp19 )*(((-1 )+tmp19 )>(0 )))))+(((0 )*((0 )>=((-1 )+tmp19 ))+((-1 )+tmp19 )*(((-1 )+tmp19 )>(0 ))))*((((0 )*((0 )>=((-1 )+tmp19 ))+((-1 )+tmp19 )*(((-1 )+tmp19 )>(0 ))))<((-1 )+ks0 )))+ks0 *ks1 *r0_2 +(((-1 )+ks1 )*(((-1 )+ks1 )<=(((0 )*((0 )>=((-1 )+tmp39 ))+((-1 )+tmp39 )*(((-1 )+tmp39 )>(0 )))))+(((0 )*((0 )>=((-1 )+tmp39 ))+((-1 )+tmp39 )*(((-1 )+tmp39 )>(0 ))))*((((0 )*((0 )>=((-1 )+tmp39 ))+((-1 )+tmp39 )*(((-1 )+tmp39 )>(0 ))))<((-1 )+ks1 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp47 =triton_helpers .maximum (tmp46 ,tmp45 )\n        tmp48 =libdevice .tanh (tmp47 )\n        tmp49 =tmp47 -tmp48 \n        tmp50 =1.0 \n        tmp51 =tmp50 -tmp49 \n        tmp52 =0.0 \n        tmp53 =triton_helpers .maximum (tmp51 ,tmp52 )\n        tmp54 =tl .full ([1 ,1 ],False ,tl .int1 )\n        tmp55 =tl .where (tmp54 ,tmp53 ,tmp52 )\n        tmp56 =tl .full ([1 ,1 ],True ,tl .int1 )\n        tmp57 =tl .where (tmp56 ,tmp49 ,tmp52 )\n        tmp58 =tmp55 +tmp57 \n        tmp59 =tl .broadcast_to (tmp58 ,[XBLOCK ,R0_BLOCK ])\n        tmp61 =_tmp60 +tmp59 \n        _tmp60 =tl .where (r0_mask ,tmp61 ,_tmp60 )\n    tmp60 =tl .sum (_tmp60 ,1 )[:,None ]\n    tmp62 =196 *ks2 \n    tmp63 =tmp62 .to (tl .float32 )\n    tmp64 =tmp60 /tmp63 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp64 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,2 ),(2 *s0 ,2 ,1 ),torch .float32 )\n\n        triton_poi_fused_rand_0_xnumel =2 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (triton_poi_fused_rand_0_xnumel )](buf0 ,buf1 ,0 ,6 ,XBLOCK =8 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf3 =empty_strided_cuda ((),(),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        196 *s0 \n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_fill_fractional_max_pool2d_mean_ne_replication_pad2d_sub_where_zeros_like_1 [grid (1 )](buf4 ,buf1 ,arg3_1 ,28 ,28 ,3 ,1 ,588 ,XBLOCK =1 ,R0_BLOCK =1024 ,num_warps =8 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =28 \n    arg2_1 =28 \n    arg3_1 =rand_strided ((1 ,3 ,28 ,28 ),(2352 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c188e188-4c9b-4234-943c-4abe22b65a1b",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MultiLabelSoftMarginLoss', 'MultiLabelMarginLoss', 'FractionalMaxPool2d', 'SyncBatchNorm', 'CrossMapLRN2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.fractional_max_pool = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.sync_batch_norm = nn.SyncBatchNorm(64)\n        self.cross_map_lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.multi_label_soft_margin_loss = nn.MultiLabelSoftMarginLoss()\n        self.multi_label_margin_loss = nn.MultiLabelMarginLoss()\n\n    def forward(self, x):\n        # Apply FractionalMaxPool2d\n        x = self.fractional_max_pool(x)\n        \n        # Apply SyncBatchNorm\n        x = self.sync_batch_norm(x)\n        \n        # Apply CrossMapLRN2d\n        x = self.cross_map_lrn(x)\n        \n        # Reshape for loss functions\n        x = x.view(x.size(0), -1)\n        \n        # Dummy target for loss functions (assuming binary classification)\n        target = torch.randint(0, 2, (x.size(0), x.size(1))).float()\n        \n        # Apply MultiLabelSoftMarginLoss\n        loss1 = self.multi_label_soft_margin_loss(x, target)\n        \n        # Apply MultiLabelMarginLoss\n        loss2 = self.multi_label_margin_loss(x, target.long())\n        \n        # Return the average of the two losses\n        return (loss1 + loss2) / 2\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 28, 28).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_functional_fractional_max_pool2d_native_batch_norm_backward_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,out_ptr0 ,out_ptr3 ,out_ptr4 ,out_ptr5 ,out_ptr7 ,out_ptr9 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =64 \n    r0_numel =196 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp49_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp49_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp49_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index //14 \n        r0_1 =(r0_index %14 )\n        r0_3 =r0_index \n        tmp1 =((-2 )+ks0 )/13 \n        tmp2 =tmp1 .to (tl .float32 )\n        tmp3 =r0_2 \n        tmp4 =tmp3 .to (tl .float32 )\n        tmp5 =tmp4 +tmp0 \n        tmp6 =tmp5 *tmp2 \n        tmp7 =libdevice .floor (tmp6 )\n        tmp8 =tmp0 *tmp2 \n        tmp9 =libdevice .floor (tmp8 )\n        tmp10 =tmp7 -tmp9 \n        tmp11 =tmp10 .to (tl .int64 )\n        tmp12 =tl .full ([1 ,1 ],13 ,tl .int64 )\n        tmp13 =tmp4 <tmp12 \n        tmp14 =(-2 )+ks0 \n        tmp15 =tl .where (tmp13 ,tmp11 ,tmp14 )\n        tmp16 =ks0 \n        tmp17 =tmp15 +tmp16 \n        tmp18 =tmp15 <0 \n        tmp19 =tl .where (tmp18 ,tmp17 ,tmp15 )\n        tl .device_assert (((0 <=tmp19 )&(tmp19 <ks0 ))|~(r0_mask &xmask ),\"index out of bounds: 0 <= tmp19 < ks0\")\n        tmp22 =((-2 )+ks1 )/13 \n        tmp23 =tmp22 .to (tl .float32 )\n        tmp24 =r0_1 \n        tmp25 =tmp24 .to (tl .float32 )\n        tmp26 =tmp25 +tmp21 \n        tmp27 =tmp26 *tmp23 \n        tmp28 =libdevice .floor (tmp27 )\n        tmp29 =tmp21 *tmp23 \n        tmp30 =libdevice .floor (tmp29 )\n        tmp31 =tmp28 -tmp30 \n        tmp32 =tmp31 .to (tl .int64 )\n        tmp33 =tmp25 <tmp12 \n        tmp34 =(-2 )+ks1 \n        tmp35 =tl .where (tmp33 ,tmp32 ,tmp34 )\n        tmp36 =ks1 \n        tmp37 =tmp35 +tmp36 \n        tmp38 =tmp35 <0 \n        tmp39 =tl .where (tmp38 ,tmp37 ,tmp35 )\n        tl .device_assert (((0 <=tmp39 )&(tmp39 <ks1 ))|~(r0_mask &xmask ),\"index out of bounds: 0 <= tmp39 < ks1\")\n        tmp41 =tl .load (in_ptr1 +(tmp39 +ks1 *tmp19 +ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp42 =tl .load (in_ptr1 +(1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp43 =triton_helpers .maximum (tmp42 ,tmp41 )\n        tmp44 =tl .load (in_ptr1 +(ks1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp45 =triton_helpers .maximum (tmp44 ,tmp43 )\n        tmp46 =tl .load (in_ptr1 +(1 +ks1 +tmp39 +ks1 *tmp19 +ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp47 =triton_helpers .maximum (tmp46 ,tmp45 )\n        tmp48 =tl .broadcast_to (tmp47 ,[XBLOCK ,R0_BLOCK ])\n        tmp49_mean_next ,tmp49_m2_next ,tmp49_weight_next =triton_helpers .welford_reduce (\n        tmp48 ,tmp49_mean ,tmp49_m2 ,tmp49_weight ,roffset ==0 \n        )\n        tmp49_mean =tl .where (r0_mask &xmask ,tmp49_mean_next ,tmp49_mean )\n        tmp49_m2 =tl .where (r0_mask &xmask ,tmp49_m2_next ,tmp49_m2 )\n        tmp49_weight =tl .where (r0_mask &xmask ,tmp49_weight_next ,tmp49_weight )\n        tl .store (out_ptr0 +(r0_3 +196 *x0 ),tmp47 ,r0_mask &xmask )\n    tmp52 ,tmp53 ,tmp54 =triton_helpers .welford (tmp49_mean ,tmp49_m2 ,tmp49_weight ,1 )\n    tmp49 =tmp52 [:,None ]\n    tmp50 =tmp53 [:,None ]\n    tmp51 =tmp54 [:,None ]\n    tmp63 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp65 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_3 =r0_index \n        tmp55 =tl .load (out_ptr0 +(r0_3 +196 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp56 =tmp55 -tmp49 \n        tmp57 =196.0 \n        tmp58 =tmp50 /tmp57 \n        tmp59 =1e-05 \n        tmp60 =tmp58 +tmp59 \n        tmp61 =libdevice .rsqrt (tmp60 )\n        tmp62 =tmp56 *tmp61 \n        tmp64 =tmp62 *tmp63 \n        tmp66 =tmp64 +tmp65 \n        tl .store (out_ptr3 +(r0_3 +196 *x0 ),tmp66 ,r0_mask &xmask )\n        tl .store (out_ptr4 +(r0_3 +196 *x0 ),tmp56 ,r0_mask &xmask )\n    tmp76 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp81 =tl .load (in_ptr5 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp67 =196.0 \n    tmp68 =tmp50 /tmp67 \n    tmp69 =1e-05 \n    tmp70 =tmp68 +tmp69 \n    tmp71 =libdevice .rsqrt (tmp70 )\n    tmp72 =1.005128205128205 \n    tmp73 =tmp68 *tmp72 \n    tmp74 =0.1 \n    tmp75 =tmp73 *tmp74 \n    tmp77 =0.9 \n    tmp78 =tmp76 *tmp77 \n    tmp79 =tmp75 +tmp78 \n    tmp80 =tmp49 *tmp74 \n    tmp82 =tmp81 *tmp77 \n    tmp83 =tmp80 +tmp82 \n    tl .store (out_ptr5 +(x0 ),tmp71 ,xmask )\n    tl .store (out_ptr7 +(x0 ),tmp79 ,xmask )\n    tl .store (out_ptr9 +(x0 ),tmp83 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_2 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .full ([1 ],1 ,tl .int64 )\n    tmp3 =tmp1 +tmp2 \n    tl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 =args \n    args .clear ()\n    s1 =primals_1 \n    s2 =primals_2 \n    assert_size_stride (primals_3 ,(1 ,64 ,s1 ,s2 ),(64 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    assert_size_stride (primals_4 ,(),())\n    assert_size_stride (primals_5 ,(64 ,),(1 ,))\n    assert_size_stride (primals_6 ,(64 ,),(1 ,))\n    assert_size_stride (primals_7 ,(64 ,),(1 ,))\n    assert_size_stride (primals_8 ,(64 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,64 ,2 ),(128 ,2 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_0 [grid (128 )](buf0 ,buf1 ,0 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,64 ,14 ,14 ),(12544 ,196 ,14 ,1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,64 ,14 ,14 ),(12544 ,196 ,14 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,64 ,14 ,14 ),(12544 ,196 ,14 ,1 ),torch .float32 )\n        buf6 =empty_strided_cuda ((1 ,64 ,1 ,1 ),(64 ,1 ,64 ,64 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_functional_fractional_max_pool2d_native_batch_norm_backward_1 [grid (64 )](buf1 ,primals_3 ,primals_7 ,primals_8 ,primals_6 ,primals_5 ,buf2 ,buf7 ,buf8 ,buf6 ,primals_6 ,primals_5 ,28 ,28 ,64 ,196 ,XBLOCK =1 ,R0_BLOCK =256 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n        del buf2 \n        del primals_3 \n        del primals_5 \n        del primals_6 \n        del primals_7 \n        del primals_8 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_2 [grid (1 )](primals_4 ,primals_4 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_4 \n    return (buf7 ,reinterpret_tensor (buf6 ,(64 ,),(1 ,),0 ),buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =28 \n    primals_2 =28 \n    primals_3 =rand_strided ((1 ,64 ,28 ,28 ),(50176 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_5 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c18d1d0e-13eb-4afb-9b0f-bc2867b0f42a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PoissonNLLLoss', 'ReflectionPad1d', 'CosineSimilarity']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reflection_pad1 = nn.ReflectionPad1d(2)\n        self.reflection_pad2 = nn.ReflectionPad1d(3)\n        self.cosine_sim = nn.CosineSimilarity(dim=1)\n        self.loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Apply ReflectionPad1d twice\n        x = self.reflection_pad1(x)\n        x = self.reflection_pad2(x)\n        \n        # Reshape the input to have two tensors for CosineSimilarity\n        x1 = x[:, :x.shape[1]//2, :]\n        x2 = x[:, x.shape[1]//2:, :]\n        \n        # Compute cosine similarity\n        x = self.cosine_sim(x1, x2)\n        \n        # Compute PoissonNLLLoss (requires target, but for simplicity, we use x as both input and target)\n        loss = self.loss(x, x)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 20).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_linalg_vector_norm_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp3 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(ks1 *r0_1 +ks1 *(ks0 //2 )+(tl .where ((-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-2 )+(tl .where (3 +ks1 +((-1 )*tl_math .abs (3 +ks1 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks1 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks1 ,3 +ks1 +((-1 )*tl_math .abs (3 +ks1 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-2 )+(tl .where (3 +ks1 +((-1 )*tl_math .abs (3 +ks1 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks1 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks1 ,3 +ks1 +((-1 )*tl_math .abs (3 +ks1 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))+2 *ks1 ,(-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-2 )+(tl .where (3 +ks1 +((-1 )*tl_math .abs (3 +ks1 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks1 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks1 ,3 +ks1 +((-1 )*tl_math .abs (3 +ks1 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tmp0 *tmp0 \n        tmp2 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n        tmp4 =_tmp3 +tmp2 \n        _tmp3 =tl .where (r0_mask &xmask ,tmp4 ,_tmp3 )\n    tmp3 =tl .sum (_tmp3 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp3 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_clamp_min_div_linalg_vector_norm_mul_sum_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp3 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(ks0 *r0_1 +(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+(tl .where (3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks0 ,3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+(tl .where (3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks0 ,3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+(tl .where (3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks0 ,3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tmp0 *tmp0 \n        tmp2 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n        tmp4 =_tmp3 +tmp2 \n        _tmp3 =tl .where (r0_mask &xmask ,tmp4 ,_tmp3 )\n    tmp3 =tl .sum (_tmp3 ,1 )[:,None ]\n    tmp11 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    _tmp17 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp5 =tl .load (in_ptr0 +(ks0 *r0_1 +(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+(tl .where (3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks0 ,3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+(tl .where (3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks0 ,3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+(tl .where (3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks0 ,3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp10 =tl .load (in_ptr0 +(ks0 *r0_1 +ks0 *(ks1 //2 )+(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+(tl .where (3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks0 ,3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+(tl .where (3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks0 ,3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-2 )+(tl .where (3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))<0 ,7 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 ))))+2 *ks0 ,3 +ks0 +((-1 )*tl_math .abs (3 +ks0 +((-1 )*tl_math .abs ((-3 )+x0 )))))))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp6 =libdevice .sqrt (tmp3 )\n        tmp7 =1e-08 \n        tmp8 =triton_helpers .maximum (tmp6 ,tmp7 )\n        tmp9 =tmp5 /tmp8 \n        tmp12 =libdevice .sqrt (tmp11 )\n        tmp13 =triton_helpers .maximum (tmp12 ,tmp7 )\n        tmp14 =tmp10 /tmp13 \n        tmp15 =tmp9 *tmp14 \n        tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ,R0_BLOCK ])\n        tmp18 =_tmp17 +tmp16 \n        _tmp17 =tl .where (r0_mask &xmask ,tmp18 ,_tmp17 )\n    tmp17 =tl .sum (_tmp17 ,1 )[:,None ]\n    tl .store (in_out_ptr0 +(x0 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_exp_mean_mul_sub_2 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp5 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl_math .exp (tmp0 )\n        tmp2 =tmp0 *tmp0 \n        tmp3 =tmp1 -tmp2 \n        tmp4 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n        tmp6 =_tmp5 +tmp4 \n        _tmp5 =tl .where (r0_mask ,tmp6 ,_tmp5 )\n    tmp5 =tl .sum (_tmp5 ,1 )[:,None ]\n    tmp7 =10 +ks0 \n    tmp8 =tmp7 .to (tl .float32 )\n    tmp9 =tmp5 /tmp8 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp9 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,1 ,10 +s1 ),(10 +s1 ,10 +s1 ,1 ),torch .float32 )\n\n        triton_red_fused_linalg_vector_norm_0_xnumel =10 +s1 \n        s0 +((-1 )*(s0 //2 ))\n        get_raw_stream (0 )\n        triton_red_fused_linalg_vector_norm_0 [grid (triton_red_fused_linalg_vector_norm_0_xnumel )](arg2_1 ,buf1 ,10 ,20 ,30 ,5 ,XBLOCK =32 ,R0_BLOCK =8 ,num_warps =2 ,num_stages =1 )\n        buf0 =empty_strided_cuda ((1 ,1 ,10 +s1 ),(10 +s1 ,10 +s1 ,1 ),torch .float32 )\n        buf2 =reinterpret_tensor (buf0 ,(1 ,10 +s1 ),(10 +s1 ,1 ),0 );del buf0 \n\n        triton_red_fused_clamp_min_div_linalg_vector_norm_mul_sum_1_xnumel =10 +s1 \n        s0 //2 \n        get_raw_stream (0 )\n        triton_red_fused_clamp_min_div_linalg_vector_norm_mul_sum_1 [grid (triton_red_fused_clamp_min_div_linalg_vector_norm_mul_sum_1_xnumel )](buf2 ,arg2_1 ,buf1 ,20 ,10 ,30 ,5 ,XBLOCK =1 ,R0_BLOCK =8 ,num_warps =2 ,num_stages =1 )\n        del arg2_1 \n        del buf1 \n        buf3 =empty_strided_cuda ((),(),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        10 +s1 \n        get_raw_stream (0 )\n        triton_red_fused_exp_mean_mul_sub_2 [grid (1 )](buf4 ,buf2 ,20 ,1 ,30 ,XBLOCK =1 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n        del buf2 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =20 \n    arg2_1 =rand_strided ((1 ,10 ,20 ),(200 ,20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c2185191-15da-4602-83f9-20ef9970abd9",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['HingeEmbeddingLoss', 'ZeroPad2d', 'BCELoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1 = nn.ZeroPad2d(2)\n        self.pad2 = nn.ZeroPad2d(1)\n        self.hinge_loss = nn.HingeEmbeddingLoss()\n        self.bce_loss = nn.BCELoss()\n\n    def forward(self, x):\n        # Apply ZeroPad2d layers\n        x = self.pad1(x)\n        x = self.pad2(x)\n        \n        # Flatten the input for loss computation\n        x_flat = x.view(-1)\n        \n        # Create a dummy target tensor for loss computation\n        target = torch.ones_like(x_flat)\n        \n        # Compute HingeEmbeddingLoss\n        hinge_loss = self.hinge_loss(x_flat, target)\n        \n        # Compute BCELoss\n        bce_loss = self.bce_loss(torch.sigmoid(x_flat), target)\n        \n        # Return the sum of the losses as the output\n        return hinge_loss + bce_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_binary_cross_entropy_clamp_min_fill_mean_ne_ones_like_sigmoid_sub_where_zeros_like_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp39 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp52 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =1.0 \n        tmp1 =tmp0 !=tmp0 \n        tmp2 =(-1 )+(((r0_0 //(6 +ks1 ))%(6 +ks0 )))\n        tmp3 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp4 =tmp2 >=tmp3 \n        tmp5 =4 +ks0 \n        tmp6 =tmp2 <tmp5 \n        tmp7 =(-1 )+((r0_0 %(6 +ks1 )))\n        tmp8 =tmp7 >=tmp3 \n        tmp9 =4 +ks1 \n        tmp10 =tmp7 <tmp9 \n        tmp11 =tmp4 &tmp6 \n        tmp12 =tmp11 &tmp8 \n        tmp13 =tmp12 &tmp10 \n        tmp14 =tl .broadcast_to ((-3 )+(((r0_0 //(6 +ks1 ))%(6 +ks0 ))),[XBLOCK ,R0_BLOCK ])\n        tmp15 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp16 =tmp14 >=tmp15 \n        tmp17 =tl .broadcast_to (ks0 ,[XBLOCK ,R0_BLOCK ])\n        tmp18 =tmp14 <tmp17 \n        tmp19 =tl .broadcast_to ((-3 )+((r0_0 %(6 +ks1 ))),[XBLOCK ,R0_BLOCK ])\n        tmp20 =tmp19 >=tmp15 \n        tmp21 =tl .broadcast_to (ks1 ,[XBLOCK ,R0_BLOCK ])\n        tmp22 =tmp19 <tmp21 \n        tmp23 =tmp16 &tmp18 \n        tmp24 =tmp23 &tmp20 \n        tmp25 =tmp24 &tmp22 \n        tmp26 =tmp25 &tmp13 \n        tmp27 =tl .load (in_ptr0 +(tl .broadcast_to ((-3 )+((-3 )*ks1 )+ks1 *(((r0_0 //(6 +ks1 ))%(6 +ks0 )))+ks0 *ks1 *(r0_0 //(36 +6 *ks0 +6 *ks1 +ks0 *ks1 ))+((r0_0 %(6 +ks1 ))),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp26 ,eviction_policy ='evict_last',other =0.0 )\n        tmp28 =tl .full (tmp27 .shape ,0.0 ,tmp27 .dtype )\n        tmp29 =tl .where (tmp13 ,tmp27 ,tmp28 )\n        tmp30 =tmp0 -tmp29 \n        tmp31 =0.0 \n        tmp32 =triton_helpers .maximum (tmp30 ,tmp31 )\n        tmp33 =tl .where (tmp1 ,tmp32 ,tmp31 )\n        tmp34 =-1.0 \n        tmp35 =tmp0 !=tmp34 \n        tmp36 =tl .where (tmp35 ,tmp29 ,tmp31 )\n        tmp37 =tmp33 +tmp36 \n        tmp38 =tl .broadcast_to (tmp37 ,[XBLOCK ,R0_BLOCK ])\n        tmp40 =_tmp39 +tmp38 \n        _tmp39 =tl .where (r0_mask ,tmp40 ,_tmp39 )\n        tmp41 =tl .sigmoid (tmp29 )\n        tmp42 =-tmp41 \n        tmp43 =libdevice .log1p (tmp42 )\n        tmp44 =-100.0 \n        tmp45 =triton_helpers .maximum (tmp43 ,tmp44 )\n        tmp46 =tmp31 *tmp45 \n        tmp47 =tl_math .log (tmp41 )\n        tmp48 =triton_helpers .maximum (tmp47 ,tmp44 )\n        tmp49 =tmp0 *tmp48 \n        tmp50 =tmp46 -tmp49 \n        tmp51 =tl .broadcast_to (tmp50 ,[XBLOCK ,R0_BLOCK ])\n        tmp53 =_tmp52 +tmp51 \n        _tmp52 =tl .where (r0_mask ,tmp53 ,_tmp52 )\n    tmp39 =tl .sum (_tmp39 ,1 )[:,None ]\n    tmp52 =tl .sum (_tmp52 ,1 )[:,None ]\n    tmp54 =36 *ks2 +6 *ks0 *ks2 +6 *ks1 *ks2 +ks0 *ks1 *ks2 \n    tmp55 =tmp54 .to (tl .float32 )\n    tmp56 =tmp39 /tmp55 \n    tmp57 =tmp52 /tmp55 \n    tmp58 =tmp56 +tmp57 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp58 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((),(),torch .float32 )\n        buf2 =buf0 ;del buf0 \n\n        36 *s0 +6 *s0 *s1 +6 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused_add_binary_cross_entropy_clamp_min_fill_mean_ne_ones_like_sigmoid_sub_where_zeros_like_0 [grid (1 )](buf2 ,arg3_1 ,32 ,32 ,3 ,1 ,4332 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg3_1 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c2a9cf83-f713-44ec-8393-f1ed522cc651",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Unfold', 'AdaptiveAvgPool1d', 'LSTM', 'AvgPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.unfold = nn.Unfold(kernel_size=(3, 3), stride=(1, 1))\n        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=10)\n        self.lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.avg_pool1d = nn.AvgPool1d(kernel_size=2, stride=2)\n        self.fc = nn.Linear(20, 10)\n\n    def forward(self, x):\n        # Unfold the input to extract patches\n        x = self.unfold(x)\n        \n        # Reshape to fit AdaptiveAvgPool1d\n        x = x.view(x.size(0), x.size(1), -1)\n        \n        # Apply AdaptiveAvgPool1d\n        x = self.adaptive_avg_pool1d(x)\n        \n        # Reshape for LSTM\n        x = x.permute(0, 2, 1)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Apply AvgPool1d\n        x = x.permute(0, 2, 1)\n        x = self.avg_pool1d(x)\n        \n        # Reshape for fully connected layer\n        x = x.view(x.size(0), -1)\n        \n        # Apply fully connected layer\n        x = self.fc(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_im2col_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ynumel ,xnumel ,YBLOCK :tl .constexpr ,XBLOCK :tl .constexpr ):\n    yoffset =(tl .program_id (1 )+tl .program_id (2 )*tl .num_programs (1 ))*YBLOCK \n    yindex =yoffset +tl .arange (0 ,YBLOCK )[None ,:]\n    ymask =yindex <ynumel \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    x4 =xindex //ks0 \n    y1 =((yindex //3 )%3 )\n    x3 =(xindex %ks0 )\n    y0 =(yindex %3 )\n    y2 =yindex //9 \n    x7 =xindex \n    y6 =yindex \n    tl .device_assert ((x4 +y1 <ks1 )|~(xmask &ymask ),\"index out of bounds: x4 + y1 < ks1\")\n    tl .device_assert ((x3 +y0 <ks2 )|~(xmask &ymask ),\"index out of bounds: x3 + y0 < ks2\")\n    tmp2 =tl .load (in_ptr0 +(x3 +y0 +ks2 *x4 +ks2 *y1 +ks1 *ks2 *y2 ),xmask &ymask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x7 +4 *y6 +((-2 )*ks1 *y6 )+((-2 )*ks2 *y6 )+ks1 *ks2 *y6 ),tmp2 ,xmask &ymask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(((-2 )*(x0 //ks1 ))+4 *x1 +ks3 *(x0 //ks1 )+((-2 )*ks2 *x1 )+((-2 )*ks3 *x1 )+ks2 *ks3 *x1 +((x0 %ks1 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        (-2 )+s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,3 ,3 ,(-2 )+s1 ,(-2 )+s2 ),(36 *s0 +((-18 )*s0 *s1 )+((-18 )*s0 *s2 )+9 *s0 *s1 *s2 ,36 +((-18 )*s1 )+((-18 )*s2 )+9 *s1 *s2 ,12 +((-6 )*s1 )+((-6 )*s2 )+3 *s1 *s2 ,4 +((-2 )*s1 )+((-2 )*s2 )+s1 *s2 ,(-2 )+s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_im2col_0_ynumel =9 *s0 \n        triton_poi_fused_im2col_0_xnumel =4 +((-2 )*s1 )+((-2 )*s2 )+s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_im2col_0 [grid (triton_poi_fused_im2col_0_ynumel ,triton_poi_fused_im2col_0_xnumel )](arg3_1 ,buf0 ,30 ,32 ,32 ,27 ,900 ,XBLOCK =256 ,YBLOCK =1 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        4 +((-2 )*s1 )+((-2 )*s2 )+s1 *s2 \n        buf1 =empty_strided_cuda ((1 ,9 *s0 ,1 ,4 +((-2 )*s1 )+((-2 )*s2 )+s1 *s2 ),(36 *s0 +((-18 )*s0 *s1 )+((-18 )*s0 *s2 )+9 *s0 *s1 *s2 ,4 +((-2 )*s1 )+((-2 )*s2 )+s1 *s2 ,36 *s0 +((-18 )*s0 *s1 )+((-18 )*s0 *s2 )+9 *s0 *s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__adaptive_avg_pool2d_1_xnumel =36 *s0 +((-18 )*s0 *s1 )+((-18 )*s0 *s2 )+9 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_1 [grid (triton_poi_fused__adaptive_avg_pool2d_1_xnumel )](buf0 ,buf1 ,900 ,30 ,32 ,32 ,24300 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n\n        buf2 =torch .ops .aten .avg_pool2d .default (buf1 ,[1 ,90 ],[1 ,90 ],[0 ,0 ],False ,True ,None )\n        del buf1 \n        buf3 =buf2 \n        del buf2 \n    return (reinterpret_tensor (buf3 ,(1 ,1 +(((-86 )+((-2 )*s1 )+((-2 )*s2 )+s1 *s2 )//90 ),9 *s0 ),(9 *s0 +9 *s0 *(((-86 )+((-2 )*s1 )+((-2 )*s2 )+s1 *s2 )//90 ),1 ,1 +(((-86 )+((-2 )*s1 )+((-2 )*s2 )+s1 *s2 )//90 )),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c3b68e9e-467d-46fa-bba2-9b0832182cdd",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Upsample', 'TripletMarginLoss', 'LSTM', 'MarginRankingLoss', 'MaxPool2d', 'Unflatten', 'LazyConvTranspose3d', 'MultiLabelMarginLoss', 'ZeroPad3d', 'AvgPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(16, 8, 8))\n        self.lazy_conv_transpose3d = nn.LazyConvTranspose3d(out_channels=32, kernel_size=3, stride=2)\n        self.zero_pad3d = nn.ZeroPad3d(padding=1)\n        self.avgpool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.triplet_margin_loss = nn.TripletMarginLoss()\n        self.margin_ranking_loss = nn.MarginRankingLoss()\n        self.multi_label_margin_loss = nn.MultiLabelMarginLoss()\n\n    def forward(self, x):\n        # Upsample the input\n        x = self.upsample(x)\n        \n        # Reshape for LSTM\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1).transpose(1, 2)\n        \n        # Pass through LSTM\n        x, _ = self.lstm(x)\n        \n        # Reshape back to 4D\n        x = x.transpose(1, 2).view(batch_size, -1, height, width)\n        \n        # Apply MaxPool2d\n        x = self.maxpool2d(x)\n        \n        # Unflatten\n        x = self.unflatten(x)\n        \n        # Reshape for ConvTranspose3d\n        x = x.unsqueeze(2)  # Add a dummy dimension for 3D convolution\n        x = self.lazy_conv_transpose3d(x)\n        \n        # ZeroPad3d\n        x = self.zero_pad3d(x)\n        \n        # Reshape back to 4D\n        x = x.squeeze(2)\n        \n        # Apply AvgPool2d\n        x = self.avgpool2d(x)\n        \n        # Compute losses (dummy example)\n        anchor = x[:, 0, :, :].unsqueeze(1)\n        positive = x[:, 1, :, :].unsqueeze(1)\n        negative = x[:, 2, :, :].unsqueeze(1)\n        triplet_loss = self.triplet_margin_loss(anchor, positive, negative)\n        \n        input1 = x[:, 0, :, :].unsqueeze(1)\n        input2 = x[:, 1, :, :].unsqueeze(1)\n        target = torch.ones_like(input1)\n        margin_ranking_loss = self.margin_ranking_loss(input1, input2, target)\n        \n        multi_label_margin_loss = self.multi_label_margin_loss(x.view(batch_size, -1), torch.randint(0, 2, (batch_size, x.size(1))))\n        \n        # Return the final output and losses\n        return x, triplet_loss, margin_ranking_loss, multi_label_margin_loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__unsafe_index_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .full ([1 ],2.0 ,tl .float64 )\n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float64 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tmp2 /tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =x1 \n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp7 *tmp5 \n    tmp9 =tmp8 .to (tl .int64 )\n    tmp10 =tmp9 +tmp1 \n    tmp11 =tmp9 <0 \n    tmp12 =tl .where (tmp11 ,tmp10 ,tmp9 )\n    tmp13 =ks3 \n    tmp14 =tmp13 .to (tl .float64 )\n    tmp15 =tmp0 *tmp14 \n    tmp16 =tmp14 /tmp15 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =x0 \n    tmp19 =tmp18 .to (tl .float32 )\n    tmp20 =tmp19 *tmp17 \n    tmp21 =tmp20 .to (tl .int64 )\n    tmp22 =tmp21 +tmp13 \n    tmp23 =tmp21 <0 \n    tmp24 =tl .where (tmp23 ,tmp22 ,tmp21 )\n    tmp25 =tl .load (in_ptr0 +(tmp24 +ks3 *tmp12 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x4 ),tmp25 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 *s2 \n        2 *s1 \n        4 *s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,2 *s1 ,2 *s2 ),(4 *s0 *s1 *s2 ,4 *s1 *s2 ,2 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__unsafe_index_0_xnumel =4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__unsafe_index_0 [grid (triton_poi_fused__unsafe_index_0_xnumel )](arg3_1 ,buf0 ,32 ,64 ,64 ,32 ,4096 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,4 *s1 *s2 ,s0 ),(4 *s0 *s1 *s2 ,1 ,4 *s1 *s2 ),0 ),2 *s1 ,2 *s2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c60840a8-1ece-4741-a9a3-8b0562100584",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ZeroPad3d', 'GaussianNLLLoss', 'Tanhshrink', 'UpsamplingBilinear2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad = nn.ZeroPad3d(padding=1)\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.tanhshrink = nn.Tanhshrink()\n        self.gaussian_nll_loss = nn.GaussianNLLLoss()\n\n    def forward(self, x):\n        # Apply ZeroPad3d to the input\n        x = self.zero_pad(x)\n        \n        # Reshape the tensor to fit the UpsamplingBilinear2d input requirements\n        # Assuming the input is 5D (batch, channels, depth, height, width), we need to convert it to 4D (batch, channels, height, width)\n        # by selecting a specific depth slice or averaging over depth\n        x = x.mean(dim=2)  # Average over the depth dimension to get a 4D tensor\n        \n        # Apply UpsamplingBilinear2d\n        x = self.upsample(x)\n        \n        # Apply Tanhshrink\n        x = self.tanhshrink(x)\n        \n        # Reshape the tensor to fit the GaussianNLLLoss input requirements\n        # GaussianNLLLoss expects input of shape (N, *) and target of shape (N, *)\n        # For simplicity, we'll assume the target is a tensor of zeros with the same shape as x\n        target = torch.zeros_like(x)\n        \n        # Apply GaussianNLLLoss\n        loss = self.gaussian_nll_loss(x, target, torch.ones_like(x))\n        \n        # Return the loss as the output (since GaussianNLLLoss returns a scalar)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 16, 32, 32).cuda()  # Example input: (batch, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_constant_pad_nd_mean_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks5 \n    _tmp20 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    x4 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_3 =r0_index \n        tmp0 =(-1 )+r0_3 \n        tmp1 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp2 =tmp0 >=tmp1 \n        tmp3 =ks0 \n        tmp4 =tmp0 <tmp3 \n        tmp5 =(-1 )+x1 \n        tmp6 =tmp5 >=tmp1 \n        tmp7 =ks3 \n        tmp8 =tmp5 <tmp7 \n        tmp9 =(-1 )+x0 \n        tmp10 =tmp9 >=tmp1 \n        tmp11 =ks4 \n        tmp12 =tmp9 <tmp11 \n        tmp13 =tmp2 &tmp4 \n        tmp14 =tmp13 &tmp6 \n        tmp15 =tmp14 &tmp8 \n        tmp16 =tmp15 &tmp10 \n        tmp17 =tmp16 &tmp12 \n        tmp18 =tl .load (in_ptr0 +((-1 )+x0 +((-1 )*ks4 )+ks4 *x1 +((-1 )*ks3 *ks4 )+ks3 *ks4 *r0_3 +ks0 *ks3 *ks4 *x2 ),r0_mask &tmp17 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp19 =tl .broadcast_to (tmp18 ,[XBLOCK ,R0_BLOCK ])\n        tmp21 =_tmp20 +tmp19 \n        _tmp20 =tl .where (r0_mask &xmask ,tmp21 ,_tmp20 )\n    tmp20 =tl .sum (_tmp20 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x4 ),tmp20 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__unsafe_index_constant_pad_nd_mean_sub_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x6 =xindex //ks4 \n    x3 =xindex \n    tmp0 =2.0 \n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =tmp0 +tmp2 \n    tmp4 =tmp3 .to (tl .float64 )\n    tmp5 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp6 =tmp5 +tmp4 \n    tmp7 =tmp0 *tmp2 \n    tmp8 =4.0 \n    tmp9 =tmp8 +tmp7 \n    tmp10 =tmp9 .to (tl .float64 )\n    tmp11 =tmp5 +tmp10 \n    tmp12 =tmp6 /tmp11 \n    tmp13 =tmp12 .to (tl .float32 )\n    tmp14 =x1 \n    tmp15 =tmp14 .to (tl .float32 )\n    tmp16 =tmp15 *tmp13 \n    tmp17 =0.0 \n    tmp18 =triton_helpers .maximum (tmp16 ,tmp17 )\n    tmp19 =tmp18 .to (tl .int64 )\n    tmp20 =tl .full ([1 ],1 ,tl .int64 )\n    tmp21 =tmp19 +tmp20 \n    tmp22 =1 +ks0 \n    tmp23 =triton_helpers .minimum (tmp21 ,tmp22 )\n    tmp24 =ks3 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp0 +tmp25 \n    tmp27 =tmp26 .to (tl .float64 )\n    tmp28 =tmp5 +tmp27 \n    tmp29 =tmp0 *tmp25 \n    tmp30 =tmp8 +tmp29 \n    tmp31 =tmp30 .to (tl .float64 )\n    tmp32 =tmp5 +tmp31 \n    tmp33 =tmp28 /tmp32 \n    tmp34 =tmp33 .to (tl .float32 )\n    tmp35 =x0 \n    tmp36 =tmp35 .to (tl .float32 )\n    tmp37 =tmp36 *tmp34 \n    tmp38 =triton_helpers .maximum (tmp37 ,tmp17 )\n    tmp39 =tmp38 .to (tl .int64 )\n    tmp40 =tl .load (in_ptr0 +(tmp39 +2 *tmp23 +4 *x6 +ks3 *tmp23 +2 *ks0 *x6 +2 *ks3 *x6 +ks0 *ks3 *x6 ),xmask ,eviction_policy ='evict_last')\n    tmp41 =2 +ks5 \n    tmp42 =tmp41 .to (tl .float32 )\n    tmp43 =tmp40 /tmp42 \n    tmp44 =tmp39 +tmp20 \n    tmp45 =1 +ks3 \n    tmp46 =triton_helpers .minimum (tmp44 ,tmp45 )\n    tmp47 =tl .load (in_ptr0 +(tmp46 +2 *tmp23 +4 *x6 +ks3 *tmp23 +2 *ks0 *x6 +2 *ks3 *x6 +ks0 *ks3 *x6 ),xmask ,eviction_policy ='evict_last')\n    tmp48 =tmp47 /tmp42 \n    tmp49 =tmp48 -tmp43 \n    tmp50 =tl .load (in_ptr0 +(tmp39 +2 *tmp19 +4 *x6 +ks3 *tmp19 +2 *ks0 *x6 +2 *ks3 *x6 +ks0 *ks3 *x6 ),xmask ,eviction_policy ='evict_last')\n    tmp51 =tmp50 /tmp42 \n    tmp52 =tl .load (in_ptr0 +(tmp46 +2 *tmp19 +4 *x6 +ks3 *tmp19 +2 *ks0 *x6 +2 *ks3 *x6 +ks0 *ks3 *x6 ),xmask ,eviction_policy ='evict_last')\n    tmp53 =tmp52 /tmp42 \n    tmp54 =tmp53 -tmp51 \n    tl .store (out_ptr0 +(x3 ),tmp43 ,xmask )\n    tl .store (out_ptr1 +(x3 ),tmp49 ,xmask )\n    tl .store (out_ptr2 +(x3 ),tmp51 ,xmask )\n    tl .store (out_ptr3 +(x3 ),tmp54 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_arange_clamp_mul_sub_view_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %ks1 )\n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr1 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp30 =tl .load (in_ptr2 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =2.0 \n    tmp3 =ks0 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =tmp2 +tmp4 \n    tmp6 =tmp5 .to (tl .float64 )\n    tmp7 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp8 =tmp7 +tmp6 \n    tmp9 =tmp2 *tmp4 \n    tmp10 =4.0 \n    tmp11 =tmp10 +tmp9 \n    tmp12 =tmp11 .to (tl .float64 )\n    tmp13 =tmp7 +tmp12 \n    tmp14 =tmp8 /tmp13 \n    tmp15 =tmp14 .to (tl .float32 )\n    tmp16 =x0 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =tmp17 *tmp15 \n    tmp19 =0.0 \n    tmp20 =triton_helpers .maximum (tmp18 ,tmp19 )\n    tmp21 =tmp20 .to (tl .int64 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 -tmp22 \n    tmp24 =triton_helpers .maximum (tmp23 ,tmp19 )\n    tmp25 =1.0 \n    tmp26 =triton_helpers .minimum (tmp24 ,tmp25 )\n    tmp27 =tmp1 *tmp26 \n    tmp28 =tmp0 +tmp27 \n    tmp31 =tmp30 *tmp26 \n    tmp32 =tmp29 +tmp31 \n    tmp33 =tmp28 -tmp32 \n    tl .store (in_out_ptr0 +(x2 ),tmp33 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_arange_clamp_mul_sub_view_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x3 =xindex \n    x0 =(xindex %ks1 )\n    x1 =((xindex //ks1 )%ks3 )\n    tmp0 =tl .load (in_out_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =2.0 \n    tmp3 =ks0 \n    tmp4 =tmp3 .to (tl .float32 )\n    tmp5 =tmp2 +tmp4 \n    tmp6 =tmp5 .to (tl .float64 )\n    tmp7 =tl .full ([1 ],-1.0 ,tl .float64 )\n    tmp8 =tmp7 +tmp6 \n    tmp9 =tmp2 *tmp4 \n    tmp10 =4.0 \n    tmp11 =tmp10 +tmp9 \n    tmp12 =tmp11 .to (tl .float64 )\n    tmp13 =tmp7 +tmp12 \n    tmp14 =tmp8 /tmp13 \n    tmp15 =tmp14 .to (tl .float32 )\n    tmp16 =x0 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =tmp17 *tmp15 \n    tmp19 =0.0 \n    tmp20 =triton_helpers .maximum (tmp18 ,tmp19 )\n    tmp21 =tmp20 .to (tl .int64 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 -tmp22 \n    tmp24 =triton_helpers .maximum (tmp23 ,tmp19 )\n    tmp25 =1.0 \n    tmp26 =triton_helpers .minimum (tmp24 ,tmp25 )\n    tmp27 =tmp1 *tmp26 \n    tmp28 =tmp0 +tmp27 \n    tmp30 =ks2 \n    tmp31 =tmp30 .to (tl .float32 )\n    tmp32 =tmp2 +tmp31 \n    tmp33 =tmp32 .to (tl .float64 )\n    tmp34 =tmp7 +tmp33 \n    tmp35 =tmp2 *tmp31 \n    tmp36 =tmp10 +tmp35 \n    tmp37 =tmp36 .to (tl .float64 )\n    tmp38 =tmp7 +tmp37 \n    tmp39 =tmp34 /tmp38 \n    tmp40 =tmp39 .to (tl .float32 )\n    tmp41 =x1 \n    tmp42 =tmp41 .to (tl .float32 )\n    tmp43 =tmp42 *tmp40 \n    tmp44 =triton_helpers .maximum (tmp43 ,tmp19 )\n    tmp45 =tmp44 .to (tl .int64 )\n    tmp46 =tmp45 .to (tl .float32 )\n    tmp47 =tmp44 -tmp46 \n    tmp48 =triton_helpers .maximum (tmp47 ,tmp19 )\n    tmp49 =triton_helpers .minimum (tmp48 ,tmp25 )\n    tmp50 =tmp29 *tmp49 \n    tmp51 =tmp28 +tmp50 \n    tl .store (in_out_ptr0 +(x3 ),tmp51 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_sub_tanh_4 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =libdevice .tanh (tmp0 )\n    tmp2 =tmp0 -tmp1 \n    tl .store (in_out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_zeros_like_5 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_ones_like_6 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =1.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +s3 \n        2 +s2 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s2 ,2 +s3 ),(4 *s0 +2 *s0 *s2 +2 *s0 *s3 +s0 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_red_fused_constant_pad_nd_mean_0_xnumel =4 *s0 +2 *s0 *s2 +2 *s0 *s3 +s0 *s2 *s3 \n        2 +s1 \n        get_raw_stream (0 )\n        triton_red_fused_constant_pad_nd_mean_0 [grid (triton_red_fused_constant_pad_nd_mean_0_xnumel )](arg4_1 ,buf0 ,16 ,34 ,34 ,32 ,32 ,1156 ,3468 ,18 ,XBLOCK =64 ,R0_BLOCK =32 ,num_warps =16 ,num_stages =1 )\n        del arg4_1 \n        4 +2 *s3 \n        4 +2 *s2 \n        16 +8 *s2 +8 *s3 +4 *s2 *s3 \n        buf1 =empty_strided_cuda ((1 ,s0 ,4 +2 *s2 ,4 +2 *s3 ),(16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 ,16 +8 *s2 +8 *s3 +4 *s2 *s3 ,4 +2 *s3 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,4 +2 *s2 ,4 +2 *s3 ),(16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 ,16 +8 *s2 +8 *s3 +4 *s2 *s3 ,4 +2 *s3 ,1 ),torch .float32 )\n        buf3 =empty_strided_cuda ((1 ,s0 ,4 +2 *s2 ,4 +2 *s3 ),(16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 ,16 +8 *s2 +8 *s3 +4 *s2 *s3 ,4 +2 *s3 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,s0 ,4 +2 *s2 ,4 +2 *s3 ),(16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 ,16 +8 *s2 +8 *s3 +4 *s2 *s3 ,4 +2 *s3 ,1 ),torch .float32 )\n\n        triton_poi_fused__unsafe_index_constant_pad_nd_mean_sub_1_xnumel =16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused__unsafe_index_constant_pad_nd_mean_sub_1 [grid (triton_poi_fused__unsafe_index_constant_pad_nd_mean_sub_1_xnumel )](buf0 ,buf1 ,buf2 ,buf3 ,buf4 ,32 ,68 ,68 ,32 ,4624 ,16 ,13872 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        buf5 =buf1 ;del buf1 \n\n        triton_poi_fused__to_copy_add_arange_clamp_mul_sub_view_2_xnumel =16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_arange_clamp_mul_sub_view_2 [grid (triton_poi_fused__to_copy_add_arange_clamp_mul_sub_view_2_xnumel )](buf5 ,buf2 ,buf3 ,buf4 ,32 ,68 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        buf6 =buf3 ;del buf3 \n\n        triton_poi_fused__to_copy_add_arange_clamp_mul_sub_view_3_xnumel =16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_arange_clamp_mul_sub_view_3 [grid (triton_poi_fused__to_copy_add_arange_clamp_mul_sub_view_3_xnumel )](buf6 ,buf4 ,buf5 ,32 ,68 ,32 ,68 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf7 =buf6 ;del buf6 \n\n        triton_poi_fused_sub_tanh_4_xnumel =16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_sub_tanh_4 [grid (triton_poi_fused_sub_tanh_4_xnumel )](buf7 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf8 =buf5 ;del buf5 \n\n        triton_poi_fused_zeros_like_5_xnumel =16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_zeros_like_5 [grid (triton_poi_fused_zeros_like_5_xnumel )](buf8 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf9 =buf4 ;del buf4 \n\n        triton_poi_fused_ones_like_6_xnumel =16 *s0 +8 *s0 *s2 +8 *s0 *s3 +4 *s0 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_ones_like_6 [grid (triton_poi_fused_ones_like_6_xnumel )](buf9 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n    return (buf7 ,buf8 ,buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =16 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,16 ,32 ,32 ),(49152 ,16384 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c616ae91-74f8-4100-b597-0cf3d862b36b",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CrossMapLRN2d', 'BatchNorm3d', 'Mish', 'SyncBatchNorm', 'ConstantPad2d', 'SELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad2d(2, 3.0)\n        self.lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.bn3d = nn.BatchNorm3d(10)\n        self.sync_bn = nn.SyncBatchNorm(20)\n        self.mish = nn.Mish()\n        self.selu = nn.SELU()\n\n    def forward(self, x):\n        # Assuming input is 4D (batch, channels, height, width)\n        x = self.pad(x)  # Apply padding\n        x = self.lrn(x)  # Apply CrossMapLRN2d\n        x = x.unsqueeze(2)  # Add a dimension to make it 5D for BatchNorm3d\n        x = self.bn3d(x)  # Apply BatchNorm3d\n        x = x.squeeze(2)  # Remove the added dimension\n        x = x.unsqueeze(1)  # Add a dimension to make it 5D for SyncBatchNorm\n        x = self.sync_bn(x)  # Apply SyncBatchNorm\n        x = x.squeeze(1)  # Remove the added dimension\n        x = self.mish(x)  # Apply Mish activation\n        x = self.selu(x)  # Apply SELU activation\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels, 64x64 spatial dimensions\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =3.0 )\n    tl .store (out_ptr0 +(x4 ),tmp12 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg3_1 ,buf0 ,36 ,36 ,32 ,32 ,1296 ,3888 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c6e612b7-31fb-42e9-9064-e7b7224fb3e8",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardtanh', 'Sequential', 'AlphaDropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardtanh = nn.Hardtanh(min_val=-1.0, max_val=1.0)\n        self.sequential = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 16)\n        )\n        self.alpha_dropout = nn.AlphaDropout(p=0.5)\n\n    def forward(self, x):\n        # Flatten the input to fit the linear layers in the sequential module\n        x = x.view(x.size(0), -1)\n        x = self.hardtanh(x)\n        x = self.sequential(x)\n        x = self.alpha_dropout(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 128).cuda()  # Assuming input shape (batch_size, 128)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_hardtanh_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =-1.0 \n    tmp2 =triton_helpers .maximum (tmp0 ,tmp1 )\n    tmp3 =1.0 \n    tmp4 =triton_helpers .minimum (tmp2 ,tmp3 )\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_relu_1 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_relu_2 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_addmm_bernoulli_mul_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp5 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp6 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =tmp4 .to (tl .float32 )\n    tmp9 =0.8864048946659319 \n    tmp10 =tmp8 *tmp9 \n    tmp11 =tmp7 *tmp10 \n    tmp12 =-1.0 \n    tmp13 =tmp8 +tmp12 \n    tmp14 =1.558387861036063 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =0.7791939305180315 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =tmp11 +tmp17 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(x0 ),tmp18 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_2 ,(64 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_3 ,(64 ,),(1 ,))\n    assert_size_stride (primals_4 ,(32 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_5 ,(32 ,),(1 ,))\n    assert_size_stride (primals_6 ,(16 ,32 ),(32 ,1 ))\n    assert_size_stride (primals_7 ,(16 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_hardtanh_0 [grid (128 )](primals_1 ,buf0 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_1 \n        buf1 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf0 ,reinterpret_tensor (primals_2 ,(128 ,64 ),(1 ,128 ),0 ),out =buf1 )\n        del primals_2 \n        buf2 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_relu_1 [grid (64 )](buf2 ,primals_3 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        del primals_3 \n        buf3 =empty_strided_cuda ((1 ,32 ),(32 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf2 ,reinterpret_tensor (primals_4 ,(64 ,32 ),(1 ,64 ),0 ),out =buf3 )\n        buf4 =buf3 ;del buf3 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_relu_2 [grid (32 )](buf4 ,primals_5 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del primals_5 \n        buf5 =empty_strided_cuda ((1 ,16 ),(16 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf4 ,reinterpret_tensor (primals_6 ,(32 ,16 ),(1 ,32 ),0 ),out =buf5 )\n        buf6 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf6 )\n        buf8 =empty_strided_cuda ((1 ,16 ),(16 ,1 ),torch .bool )\n        buf9 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_addmm_bernoulli_mul_3 [grid (16 )](buf9 ,buf6 ,primals_7 ,buf8 ,0 ,16 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf6 \n        del primals_7 \n    return (buf9 ,buf0 ,buf2 ,buf4 ,buf8 ,primals_6 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((64 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((32 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((16 ,32 ),(32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c76edd5e-e8cb-48d3-a836-e95e7a6cc305",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool3d', 'ReflectionPad3d', 'PoissonNLLLoss', 'AdaptiveLogSoftmaxWithLoss', 'CosineEmbeddingLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool3d = nn.MaxPool3d(kernel_size=2, stride=2)\n        self.reflectionpad3d = nn.ReflectionPad3d(padding=1)\n        self.poisson_nll_loss = nn.PoissonNLLLoss()\n        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(in_features=128, n_classes=10, cutoffs=[5])\n        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()\n\n    def forward(self, x):\n        # Apply ReflectionPad3d\n        x = self.reflectionpad3d(x)\n        \n        # Apply MaxPool3d\n        x = self.maxpool3d(x)\n        \n        # Reshape the tensor to fit the input shape of AdaptiveLogSoftmaxWithLoss\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x = x[:, :128]  # Ensure the input has 128 features\n        \n        # Apply AdaptiveLogSoftmaxWithLoss\n        target = torch.randint(0, 10, (x.size(0),), device=x.device)\n        output, _ = self.adaptive_log_softmax(x, target)\n        \n        # Apply CosineEmbeddingLoss\n        input1 = torch.randn_like(output)\n        input2 = torch.randn_like(output)\n        target_cosine = torch.ones(output.size(0), device=output.device)\n        loss = self.cosine_embedding_loss(input1, input2, target_cosine)\n        \n        # Apply PoissonNLLLoss\n        log_input = torch.log(output + 1e-8)  # Ensure log_input is positive\n        target_poisson = torch.randn_like(log_input)\n        loss += self.poisson_nll_loss(log_input, target_poisson)\n        \n        return output, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape for 3D operations\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_reflection_pad3d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =((xindex //ks2 )%ks3 )\n    x3 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks7 *(tl .where ((-1 )+ks6 +((-1 )*tl_math .abs (1 +((-1 )*ks6 )+tl_math .abs ((-1 )+x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks6 )+tl_math .abs ((-1 )+x1 )))+2 *ks6 ,(-1 )+ks6 +((-1 )*tl_math .abs (1 +((-1 )*ks6 )+tl_math .abs ((-1 )+x1 )))))+ks6 *ks7 *(tl .where ((-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+x2 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+x2 )))+2 *ks5 ,(-1 )+ks5 +((-1 )*tl_math .abs (1 +((-1 )*ks5 )+tl_math .abs ((-1 )+x2 )))))+ks5 *ks6 *ks7 *x3 +(tl .where ((-1 )+ks7 +((-1 )*tl_math .abs (1 +((-1 )*ks7 )+tl_math .abs ((-1 )+x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks7 )+tl_math .abs ((-1 )+x0 )))+2 *ks7 ,(-1 )+ks7 +((-1 )*tl_math .abs (1 +((-1 )*ks7 )+tl_math .abs ((-1 )+x0 )))))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x4 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_randint_1 (in_out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_out_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .full ([1 ],0 ,tl .int64 )\n    tmp3 =tl .full ([1 ],10 ,tl .int64 )\n    tmp4 =triton_helpers .randint64 (tmp0 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp3 )\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp4 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +s3 \n        2 +s2 \n        4 +2 *s2 +2 *s3 +s2 *s3 \n        2 +s1 \n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        buf0 =empty_strided_cuda ((1 ,s0 ,2 +s1 ,2 +s2 ,2 +s3 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,4 +2 *s2 +2 *s3 +s2 *s3 ,2 +s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_reflection_pad3d_0_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_reflection_pad3d_0 [grid (triton_poi_fused_reflection_pad3d_0_xnumel )](arg4_1 ,buf0 ,34 ,34 ,1156 ,34 ,39304 ,32 ,32 ,32 ,117912 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg4_1 \n\n        buf1 =torch .ops .aten .max_pool3d_with_indices .default (buf0 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del buf0 \n        buf2 =buf1 [0 ]\n        del buf1 \n        buf4 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf4 )\n        buf5 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_poi_fused_randint_1 [grid (1 )](buf5 ,0 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n    return (reinterpret_tensor (buf2 ,(1 ,128 ),(s0 +s0 *(s1 //2 )+s0 *(s2 //2 )+s0 *(s3 //2 )+s0 *(s1 //2 )*(s2 //2 )+s0 *(s1 //2 )*(s3 //2 )+s0 *(s2 //2 )*(s3 //2 )+s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),1 ),0 ),buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c952610e-9efe-4e17-b436-890520fa164a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LogSoftmax', 'TripletMarginWithDistanceLoss', 'MSELoss', 'LogSigmoid', 'Hardtanh', 'Softplus']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardtanh1 = nn.Hardtanh()\n        self.hardtanh2 = nn.Hardtanh()\n        self.softplus1 = nn.Softplus()\n        self.softplus2 = nn.Softplus()\n        self.log_sigmoid = nn.LogSigmoid()\n        self.log_softmax = nn.LogSoftmax(dim=1)\n        self.triplet_loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n        self.mse_loss = nn.MSELoss()\n\n    def forward(self, x):\n        # Apply Hardtanh twice\n        x = self.hardtanh1(x)\n        x = self.hardtanh2(x)\n        \n        # Apply Softplus twice\n        x = self.softplus1(x)\n        x = self.softplus2(x)\n        \n        # Apply LogSigmoid\n        x = self.log_sigmoid(x)\n        \n        # Reshape for LogSoftmax\n        x = x.view(x.size(0), -1)\n        x = self.log_softmax(x)\n        \n        # Generate anchor, positive, and negative samples for TripletMarginWithDistanceLoss\n        anchor = x[:x.size(0)//2]\n        positive = x[x.size(0)//2:]\n        negative = torch.flip(positive, dims=[0])\n        \n        # Compute TripletMarginWithDistanceLoss\n        triplet_loss = self.triplet_loss(anchor, positive, negative)\n        \n        # Compute MSELoss between anchor and positive\n        mse_loss = self.mse_loss(anchor, positive)\n        \n        # Return the final output and the losses\n        return x, triplet_loss, mse_loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(10, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax_0 (in_ptr0 ,out_ptr2 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp28 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *ks2 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =-1.0 \n        tmp2 =triton_helpers .maximum (tmp0 ,tmp1 )\n        tmp3 =1.0 \n        tmp4 =triton_helpers .minimum (tmp2 ,tmp3 )\n        tmp5 =triton_helpers .maximum (tmp4 ,tmp1 )\n        tmp6 =triton_helpers .minimum (tmp5 ,tmp3 )\n        tmp7 =tmp6 *tmp3 \n        tmp8 =20.0 \n        tmp9 =tmp7 >tmp8 \n        tmp10 =tl_math .exp (tmp7 )\n        tmp11 =libdevice .log1p (tmp10 )\n        tmp12 =tmp11 *tmp3 \n        tmp13 =tl .where (tmp9 ,tmp6 ,tmp12 )\n        tmp14 =tmp13 *tmp3 \n        tmp15 =tmp14 >tmp8 \n        tmp16 =tl_math .exp (tmp14 )\n        tmp17 =libdevice .log1p (tmp16 )\n        tmp18 =tmp17 *tmp3 \n        tmp19 =tl .where (tmp15 ,tmp13 ,tmp18 )\n        tmp20 =0.0 \n        tmp21 =triton_helpers .minimum (tmp20 ,tmp19 )\n        tmp22 =tl_math .abs (tmp19 )\n        tmp23 =-tmp22 \n        tmp24 =tl_math .exp (tmp23 )\n        tmp25 =libdevice .log1p (tmp24 )\n        tmp26 =tmp21 -tmp25 \n        tmp27 =tl .broadcast_to (tmp26 ,[XBLOCK ,R0_BLOCK ])\n        tmp29 =triton_helpers .maximum (_tmp28 ,tmp27 )\n        _tmp28 =tl .where (r0_mask &xmask ,tmp29 ,_tmp28 )\n    tmp28 =triton_helpers .max2 (_tmp28 ,1 )[:,None ]\n    _tmp60 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp30 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *ks2 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp31 =-1.0 \n        tmp32 =triton_helpers .maximum (tmp30 ,tmp31 )\n        tmp33 =1.0 \n        tmp34 =triton_helpers .minimum (tmp32 ,tmp33 )\n        tmp35 =triton_helpers .maximum (tmp34 ,tmp31 )\n        tmp36 =triton_helpers .minimum (tmp35 ,tmp33 )\n        tmp37 =tmp36 *tmp33 \n        tmp38 =20.0 \n        tmp39 =tmp37 >tmp38 \n        tmp40 =tl_math .exp (tmp37 )\n        tmp41 =libdevice .log1p (tmp40 )\n        tmp42 =tmp41 *tmp33 \n        tmp43 =tl .where (tmp39 ,tmp36 ,tmp42 )\n        tmp44 =tmp43 *tmp33 \n        tmp45 =tmp44 >tmp38 \n        tmp46 =tl_math .exp (tmp44 )\n        tmp47 =libdevice .log1p (tmp46 )\n        tmp48 =tmp47 *tmp33 \n        tmp49 =tl .where (tmp45 ,tmp43 ,tmp48 )\n        tmp50 =0.0 \n        tmp51 =triton_helpers .minimum (tmp50 ,tmp49 )\n        tmp52 =tl_math .abs (tmp49 )\n        tmp53 =-tmp52 \n        tmp54 =tl_math .exp (tmp53 )\n        tmp55 =libdevice .log1p (tmp54 )\n        tmp56 =tmp51 -tmp55 \n        tmp57 =tmp56 -tmp28 \n        tmp58 =tl_math .exp (tmp57 )\n        tmp59 =tl .broadcast_to (tmp58 ,[XBLOCK ,R0_BLOCK ])\n        tmp61 =_tmp60 +tmp59 \n        _tmp60 =tl .where (r0_mask &xmask ,tmp61 ,_tmp60 )\n    tmp60 =tl .sum (_tmp60 ,1 )[:,None ]\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp62 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *ks2 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp63 =-1.0 \n        tmp64 =triton_helpers .maximum (tmp62 ,tmp63 )\n        tmp65 =1.0 \n        tmp66 =triton_helpers .minimum (tmp64 ,tmp65 )\n        tmp67 =triton_helpers .maximum (tmp66 ,tmp63 )\n        tmp68 =triton_helpers .minimum (tmp67 ,tmp65 )\n        tmp69 =tmp68 *tmp65 \n        tmp70 =20.0 \n        tmp71 =tmp69 >tmp70 \n        tmp72 =tl_math .exp (tmp69 )\n        tmp73 =libdevice .log1p (tmp72 )\n        tmp74 =tmp73 *tmp65 \n        tmp75 =tl .where (tmp71 ,tmp68 ,tmp74 )\n        tmp76 =tmp75 *tmp65 \n        tmp77 =tmp76 >tmp70 \n        tmp78 =tl_math .exp (tmp76 )\n        tmp79 =libdevice .log1p (tmp78 )\n        tmp80 =tmp79 *tmp65 \n        tmp81 =tl .where (tmp77 ,tmp75 ,tmp80 )\n        tmp82 =0.0 \n        tmp83 =triton_helpers .minimum (tmp82 ,tmp81 )\n        tmp84 =tl_math .abs (tmp81 )\n        tmp85 =-tmp84 \n        tmp86 =tl_math .exp (tmp85 )\n        tmp87 =libdevice .log1p (tmp86 )\n        tmp88 =tmp83 -tmp87 \n        tmp89 =tmp88 -tmp28 \n        tmp90 =tl_math .log (tmp60 )\n        tmp91 =tmp89 -tmp90 \n        tl .store (out_ptr2 +(r0_1 +ks0 *ks1 *ks2 *x0 ),tmp91 ,r0_mask &xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_flip_norm_sub_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp7 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp14 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *ks2 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *ks2 *x0 +ks0 *ks1 *ks2 *(ks3 //2 )),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp9 =tl .load (in_ptr0 +(r0_1 +((-1 )*ks0 *ks1 *ks2 )+ks0 *ks1 *ks2 *ks3 +((-1 )*ks0 *ks1 *ks2 *x0 )),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp3 =1e-06 \n        tmp4 =tmp2 +tmp3 \n        tmp5 =tmp4 *tmp4 \n        tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n        tmp8 =_tmp7 +tmp6 \n        _tmp7 =tl .where (r0_mask &xmask ,tmp8 ,_tmp7 )\n        tmp10 =tmp0 -tmp9 \n        tmp11 =tmp10 +tmp3 \n        tmp12 =tmp11 *tmp11 \n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =_tmp14 +tmp13 \n        _tmp14 =tl .where (r0_mask &xmask ,tmp15 ,_tmp14 )\n    tmp7 =tl .sum (_tmp7 ,1 )[:,None ]\n    tmp14 =tl .sum (_tmp14 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_mean_norm_sub_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp10 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =libdevice .sqrt (tmp0 )\n        tmp2 =1.0 \n        tmp3 =tmp1 +tmp2 \n        tmp5 =libdevice .sqrt (tmp4 )\n        tmp6 =tmp3 -tmp5 \n        tmp7 =0.0 \n        tmp8 =triton_helpers .maximum (tmp6 ,tmp7 )\n        tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n        tmp11 =_tmp10 +tmp9 \n        _tmp10 =tl .where (r0_mask ,tmp11 ,_tmp10 )\n    tmp10 =tl .sum (_tmp10 ,1 )[:,None ]\n    tmp12 =ks0 //2 \n    tmp13 =tmp12 .to (tl .float32 )\n    tmp14 =tmp10 /tmp13 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp14 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_mse_loss_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp10 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *(triton_helpers .div_floor_integer (1 +ks1 *ks2 *ks3 *(ks0 //2 ),2 ))\n        tmp1 =ks1 *ks2 *ks3 *(ks0 //2 )\n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *(triton_helpers .div_floor_integer (1 +ks1 *ks2 *ks3 *(ks0 //2 ),2 )))%(ks1 *ks2 *ks3 *(ks0 //2 )))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr0 +(ks1 *ks2 *ks3 *(ks0 //2 )+(((r0_1 +x0 *(triton_helpers .div_floor_integer (1 +ks1 *ks2 *ks3 *(ks0 //2 ),2 )))%(ks1 *ks2 *ks3 *(ks0 //2 ))))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tmp3 -tmp4 \n        tmp6 =tmp5 *tmp5 \n        tmp7 =tl .full (tmp6 .shape ,0 ,tmp6 .dtype )\n        tmp8 =tl .where (tmp2 ,tmp6 ,tmp7 )\n        tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n        tmp11 =_tmp10 +tmp9 \n        _tmp10 =tl .where (r0_mask &xmask ,tmp11 ,_tmp10 )\n    tmp10 =tl .sum (_tmp10 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_mse_loss_4 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =ks1 *ks2 *ks3 *(ks0 //2 )\n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp6 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(s0 ,s1 ,s2 ,s3 ),(s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf2 =empty_strided_cuda ((s0 ,s1 *s2 *s3 ),(s1 *s2 *s3 ,1 ),torch .float32 )\n\n        s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_red_fused__log_softmax_0 [grid (s0 )](arg4_1 ,buf2 ,3 ,32 ,32 ,10 ,3072 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg4_1 \n        buf3 =empty_strided_cuda ((s0 //2 ,),(1 ,),torch .float32 )\n        buf4 =empty_strided_cuda ((s0 //2 ,),(1 ,),torch .float32 )\n\n        triton_red_fused_add_flip_norm_sub_1_xnumel =s0 //2 \n        s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_red_fused_add_flip_norm_sub_1 [grid (triton_red_fused_add_flip_norm_sub_1_xnumel )](buf2 ,buf3 ,buf4 ,3 ,32 ,32 ,10 ,5 ,3072 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((),(),torch .float32 )\n        buf8 =buf5 ;del buf5 \n\n        s0 //2 \n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_mean_norm_sub_2 [grid (1 )](buf8 ,buf3 ,buf4 ,10 ,1 ,5 ,XBLOCK =1 ,R0_BLOCK =8 ,num_warps =2 ,num_stages =1 )\n        del buf3 \n        del buf4 \n        buf6 =empty_strided_cuda ((2 ,),(1 ,),torch .float32 )\n\n        (1 +s1 *s2 *s3 *(s0 //2 ))//2 \n        get_raw_stream (0 )\n        triton_red_fused_mse_loss_3 [grid (2 )](buf2 ,buf6 ,10 ,3 ,32 ,32 ,2 ,7680 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf7 =empty_strided_cuda ((),(),torch .float32 )\n        buf9 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_per_fused_mse_loss_4 [grid (1 )](buf9 ,buf6 ,10 ,3 ,32 ,32 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf6 \n    return (buf2 ,buf8 ,buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =3 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((10 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "c9a72bbc-6030-4817-b36b-eef58b832fc6",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PixelShuffle', 'LeakyReLU', 'MultiMarginLoss', 'Hardswish', 'NLLLoss2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pixel_shuffle = nn.PixelShuffle(2)\n        self.leaky_relu = nn.LeakyReLU(0.1)\n        self.hardswish = nn.Hardswish()\n        self.multi_margin_loss = nn.MultiMarginLoss()\n        self.nll_loss_2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Apply PixelShuffle\n        x = self.pixel_shuffle(x)\n        \n        # Apply LeakyReLU\n        x = self.leaky_relu(x)\n        \n        # Apply Hardswish\n        x = self.hardswish(x)\n        \n        # Reshape for MultiMarginLoss\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        target = torch.randint(0, x.size(1), (x.size(0),)).to(x.device)\n        loss1 = self.multi_margin_loss(x, target)\n        \n        # Reshape for NLLLoss2d\n        x = x.view(x.size(0), 1, int(x.size(1) ** 0.5), int(x.size(1) ** 0.5))  # Reshape to 4D\n        target = torch.randint(0, x.size(1), (x.size(0), x.size(2), x.size(3))).to(x.device)\n        loss2 = self.nll_loss_2d(x, target)\n        \n        # Return the sum of the losses\n        return loss1 + loss2\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 4, 16, 16).cuda()  # Input shape compatible with PixelShuffle\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nfrom ctypes import c_void_p ,c_long ,c_int \nimport torch \nimport math \nimport random \nimport os \nimport tempfile \nfrom math import inf ,nan \nfrom cmath import nanj \nfrom torch ._inductor .hooks import run_intermediate_hooks \nfrom torch ._inductor .utils import maybe_profile \nfrom torch ._inductor .codegen .memory_planning import _align as align \nfrom torch import device ,empty_strided \nfrom torch ._inductor .async_compile import AsyncCompile \nfrom torch ._inductor .select_algorithm import extern_kernels \nfrom torch ._inductor .codegen .multi_kernel import MultiKernelCall \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\nsplit_scan_grid ,\ngrid_combo_kernels ,\nstart_graph ,\nend_graph ,\ncooperative_reduction_grid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nextern \"C\"void kernel (const int64_t *in_ptr0 ,\nint64_t *out_ptr0 ,\nconst int64_t ks0 ,\nconst int64_t ks1 )\n{\n{\n{\n{\nauto tmp0 =in_ptr0 [static_cast <int64_t >(0 L )];\nauto tmp1 =static_cast <int32_t >(0 );\nauto tmp2 =static_cast <int64_t >(0 );\nauto tmp3 =4 L *ks0 *ks1 ;\nauto tmp4 =c10 ::convert <int64_t >(tmp3 );\nauto tmp5 =randint64_cpu (tmp0 ,tmp1 ,tmp2 ,tmp4 );\nout_ptr0 [static_cast <int64_t >(0 L )]=tmp5 ;\n}\n}\n}\n}\n''')\n\n# kernel path: /tmp/torchinductor_sahanp/5y/c5yxdby7r456cj3cg23ku4vvrtahhdyoakg3bdl55hg3fbcrkkfy.py\n# Topologically Sorted Source Nodes: [loss1], Original ATen: [aten.arange, aten.ne, aten.gather, aten.rsub, aten.add, aten.clamp_min, aten.scalar_tensor, aten.where, aten.mean]\n# Source node to ATen node mapping:\n#   loss1 => add_14, clamp_min_1, full_default, gather, iota, mean, ne_4, sub_9, where_1\n# Graph fragment:\n#   %iota : [num_users=1] = call_function[target=torch.ops.prims.iota.default](args = (%floordiv_1,), kwargs = {start: 0, step: 1, dtype: torch.int64, device: cuda:0, requires_grad: False})\n#   %ne_4 : [num_users=1] = call_function[target=torch.ops.aten.ne.Tensor](args = (%iota, %unsqueeze), kwargs = {})\n#   %gather : [num_users=1] = call_function[target=torch.ops.aten.gather.default](args = (%view_2, 1, %unsqueeze), kwargs = {})\n#   %sub_9 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1.0, %gather), kwargs = {})\n#   %add_14 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%sub_9, %view_2), kwargs = {})\n#   %clamp_min_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp_min.default](args = (%add_14, 0), kwargs = {})\n#   %full_default : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float32, layout: torch.strided, device: cuda:0, pin_memory: False})\n#   %where_1 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%ne_4, %clamp_min_1, %full_default), kwargs = {})\n#   %mean : [num_users=1] = call_function[target=torch.ops.aten.mean.default](args = (%where_1,), kwargs = {})\nimport triton\nimport triton.language as tl\nfrom triton.compiler.compiler import AttrsDescriptor\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton.jit\ndef triton_red_fused_add_arange_clamp_min_gather_mean_ne_rsub_scalar_tensor_where_1(in_ptr0, in_ptr1, out_ptr1, ks0, ks1, ks2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):\n    xnumel = 1\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_base = tl.arange(0, R0_BLOCK)[None, :]\n    rbase = r0_base\n    tmp0 = tl.load(in_ptr0 + (0))\n    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])\n    _tmp38 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)\n    for r0_offset in range(0, r0_numel, R0_BLOCK):\n        r0_index = r0_offset + r0_base\n        r0_mask = r0_index < r0_numel\n        roffset = r0_offset\n        rindex = r0_index\n        r0_0 = r0_index\n        tmp25 = tl.load(in_ptr1 + (ks2*((((((r0_0 // (2*ks2)) % (2*ks1))) // 2) % ks1)) + ks1*ks2*((((r0_0 % (2*ks2))) % 2)) + 2*ks1*ks2*(((((r0_0 // (2*ks2)) % (2*ks1))) % 2)) + (((((r0_0 % (2*ks2))) // 2) % ks2))), r0_mask, eviction_policy='evict_last', other=0.0)\n        tmp2 = r0_0\n        tmp3 = tmp2 != tmp1\n        tmp4 = 4*ks1*ks2*(ks0 // 4)\n        tmp5 = tmp1 + tmp4\n        tmp6 = tmp1 < 0\n        tmp7 = tl.where(tmp6, tmp5, tmp1)\n        tl.device_assert((0 <= tmp7) & (tmp7 < 4*ks1*ks2*(ks0 // 4)), \"index out of bounds: 0 <= tmp7 < 4*ks1*ks2*(ks0 // 4)\")\n        tmp9 = tl.load(in_ptr1 + (ks2*((((((tmp7 // (2*ks2)) % (2*ks1))) // 2) % ks1)) + ks1*ks2*((((tmp7 % (2*ks2))) % 2)) + 2*ks1*ks2*(((((tmp7 // (2*ks2)) % (2*ks1))) % 2)) + (((((tmp7 % (2*ks2))) // 2) % ks2))), None, eviction_policy='evict_last')\n        tmp10 = 0.0\n        tmp11 = tmp9 > tmp10\n        tmp12 = 0.1\n        tmp13 = tmp9 * tmp12\n        tmp14 = tl.where(tmp11, tmp9, tmp13)\n        tmp15 = 3.0\n        tmp16 = tmp14 + tmp15\n        tmp17 = triton_helpers.maximum(tmp16, tmp10)\n        tmp18 = 6.0\n        tmp19 = triton_helpers.minimum(tmp17, tmp18)\n        tmp20 = tmp14 * tmp19\n        tmp21 = 0.16666666666666666\n        tmp22 = tmp20 * tmp21\n        tmp23 = 1.0\n        tmp24 = tmp23 - tmp22\n        tmp26 = tmp25 > tmp10\n        tmp27 = tmp25 * tmp12\n        tmp28 = tl.where(tmp26, tmp25, tmp27)\n        tmp29 = tmp28 + tmp15\n        tmp30 = triton_helpers.maximum(tmp29, tmp10)\n        tmp31 = triton_helpers.minimum(tmp30, tmp18)\n        tmp32 = tmp28 * tmp31\n        tmp33 = tmp32 * tmp21\n        tmp34 = tmp24 + tmp33\n        tmp35 = triton_helpers.maximum(tmp34, tmp10)\n        tmp36 = tl.where(tmp3, tmp35, tmp10)\n        tmp37 = tl.broadcast_to(tmp36, [XBLOCK, R0_BLOCK])\n        tmp39 = _tmp38 + tmp37\n        _tmp38 = tl.where(r0_mask, tmp39, _tmp38)\n    tmp38 = tl.sum(_tmp38, 1)[:, None]\n    tl.store(out_ptr1 + (tl.full([XBLOCK, 1], 0, tl.int32)), tmp38, None)\n\n#include \"/tmp/torchinductor_sahanp/3b/c3bi5gk6mslf6u4iaqafhxm64z6u65e3eain4xlary5blqnvv6xx.h\"\nextern \"C\"  void kernel(const int64_t* in_ptr0,\n                       int64_t* out_ptr0,\n                       const int64_t ks0,\n                       const int64_t ks1,\n                       const int64_t ks2)\n{\n    {\n        #pragma GCC ivdep\n        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>((c10::div_floor_integer(static_cast<int64_t>(2L*ks1*ks2*(c10::div_floor_integer(static_cast<int64_t>(ks0), static_cast<int64_t>(2L*(c10::div_floor_integer(static_cast<int64_t>(ks0), static_cast<int64_t>(4L))))))), static_cast<int64_t>(static_cast<int64_t>(std::trunc(std::pow(static_cast<double>(4L*ks1*ks2), 0.5))))))*static_cast<int64_t>(std::trunc(std::pow(static_cast<double>(4L*ks1*ks2), 0.5)))); x0+=static_cast<int64_t>(1L))\n        {\n            {\n                {\n                    auto tmp0 = in_ptr0[static_cast<int64_t>(1L)];\n                    auto tmp1 = x0;\n                    auto tmp2 = c10::convert<int32_t>(tmp1);\n                    auto tmp3 = static_cast<int64_t>(0);\n                    auto tmp4 = static_cast<int64_t>(1);\n                    auto tmp5 = randint64_cpu(tmp0, tmp2, tmp3, tmp4);\n                    out_ptr0[static_cast<int64_t>(x0)] = tmp5;\n                }\n            }\n        }\n    }\n}\n''')\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_mean_nll_loss2d_forward_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =1 \n    rnumel =r0_numel \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    rbase =r0_base \n    _tmp27 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp33 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int64 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        rindex =r0_index \n        r0_2 =r0_index \n        r0_0 =(r0_index %ks0 )\n        r0_1 =r0_index //ks0 \n        tmp0 =tl .load (in_ptr0 +(r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp10 =tl .load (in_ptr1 +(ks2 *(((((((r0_0 +r0_1 *libdevice .trunc (libdevice .pow ((4 *ks1 *ks2 ).to (tl .float64 ),tl .full ([],0.500000000000000 ,tl .float64 ))).to (tl .int32 ))//(2 *ks2 ))%(2 *ks1 )))//2 )%ks1 ))+ks1 *ks2 *(((((r0_0 +r0_1 *libdevice .trunc (libdevice .pow ((4 *ks1 *ks2 ).to (tl .float64 ),tl .full ([],0.500000000000000 ,tl .float64 ))).to (tl .int32 ))%(2 *ks2 )))%2 ))+2 *ks1 *ks2 *((((((r0_0 +r0_1 *libdevice .trunc (libdevice .pow ((4 *ks1 *ks2 ).to (tl .float64 ),tl .full ([],0.500000000000000 ,tl .float64 ))).to (tl .int32 ))//(2 *ks2 ))%(2 *ks1 )))%2 ))+((((((r0_0 +r0_1 *libdevice .trunc (libdevice .pow ((4 *ks1 *ks2 ).to (tl .float64 ),tl .full ([],0.500000000000000 ,tl .float64 ))).to (tl .int32 ))%(2 *ks2 )))//2 )%ks2 ))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp29 =tl .load (in_ptr0 +(r0_2 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .full ([1 ,1 ],-100 ,tl .int64 )\n        tmp2 =tmp0 !=tmp1 \n        tmp3 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp4 =tl .where (tmp2 ,tmp0 ,tmp3 )\n        tmp5 =tl .full ([XBLOCK ,R0_BLOCK ],1 ,tl .int32 )\n        tmp6 =tmp4 +tmp5 \n        tmp7 =tmp4 <0 \n        tmp8 =tl .where (tmp7 ,tmp6 ,tmp4 )\n        tl .device_assert (((0 <=tmp8 )&(tmp8 <1 ))|~(r0_mask ),\"index out of bounds: 0 <= tmp8 < 1\")\n        tmp11 =0.0 \n        tmp12 =tmp10 >tmp11 \n        tmp13 =0.1 \n        tmp14 =tmp10 *tmp13 \n        tmp15 =tl .where (tmp12 ,tmp10 ,tmp14 )\n        tmp16 =3.0 \n        tmp17 =tmp15 +tmp16 \n        tmp18 =triton_helpers .maximum (tmp17 ,tmp11 )\n        tmp19 =6.0 \n        tmp20 =triton_helpers .minimum (tmp18 ,tmp19 )\n        tmp21 =tmp15 *tmp20 \n        tmp22 =0.16666666666666666 \n        tmp23 =tmp21 *tmp22 \n        tmp24 =-tmp23 \n        tmp25 =tl .where (tmp2 ,tmp24 ,tmp11 )\n        tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n        tmp28 =_tmp27 +tmp26 \n        _tmp27 =tl .where (r0_mask ,tmp28 ,_tmp27 )\n        tmp30 =tmp29 !=tmp1 \n        tmp31 =tmp30 .to (tl .int64 )\n        tmp32 =tl .broadcast_to (tmp31 ,[XBLOCK ,R0_BLOCK ])\n        tmp34 =_tmp33 +tmp32 \n        _tmp33 =tl .where (r0_mask ,tmp34 ,_tmp33 )\n    tmp27 =tl .sum (_tmp27 ,1 )[:,None ]\n    tmp33 =tl .sum (_tmp33 ,1 )[:,None ]\n    tmp35 =tl .load (in_out_ptr0 +(0 ))\n    tmp36 =tl .broadcast_to (tmp35 ,[XBLOCK ,1 ])\n    tmp37 =2 *ks1 *ks2 *(triton_helpers .div_floor_integer (ks3 ,2 *(ks3 //4 )))\n    tmp38 =tmp37 .to (tl .float32 )\n    tmp39 =tmp36 /tmp38 \n    tmp40 =tmp33 .to (tl .float32 )\n    tmp41 =tmp27 /tmp40 \n    tmp42 =tmp39 +tmp41 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp42 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    buf0 =empty_strided_cpu ((2 ,),(1 ,),torch .int64 )\n\n    aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n    buf1 =empty_strided_cpu ((1 ,),(1 ,),torch .int64 )\n    cpp_fused_randint_0 (buf0 ,buf1 ,s1 ,s2 )\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n        buf2 .copy_ (buf1 ,False )\n        del buf1 \n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n\n        triton_red_fused_add_arange_clamp_min_gather_mean_ne_rsub_scalar_tensor_where_1_r0_numel =2 *s1 *s2 *(s0 //(2 *(s0 //4 )))\n        stream0 =get_raw_stream (0 )\n        triton_red_fused_add_arange_clamp_min_gather_mean_ne_rsub_scalar_tensor_where_1 [grid (1 )](buf2 ,arg3_1 ,buf4 ,4 ,16 ,16 ,1 ,1024 ,XBLOCK =1 ,R0_BLOCK =1024 ,num_warps =8 ,num_stages =1 )\n        del buf2 \n    buf5 =empty_strided_cpu ((1 ,math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 ),(2 *s1 *s2 *(s0 //(2 *(s0 //4 ))))//(math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 ))),(((2 *s1 *s2 *(s0 //(2 *(s0 //4 ))))//(math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 )))*math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 ),(2 *s1 *s2 *(s0 //(2 *(s0 //4 ))))//(math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 )),1 ),torch .int64 )\n    cpp_fused_randint_2 (buf0 ,buf5 ,s0 ,s1 ,s2 )\n    del buf0 \n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf6 =empty_strided_cuda ((1 ,math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 ),(2 *s1 *s2 *(s0 //(2 *(s0 //4 ))))//(math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 ))),(((2 *s1 *s2 *(s0 //(2 *(s0 //4 ))))//(math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 )))*math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 ),(2 *s1 *s2 *(s0 //(2 *(s0 //4 ))))//(math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 )),1 ),torch .int64 )\n        buf6 .copy_ (buf5 ,False )\n        del buf5 \n        ps0 =(2 *s1 *s2 *(s0 //(2 *(s0 //4 ))))//(math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 ))\n        buf9 =buf4 ;del buf4 \n\n        triton_red_fused_add_mean_nll_loss2d_forward_3_r0_numel =((2 *s1 *s2 *(s0 //(2 *(s0 //4 ))))//(math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 )))*math .trunc (torch .sym_float (4 *s1 *s2 )**0.5 )\n        stream0 =get_raw_stream (0 )\n        triton_red_fused_add_mean_nll_loss2d_forward_3 [grid (1 )](buf9 ,buf6 ,arg3_1 ,32 ,16 ,16 ,4 ,1 ,1024 ,XBLOCK =1 ,R0_BLOCK =1024 ,num_warps =8 ,num_stages =1 )\n        del arg3_1 \n        del buf6 \n    return (buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =4 \n    arg1_1 =16 \n    arg2_1 =16 \n    arg3_1 =rand_strided ((1 ,4 ,16 ,16 ),(1024 ,256 ,16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ca84df1f-3ed4-44e3-a24b-93d47f162367",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CrossMapLRN2d', 'TripletMarginWithDistanceLoss', 'TransformerEncoder']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=64, nhead=8), num_layers=3\n        )\n        self.loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n\n    def forward(self, x):\n        # Apply CrossMapLRN2d\n        x = self.lrn(x)\n        \n        # Reshape for TransformerEncoder\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1).permute(2, 0, 1)  # (seq_len, batch_size, d_model)\n        \n        # Apply TransformerEncoder\n        x = self.transformer_encoder(x)\n        \n        # Reshape back to original shape\n        x = x.permute(1, 2, 0).view(batch_size, channels, height, width)\n        \n        # Generate anchor, positive, and negative samples for TripletMarginWithDistanceLoss\n        anchor = x[:, :, :height//2, :width//2].reshape(batch_size, -1)\n        positive = x[:, :, :height//2, width//2:].reshape(batch_size, -1)\n        negative = x[:, :, height//2:, :width//2].reshape(batch_size, -1)\n        \n        # Compute TripletMarginWithDistanceLoss\n        loss = self.loss(anchor, positive, negative)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =(xindex %64 )\n    x1 =((xindex //64 )%1024 )\n    x2 =xindex //65536 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +64 *x2 +192 *x1 ),None )\n    tmp1 =tl .load (in_ptr1 +(x0 +64 *x2 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (out_ptr0 +(x3 ),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,out_ptr6 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =1024 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(x0 +1024 *r0_1 ),xmask ,other =0.0 )\n    tmp7 =tl .load (in_ptr2 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp8 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr5 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +64 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp14 ,0 )\n    tmp17 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp19 =tl .where (xmask ,tmp17 ,0 )\n    tmp20 =tl .sum (tmp19 ,1 )[:,None ]\n    tmp21 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 /tmp22 \n    tmp24 =tmp14 -tmp23 \n    tmp25 =tmp24 *tmp24 \n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (xmask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =tmp13 -tmp23 \n    tmp31 =64.0 \n    tmp32 =tmp29 /tmp31 \n    tmp33 =1e-05 \n    tmp34 =tmp32 +tmp33 \n    tmp35 =libdevice .rsqrt (tmp34 )\n    tmp36 =tmp30 *tmp35 \n    tmp38 =tmp36 *tmp37 \n    tmp40 =tmp38 +tmp39 \n    tmp41 =0.015625 \n    tmp42 =tmp35 *tmp41 \n    tl .store (out_ptr1 +(r0_1 +64 *x0 ),tmp4 ,xmask )\n    tl .store (out_ptr4 +(r0_1 +64 *x0 ),tmp40 ,xmask )\n    tl .store (out_ptr5 +(x0 +1024 *r0_1 ),tmp36 ,xmask )\n    tl .store (out_ptr6 +(x0 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_relu_threshold_backward_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x2 =xindex \n    x0 =(xindex %2048 )\n    tmp0 =tl .load (in_ptr0 +(x2 ),None )\n    tmp1 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp5 =0.0 \n    tmp6 =tmp4 <=tmp5 \n    tl .store (out_ptr0 +(x2 ),tmp6 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    x1 =(xindex %2048 )\n    tmp6 =tl .load (in_out_ptr0 +(x0 ),None )\n    tmp7 =tl .load (in_ptr1 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,None )\n    tl .store (in_out_ptr0 +(x0 ),tmp13 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =1024 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp7 =tl .load (in_out_ptr0 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp8 =tl .load (in_ptr2 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr3 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr4 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +64 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp14 ,0 )\n    tmp17 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp19 =tl .where (xmask ,tmp17 ,0 )\n    tmp20 =tl .sum (tmp19 ,1 )[:,None ]\n    tmp21 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 /tmp22 \n    tmp24 =tmp14 -tmp23 \n    tmp25 =tmp24 *tmp24 \n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (xmask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =tmp13 -tmp23 \n    tmp31 =64.0 \n    tmp32 =tmp29 /tmp31 \n    tmp33 =1e-05 \n    tmp34 =tmp32 +tmp33 \n    tmp35 =libdevice .rsqrt (tmp34 )\n    tmp36 =tmp30 *tmp35 \n    tmp38 =tmp36 *tmp37 \n    tmp40 =tmp38 +tmp39 \n    tmp41 =0.015625 \n    tmp42 =tmp35 *tmp41 \n    tl .store (out_ptr1 +(r0_1 +64 *x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(r0_1 +64 *x0 ),tmp36 ,xmask )\n    tl .store (out_ptr4 +(r0_1 +64 *x0 ),tmp40 ,xmask )\n    tl .store (out_ptr5 +(x0 ),tmp42 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_threshold_backward_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    x1 =(xindex %2048 )\n    tmp6 =tl .load (in_ptr1 +(x0 ),None )\n    tmp7 =tl .load (in_ptr2 +(x1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.0 \n    tmp15 =tmp10 <=tmp14 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,None )\n    tl .store (out_ptr2 +(x0 ),tmp13 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp15 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_6 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr4 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =1024 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp5 =tl .load (in_ptr1 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp7 =tl .load (in_out_ptr0 +(r0_1 +64 *x0 ),xmask ,other =0.0 )\n    tmp8 =tl .load (in_ptr2 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +64 *x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp14 ,0 )\n    tmp17 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp19 =tl .where (xmask ,tmp17 ,0 )\n    tmp20 =tl .sum (tmp19 ,1 )[:,None ]\n    tmp21 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp20 /tmp22 \n    tmp24 =tmp14 -tmp23 \n    tmp25 =tmp24 *tmp24 \n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ,R0_BLOCK ])\n    tmp28 =tl .where (xmask ,tmp26 ,0 )\n    tmp29 =tl .sum (tmp28 ,1 )[:,None ]\n    tmp30 =tmp13 -tmp23 \n    tmp31 =64.0 \n    tmp32 =tmp29 /tmp31 \n    tmp33 =1e-05 \n    tmp34 =tmp32 +tmp33 \n    tmp35 =libdevice .rsqrt (tmp34 )\n    tmp36 =tmp30 *tmp35 \n    tmp37 =0.015625 \n    tmp38 =tmp35 *tmp37 \n    tl .store (out_ptr1 +(r0_1 +64 *x0 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(r0_1 +64 *x0 ),tmp36 ,xmask )\n    tl .store (out_ptr4 +(x0 ),tmp38 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_norm_sub_7 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_2 =r0_index \n    x0 =(xindex %2 )\n    x1 =xindex //2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(32 *x0 +64 *((r0_2 %16 ))+2048 *((((r0_2 +128 *x1 )//16 )%16 ))+((r0_2 +128 *x1 )//256 )),xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(32 *x0 +((r0_2 +128 *x1 )//256 )),xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp3 =tl .load (in_ptr2 +(32 *x0 +((r0_2 +128 *x1 )//256 )),xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp5 =tl .load (in_ptr0 +(1024 +32 *x0 +64 *((r0_2 %16 ))+2048 *((((r0_2 +128 *x1 )//16 )%16 ))+((r0_2 +128 *x1 )//256 )),xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp16 =tl .load (in_ptr0 +(32768 +32 *x0 +64 *((r0_2 %16 ))+2048 *((((r0_2 +128 *x1 )//16 )%16 ))+((r0_2 +128 *x1 )//256 )),xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp2 =tmp0 *tmp1 \n    tmp4 =tmp2 +tmp3 \n    tmp6 =tmp5 *tmp1 \n    tmp7 =tmp6 +tmp3 \n    tmp8 =tmp4 -tmp7 \n    tmp9 =1e-06 \n    tmp10 =tmp8 +tmp9 \n    tmp11 =tmp10 *tmp10 \n    tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n    tmp14 =tl .where (xmask ,tmp12 ,0 )\n    tmp15 =tl .sum (tmp14 ,1 )[:,None ]\n    tmp17 =tmp16 *tmp1 \n    tmp18 =tmp17 +tmp3 \n    tmp19 =tmp4 -tmp18 \n    tmp20 =tmp19 +tmp9 \n    tmp21 =tmp20 *tmp20 \n    tmp22 =tl .broadcast_to (tmp21 ,[XBLOCK ,R0_BLOCK ])\n    tmp24 =tl .where (xmask ,tmp22 ,0 )\n    tmp25 =tl .sum (tmp24 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp15 ,xmask )\n    tl .store (out_ptr1 +(x3 ),tmp25 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_norm_sub_8 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =2 \n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +2 *r0_1 ),xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (xmask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_clamp_min_mean_norm_sub_9 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp4 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .sum (tmp5 ,1 )[:,None ]\n    tmp8 =libdevice .sqrt (tmp7 )\n    tmp9 =libdevice .sqrt (tmp3 )\n    tmp10 =1.0 \n    tmp11 =tmp8 +tmp10 \n    tmp12 =tmp11 -tmp9 \n    tmp13 =0.0 \n    tmp14 =triton_helpers .maximum (tmp12 ,tmp13 )\n    tmp15 =tmp14 /tmp10 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp8 ,None )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp9 ,None )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp15 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 ,primals_27 ,primals_28 ,primals_29 ,primals_30 ,primals_31 ,primals_32 ,primals_33 ,primals_34 ,primals_35 ,primals_36 ,primals_37 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,64 ,32 ,32 ),(65536 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_2 ,(192 ,),(1 ,))\n    assert_size_stride (primals_3 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_4 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_5 ,(64 ,),(1 ,))\n    assert_size_stride (primals_6 ,(64 ,),(1 ,))\n    assert_size_stride (primals_7 ,(64 ,),(1 ,))\n    assert_size_stride (primals_8 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_9 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_10 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_11 ,(64 ,),(1 ,))\n    assert_size_stride (primals_12 ,(64 ,),(1 ,))\n    assert_size_stride (primals_13 ,(64 ,),(1 ,))\n    assert_size_stride (primals_14 ,(192 ,),(1 ,))\n    assert_size_stride (primals_15 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_16 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_17 ,(64 ,),(1 ,))\n    assert_size_stride (primals_18 ,(64 ,),(1 ,))\n    assert_size_stride (primals_19 ,(64 ,),(1 ,))\n    assert_size_stride (primals_20 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_21 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_22 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_23 ,(64 ,),(1 ,))\n    assert_size_stride (primals_24 ,(64 ,),(1 ,))\n    assert_size_stride (primals_25 ,(64 ,),(1 ,))\n    assert_size_stride (primals_26 ,(192 ,),(1 ,))\n    assert_size_stride (primals_27 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_28 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_29 ,(64 ,),(1 ,))\n    assert_size_stride (primals_30 ,(64 ,),(1 ,))\n    assert_size_stride (primals_31 ,(64 ,),(1 ,))\n    assert_size_stride (primals_32 ,(2048 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_33 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_34 ,(64 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_35 ,(64 ,),(1 ,))\n    assert_size_stride (primals_36 ,(64 ,),(1 ,))\n    assert_size_stride (primals_37 ,(64 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf8 =empty_strided_cuda ((9 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[9 ],out =buf8 )\n        buf0 =empty_strided_cuda ((1024 ,192 ),(192 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1024 ,64 ),(1 ,1024 ),0 ),reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf0 )\n        del primals_3 \n        buf1 =empty_strided_cuda ((3 ,1024 ,1 ,64 ),(65536 ,64 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (196608 )](buf0 ,primals_2 ,buf1 ,196608 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del primals_2 \n\n        buf2 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf1 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),0 ),reinterpret_tensor (buf1 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),65536 ),reinterpret_tensor (buf1 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),131072 ),None ,True ,0.1 )\n        buf3 =buf2 [0 ]\n        buf4 =buf2 [1 ]\n        buf5 =buf2 [2 ]\n        buf6 =buf2 [3 ]\n        del buf2 \n        buf7 =empty_strided_cuda ((1024 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf3 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_4 ,(64 ,64 ),(1 ,64 ),0 ),out =buf7 )\n        buf10 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .bool )\n        buf14 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .float32 )\n        buf96 =empty_strided_cuda ((1024 ,1 ,64 ),(1 ,65536 ,1024 ),torch .float32 )\n        buf97 =empty_strided_cuda ((1024 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_1 [grid (1024 )](buf8 ,primals_1 ,buf7 ,primals_5 ,primals_6 ,primals_7 ,buf10 ,buf14 ,buf96 ,buf97 ,0 ,1024 ,64 ,XBLOCK =8 ,num_warps =4 ,num_stages =1 )\n        del primals_5 \n        del primals_7 \n        buf15 =empty_strided_cuda ((1024 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf14 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_8 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf15 )\n        buf95 =empty_strided_cuda ((1024 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_relu_threshold_backward_2 [grid (2097152 )](buf15 ,primals_9 ,buf95 ,2097152 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        buf17 =empty_strided_cuda ((1024 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n        buf18 =reinterpret_tensor (buf15 ,(1024 ,1 ,2048 ),(2048 ,2048 ,1 ),0 );del buf15 \n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_3 [grid (2097152 )](buf18 ,buf8 ,primals_9 ,buf17 ,1 ,2097152 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del primals_9 \n        buf19 =buf7 ;del buf7 \n\n        extern_kernels .mm (reinterpret_tensor (buf18 ,(1024 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_10 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf19 )\n        buf21 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .bool )\n        buf25 =reinterpret_tensor (buf19 ,(1024 ,1 ,64 ),(64 ,64 ,1 ),0 );del buf19 \n        buf26 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .float32 )\n        buf94 =empty_strided_cuda ((1024 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 [grid (1024 )](buf25 ,buf8 ,buf14 ,primals_11 ,primals_12 ,primals_13 ,buf21 ,buf26 ,buf94 ,6 ,1024 ,64 ,XBLOCK =8 ,num_warps =4 ,num_stages =1 )\n        del primals_11 \n        del primals_13 \n        buf27 =buf0 ;del buf0 \n\n        extern_kernels .mm (reinterpret_tensor (buf26 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_15 ,(64 ,192 ),(1 ,64 ),0 ),out =buf27 )\n        buf28 =empty_strided_cuda ((3 ,1024 ,1 ,64 ),(65536 ,64 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (196608 )](buf27 ,primals_14 ,buf28 ,196608 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del primals_14 \n\n        buf29 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf28 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),0 ),reinterpret_tensor (buf28 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),65536 ),reinterpret_tensor (buf28 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),131072 ),None ,True ,0.1 )\n        buf30 =buf29 [0 ]\n        buf31 =buf29 [1 ]\n        buf32 =buf29 [2 ]\n        buf33 =buf29 [3 ]\n        del buf29 \n        buf34 =empty_strided_cuda ((1024 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf30 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_16 ,(64 ,64 ),(1 ,64 ),0 ),out =buf34 )\n        buf36 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .bool )\n        buf40 =reinterpret_tensor (buf34 ,(1024 ,1 ,64 ),(64 ,64 ,1 ),0 );del buf34 \n        buf41 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .float32 )\n        buf93 =empty_strided_cuda ((1024 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 [grid (1024 )](buf40 ,buf8 ,buf26 ,primals_17 ,primals_18 ,primals_19 ,buf36 ,buf41 ,buf93 ,6 ,1024 ,64 ,XBLOCK =8 ,num_warps =4 ,num_stages =1 )\n        del primals_17 \n        del primals_19 \n        buf42 =empty_strided_cuda ((1024 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf41 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_20 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf42 )\n        buf44 =empty_strided_cuda ((1024 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n        buf45 =empty_strided_cuda ((1024 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .float32 )\n        buf92 =empty_strided_cuda ((1024 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_5 [grid (2097152 )](buf8 ,buf42 ,primals_21 ,buf44 ,buf45 ,buf92 ,7 ,2097152 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_21 \n        buf46 =empty_strided_cuda ((1024 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf45 ,(1024 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_22 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf46 )\n        buf48 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .bool )\n        buf52 =reinterpret_tensor (buf46 ,(1024 ,1 ,64 ),(64 ,64 ,1 ),0 );del buf46 \n        buf53 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .float32 )\n        buf91 =empty_strided_cuda ((1024 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 [grid (1024 )](buf52 ,buf8 ,buf41 ,primals_23 ,primals_24 ,primals_25 ,buf48 ,buf53 ,buf91 ,6 ,1024 ,64 ,XBLOCK =8 ,num_warps =4 ,num_stages =1 )\n        del primals_23 \n        del primals_25 \n        buf54 =buf27 ;del buf27 \n\n        extern_kernels .mm (reinterpret_tensor (buf53 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_27 ,(64 ,192 ),(1 ,64 ),0 ),out =buf54 )\n        buf55 =empty_strided_cuda ((3 ,1024 ,1 ,64 ),(65536 ,64 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (196608 )](buf54 ,primals_26 ,buf55 ,196608 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del buf54 \n        del primals_26 \n\n        buf56 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf55 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),0 ),reinterpret_tensor (buf55 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),65536 ),reinterpret_tensor (buf55 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),131072 ),None ,True ,0.1 )\n        buf57 =buf56 [0 ]\n        buf58 =buf56 [1 ]\n        buf59 =buf56 [2 ]\n        buf60 =buf56 [3 ]\n        del buf56 \n        buf61 =empty_strided_cuda ((1024 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf57 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_28 ,(64 ,64 ),(1 ,64 ),0 ),out =buf61 )\n        buf63 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .bool )\n        buf67 =reinterpret_tensor (buf61 ,(1024 ,1 ,64 ),(64 ,64 ,1 ),0 );del buf61 \n        buf68 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .float32 )\n        buf90 =empty_strided_cuda ((1024 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 [grid (1024 )](buf67 ,buf8 ,buf53 ,primals_29 ,primals_30 ,primals_31 ,buf63 ,buf68 ,buf90 ,6 ,1024 ,64 ,XBLOCK =8 ,num_warps =4 ,num_stages =1 )\n        del primals_29 \n        del primals_31 \n        buf69 =buf42 ;del buf42 \n\n        extern_kernels .mm (reinterpret_tensor (buf68 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_32 ,(64 ,2048 ),(1 ,64 ),0 ),out =buf69 )\n        buf71 =empty_strided_cuda ((1024 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n        buf72 =empty_strided_cuda ((1024 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .float32 )\n        buf89 =empty_strided_cuda ((1024 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_5 [grid (2097152 )](buf8 ,buf69 ,primals_33 ,buf71 ,buf72 ,buf89 ,7 ,2097152 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf69 \n        del primals_33 \n        buf73 =empty_strided_cuda ((1024 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf72 ,(1024 ,2048 ),(2048 ,1 ),0 ),reinterpret_tensor (primals_34 ,(2048 ,64 ),(1 ,2048 ),0 ),out =buf73 )\n        buf75 =empty_strided_cuda ((1024 ,1 ,64 ),(64 ,64 ,1 ),torch .bool )\n        buf79 =reinterpret_tensor (buf73 ,(1024 ,1 ,64 ),(64 ,64 ,1 ),0 );del buf73 \n        buf88 =empty_strided_cuda ((1024 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_6 [grid (1024 )](buf79 ,buf8 ,buf68 ,primals_35 ,buf75 ,buf88 ,8 ,1024 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf8 \n        del primals_35 \n        buf80 =empty_strided_cuda ((1 ,2 ,64 ),(128 ,1 ,2 ),torch .float32 )\n        buf84 =empty_strided_cuda ((1 ,2 ,64 ),(128 ,1 ,2 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_norm_sub_7 [grid (128 )](buf79 ,primals_36 ,primals_37 ,buf80 ,buf84 ,128 ,128 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf81 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_norm_sub_8 [grid (2 )](buf80 ,buf81 ,2 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf80 \n        buf85 =empty_strided_cuda ((1 ,2 ),(2 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_norm_sub_8 [grid (2 )](buf84 ,buf85 ,2 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf84 \n        buf86 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf82 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf83 =buf82 ;del buf82 \n        buf87 =buf86 ;del buf86 \n        buf98 =empty_strided_cuda ((),(),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_clamp_min_mean_norm_sub_9 [grid (1 )](buf83 ,buf87 ,buf85 ,buf81 ,buf98 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf81 \n        del buf85 \n    return (buf98 ,primals_6 ,primals_12 ,primals_18 ,primals_24 ,primals_30 ,primals_36 ,primals_37 ,reinterpret_tensor (primals_1 ,(1024 ,64 ),(1 ,1024 ),0 ),reinterpret_tensor (buf1 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),0 ),reinterpret_tensor (buf1 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),65536 ),reinterpret_tensor (buf1 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),131072 ),buf3 ,buf4 ,buf5 ,buf6 ,buf10 ,reinterpret_tensor (buf14 ,(1024 ,64 ),(64 ,1 ),0 ),buf17 ,reinterpret_tensor (buf18 ,(1024 ,2048 ),(2048 ,1 ),0 ),buf21 ,buf25 ,reinterpret_tensor (buf26 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (buf28 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),0 ),reinterpret_tensor (buf28 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),65536 ),reinterpret_tensor (buf28 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),131072 ),buf30 ,buf31 ,buf32 ,buf33 ,buf36 ,buf40 ,reinterpret_tensor (buf41 ,(1024 ,64 ),(64 ,1 ),0 ),buf44 ,reinterpret_tensor (buf45 ,(1024 ,2048 ),(2048 ,1 ),0 ),buf48 ,buf52 ,reinterpret_tensor (buf53 ,(1024 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (buf55 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),0 ),reinterpret_tensor (buf55 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),65536 ),reinterpret_tensor (buf55 ,(1 ,8 ,1024 ,8 ),(64 ,8 ,64 ,1 ),131072 ),buf57 ,buf58 ,buf59 ,buf60 ,buf63 ,buf67 ,reinterpret_tensor (buf68 ,(1024 ,64 ),(64 ,1 ),0 ),buf71 ,reinterpret_tensor (buf72 ,(1024 ,2048 ),(2048 ,1 ),0 ),buf75 ,buf79 ,buf83 ,buf87 ,buf88 ,primals_34 ,buf89 ,primals_32 ,buf90 ,primals_28 ,primals_27 ,buf91 ,primals_22 ,buf92 ,primals_20 ,buf93 ,primals_16 ,primals_15 ,buf94 ,primals_10 ,buf95 ,primals_8 ,buf96 ,buf97 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,64 ,32 ,32 ),(65536 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_18 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_22 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_23 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_24 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_25 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_26 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_27 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_28 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_29 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_30 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_31 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_32 =rand_strided ((2048 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_33 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_34 =rand_strided ((64 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_35 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_36 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_37 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 ,primals_27 ,primals_28 ,primals_29 ,primals_30 ,primals_31 ,primals_32 ,primals_33 ,primals_34 ,primals_35 ,primals_36 ,primals_37 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "caf76695-51fa-437f-ba53-39bcacddbb40",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Linear', 'CosineEmbeddingLoss', 'Hardshrink', 'Upsample', 'LayerNorm']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.linear1 = nn.Linear(128, 256)\n        self.linear2 = nn.Linear(256, 512)\n        self.linear3 = nn.Linear(512, 256)\n        self.linear4 = nn.Linear(256, 128)\n        self.linear5 = nn.Linear(128, 64)\n        \n        self.hardshrink = nn.Hardshrink()\n        \n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n        \n        self.layer_norm1 = nn.LayerNorm(256)\n        self.layer_norm2 = nn.LayerNorm(512)\n        self.layer_norm3 = nn.LayerNorm(256)\n        self.layer_norm4 = nn.LayerNorm(128)\n        self.layer_norm5 = nn.LayerNorm(64)\n        \n        self.cosine_loss = nn.CosineEmbeddingLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, *), flatten it\n        x = x.view(x.size(0), -1)\n        \n        # Apply linear layers with layer normalization\n        x = self.layer_norm1(F.relu(self.linear1(x)))\n        x = self.layer_norm2(F.relu(self.linear2(x)))\n        x = self.layer_norm3(F.relu(self.linear3(x)))\n        x = self.layer_norm4(F.relu(self.linear4(x)))\n        x = self.layer_norm5(F.relu(self.linear5(x)))\n        \n        # Apply Hardshrink\n        x = self.hardshrink(x)\n        \n        # Reshape for upsampling\n        x = x.view(x.size(0), 1, 8, 8)\n        x = self.upsample(x)\n        \n        # Flatten again for cosine embedding loss\n        x = x.view(x.size(0), -1)\n        \n        # Dummy target for cosine embedding loss\n        target = torch.ones(x.size(0)).to(x.device)\n        \n        # Compute cosine embedding loss (for demonstration purposes, we return the loss)\n        loss = self.cosine_loss(x, x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 128).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_native_layer_norm_relu_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp23 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp25 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp3 =tl .broadcast_to (tmp2 ,[R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp3 ,[R0_BLOCK ])\n    tmp7 =triton_helpers .promote_to_tensor (tl .sum (tmp5 ,0 ))\n    tmp8 =tl .full ([1 ],256 ,tl .int32 )\n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp7 /tmp9 \n    tmp11 =tmp3 -tmp10 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tl .broadcast_to (tmp12 ,[R0_BLOCK ])\n    tmp15 =triton_helpers .promote_to_tensor (tl .sum (tmp13 ,0 ))\n    tmp16 =256.0 \n    tmp17 =tmp15 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tmp21 =tmp2 -tmp10 \n    tmp22 =tmp21 *tmp20 \n    tmp24 =tmp22 *tmp23 \n    tmp26 =tmp24 +tmp25 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp20 ,None )\n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp26 ,None )\n    tl .store (out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp10 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_native_layer_norm_relu_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =512 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp23 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp25 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp3 =tl .broadcast_to (tmp2 ,[R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp3 ,[R0_BLOCK ])\n    tmp7 =triton_helpers .promote_to_tensor (tl .sum (tmp5 ,0 ))\n    tmp8 =tl .full ([1 ],512 ,tl .int32 )\n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp7 /tmp9 \n    tmp11 =tmp3 -tmp10 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tl .broadcast_to (tmp12 ,[R0_BLOCK ])\n    tmp15 =triton_helpers .promote_to_tensor (tl .sum (tmp13 ,0 ))\n    tmp16 =512.0 \n    tmp17 =tmp15 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tmp21 =tmp2 -tmp10 \n    tmp22 =tmp21 *tmp20 \n    tmp24 =tmp22 *tmp23 \n    tmp26 =tmp24 +tmp25 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp20 ,None )\n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp26 ,None )\n    tl .store (out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp10 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_native_layer_norm_relu_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp23 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp25 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp1 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .sum (tmp5 ,1 )[:,None ]\n    tmp8 =tl .full ([XBLOCK ,1 ],128 ,tl .int32 )\n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp7 /tmp9 \n    tmp11 =tmp3 -tmp10 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n    tmp15 =tl .sum (tmp13 ,1 )[:,None ]\n    tmp16 =128.0 \n    tmp17 =tmp15 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tmp21 =tmp2 -tmp10 \n    tmp22 =tmp21 *tmp20 \n    tmp24 =tmp22 *tmp23 \n    tmp26 =tmp24 +tmp25 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp20 ,None )\n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp26 ,None )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp10 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_abs_le_native_layer_norm_relu_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =64 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp23 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp25 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp1 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .sum (tmp5 ,1 )[:,None ]\n    tmp8 =tl .full ([XBLOCK ,1 ],64 ,tl .int32 )\n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp7 /tmp9 \n    tmp11 =tmp3 -tmp10 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n    tmp15 =tl .sum (tmp13 ,1 )[:,None ]\n    tmp16 =64.0 \n    tmp17 =tmp15 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tmp21 =tmp2 -tmp10 \n    tmp22 =tmp21 *tmp20 \n    tmp24 =tmp22 *tmp23 \n    tmp26 =tmp24 +tmp25 \n    tmp27 =tl_math .abs (tmp26 )\n    tmp28 =0.5 \n    tmp29 =tmp27 <=tmp28 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp20 ,None )\n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp26 ,None )\n    tl .store (out_ptr2 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp29 ,None )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp10 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_arange_mul_4 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =x0 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.5 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =tmp3 .to (tl .int32 )\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__unsafe_index_add_clamp_min_div_eq_fill_mean_mul_sqrt_sub_sum_where_zeros_like_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index //16 \n    r0_0 =(r0_index %16 )\n    r0_2 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(r0_0 ),None ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([R0_BLOCK ],8 ,tl .int32 )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tmp0 <0 \n    tmp4 =tl .where (tmp3 ,tmp2 ,tmp0 )\n    tmp6 =tmp5 +tmp1 \n    tmp7 =tmp5 <0 \n    tmp8 =tl .where (tmp7 ,tmp6 ,tmp5 )\n    tmp9 =tl .load (in_ptr1 +(tmp8 +8 *tmp4 ),None ,eviction_policy ='evict_last').to (tl .int1 )\n    tmp10 =tl .load (in_ptr2 +(tmp8 +8 *tmp4 ),None ,eviction_policy ='evict_last')\n    tmp11 =0.0 \n    tmp12 =tl .where (tmp9 ,tmp11 ,tmp10 )\n    tmp13 =tmp12 *tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[R0_BLOCK ])\n    tmp16 =triton_helpers .promote_to_tensor (tl .sum (tmp14 ,0 ))\n    tmp17 =9.999999960041972e-13 \n    tmp18 =tmp16 +tmp17 \n    tmp19 =tmp18 *tmp18 \n    tmp20 =libdevice .sqrt (tmp19 )\n    tmp21 =tmp16 /tmp20 \n    tmp22 =1.0 \n    tmp23 =tmp22 -tmp21 \n    tmp24 =tl .full ([1 ],True ,tl .int1 )\n    tmp25 =tl .where (tmp24 ,tmp23 ,tmp11 )\n    tmp26 =tmp21 -tmp11 \n    tmp27 =triton_helpers .maximum (tmp26 ,tmp11 )\n    tmp28 =tl .full ([1 ],False ,tl .int1 )\n    tmp29 =tl .where (tmp28 ,tmp27 ,tmp11 )\n    tmp30 =tmp25 +tmp29 \n    tmp31 =tmp30 /tmp22 \n    tl .store (out_ptr0 +(tl .broadcast_to (r0_2 ,[R0_BLOCK ])),tmp12 ,None )\n    tl .store (out_ptr2 +(tl .full ([1 ],0 ,tl .int32 )),tmp31 ,None )\n    tl .store (out_ptr1 +(tl .full ([1 ],0 ,tl .int32 )),tmp16 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_2 ,(256 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_3 ,(256 ,),(1 ,))\n    assert_size_stride (primals_4 ,(256 ,),(1 ,))\n    assert_size_stride (primals_5 ,(256 ,),(1 ,))\n    assert_size_stride (primals_6 ,(512 ,256 ),(256 ,1 ))\n    assert_size_stride (primals_7 ,(512 ,),(1 ,))\n    assert_size_stride (primals_8 ,(512 ,),(1 ,))\n    assert_size_stride (primals_9 ,(512 ,),(1 ,))\n    assert_size_stride (primals_10 ,(256 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_11 ,(256 ,),(1 ,))\n    assert_size_stride (primals_12 ,(256 ,),(1 ,))\n    assert_size_stride (primals_13 ,(256 ,),(1 ,))\n    assert_size_stride (primals_14 ,(128 ,256 ),(256 ,1 ))\n    assert_size_stride (primals_15 ,(128 ,),(1 ,))\n    assert_size_stride (primals_16 ,(128 ,),(1 ,))\n    assert_size_stride (primals_17 ,(128 ,),(1 ,))\n    assert_size_stride (primals_18 ,(64 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_19 ,(64 ,),(1 ,))\n    assert_size_stride (primals_20 ,(64 ,),(1 ,))\n    assert_size_stride (primals_21 ,(64 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_3 ,primals_1 ,reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf0 )\n        del primals_2 \n        del primals_3 \n        buf1 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf4 =buf2 ;del buf2 \n        buf5 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_native_layer_norm_relu_0 [grid (1 )](buf4 ,buf0 ,primals_4 ,primals_5 ,buf1 ,buf5 ,1 ,256 ,num_warps =2 ,num_stages =1 )\n        del primals_5 \n        buf6 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_7 ,buf5 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),alpha =1 ,beta =1 ,out =buf6 )\n        del primals_7 \n        buf7 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf10 =buf8 ;del buf8 \n        buf11 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_native_layer_norm_relu_1 [grid (1 )](buf10 ,buf6 ,primals_8 ,primals_9 ,buf7 ,buf11 ,1 ,512 ,num_warps =4 ,num_stages =1 )\n        del primals_9 \n        buf12 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_11 ,buf11 ,reinterpret_tensor (primals_10 ,(512 ,256 ),(1 ,512 ),0 ),alpha =1 ,beta =1 ,out =buf12 )\n        del primals_11 \n        buf13 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf14 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf16 =buf14 ;del buf14 \n        buf17 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_native_layer_norm_relu_0 [grid (1 )](buf16 ,buf12 ,primals_12 ,primals_13 ,buf13 ,buf17 ,1 ,256 ,num_warps =2 ,num_stages =1 )\n        del primals_13 \n        buf18 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_15 ,buf17 ,reinterpret_tensor (primals_14 ,(256 ,128 ),(1 ,256 ),0 ),alpha =1 ,beta =1 ,out =buf18 )\n        del primals_15 \n        buf19 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf20 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf22 =buf20 ;del buf20 \n        buf23 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_native_layer_norm_relu_2 [grid (1 )](buf22 ,buf18 ,primals_16 ,primals_17 ,buf19 ,buf23 ,1 ,128 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_17 \n        buf24 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_19 ,buf23 ,reinterpret_tensor (primals_18 ,(128 ,64 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf24 )\n        del primals_19 \n        buf25 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf26 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf28 =buf26 ;del buf26 \n        buf29 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n        buf30 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_per_fused_abs_le_native_layer_norm_relu_3 [grid (1 )](buf28 ,buf24 ,primals_20 ,primals_21 ,buf25 ,buf29 ,buf30 ,1 ,64 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_21 \n        buf31 =empty_strided_cuda ((16 ,),(1 ,),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_arange_mul_4 [grid (16 )](buf31 ,16 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        buf32 =empty_strided_cuda ((1 ,1 ,16 ,16 ),(256 ,256 ,16 ,1 ),torch .float32 )\n        buf33 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf34 =empty_strided_cuda ((),(),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__unsafe_index_add_clamp_min_div_eq_fill_mean_mul_sqrt_sub_sum_where_zeros_like_5 [grid (1 )](buf31 ,buf30 ,buf29 ,buf32 ,buf33 ,buf34 ,1 ,256 ,num_warps =2 ,num_stages =1 )\n        del buf29 \n    return (buf34 ,primals_4 ,primals_8 ,primals_12 ,primals_16 ,primals_20 ,primals_1 ,buf0 ,buf1 ,buf4 ,buf5 ,buf6 ,buf7 ,buf10 ,buf11 ,buf12 ,buf13 ,buf16 ,buf17 ,buf18 ,buf19 ,buf22 ,buf23 ,buf24 ,buf25 ,buf28 ,buf30 ,buf31 ,buf32 ,buf33 ,primals_18 ,primals_14 ,primals_10 ,primals_6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((256 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((512 ,256 ),(256 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((256 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((128 ,256 ),(256 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_18 =rand_strided ((64 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "cb07b6b3-8154-4aae-9840-71d56c4f3c6f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ModuleDict', 'LPPool3d', 'MultiMarginLoss', 'Mish', 'FeatureAlphaDropout', 'SoftMarginLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.module_dict = nn.ModuleDict({\n            'lppool3d_1': nn.LPPool3d(norm_type=2, kernel_size=2, stride=2),\n            'lppool3d_2': nn.LPPool3d(norm_type=2, kernel_size=2, stride=2),\n            'mish_1': nn.Mish(),\n            'mish_2': nn.Mish(),\n            'feature_alpha_dropout': nn.FeatureAlphaDropout(p=0.5),\n        })\n        self.multi_margin_loss = nn.MultiMarginLoss()\n        self.soft_margin_loss = nn.SoftMarginLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, depth, height, width)\n        x = self.module_dict['lppool3d_1'](x)\n        x = self.module_dict['mish_1'](x)\n        x = self.module_dict['feature_alpha_dropout'](x)\n        x = self.module_dict['lppool3d_2'](x)\n        x = self.module_dict['mish_2'](x)\n        \n        # Flatten the tensor for loss computation\n        x = x.view(x.size(0), -1)\n        \n        # Dummy target for loss computation\n        target = torch.randint(0, 10, (x.size(0),), device=x.device)\n        \n        # Compute losses\n        multi_margin_loss = self.multi_margin_loss(x, target)\n        soft_margin_loss = self.soft_margin_loss(x, target.float())\n        \n        # Return the average of the two losses\n        return (multi_margin_loss + soft_margin_loss) / 2\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_abs_add_avg_pool3d_bernoulli_mish_mul_pow_relu_sign_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =((xindex //ks2 )%ks3 )\n    x3 =xindex //ks4 \n    x5 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(ks7 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr0 +(1 +ks7 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp14 =tl .load (in_ptr0 +(1 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr0 +(ks7 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp20 =tl .load (in_ptr0 +(1 +ks7 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp45 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tmp0 *tmp0 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tmp3 +tmp1 \n    tmp6 =tmp5 *tmp5 \n    tmp7 =tmp6 +tmp4 \n    tmp9 =tmp8 *tmp8 \n    tmp10 =tmp9 +tmp7 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tmp12 +tmp10 \n    tmp15 =tmp14 *tmp14 \n    tmp16 =tmp15 +tmp13 \n    tmp18 =tmp17 *tmp17 \n    tmp19 =tmp18 +tmp16 \n    tmp21 =tmp20 *tmp20 \n    tmp22 =tmp21 +tmp19 \n    tmp23 =0.125 \n    tmp24 =tmp22 *tmp23 \n    tmp25 =tl .full ([1 ],0 ,tl .int32 )\n    tmp26 =tmp25 <tmp24 \n    tmp27 =tmp26 .to (tl .int8 )\n    tmp28 =tmp24 <tmp25 \n    tmp29 =tmp28 .to (tl .int8 )\n    tmp30 =tmp27 -tmp29 \n    tmp31 =tmp30 .to (tmp24 .dtype )\n    tmp32 =tl_math .abs (tmp24 )\n    tmp33 =triton_helpers .maximum (tmp25 ,tmp32 )\n    tmp34 =tmp31 *tmp33 \n    tmp35 =8.0 \n    tmp36 =tmp34 *tmp35 \n    tmp37 =libdevice .sqrt (tmp36 )\n    tmp38 =20.0 \n    tmp39 =tmp37 >tmp38 \n    tmp40 =tl_math .exp (tmp37 )\n    tmp41 =libdevice .log1p (tmp40 )\n    tmp42 =tl .where (tmp39 ,tmp37 ,tmp41 )\n    tmp43 =libdevice .tanh (tmp42 )\n    tmp44 =tmp37 *tmp43 \n    tmp46 =0.5 \n    tmp47 =tmp45 <tmp46 \n    tmp48 =tmp47 .to (tl .float32 )\n    tmp49 =0.8864048946659319 \n    tmp50 =tmp48 *tmp49 \n    tmp51 =tmp44 *tmp50 \n    tmp52 =-1.0 \n    tmp53 =tmp48 +tmp52 \n    tmp54 =1.558387861036063 \n    tmp55 =tmp53 *tmp54 \n    tmp56 =0.7791939305180315 \n    tmp57 =tmp55 +tmp56 \n    tmp58 =tmp51 +tmp57 \n    tmp59 =tmp58 *tmp58 \n    tl .store (in_out_ptr0 +(x5 ),tmp59 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_abs_add_avg_pool3d_bernoulli_mish_mul_pow_relu_sign_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,ks8 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =((xindex //ks2 )%ks3 )\n    x3 =xindex //ks4 \n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(ks5 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(1 +ks5 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(ks8 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr0 +(1 +ks8 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(ks5 +ks8 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr0 +(1 +ks5 +ks8 +2 *x0 +2 *ks5 *x1 +2 *ks5 *ks6 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp8 =tmp7 +tmp6 \n    tmp10 =tmp9 +tmp8 \n    tmp12 =tmp11 +tmp10 \n    tmp14 =tmp13 +tmp12 \n    tmp15 =0.125 \n    tmp16 =tmp14 *tmp15 \n    tl .store (out_ptr0 +(x4 ),tmp16 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__to_copy_add_arange_clamp_min_div_gather_mean_ne_randint_rsub_scalar_tensor_soft_margin_loss_where_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,load_seed_offset ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp57 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp65 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp32 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp2 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp3 =tl .full ([1 ,1 ],10 ,tl .int64 )\n        tmp4 =triton_helpers .randint64 (tmp0 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp3 )\n        tmp5 =ks1 *ks2 *ks3 *ks4 \n        tmp6 =tmp4 +tmp5 \n        tmp7 =tmp4 <0 \n        tmp8 =tl .where (tmp7 ,tmp6 ,tmp4 )\n        tl .device_assert ((0 <=tmp8 )&(tmp8 <ks4 *(ks5 //4 )*(ks6 //4 )*(ks7 //4 )),\"index out of bounds: 0 <= tmp8 < ks4*(ks5 // 4)*(ks6 // 4)*(ks7 // 4)\")\n        tmp10 =tl .load (in_ptr1 +((tmp8 %(ks1 *ks2 *ks3 *ks4 ))),None ,eviction_policy ='evict_last')\n        tmp11 =tmp1 <tmp10 \n        tmp12 =tmp11 .to (tl .int8 )\n        tmp13 =tmp10 <tmp1 \n        tmp14 =tmp13 .to (tl .int8 )\n        tmp15 =tmp12 -tmp14 \n        tmp16 =tmp15 .to (tmp10 .dtype )\n        tmp17 =tl_math .abs (tmp10 )\n        tmp18 =triton_helpers .maximum (tmp1 ,tmp17 )\n        tmp19 =tmp16 *tmp18 \n        tmp20 =8.0 \n        tmp21 =tmp19 *tmp20 \n        tmp22 =libdevice .sqrt (tmp21 )\n        tmp23 =20.0 \n        tmp24 =tmp22 >tmp23 \n        tmp25 =tl_math .exp (tmp22 )\n        tmp26 =libdevice .log1p (tmp25 )\n        tmp27 =tl .where (tmp24 ,tmp22 ,tmp26 )\n        tmp28 =libdevice .tanh (tmp27 )\n        tmp29 =tmp22 *tmp28 \n        tmp30 =1.0 \n        tmp31 =tmp30 -tmp29 \n        tmp33 =tmp1 <tmp32 \n        tmp34 =tmp33 .to (tl .int8 )\n        tmp35 =tmp32 <tmp1 \n        tmp36 =tmp35 .to (tl .int8 )\n        tmp37 =tmp34 -tmp36 \n        tmp38 =tmp37 .to (tmp32 .dtype )\n        tmp39 =tl_math .abs (tmp32 )\n        tmp40 =triton_helpers .maximum (tmp1 ,tmp39 )\n        tmp41 =tmp38 *tmp40 \n        tmp42 =tmp41 *tmp20 \n        tmp43 =libdevice .sqrt (tmp42 )\n        tmp44 =tmp43 >tmp23 \n        tmp45 =tl_math .exp (tmp43 )\n        tmp46 =libdevice .log1p (tmp45 )\n        tmp47 =tl .where (tmp44 ,tmp43 ,tmp46 )\n        tmp48 =libdevice .tanh (tmp47 )\n        tmp49 =tmp43 *tmp48 \n        tmp50 =tmp31 +tmp49 \n        tmp51 =r0_0 \n        tmp52 =tmp51 !=tmp4 \n        tmp53 =0.0 \n        tmp54 =triton_helpers .maximum (tmp50 ,tmp53 )\n        tmp55 =tl .where (tmp52 ,tmp54 ,tmp53 )\n        tmp56 =tl .broadcast_to (tmp55 ,[XBLOCK ,R0_BLOCK ])\n        tmp58 =_tmp57 +tmp56 \n        _tmp57 =tl .where (r0_mask ,tmp58 ,_tmp57 )\n        tmp59 =-tmp49 \n        tmp60 =tmp4 .to (tl .float32 )\n        tmp61 =tmp59 *tmp60 \n        tmp62 =tl_math .exp (tmp61 )\n        tmp63 =libdevice .log1p (tmp62 )\n        tmp64 =tl .broadcast_to (tmp63 ,[XBLOCK ,R0_BLOCK ])\n        tmp66 =_tmp65 +tmp64 \n        _tmp65 =tl .where (r0_mask ,tmp66 ,_tmp65 )\n    tmp57 =tl .sum (_tmp57 ,1 )[:,None ]\n    tmp65 =tl .sum (_tmp65 ,1 )[:,None ]\n    tmp67 =ks1 *ks2 *ks3 *ks4 \n    tmp68 =tmp67 .to (tl .float32 )\n    tmp69 =tmp57 /tmp68 \n    tmp70 =tmp65 /tmp68 \n    tmp71 =tmp69 +tmp70 \n    tmp72 =0.5 \n    tmp73 =tmp71 *tmp72 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp73 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf1 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf1 ,buf2 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        s3 //2 \n        s2 //2 \n        (s2 //2 )*(s3 //2 )\n        s1 //2 \n        (s1 //2 )*(s2 //2 )*(s3 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ,s3 //2 ),(s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),(s1 //2 )*(s2 //2 )*(s3 //2 ),(s2 //2 )*(s3 //2 ),s3 //2 ,1 ),torch .float32 )\n        buf3 =buf0 ;del buf0 \n\n        triton_poi_fused__to_copy_abs_add_avg_pool3d_bernoulli_mish_mul_pow_relu_sign_1_xnumel =s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_abs_add_avg_pool3d_bernoulli_mish_mul_pow_relu_sign_1 [grid (triton_poi_fused__to_copy_abs_add_avg_pool3d_bernoulli_mish_mul_pow_relu_sign_1_xnumel )](buf3 ,arg4_1 ,buf2 ,16 ,16 ,256 ,16 ,4096 ,32 ,32 ,32 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg4_1 \n        del buf2 \n        s3 //4 \n        s2 //4 \n        (s2 //4 )*(s3 //4 )\n        s1 //4 \n        (s1 //4 )*(s2 //4 )*(s3 //4 )\n        buf4 =empty_strided_cuda ((1 ,s0 ,s1 //4 ,s2 //4 ,s3 //4 ),(s0 *(s1 //4 )*(s2 //4 )*(s3 //4 ),(s1 //4 )*(s2 //4 )*(s3 //4 ),(s2 //4 )*(s3 //4 ),s3 //4 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_abs_add_avg_pool3d_bernoulli_mish_mul_pow_relu_sign_2_xnumel =s0 *(s1 //4 )*(s2 //4 )*(s3 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_abs_add_avg_pool3d_bernoulli_mish_mul_pow_relu_sign_2 [grid (triton_poi_fused__to_copy_abs_add_avg_pool3d_bernoulli_mish_mul_pow_relu_sign_2_xnumel )](buf3 ,buf4 ,8 ,8 ,64 ,8 ,512 ,16 ,16 ,16 ,256 ,1536 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        buf6 =empty_strided_cuda ((),(),torch .float32 )\n        buf8 =buf6 ;del buf6 \n\n        s0 *(s1 //4 )*(s2 //4 )*(s3 //4 )\n        get_raw_stream (0 )\n        triton_red_fused__to_copy_add_arange_clamp_min_div_gather_mean_ne_randint_rsub_scalar_tensor_soft_margin_loss_where_3 [grid (1 )](buf8 ,buf1 ,buf4 ,1 ,8 ,8 ,8 ,3 ,32 ,32 ,32 ,1 ,1536 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf1 \n        del buf4 \n    return (buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "cd3566a2-e2b6-4ced-87b8-adf97efec741",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ZeroPad3d', 'CircularPad1d', 'LogSigmoid', 'Dropout2d', 'MaxPool1d', 'ReplicationPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad3d = nn.ZeroPad3d(1)\n        self.circular_pad1d = nn.CircularPad1d(2)\n        self.log_sigmoid = nn.LogSigmoid()\n        self.dropout2d = nn.Dropout2d(p=0.5)\n        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.replication_pad3d = nn.ReplicationPad3d(1)\n        \n    def forward(self, x):\n        # Apply ZeroPad3d\n        x = self.zero_pad3d(x)\n        \n        # Reshape to 1D for CircularPad1d\n        x = x.view(x.size(0), -1)\n        x = self.circular_pad1d(x)\n        \n        # Reshape back to 3D for Dropout2d\n        x = x.view(x.size(0), 1, -1, x.size(1))\n        x = self.dropout2d(x)\n        \n        # Reshape to 1D for MaxPool1d\n        x = x.view(x.size(0), -1)\n        x = self.max_pool1d(x)\n        \n        # Reshape to 3D for ReplicationPad3d\n        x = x.view(x.size(0), 1, -1, x.size(1))\n        x = self.replication_pad3d(x)\n        \n        # Apply LogSigmoid\n        x = self.log_sigmoid(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape: (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],2 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =x0 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .broadcast_to (2 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 ,[XBLOCK ])\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =(-1 )+(((((-2 )+x0 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 )))\n    tmp11 =tl .full ([1 ],0 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .broadcast_to (ks1 ,[XBLOCK ])\n    tmp14 =tmp10 <tmp13 \n    tmp15 =(-1 )+(((((-2 )+x0 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//(2 +ks3 ))%(2 +ks2 )))\n    tmp16 =tmp15 >=tmp11 \n    tmp17 =tl .broadcast_to (ks2 ,[XBLOCK ])\n    tmp18 =tmp15 <tmp17 \n    tmp19 =(-1 )+((((-2 )+x0 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )%(2 +ks3 )))\n    tmp20 =tmp19 >=tmp11 \n    tmp21 =tl .broadcast_to (ks3 ,[XBLOCK ])\n    tmp22 =tmp19 <tmp21 \n    tmp23 =tmp12 &tmp14 \n    tmp24 =tmp23 &tmp16 \n    tmp25 =tmp24 &tmp18 \n    tmp26 =tmp25 &tmp20 \n    tmp27 =tmp26 &tmp22 \n    tmp28 =tmp27 &tmp9 \n    tmp29 =tl .load (in_ptr0 +((-1 )+((-1 )*ks3 )+ks3 *(((((-2 )+x0 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//(2 +ks3 ))%(2 +ks2 )))+((-1 )*ks2 *ks3 )+ks2 *ks3 *(((((-2 )+x0 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 )))+ks1 *ks2 *ks3 *(((((-2 )+x0 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )//(8 +4 *ks1 +4 *ks2 +4 *ks3 +2 *ks1 *ks2 +2 *ks1 *ks3 +2 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+((((-2 )+x0 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 )%(2 +ks3 )))),tmp28 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp30 =tl .full (tmp29 .shape ,0.0 ,tmp29 .dtype )\n    tmp31 =tl .where (tmp9 ,tmp29 ,tmp30 )\n    tmp32 =float (\"nan\")\n    tmp33 =tl .where (tmp8 ,tmp31 ,tmp32 )\n    tmp34 =tl .full (tmp33 .shape ,0.0 ,tmp33 .dtype )\n    tmp35 =tl .where (tmp2 ,tmp33 ,tmp34 )\n    tmp36 =tmp0 >=tmp1 \n    tmp37 =2 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n    tmp38 =tmp0 <tmp37 \n    tmp39 =tmp36 &tmp38 \n    tmp40 =(-1 )+(((((-2 )+x0 )//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 )))\n    tmp41 =tl .full ([1 ],0 ,tl .int64 )\n    tmp42 =tmp40 >=tmp41 \n    tmp43 =tl .broadcast_to (ks1 ,[XBLOCK ])\n    tmp44 =tmp40 <tmp43 \n    tmp45 =(-1 )+(((((-2 )+x0 )//(2 +ks3 ))%(2 +ks2 )))\n    tmp46 =tmp45 >=tmp41 \n    tmp47 =tl .broadcast_to (ks2 ,[XBLOCK ])\n    tmp48 =tmp45 <tmp47 \n    tmp49 =(-1 )+((((-2 )+x0 )%(2 +ks3 )))\n    tmp50 =tmp49 >=tmp41 \n    tmp51 =tl .broadcast_to (ks3 ,[XBLOCK ])\n    tmp52 =tmp49 <tmp51 \n    tmp53 =tmp42 &tmp44 \n    tmp54 =tmp53 &tmp46 \n    tmp55 =tmp54 &tmp48 \n    tmp56 =tmp55 &tmp50 \n    tmp57 =tmp56 &tmp52 \n    tmp58 =tmp57 &tmp39 \n    tmp59 =tl .load (in_ptr0 +((-1 )+((-1 )*ks3 )+ks3 *(((((-2 )+x0 )//(2 +ks3 ))%(2 +ks2 )))+((-1 )*ks2 *ks3 )+ks2 *ks3 *(((((-2 )+x0 )//(4 +2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks1 )))+ks1 *ks2 *ks3 *(((((-2 )+x0 )//(8 +4 *ks1 +4 *ks2 +4 *ks3 +2 *ks1 *ks2 +2 *ks1 *ks3 +2 *ks2 *ks3 +ks1 *ks2 *ks3 ))%ks0 ))+((((-2 )+x0 )%(2 +ks3 )))),tmp58 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp60 =tl .full (tmp59 .shape ,0.0 ,tmp59 .dtype )\n    tmp61 =tl .where (tmp39 ,tmp59 ,tmp60 )\n    tmp62 =float (\"nan\")\n    tmp63 =tl .where (tmp39 ,tmp61 ,tmp62 )\n    tmp64 =tl .where (tmp2 ,tmp35 ,tmp63 )\n    tl .store (out_ptr0 +(x0 ),tmp64 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp4 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp6 =tl .load (in_ptr1 +(0 ))\n    tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ])\n    tmp0 =x0 \n    tmp1 =2 +8 *ks0 +4 *ks0 *ks1 +4 *ks0 *ks2 +4 *ks0 *ks3 +2 *ks0 *ks1 *ks2 +2 *ks0 *ks1 *ks3 +2 *ks0 *ks2 *ks3 +ks0 *ks1 *ks2 *ks3 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .load (in_ptr0 +(x0 +((-8 )*ks0 )+((-4 )*ks0 *ks1 )+((-4 )*ks0 *ks2 )+((-4 )*ks0 *ks3 )+((-2 )*ks0 *ks1 *ks2 )+((-2 )*ks0 *ks1 *ks3 )+((-2 )*ks0 *ks2 *ks3 )+((-1 )*ks0 *ks1 *ks2 *ks3 )),tmp2 &xmask ,other =0.0 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp8 =0.5 \n    tmp9 =tmp7 <tmp8 \n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =2.0 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 *tmp12 \n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_log_sigmoid_forward_replication_pad3d_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))<=(1 +4 *ks1 +2 *ks1 *ks2 +2 *ks1 *ks3 +2 *ks1 *ks4 +ks1 *ks2 *ks3 +ks1 *ks2 *ks4 +ks1 *ks3 *ks4 +((ks1 *ks2 *ks3 *ks4 )//2 )))+(1 +4 *ks1 +2 *ks1 *ks2 +2 *ks1 *ks3 +2 *ks1 *ks4 +ks1 *ks2 *ks3 +ks1 *ks2 *ks4 +ks1 *ks3 *ks4 +((ks1 *ks2 *ks3 *ks4 )//2 ))*((1 +4 *ks1 +2 *ks1 *ks2 +2 *ks1 *ks3 +2 *ks1 *ks4 +ks1 *ks2 *ks3 +ks1 *ks2 *ks4 +ks1 *ks3 *ks4 +((ks1 *ks2 *ks3 *ks4 )//2 ))<(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))*((((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))<=(1 +4 *ks1 +2 *ks1 *ks2 +2 *ks1 *ks3 +2 *ks1 *ks4 +ks1 *ks2 *ks3 +ks1 *ks2 *ks4 +ks1 *ks3 *ks4 +((ks1 *ks2 *ks3 *ks4 )//2 )))+(1 +4 *ks1 +2 *ks1 *ks2 +2 *ks1 *ks3 +2 *ks1 *ks4 +ks1 *ks2 *ks3 +ks1 *ks2 *ks4 +ks1 *ks3 *ks4 +((ks1 *ks2 *ks3 *ks4 )//2 ))*((1 +4 *ks1 +2 *ks1 *ks2 +2 *ks1 *ks3 +2 *ks1 *ks4 +ks1 *ks2 *ks3 +ks1 *ks2 *ks4 +ks1 *ks3 *ks4 +((ks1 *ks2 *ks3 *ks4 )//2 ))<(((0 )*((0 )>=((-1 )+x0 ))+((-1 )+x0 )*(((-1 )+x0 )>(0 ))))))),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp3 =0.0 \n    tmp4 =triton_helpers .minimum (tmp3 ,tmp2 )\n    tmp5 =tl_math .abs (tmp2 )\n    tmp6 =-tmp5 \n    tmp7 =tl_math .exp (tmp6 )\n    tmp8 =libdevice .log1p (tmp7 )\n    tmp9 =tmp4 -tmp8 \n    tl .store (out_ptr0 +(x2 ),tmp9 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,4 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ),(4 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_0_xnumel =4 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (triton_poi_fused_copy_0_xnumel )](arg4_1 ,buf1 ,3 ,32 ,32 ,32 ,117916 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg4_1 \n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,1 ,1 ,1 ),(1 ,1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_1 [grid (1 )](buf2 ,buf3 ,0 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf2 \n        buf4 =empty_strided_cuda ((1 ,1 ,1 ,4 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ),(4 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,4 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,4 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_bernoulli_div_mul_2_xnumel =4 +8 *s0 +4 *s0 *s1 +4 *s0 *s2 +4 *s0 *s3 +2 *s0 *s1 *s2 +2 *s0 *s1 *s3 +2 *s0 *s2 *s3 +s0 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_2 [grid (triton_poi_fused__to_copy_bernoulli_div_mul_2_xnumel )](buf1 ,buf3 ,buf4 ,3 ,32 ,32 ,32 ,117916 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf1 \n        del buf3 \n        4 +4 *s0 +2 *s0 *s1 +2 *s0 *s2 +2 *s0 *s3 +s0 *s1 *s2 +s0 *s1 *s3 +s0 *s2 *s3 +((s0 *s1 *s2 *s3 )//2 )\n        buf5 =empty_strided_cuda ((1 ,3 ,3 ,4 +4 *s0 +2 *s0 *s1 +2 *s0 *s2 +2 *s0 *s3 +s0 *s1 *s2 +s0 *s1 *s3 +s0 *s2 *s3 +((s0 *s1 *s2 *s3 )//2 )),(36 +9 *((s0 *s1 *s2 *s3 )//2 )+36 *s0 +18 *s0 *s1 +18 *s0 *s2 +18 *s0 *s3 +9 *s0 *s1 *s2 +9 *s0 *s1 *s3 +9 *s0 *s2 *s3 ,12 +3 *((s0 *s1 *s2 *s3 )//2 )+12 *s0 +6 *s0 *s1 +6 *s0 *s2 +6 *s0 *s3 +3 *s0 *s1 *s2 +3 *s0 *s1 *s3 +3 *s0 *s2 *s3 ,4 +4 *s0 +2 *s0 *s1 +2 *s0 *s2 +2 *s0 *s3 +s0 *s1 *s2 +s0 *s1 *s3 +s0 *s2 *s3 +((s0 *s1 *s2 *s3 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_log_sigmoid_forward_replication_pad3d_3_xnumel =36 +9 *((s0 *s1 *s2 *s3 )//2 )+36 *s0 +18 *s0 *s1 +18 *s0 *s2 +18 *s0 *s3 +9 *s0 *s1 *s2 +9 *s0 *s1 *s3 +9 *s0 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused_log_sigmoid_forward_replication_pad3d_3 [grid (triton_poi_fused_log_sigmoid_forward_replication_pad3d_3_xnumel )](buf4 ,buf5 ,58960 ,3 ,32 ,32 ,32 ,530640 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf4 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "cde8ab50-15e8-40dc-b0e3-604134ee6bd7",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['UpsamplingNearest2d', 'HuberLoss', 'Sigmoid', 'PairwiseDistance']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample1 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.upsample2 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.sigmoid = nn.Sigmoid()\n        self.pairwise_distance = nn.PairwiseDistance(p=2)\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Upsample the input twice\n        x = self.upsample1(x)\n        x = self.upsample2(x)\n        \n        # Apply sigmoid activation\n        x = self.sigmoid(x)\n        \n        # Reshape the tensor to have two dimensions for pairwise distance\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x1 = x[:, :x.size(1)//2]  # Split into two halves\n        x2 = x[:, x.size(1)//2:]\n        \n        # Compute pairwise distance between the two halves\n        dist = self.pairwise_distance(x1, x2)\n        \n        # Compute Huber loss between the distance and a target (e.g., zeros)\n        target = torch.zeros_like(dist)\n        loss = self.huber_loss(dist, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__unsafe_index_sigmoid_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks1 )%ks2 )\n    x0 =(xindex %ks1 )\n    x2 =xindex //ks4 \n    x3 =xindex \n    tmp0 =2.0 \n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =tmp0 *tmp2 \n    tmp4 =tmp3 .to (tl .float64 )\n    tmp5 =tl .full ([1 ],2.0 ,tl .float64 )\n    tmp6 =tmp5 *tmp4 \n    tmp7 =tmp4 /tmp6 \n    tmp8 =tmp7 .to (tl .float32 )\n    tmp9 =x1 \n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp10 *tmp8 \n    tmp12 =tmp11 .to (tl .int64 )\n    tmp13 =2 *ks0 \n    tmp14 =tmp12 +tmp13 \n    tmp15 =tmp12 <0 \n    tmp16 =tl .where (tmp15 ,tmp14 ,tmp12 )\n    tmp17 =ks3 \n    tmp18 =tmp17 .to (tl .float32 )\n    tmp19 =tmp0 *tmp18 \n    tmp20 =tmp19 .to (tl .float64 )\n    tmp21 =tmp5 *tmp20 \n    tmp22 =tmp20 /tmp21 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =x0 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 *tmp23 \n    tmp27 =tmp26 .to (tl .int64 )\n    tmp28 =2 *ks3 \n    tmp29 =tmp27 +tmp28 \n    tmp30 =tmp27 <0 \n    tmp31 =tl .where (tmp30 ,tmp29 ,tmp27 )\n    tmp32 =tmp1 .to (tl .float64 )\n    tmp33 =tmp5 *tmp32 \n    tmp34 =tmp32 /tmp33 \n    tmp35 =tmp34 .to (tl .float32 )\n    tmp36 =tmp16 \n    tmp37 =tmp36 .to (tl .float32 )\n    tmp38 =tmp37 *tmp35 \n    tmp39 =tmp38 .to (tl .int64 )\n    tmp40 =tmp39 +tmp1 \n    tmp41 =tmp39 <0 \n    tmp42 =tl .where (tmp41 ,tmp40 ,tmp39 )\n    tmp43 =tmp17 .to (tl .float64 )\n    tmp44 =tmp5 *tmp43 \n    tmp45 =tmp43 /tmp44 \n    tmp46 =tmp45 .to (tl .float32 )\n    tmp47 =tmp31 \n    tmp48 =tmp47 .to (tl .float32 )\n    tmp49 =tmp48 *tmp46 \n    tmp50 =tmp49 .to (tl .int64 )\n    tmp51 =tmp50 +tmp17 \n    tmp52 =tmp50 <0 \n    tmp53 =tl .where (tmp52 ,tmp51 ,tmp50 )\n    tmp54 =tl .load (in_ptr0 +(tmp53 +ks3 *tmp42 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp55 =tl .sigmoid (tmp54 )\n    tl .store (out_ptr0 +(x3 ),tmp55 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_norm_sub_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp12 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((2 +8 *ks0 *ks1 *ks2 )//3 )\n        tmp1 =8 *ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((2 +8 *ks0 *ks1 *ks2 )//3 ))%(16 *ks0 *ks1 *ks2 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr0 +(4 *ks2 *((((r0_1 +x0 *((2 +8 *ks0 *ks1 *ks2 )//3 )+8 *ks0 *ks1 *ks2 )//ks3 )%(4 *ks0 *ks1 )))+(((r0_1 +x0 *((2 +8 *ks0 *ks1 *ks2 )//3 ))%ks3 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tmp3 -tmp4 \n        tmp6 =1e-06 \n        tmp7 =tmp5 +tmp6 \n        tmp8 =tmp7 *tmp7 \n        tmp9 =tl .full (tmp8 .shape ,0 ,tmp8 .dtype )\n        tmp10 =tl .where (tmp2 ,tmp8 ,tmp9 )\n        tmp11 =tl .broadcast_to (tmp10 ,[XBLOCK ,R0_BLOCK ])\n        tmp13 =_tmp12 +tmp11 \n        _tmp12 =tl .where (r0_mask &xmask ,tmp13 ,_tmp12 )\n    tmp12 =tl .sum (_tmp12 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_huber_loss_norm_sub_2 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =3 \n    R0_BLOCK :tl .constexpr =4 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =libdevice .sqrt (tmp4 )\n    tmp6 =tl_math .abs (tmp5 )\n    tmp7 =1.0 \n    tmp8 =tmp6 <tmp7 \n    tmp9 =0.5 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =tmp10 *tmp6 \n    tmp12 =tmp6 -tmp9 \n    tmp13 =tmp12 *tmp7 \n    tmp14 =tl .where (tmp8 ,tmp11 ,tmp13 )\n    tmp15 =tmp14 /tmp7 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp15 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 *s2 \n        4 *s1 \n        16 *s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 *s1 ,4 *s2 ),(16 *s0 *s1 *s2 ,16 *s1 *s2 ,4 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__unsafe_index_sigmoid_0_xnumel =16 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__unsafe_index_sigmoid_0 [grid (triton_poi_fused__unsafe_index_sigmoid_0_xnumel )](arg3_1 ,buf0 ,32 ,128 ,128 ,32 ,16384 ,49152 ,XBLOCK =512 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf1 =empty_strided_cuda ((1 ,3 ),(3 ,1 ),torch .float32 )\n\n        (2 +8 *s0 *s1 *s2 )//3 \n        get_raw_stream (0 )\n        triton_red_fused_add_norm_sub_1 [grid (3 )](buf0 ,buf1 ,3 ,32 ,32 ,128 ,3 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf3 =reinterpret_tensor (buf2 ,(),(),0 );del buf2 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_huber_loss_norm_sub_2 [grid (1 )](buf3 ,buf1 ,1 ,3 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ce6deaeb-1144-4b81-8b55-8b6071ef332e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool3d', 'PixelShuffle', 'GLU', 'NLLLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool3d = nn.MaxPool3d(kernel_size=2, stride=2)\n        self.pixelshuffle = nn.PixelShuffle(upscale_factor=2)\n        self.glu = nn.GLU(dim=1)\n        self.nllloss = nn.NLLLoss()\n\n    def forward(self, x):\n        # Apply MaxPool3d\n        x = self.maxpool3d(x)\n        \n        # Reshape to 4D tensor for PixelShuffle\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size, channels * depth, height, width)\n        \n        # Apply PixelShuffle\n        x = self.pixelshuffle(x)\n        \n        # Reshape back to 5D tensor for GLU\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels // 2, 2, height, width)\n        \n        # Apply GLU\n        x = self.glu(x)\n        \n        # Reshape to 2D tensor for NLLLoss\n        x = x.view(batch_size, -1)\n        \n        # Apply NLLLoss (assuming target is provided externally)\n        # Note: NLLLoss requires a target, which is not part of the input tensor.\n        # For the purpose of this model, we will return the log probabilities.\n        # In practice, you would compute the loss using the target.\n        log_probs = F.log_softmax(x, dim=1)\n        return log_probs\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 32, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax_0 (in_ptr0 ,out_ptr2 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp5 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +((ks1 //2 )*((((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))//2 )%(ks0 //2 )))+(ks0 //2 )*(ks1 //2 )*((((r0_0 %(2 *(ks1 //2 ))))%2 ))+2 *(ks0 //2 )*(ks1 //2 )*(((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))%2 ))+4 *(ks0 //2 )*(ks1 //2 )*(triton_helpers .div_floor_integer (r0_0 ,4 *(ks0 //2 )*(ks1 //2 )))+(((((r0_0 %(2 *(ks1 //2 ))))//2 )%(ks1 //2 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +((ks1 //2 )*((((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))//2 )%(ks0 //2 )))+8 *(ks0 //2 )*(ks1 //2 )+(ks0 //2 )*(ks1 //2 )*((((r0_0 %(2 *(ks1 //2 ))))%2 ))+2 *(ks0 //2 )*(ks1 //2 )*(((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))%2 ))+4 *(ks0 //2 )*(ks1 //2 )*(triton_helpers .div_floor_integer (r0_0 ,4 *(ks0 //2 )*(ks1 //2 )))+(((((r0_0 %(2 *(ks1 //2 ))))//2 )%(ks1 //2 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tl .sigmoid (tmp1 )\n        tmp3 =tmp0 *tmp2 \n        tmp4 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n        tmp6 =triton_helpers .maximum (_tmp5 ,tmp4 )\n        _tmp5 =tl .where (r0_mask ,tmp6 ,_tmp5 )\n    tmp5 =triton_helpers .max2 (_tmp5 ,1 )[:,None ]\n    _tmp14 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp7 =tl .load (in_ptr0 +((ks1 //2 )*((((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))//2 )%(ks0 //2 )))+(ks0 //2 )*(ks1 //2 )*((((r0_0 %(2 *(ks1 //2 ))))%2 ))+2 *(ks0 //2 )*(ks1 //2 )*(((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))%2 ))+4 *(ks0 //2 )*(ks1 //2 )*(triton_helpers .div_floor_integer (r0_0 ,4 *(ks0 //2 )*(ks1 //2 )))+(((((r0_0 %(2 *(ks1 //2 ))))//2 )%(ks1 //2 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp8 =tl .load (in_ptr0 +((ks1 //2 )*((((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))//2 )%(ks0 //2 )))+8 *(ks0 //2 )*(ks1 //2 )+(ks0 //2 )*(ks1 //2 )*((((r0_0 %(2 *(ks1 //2 ))))%2 ))+2 *(ks0 //2 )*(ks1 //2 )*(((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))%2 ))+4 *(ks0 //2 )*(ks1 //2 )*(triton_helpers .div_floor_integer (r0_0 ,4 *(ks0 //2 )*(ks1 //2 )))+(((((r0_0 %(2 *(ks1 //2 ))))//2 )%(ks1 //2 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp9 =tl .sigmoid (tmp8 )\n        tmp10 =tmp7 *tmp9 \n        tmp11 =tmp10 -tmp5 \n        tmp12 =tl_math .exp (tmp11 )\n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =_tmp14 +tmp13 \n        _tmp14 =tl .where (r0_mask ,tmp15 ,_tmp14 )\n    tmp14 =tl .sum (_tmp14 ,1 )[:,None ]\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp16 =tl .load (in_ptr0 +((ks1 //2 )*((((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))//2 )%(ks0 //2 )))+(ks0 //2 )*(ks1 //2 )*((((r0_0 %(2 *(ks1 //2 ))))%2 ))+2 *(ks0 //2 )*(ks1 //2 )*(((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))%2 ))+4 *(ks0 //2 )*(ks1 //2 )*(triton_helpers .div_floor_integer (r0_0 ,4 *(ks0 //2 )*(ks1 //2 )))+(((((r0_0 %(2 *(ks1 //2 ))))//2 )%(ks1 //2 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp17 =tl .load (in_ptr0 +((ks1 //2 )*((((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))//2 )%(ks0 //2 )))+8 *(ks0 //2 )*(ks1 //2 )+(ks0 //2 )*(ks1 //2 )*((((r0_0 %(2 *(ks1 //2 ))))%2 ))+2 *(ks0 //2 )*(ks1 //2 )*(((((r0_0 //(2 *(ks1 //2 )))%(2 *(ks0 //2 ))))%2 ))+4 *(ks0 //2 )*(ks1 //2 )*(triton_helpers .div_floor_integer (r0_0 ,4 *(ks0 //2 )*(ks1 //2 )))+(((((r0_0 %(2 *(ks1 //2 ))))//2 )%(ks1 //2 )))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp18 =tl .sigmoid (tmp17 )\n        tmp19 =tmp16 *tmp18 \n        tmp20 =tmp19 -tmp5 \n        tmp21 =tl_math .log (tmp14 )\n        tmp22 =tmp20 -tmp21 \n        tl .store (out_ptr2 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp22 ,r0_mask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .max_pool3d_with_indices .default (arg3_1 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del arg3_1 \n        buf1 =buf0 [0 ]\n        del buf0 \n        buf5 =empty_strided_cuda ((1 ,8 *(s1 //2 )*(s2 //2 )),(2 *(s0 //(4 *(s0 //8 )))*(s0 //(8 *(s0 //16 )))*(s1 //2 )*(s2 //2 ),1 ),torch .float32 )\n\n        8 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_red_fused__log_softmax_0 [grid (1 )](buf1 ,buf5 ,32 ,32 ,1 ,2048 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf1 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =32 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,1 ,32 ,32 ,32 ),(32768 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ceb5085c-d8e4-4483-b34f-2178d9956bbc",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softshrink', 'GRU', 'InstanceNorm3d', 'LazyInstanceNorm2d', 'MultiMarginLoss', 'Flatten', 'PairwiseDistance', 'TransformerEncoderLayer', 'AvgPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softshrink = nn.Softshrink(lambd=0.5)\n        self.gru = nn.GRU(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.instance_norm3d = nn.InstanceNorm3d(num_features=32)\n        self.lazy_instance_norm2d = nn.LazyInstanceNorm2d()\n        self.multi_margin_loss = nn.MultiMarginLoss(p=1, margin=1.0)\n        self.flatten = nn.Flatten()\n        self.pairwise_distance = nn.PairwiseDistance(p=2)\n        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n        self.avg_pool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, depth, height, width)\n        x = self.instance_norm3d(x)  # Apply InstanceNorm3d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape for GRU\n        x, _ = self.gru(x)  # Apply GRU\n        x = x.view(x.size(0), x.size(1), int(x.size(2)**0.5), int(x.size(2)**0.5))  # Reshape back\n        x = self.lazy_instance_norm2d(x)  # Apply LazyInstanceNorm2d\n        x = self.avg_pool2d(x)  # Apply AvgPool2d\n        x = self.softshrink(x)  # Apply Softshrink\n        x = self.flatten(x)  # Flatten the tensor\n        x = x.unsqueeze(0)  # Add batch dimension for TransformerEncoderLayer\n        x = self.transformer_encoder_layer(x)  # Apply TransformerEncoderLayer\n        x = x.squeeze(0)  # Remove batch dimension\n        x = x.view(x.size(0), -1)  # Reshape for PairwiseDistance\n        x = self.pairwise_distance(x, x)  # Apply PairwiseDistance\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 32, 64, 64, 64).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %32 )\n    x1 =xindex //32 \n    tmp13_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp13_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp13_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =r0_2 +x0 *((31 +ks0 *ks1 *ks2 )//32 )\n        tmp1 =ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(ks0 *ks1 *ks2 *x1 +(((r0_2 +x0 *((31 +ks0 *ks1 *ks2 )//32 ))%(ks0 *ks1 *ks2 )))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =0.0 \n        tmp5 =tl .full (tmp4 .shape ,0 ,tmp4 .dtype )\n        tmp6 =tl .where (tmp2 ,tmp4 ,tmp5 )\n        tmp7 =1.0 \n        tmp8 =tl .full (tmp7 .shape ,0 ,tmp7 .dtype )\n        tmp9 =tl .where (tmp2 ,tmp7 ,tmp8 )\n        tmp10 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n        tmp11 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp13_mean_next ,tmp13_m2_next ,tmp13_weight_next =triton_helpers .welford_combine (\n        tmp13_mean ,tmp13_m2 ,tmp13_weight ,\n        tmp10 ,tmp11 ,tmp12 \n        )\n        tmp13_mean =tl .where (r0_mask &xmask ,tmp13_mean_next ,tmp13_mean )\n        tmp13_m2 =tl .where (r0_mask &xmask ,tmp13_m2_next ,tmp13_m2 )\n        tmp13_weight =tl .where (r0_mask &xmask ,tmp13_weight_next ,tmp13_weight )\n    tmp16 ,tmp17 ,tmp18 =triton_helpers .welford (tmp13_mean ,tmp13_m2 ,tmp13_weight ,1 )\n    tmp13 =tmp16 [:,None ]\n    tmp14 =tmp17 [:,None ]\n    tmp15 =tmp18 [:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp13 ,xmask )\n    tl .store (out_ptr1 +(x3 ),tmp14 ,xmask )\n    tl .store (out_ptr2 +(x3 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    R0_BLOCK :tl .constexpr =32 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +32 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 +32 *x0 ),xmask ,other =0.0 )\n    tmp2 =tl .load (in_ptr2 +(r0_1 +32 *x0 ),xmask ,other =0.0 )\n    tmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (xmask ,tmp3 ,0 )\n    tmp8 =tl .where (xmask ,tmp4 ,0 )\n    tmp9 =tl .where (xmask ,tmp5 ,0 )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\n    tmp13 =tmp10 [:,None ]\n    tmp14 =tmp11 [:,None ]\n    tmp15 =tmp12 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp4 =ks0 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tmp7 =1e-05 \n    tmp8 =tmp6 +tmp7 \n    tmp9 =libdevice .rsqrt (tmp8 )\n    tmp10 =tmp2 *tmp9 \n    tl .store (out_ptr0 +(x2 ),tmp10 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    s2 =arg1_1 \n    s3 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,32 ,s1 ,s2 ,s3 ),(32 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ,32 ),(1024 ,32 ,1024 ,1024 ,1024 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ,32 ),(1024 ,32 ,1024 ,1024 ,1024 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ,32 ),(1024 ,32 ,1024 ,1024 ,1024 ,1 ),torch .float32 )\n\n        (31 +s1 *s2 *s3 )//32 \n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_0 [grid (1024 )](arg3_1 ,buf0 ,buf1 ,buf2 ,64 ,64 ,64 ,1024 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,32 ,32 ,32 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,32 ,32 ,32 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_1 [grid (32 )](buf0 ,buf1 ,buf2 ,buf3 ,buf4 ,32 ,32 ,XBLOCK =8 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n        del buf1 \n        del buf2 \n        s1 *s2 *s3 \n        buf6 =empty_strided_cuda ((1 ,32 ,s1 ,s2 ,s3 ),(32 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ),torch .float32 )\n\n        triton_poi_fused__native_batch_norm_legit_2_xnumel =32 *s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_2 [grid (triton_poi_fused__native_batch_norm_legit_2_xnumel )](arg3_1 ,buf3 ,buf4 ,buf6 ,262144 ,8388608 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del arg3_1 \n        del buf3 \n        del buf4 \n    return (reinterpret_tensor (buf6 ,(1 ,32 ,s1 *s2 *s3 ),(32 *s1 *s2 *s3 ,s1 *s2 *s3 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,32 ,64 ,64 ,64 ),(8388608 ,262144 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "cf2cf5d8-b494-429b-9e6f-e706a3576a9d",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveMaxPool3d', 'TransformerDecoderLayer', 'RReLU', 'TripletMarginWithDistanceLoss', 'Hardshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((5, 5, 5))\n        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=128, nhead=8)\n        self.rrelu = nn.RReLU()\n        self.hardshrink = nn.Hardshrink()\n        \n        # Assuming the input is 3D, we need to reshape it to fit the TransformerDecoderLayer\n        self.fc1 = nn.Linear(5 * 5 * 5, 128)\n        self.fc2 = nn.Linear(128, 10)\n        \n        # TripletMarginWithDistanceLoss is a loss function, so it won't be part of the forward pass\n        self.loss_fn = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n\n    def forward(self, x):\n        # Apply AdaptiveMaxPool3d\n        x = self.adaptive_max_pool3d(x)\n        \n        # Reshape for the TransformerDecoderLayer\n        batch_size = x.size(0)\n        x = x.view(batch_size, -1)\n        x = self.fc1(x)\n        x = x.unsqueeze(0)  # Add sequence dimension for TransformerDecoderLayer\n        \n        # Apply TransformerDecoderLayer\n        memory = torch.zeros_like(x)  # Dummy memory for the decoder\n        x = self.transformer_decoder_layer(x, memory)\n        \n        # Remove sequence dimension\n        x = x.squeeze(0)\n        \n        # Apply RReLU\n        x = self.rrelu(x)\n        \n        # Apply Hardshrink\n        x = self.hardshrink(x)\n        \n        # Final fully connected layer\n        x = self.fc2(x)\n        \n        return x\n\n    def compute_loss(self, anchor, positive, negative):\n        # Compute triplet loss\n        return self.loss_fn(anchor, positive, negative)\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 10, 10, 10).cuda()  # Example input for AdaptiveMaxPool3d\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n# Example usage:\n# model = Model().cuda()\n# inputs = get_inputs()\n# output = model(*inputs)\n# loss = model.compute_loss(output, output, output)  # Example loss computation\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp5 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp7 =tl .load (in_out_ptr0 +(r0_0 ),None )\n    tmp8 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp34 =tl .load (in_ptr3 +(r0_0 ),None )\n    tmp36 =tl .load (in_ptr4 +(r0_0 ),None )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tmp16 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp18 =tl .sum (tmp16 ,1 )[:,None ]\n    tmp19 =tl .full ([XBLOCK ,1 ],128 ,tl .int32 )\n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =tmp18 /tmp20 \n    tmp22 =tmp14 -tmp21 \n    tmp23 =tmp22 *tmp22 \n    tmp24 =tl .broadcast_to (tmp23 ,[XBLOCK ,R0_BLOCK ])\n    tmp26 =tl .sum (tmp24 ,1 )[:,None ]\n    tmp27 =tmp13 -tmp21 \n    tmp28 =128.0 \n    tmp29 =tmp26 /tmp28 \n    tmp30 =1e-05 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =libdevice .rsqrt (tmp31 )\n    tmp33 =tmp27 *tmp32 \n    tmp35 =tmp33 *tmp34 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =0.0078125 \n    tmp39 =tmp32 *tmp38 \n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp4 ,None )\n    tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp33 ,None )\n    tl .store (out_ptr4 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp37 ,None )\n    tl .store (out_ptr5 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp39 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_view_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp5 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp7 =tl .load (in_out_ptr0 +(r0_0 ),None )\n    tmp8 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp34 =tl .load (in_ptr3 +(r0_0 ),None )\n    tmp36 =tl .load (in_ptr4 +(r0_0 ),None )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tmp16 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp18 =tl .sum (tmp16 ,1 )[:,None ]\n    tmp19 =tl .full ([XBLOCK ,1 ],128 ,tl .int32 )\n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =tmp18 /tmp20 \n    tmp22 =tmp14 -tmp21 \n    tmp23 =tmp22 *tmp22 \n    tmp24 =tl .broadcast_to (tmp23 ,[XBLOCK ,R0_BLOCK ])\n    tmp26 =tl .sum (tmp24 ,1 )[:,None ]\n    tmp27 =tmp13 -tmp21 \n    tmp28 =128.0 \n    tmp29 =tmp26 /tmp28 \n    tmp30 =1e-05 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =libdevice .rsqrt (tmp31 )\n    tmp33 =tmp27 *tmp32 \n    tmp35 =tmp33 *tmp34 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =0.0078125 \n    tmp39 =tmp32 *tmp38 \n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp4 ,None )\n    tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp33 ,None )\n    tl .store (out_ptr4 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp37 ,None )\n    tl .store (out_ptr5 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp39 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_threshold_backward_3 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =2048 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp6 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp7 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.0 \n    tmp15 =tmp10 <=tmp14 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr3 +(x0 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_le_rrelu_with_noise_functional_scalar_tensor_where_4 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =0.0 \n    tmp2 =tmp0 <=tmp1 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =tl .where (tmp2 ,tmp4 ,tmp0 )\n    tmp6 =tl_math .abs (tmp5 )\n    tmp7 =0.5 \n    tmp8 =tmp6 <=tmp7 \n    tmp9 =tl .where (tmp8 ,tmp1 ,tmp5 )\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp8 ,xmask )\n    tl .store (in_out_ptr0 +(x0 ),tmp9 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 =args \n    args .clear ()\n    s0 =primals_1 \n    s1 =primals_2 \n    s2 =primals_3 \n    assert_size_stride (primals_4 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    assert_size_stride (primals_5 ,(128 ,125 ),(125 ,1 ))\n    assert_size_stride (primals_6 ,(128 ,),(1 ,))\n    assert_size_stride (primals_7 ,(384 ,),(1 ,))\n    assert_size_stride (primals_8 ,(384 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_9 ,(128 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_10 ,(128 ,),(1 ,))\n    assert_size_stride (primals_11 ,(128 ,),(1 ,))\n    assert_size_stride (primals_12 ,(128 ,),(1 ,))\n    assert_size_stride (primals_13 ,(384 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_14 ,(384 ,),(1 ,))\n    assert_size_stride (primals_15 ,(128 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_16 ,(128 ,),(1 ,))\n    assert_size_stride (primals_17 ,(128 ,),(1 ,))\n    assert_size_stride (primals_18 ,(128 ,),(1 ,))\n    assert_size_stride (primals_19 ,(2048 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_20 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_21 ,(128 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_22 ,(128 ,),(1 ,))\n    assert_size_stride (primals_23 ,(128 ,),(1 ,))\n    assert_size_stride (primals_24 ,(128 ,),(1 ,))\n    assert_size_stride (primals_25 ,(10 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_26 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .adaptive_max_pool3d .default (primals_4 ,[5 ,5 ,5 ])\n        del primals_4 \n        buf1 =buf0 [0 ]\n        del buf0 \n        buf3 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_6 ,reinterpret_tensor (buf1 ,(1 ,125 ),(125 ,1 ),0 ),reinterpret_tensor (primals_5 ,(125 ,128 ),(1 ,125 ),0 ),alpha =1 ,beta =1 ,out =buf3 )\n        del primals_5 \n        del primals_6 \n        buf4 =empty_strided_cuda ((1 ,384 ),(384 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_7 ,buf3 ,reinterpret_tensor (primals_8 ,(128 ,384 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf4 )\n        del primals_7 \n\n        buf5 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf4 ,(1 ,8 ,1 ,16 ),(128 ,16 ,384 ,1 ),0 ),reinterpret_tensor (buf4 ,(1 ,8 ,1 ,16 ),(128 ,16 ,384 ,1 ),128 ),reinterpret_tensor (buf4 ,(1 ,8 ,1 ,16 ),(128 ,16 ,384 ,1 ),256 ),None ,True ,0.1 )\n        buf6 =buf5 [0 ]\n        buf7 =buf5 [1 ]\n        buf8 =buf5 [2 ]\n        buf9 =buf5 [3 ]\n        del buf5 \n        buf10 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf6 ,(1 ,128 ),(128 ,1 ),0 ),reinterpret_tensor (primals_9 ,(128 ,128 ),(1 ,128 ),0 ),out =buf10 )\n        buf11 =empty_strided_cuda ((4 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[4 ],out =buf11 )\n        buf13 =empty_strided_cuda ((1 ,1 ,128 ),(128 ,128 ,1 ),torch .bool )\n        buf17 =reinterpret_tensor (buf10 ,(1 ,1 ,128 ),(128 ,128 ,1 ),0 );del buf10 \n        buf18 =empty_strided_cuda ((1 ,1 ,128 ),(128 ,128 ,1 ),torch .float32 )\n        buf56 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_0 [grid (1 )](buf17 ,buf11 ,buf3 ,primals_10 ,primals_11 ,primals_12 ,buf13 ,buf18 ,buf56 ,3 ,1 ,128 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_10 \n        del primals_12 \n        buf19 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (reinterpret_tensor (primals_14 ,(128 ,),(1 ,),0 ),reinterpret_tensor (buf18 ,(1 ,128 ),(0 ,1 ),0 ),reinterpret_tensor (primals_13 ,(128 ,128 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf19 )\n        buf20 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_view_1 [grid (128 )](buf20 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf21 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (reinterpret_tensor (primals_14 ,(256 ,),(1 ,),128 ),buf20 ,reinterpret_tensor (primals_13 ,(128 ,256 ),(1 ,128 ),16384 ),alpha =1 ,beta =1 ,out =buf21 )\n        del primals_14 \n\n        buf22 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf19 ,(1 ,8 ,1 ,16 ),(128 ,16 ,128 ,1 ),0 ),reinterpret_tensor (buf21 ,(1 ,8 ,1 ,16 ),(128 ,16 ,256 ,1 ),0 ),reinterpret_tensor (buf21 ,(1 ,8 ,1 ,16 ),(128 ,16 ,256 ,1 ),128 ),None ,True ,0.1 )\n        buf23 =buf22 [0 ]\n        buf24 =buf22 [1 ]\n        buf25 =buf22 [2 ]\n        buf26 =buf22 [3 ]\n        del buf22 \n        buf27 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf23 ,(1 ,128 ),(128 ,1 ),0 ),reinterpret_tensor (primals_15 ,(128 ,128 ),(1 ,128 ),0 ),out =buf27 )\n        buf29 =empty_strided_cuda ((1 ,1 ,128 ),(128 ,128 ,1 ),torch .bool )\n        buf33 =reinterpret_tensor (buf27 ,(1 ,1 ,128 ),(128 ,128 ,1 ),0 );del buf27 \n        buf34 =empty_strided_cuda ((1 ,1 ,128 ),(128 ,128 ,1 ),torch .float32 )\n        buf55 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_2 [grid (1 )](buf33 ,buf11 ,buf18 ,primals_16 ,primals_17 ,primals_18 ,buf29 ,buf34 ,buf55 ,1 ,1 ,128 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_16 \n        del primals_18 \n        buf35 =empty_strided_cuda ((1 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf34 ,(1 ,128 ),(0 ,1 ),0 ),reinterpret_tensor (primals_19 ,(128 ,2048 ),(1 ,128 ),0 ),out =buf35 )\n        buf37 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n        buf38 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .float32 )\n        buf53 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_3 [grid (2048 )](buf11 ,buf35 ,primals_20 ,buf37 ,buf38 ,buf53 ,2 ,2048 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf35 \n        del primals_20 \n        buf39 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf38 ,(1 ,2048 ),(0 ,1 ),0 ),reinterpret_tensor (primals_21 ,(2048 ,128 ),(1 ,2048 ),0 ),out =buf39 )\n        buf41 =empty_strided_cuda ((1 ,1 ,128 ),(128 ,128 ,1 ),torch .bool )\n        buf45 =reinterpret_tensor (buf39 ,(1 ,1 ,128 ),(128 ,128 ,1 ),0 );del buf39 \n        buf46 =empty_strided_cuda ((1 ,1 ,128 ),(128 ,128 ,1 ),torch .float32 )\n        buf54 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_0 [grid (1 )](buf45 ,buf11 ,buf34 ,primals_22 ,primals_23 ,primals_24 ,buf41 ,buf46 ,buf54 ,3 ,1 ,128 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf11 \n        del primals_22 \n        del primals_24 \n\n        buf48 =torch .ops .aten .uniform .default (reinterpret_tensor (buf46 ,(1 ,128 ),(0 ,1 ),0 ),0.125 ,0.3333333333333333 )\n        buf49 =buf48 \n        del buf48 \n        buf47 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .bool )\n        buf50 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .bool )\n        buf51 =reinterpret_tensor (buf46 ,(1 ,128 ),(128 ,1 ),0 );del buf46 \n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_le_rrelu_with_noise_functional_scalar_tensor_where_4 [grid (128 )](buf51 ,buf49 ,buf47 ,buf50 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf52 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_26 ,buf51 ,reinterpret_tensor (primals_25 ,(128 ,10 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf52 )\n        del primals_26 \n    return (buf52 ,primals_11 ,primals_17 ,primals_23 ,reinterpret_tensor (buf1 ,(1 ,125 ),(125 ,1 ),0 ),buf3 ,reinterpret_tensor (buf4 ,(1 ,8 ,1 ,16 ),(128 ,16 ,384 ,1 ),0 ),reinterpret_tensor (buf4 ,(1 ,8 ,1 ,16 ),(128 ,16 ,384 ,1 ),128 ),reinterpret_tensor (buf4 ,(1 ,8 ,1 ,16 ),(128 ,16 ,384 ,1 ),256 ),buf6 ,buf7 ,buf8 ,buf9 ,buf13 ,buf17 ,reinterpret_tensor (buf18 ,(1 ,128 ),(128 ,1 ),0 ),buf20 ,reinterpret_tensor (buf19 ,(1 ,8 ,1 ,16 ),(128 ,16 ,128 ,1 ),0 ),reinterpret_tensor (buf21 ,(1 ,8 ,1 ,16 ),(128 ,16 ,256 ,1 ),0 ),reinterpret_tensor (buf21 ,(1 ,8 ,1 ,16 ),(128 ,16 ,256 ,1 ),128 ),buf23 ,buf24 ,buf25 ,buf26 ,buf29 ,buf33 ,reinterpret_tensor (buf34 ,(1 ,128 ),(128 ,1 ),0 ),buf37 ,reinterpret_tensor (buf38 ,(1 ,2048 ),(2048 ,1 ),0 ),buf41 ,buf45 ,buf47 ,buf49 ,buf50 ,buf51 ,primals_25 ,buf54 ,primals_21 ,buf53 ,primals_19 ,buf55 ,primals_15 ,reinterpret_tensor (primals_13 ,(128 ,128 ),(128 ,1 ),0 ),buf56 ,primals_9 ,primals_8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =10 \n    primals_2 =10 \n    primals_3 =10 \n    primals_4 =rand_strided ((1 ,1 ,10 ,10 ,10 ),(1000 ,1000 ,100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((128 ,125 ),(125 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((384 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((384 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((128 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((384 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((384 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((128 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_18 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((2048 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((128 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_22 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_23 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_24 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_25 =rand_strided ((10 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_26 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d058c33b-d6e7-4007-8b29-37309e9d29c1",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['InstanceNorm2d', 'ReplicationPad2d', 'AdaptiveAvgPool1d', 'MaxPool2d', 'GELU', 'PixelUnshuffle']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.instance_norm = nn.InstanceNorm2d(3)\n        self.replication_pad = nn.ReplicationPad2d(2)\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool1d(128)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.gelu = nn.GELU()\n        self.pixel_unshuffle = nn.PixelUnshuffle(2)\n\n    def forward(self, x):\n        # Apply InstanceNorm2d\n        x = self.instance_norm(x)\n        \n        # Apply ReplicationPad2d\n        x = self.replication_pad(x)\n        \n        # Apply MaxPool2d\n        x = self.max_pool(x)\n        \n        # Apply GELU\n        x = self.gelu(x)\n        \n        # Apply PixelUnshuffle\n        x = self.pixel_unshuffle(x)\n        \n        # Reshape for AdaptiveAvgPool1d\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels * height, width)\n        \n        # Apply AdaptiveAvgPool1d\n        x = self.adaptive_avg_pool(x)\n        \n        # Reshape back to 4D tensor\n        x = x.view(batch_size, channels, height, -1)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp3 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_replication_pad2d_view_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks4 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 )))))+(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))*((((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *x2 +(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp4 =ks3 *ks4 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tmp7 =1e-05 \n    tmp8 =tmp6 +tmp7 \n    tmp9 =libdevice .rsqrt (tmp8 )\n    tmp10 =tmp2 *tmp9 \n    tl .store (out_ptr0 +(x3 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_pixel_unshuffle_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +8 *x1 +16 *x2 +2 *ks4 *x1 +4 *ks3 *x2 +4 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +8 *x1 +16 *x2 +2 *ks4 *x1 +4 *ks3 *x2 +4 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(4 +ks4 +2 *x0 +8 *x1 +16 *x2 +2 *ks4 *x1 +4 *ks3 *x2 +4 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(5 +ks4 +2 *x0 +8 *x1 +16 *x2 +2 *ks4 *x1 +4 *ks3 *x2 +4 *ks4 *x2 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\n    tmp7 =0.5 \n    tmp8 =tmp6 *tmp7 \n    tmp9 =0.7071067811865476 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =libdevice .erf (tmp10 )\n    tmp12 =1.0 \n    tmp13 =tmp11 +tmp12 \n    tmp14 =tmp8 *tmp13 \n    tl .store (out_ptr0 +(x3 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %128 )\n    x1 =xindex //128 \n    x2 =xindex \n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =(17 *x0 )//128 \n    tmp4 =(144 +17 *x0 )//128 \n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp2 &tmp5 \n    tmp7 =tl .load (in_ptr0 +(2 *((17 *x0 )//128 )+4 *((x1 %(1 +(ks0 //4 ))))+2 *(ks1 //4 )*(triton_helpers .div_floor_integer (((x1 //(1 +(ks0 //4 )))%12 ),2 ))+4 *(ks0 //4 )*(triton_helpers .div_floor_integer (((x1 //(1 +(ks0 //4 )))%12 ),4 ))+4 *(ks1 //4 )*((x1 %(1 +(ks0 //4 ))))+4 *(ks0 //4 )*(ks1 //4 )*(triton_helpers .div_floor_integer (((x1 //(1 +(ks0 //4 )))%12 ),4 ))+(((x1 //(1 +(ks0 //4 )))%12 ))),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =1 +((17 *x0 )//128 )\n    tmp9 =tmp8 <tmp4 \n    tmp10 =tmp2 &tmp9 \n    tmp11 =tl .load (in_ptr0 +(2 +2 *((17 *x0 )//128 )+4 *((x1 %(1 +(ks0 //4 ))))+2 *(ks1 //4 )*(triton_helpers .div_floor_integer (((x1 //(1 +(ks0 //4 )))%12 ),2 ))+4 *(ks0 //4 )*(triton_helpers .div_floor_integer (((x1 //(1 +(ks0 //4 )))%12 ),4 ))+4 *(ks1 //4 )*((x1 %(1 +(ks0 //4 ))))+4 *(ks0 //4 )*(ks1 //4 )*(triton_helpers .div_floor_integer (((x1 //(1 +(ks0 //4 )))%12 ),4 ))+(((x1 //(1 +(ks0 //4 )))%12 ))),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tmp11 +tmp7 \n    tmp13 =1.0 \n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =1.0 \n    tmp17 =tl .full (tmp16 .shape ,0.0 ,tmp16 .dtype )\n    tmp18 =tl .where (tmp10 ,tmp16 ,tmp17 )\n    tmp19 =tmp18 +tmp15 \n    tmp20 =tmp12 /tmp19 \n    tl .store (out_ptr0 +(x2 ),tmp20 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    s2 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,3 ,s1 ,s2 ),(3 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n\n        s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_0 [grid (3 )](arg2_1 ,buf0 ,buf1 ,64 ,64 ,3 ,4096 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf3 =empty_strided_cuda ((1 ,3 ,4 +s1 ,4 +s2 ),(48 +12 *s1 +12 *s2 +3 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__native_batch_norm_legit_replication_pad2d_view_1_xnumel =48 +12 *s1 +12 *s2 +3 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_replication_pad2d_view_1 [grid (triton_poi_fused__native_batch_norm_legit_replication_pad2d_view_1_xnumel )](arg2_1 ,buf0 ,buf1 ,buf3 ,68 ,68 ,4624 ,64 ,64 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        del buf0 \n        del buf1 \n        2 +2 *(s2 //4 )\n        2 +2 *(s1 //4 )\n        4 +4 *(s1 //4 )+4 *(s2 //4 )+4 *(s1 //4 )*(s2 //4 )\n        buf4 =empty_strided_cuda ((1 ,3 ,2 ,2 ,1 +(s1 //4 ),1 +(s2 //4 )),(12 +12 *(s1 //4 )+12 *(s2 //4 )+12 *(s1 //4 )*(s2 //4 ),4 +4 *(s1 //4 )+4 *(s2 //4 )+4 *(s1 //4 )*(s2 //4 ),2 +2 *(s2 //4 ),1 ,4 +4 *(s2 //4 ),2 ),torch .float32 )\n\n        triton_poi_fused_pixel_unshuffle_2_xnumel =12 +12 *(s1 //4 )+12 *(s2 //4 )+12 *(s1 //4 )*(s2 //4 )\n        get_raw_stream (0 )\n        triton_poi_fused_pixel_unshuffle_2 [grid (triton_poi_fused_pixel_unshuffle_2_xnumel )](buf3 ,buf4 ,34 ,34 ,1156 ,64 ,64 ,3468 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        buf5 =empty_strided_cuda ((1 ,3 *((2 +(s1 //2 ))//(1 +(s1 //4 )))*((2 +(s2 //2 ))//(1 +(s2 //4 )))+3 *(s1 //4 )*((2 +(s1 //2 ))//(1 +(s1 //4 )))*((2 +(s2 //2 ))//(1 +(s2 //4 ))),1 ,128 ),(384 *((2 +(s1 //2 ))//(1 +(s1 //4 )))*((2 +(s2 //2 ))//(1 +(s2 //4 )))+384 *(s1 //4 )*((2 +(s1 //2 ))//(1 +(s1 //4 )))*((2 +(s2 //2 ))//(1 +(s2 //4 ))),128 ,128 ,1 ),torch .float32 )\n\n        triton_poi_fused__adaptive_avg_pool2d_3_xnumel =384 *((2 +(s1 //2 ))//(1 +(s1 //4 )))*((2 +(s2 //2 ))//(1 +(s2 //4 )))+384 *(s1 //4 )*((2 +(s1 //2 ))//(1 +(s1 //4 )))*((2 +(s2 //2 ))//(1 +(s2 //4 )))\n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_3 [grid (triton_poi_fused__adaptive_avg_pool2d_3_xnumel )](buf4 ,buf5 ,64 ,64 ,26112 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf4 \n    return (reinterpret_tensor (buf5 ,(1 ,3 *((2 +(s1 //2 ))//(1 +(s1 //4 )))*((2 +(s2 //2 ))//(1 +(s2 //4 ))),1 +(s1 //4 ),128 ),(384 *((2 +(s1 //2 ))//(1 +(s1 //4 )))*((2 +(s2 //2 ))//(1 +(s2 //4 )))+384 *(s1 //4 )*((2 +(s1 //2 ))//(1 +(s1 //4 )))*((2 +(s2 //2 ))//(1 +(s2 //4 ))),128 +128 *(s1 //4 ),128 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d0c0498d-e333-41e8-af90-d31e619c6446",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ConstantPad3d', 'ReflectionPad2d', 'CrossMapLRN2d', 'ConvTranspose1d', 'Softmax', 'Conv3d', 'ConstantPad2d', 'SyncBatchNorm']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad3d = nn.ConstantPad3d(1, 1.0)\n        self.pad2d = nn.ReflectionPad2d(2)\n        self.lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.conv_transpose1d = nn.ConvTranspose1d(10, 20, kernel_size=5, stride=2)\n        self.softmax = nn.Softmax(dim=1)\n        self.conv3d = nn.Conv3d(1, 10, kernel_size=3)\n        self.pad2d_2 = nn.ConstantPad2d(1, 2.0)\n        self.sync_bn = nn.SyncBatchNorm(10)\n\n    def forward(self, x):\n        # Assuming input is 3D, pad it with ConstantPad3d\n        x = self.pad3d(x)\n        \n        # Reshape to 4D for ReflectionPad2d\n        x = x.view(x.size(0), x.size(1), x.size(2), -1)\n        x = self.pad2d(x)\n        \n        # Apply CrossMapLRN2d\n        x = self.lrn(x)\n        \n        # Reshape to 3D for ConvTranspose1d\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.conv_transpose1d(x)\n        \n        # Apply Softmax\n        x = self.softmax(x)\n        \n        # Reshape to 5D for Conv3d\n        x = x.view(x.size(0), 1, x.size(1), x.size(2), -1)\n        x = self.conv3d(x)\n        \n        # Reshape to 4D for ConstantPad2d\n        x = x.view(x.size(0), x.size(1), x.size(2), -1)\n        x = self.pad2d_2(x)\n        \n        # Reshape to 2D for SyncBatchNorm\n        x = x.view(x.size(0), x.size(1), -1)\n        x = self.sync_bn(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 10, 10, 10).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_reflection_pad2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //ks0 \n    x0 =(xindex %ks0 )\n    x2 =xindex \n    tmp0 =(-1 )+(tl .where (1 +ks1 +((-1 )*tl_math .abs (1 +ks1 +((-1 )*tl_math .abs ((-2 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks1 +((-1 )*tl_math .abs ((-2 )+x1 ))))+2 *ks1 ,1 +ks1 +((-1 )*tl_math .abs (1 +ks1 +((-1 )*tl_math .abs ((-2 )+x1 ))))))\n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+((((tl .where (3 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+2 *ks2 +2 *ks3 +ks2 *ks3 <0 ,7 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+4 *ks2 +4 *ks3 +2 *ks2 *ks3 ,3 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))//(2 +ks3 ))%(2 +ks2 )))\n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks2 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =(-1 )+(((tl .where (3 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+2 *ks2 +2 *ks3 +ks2 *ks3 <0 ,7 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+4 *ks2 +4 *ks3 +2 *ks2 *ks3 ,3 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks3 )))\n    tmp10 =tmp9 >=tmp1 \n    tmp11 =ks3 \n    tmp12 =tmp9 <tmp11 \n    tmp13 =tmp2 &tmp4 \n    tmp14 =tmp13 &tmp6 \n    tmp15 =tmp14 &tmp8 \n    tmp16 =tmp15 &tmp10 \n    tmp17 =tmp16 &tmp12 \n    tmp18 =tl .load (in_ptr0 +((-1 )+((-1 )*ks3 )+ks3 *((((tl .where (3 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+2 *ks2 +2 *ks3 +ks2 *ks3 <0 ,7 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+4 *ks2 +4 *ks3 +2 *ks2 *ks3 ,3 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))//(2 +ks3 ))%(2 +ks2 )))+((-1 )*ks2 *ks3 )+ks2 *ks3 *(tl .where (1 +ks1 +((-1 )*tl_math .abs (1 +ks1 +((-1 )*tl_math .abs ((-2 )+x1 ))))<0 ,3 +((-1 )*tl_math .abs (1 +ks1 +((-1 )*tl_math .abs ((-2 )+x1 ))))+2 *ks1 ,1 +ks1 +((-1 )*tl_math .abs (1 +ks1 +((-1 )*tl_math .abs ((-2 )+x1 ))))))+(((tl .where (3 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+2 *ks2 +2 *ks3 +ks2 *ks3 <0 ,7 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+4 *ks2 +4 *ks3 +2 *ks2 *ks3 ,3 +((-1 )*tl_math .abs (3 +((-1 )*tl_math .abs ((-2 )+x0 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))+2 *ks2 +2 *ks3 +ks2 *ks3 ))%(2 +ks3 )))),tmp17 &xmask ,eviction_policy ='evict_last',other =1.0 )\n    tl .store (out_ptr0 +(x2 ),tmp18 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        8 +2 *s1 +2 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,1 ,6 +s0 ,8 +2 *s1 +2 *s2 +s1 *s2 ),(48 +8 *s0 +12 *s1 +12 *s2 +2 *s0 *s1 +2 *s0 *s2 +6 *s1 *s2 +s0 *s1 *s2 ,48 +8 *s0 +12 *s1 +12 *s2 +2 *s0 *s1 +2 *s0 *s2 +6 *s1 *s2 +s0 *s1 *s2 ,8 +2 *s1 +2 *s2 +s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_reflection_pad2d_0_xnumel =48 +8 *s0 +12 *s1 +12 *s2 +2 *s0 *s1 +2 *s0 *s2 +6 *s1 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_reflection_pad2d_0 [grid (triton_poi_fused_reflection_pad2d_0_xnumel )](arg3_1 ,buf0 ,148 ,10 ,10 ,10 ,2368 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =10 \n    arg2_1 =10 \n    arg3_1 =rand_strided ((1 ,1 ,10 ,10 ,10 ),(1000 ,1000 ,100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d14b03cd-db82-4bd1-9c32-23636b6e324a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReflectionPad2d', 'MultiLabelMarginLoss', 'LPPool2d', 'SoftMarginLoss', 'SiLU', 'InstanceNorm1d', 'AvgPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reflection_pad = nn.ReflectionPad2d(2)\n        self.lp_pool = nn.LPPool2d(norm_type=2, kernel_size=3, stride=2)\n        self.silu = nn.SiLU()\n        self.instance_norm = nn.InstanceNorm1d(64)\n        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.multi_label_margin_loss = nn.MultiLabelMarginLoss()\n        self.soft_margin_loss = nn.SoftMarginLoss()\n\n    def forward(self, x):\n        # Apply ReflectionPad2d\n        x = self.reflection_pad(x)\n        \n        # Apply LPPool2d\n        x = self.lp_pool(x)\n        \n        # Apply SiLU activation\n        x = self.silu(x)\n        \n        # Reshape for InstanceNorm1d\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1)  # Flatten height and width\n        x = self.instance_norm(x)\n        x = x.view(batch_size, channels, height, width)  # Reshape back\n        \n        # Apply AvgPool2d\n        x = self.avg_pool(x)\n        \n        # Compute MultiLabelMarginLoss (dummy target for demonstration)\n        target = torch.randint(0, 2, (batch_size, 10)).float()  # Dummy target\n        x_flat = x.view(batch_size, -1)\n        ml_loss = self.multi_label_margin_loss(x_flat, target)\n        \n        # Compute SoftMarginLoss (dummy target for demonstration)\n        target_soft = torch.randint(0, 2, (batch_size, 10)).float() * 2 - 1  # Dummy target\n        sm_loss = self.soft_margin_loss(x_flat, target_soft)\n        \n        # Return the average of the two losses (for demonstration purposes)\n        return (ml_loss + sm_loss) / 2\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels, 64x64 spatial dimensions\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_avg_pool2d_mul_pow_reflection_pad2d_relu_sign_silu_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp2 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+2 *x0 ))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+2 *x0 ))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+2 *x0 ))))),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp14 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-1 )+2 *x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+2 *x0 ))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+2 *x0 ))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+2 *x0 ))))),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+2 *x1 ))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+2 *x1 ))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+2 *x1 ))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp20 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+2 *x1 ))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+2 *x1 ))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+2 *x1 ))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-1 )+2 *x0 )))))),xmask ,eviction_policy ='evict_last')\n    tmp23 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+2 *x1 ))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+2 *x1 ))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+2 *x1 ))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+2 *x0 ))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+2 *x0 ))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+2 *x0 ))))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tmp0 *tmp0 \n    tmp3 =tmp2 *tmp2 \n    tmp4 =tmp3 +tmp1 \n    tmp6 =tmp5 *tmp5 \n    tmp7 =tmp6 +tmp4 \n    tmp9 =tmp8 *tmp8 \n    tmp10 =tmp9 +tmp7 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tmp12 +tmp10 \n    tmp15 =tmp14 *tmp14 \n    tmp16 =tmp15 +tmp13 \n    tmp18 =tmp17 *tmp17 \n    tmp19 =tmp18 +tmp16 \n    tmp21 =tmp20 *tmp20 \n    tmp22 =tmp21 +tmp19 \n    tmp24 =tmp23 *tmp23 \n    tmp25 =tmp24 +tmp22 \n    tmp26 =0.1111111111111111 \n    tmp27 =tmp25 *tmp26 \n    tmp28 =tl .full ([1 ],0 ,tl .int32 )\n    tmp29 =tmp28 <tmp27 \n    tmp30 =tmp29 .to (tl .int8 )\n    tmp31 =tmp27 <tmp28 \n    tmp32 =tmp31 .to (tl .int8 )\n    tmp33 =tmp30 -tmp32 \n    tmp34 =tmp33 .to (tmp27 .dtype )\n    tmp35 =tl_math .abs (tmp27 )\n    tmp36 =triton_helpers .maximum (tmp28 ,tmp35 )\n    tmp37 =tmp34 *tmp36 \n    tmp38 =9.0 \n    tmp39 =tmp37 *tmp38 \n    tmp40 =libdevice .sqrt (tmp39 )\n    tmp41 =tl .sigmoid (tmp40 )\n    tmp42 =tmp40 *tmp41 \n    tl .store (in_out_ptr0 +(x3 ),tmp42 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        (3 +s2 )//2 \n        (3 +s1 )//2 \n        ((3 +s1 )//2 )*((3 +s2 )//2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,(3 +s1 )//2 ,(3 +s2 )//2 ),(s0 *((3 +s1 )//2 )*((3 +s2 )//2 ),((3 +s1 )//2 )*((3 +s2 )//2 ),(3 +s2 )//2 ,1 ),torch .float32 )\n        buf1 =buf0 ;del buf0 \n\n        triton_poi_fused_abs_avg_pool2d_mul_pow_reflection_pad2d_relu_sign_silu_0_xnumel =s0 *((3 +s1 )//2 )*((3 +s2 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_abs_avg_pool2d_mul_pow_reflection_pad2d_relu_sign_silu_0 [grid (triton_poi_fused_abs_avg_pool2d_mul_pow_reflection_pad2d_relu_sign_silu_0_xnumel )](buf1 ,arg3_1 ,33 ,33 ,1089 ,64 ,64 ,3267 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf1 ,(1 ,s0 ,((3 +s1 )//2 )*((3 +s2 )//2 )),(s0 +s0 *((1 +s1 )//2 )+s0 *((1 +s2 )//2 )+s0 *((1 +s1 )//2 )*((1 +s2 )//2 ),((3 +s1 )//2 )*((3 +s2 )//2 ),1 ),0 ),s0 ,1 +((1 +s1 )//2 ),1 +((1 +s2 )//2 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d229afa6-1842-40c4-8995-0aa538af736c",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softmax2d', 'GRU', 'HuberLoss', 'Softsign', 'LogSigmoid']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softmax2d = nn.Softmax2d()\n        self.gru1 = nn.GRU(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.gru2 = nn.GRU(input_size=128, hidden_size=64, num_layers=1, batch_first=True)\n        self.softsign = nn.Softsign()\n        self.logsigmoid = nn.LogSigmoid()\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        \n        # Apply Softmax2d\n        x = self.softmax2d(x)\n        \n        # Reshape for GRU (batch_size, sequence_length, feature_size)\n        x = x.view(batch_size, channels, -1)  # Flatten height and width\n        x = x.permute(0, 2, 1)  # Swap dimensions to (batch_size, sequence_length, feature_size)\n        \n        # Apply first GRU\n        x, _ = self.gru1(x)\n        \n        # Apply second GRU\n        x, _ = self.gru2(x)\n        \n        # Reshape back to (batch_size, channels, height, width)\n        x = x.permute(0, 2, 1)\n        x = x.view(batch_size, -1, height, width)\n        \n        # Apply Softsign\n        x = self.softsign(x)\n        \n        # Apply LogSigmoid\n        x = self.logsigmoid(x)\n        \n        # Compute Huber Loss with a dummy target (for demonstration purposes)\n        dummy_target = torch.zeros_like(x)\n        loss = self.huber_loss(x, dummy_target)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with batch_size=1, channels=3, height=64, width=64\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(x0 +ks0 *ks1 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =triton_helpers .maximum (_tmp2 ,tmp1 )\n        _tmp2 =tl .where (r0_mask &xmask ,tmp3 ,_tmp2 )\n    tmp2 =triton_helpers .max2 (_tmp2 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp4 =tl .load (in_ptr0 +(x0 +ks0 *ks1 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp5 =tmp4 -tmp2 \n        tmp6 =tl_math .exp (tmp5 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =_tmp8 +tmp7 \n        _tmp8 =tl .where (r0_mask &xmask ,tmp9 ,_tmp8 )\n    tmp8 =tl .sum (_tmp8 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %ks0 )\n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp3 =tl_math .exp (tmp2 )\n    tmp5 =tmp3 /tmp4 \n    tl .store (out_ptr0 +(x2 ),tmp5 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,s1 ,s2 ),(s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,1 ,s1 ,s2 ),(s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_red_fused__softmax_0_xnumel =s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_0 [grid (triton_red_fused__softmax_0_xnumel )](arg3_1 ,buf0 ,buf1 ,64 ,64 ,4096 ,3 ,XBLOCK =128 ,R0_BLOCK =4 ,num_warps =4 ,num_stages =1 )\n        s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__softmax_1_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__softmax_1 [grid (triton_poi_fused__softmax_1_xnumel )](arg3_1 ,buf0 ,buf1 ,buf2 ,4096 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf0 \n        del buf1 \n    return (reinterpret_tensor (buf2 ,(1 ,s1 *s2 ,s0 ),(s0 *s1 *s2 ,1 ,s1 *s2 ),0 ),s1 ,s2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d361d4be-97ca-410e-90fc-8117178dfefe",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Container', 'BCELoss', 'MaxPool3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.container = nn.Sequential(\n            nn.MaxPool3d(kernel_size=2, stride=2),\n            nn.MaxPool3d(kernel_size=2, stride=2),\n            nn.MaxPool3d(kernel_size=2, stride=2),\n            nn.MaxPool3d(kernel_size=2, stride=2),\n            nn.MaxPool3d(kernel_size=2, stride=2)\n        )\n        self.loss = nn.BCELoss()\n\n    def forward(self, x):\n        # Apply the container with multiple MaxPool3d layers\n        x = self.container(x)\n        \n        # Flatten the output to match the expected input shape for BCELoss\n        x = x.view(x.size(0), -1)\n        \n        # Apply a sigmoid to get values between 0 and 1 for BCELoss\n        x = torch.sigmoid(x)\n        \n        # Create a dummy target tensor for BCELoss\n        target = torch.zeros_like(x)\n        \n        # Compute the loss\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 64, 64, 64).cuda()  # Assuming 3D input for MaxPool3d\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_binary_cross_entropy_sigmoid_zeros_like_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp14 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .sigmoid (tmp0 )\n        tmp2 =-tmp1 \n        tmp3 =libdevice .log1p (tmp2 )\n        tmp4 =-100.0 \n        tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n        tmp6 =-1.0 \n        tmp7 =tmp6 *tmp5 \n        tmp8 =tl_math .log (tmp1 )\n        tmp9 =triton_helpers .maximum (tmp8 ,tmp4 )\n        tmp10 =0.0 \n        tmp11 =tmp10 *tmp9 \n        tmp12 =tmp7 -tmp11 \n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =_tmp14 +tmp13 \n        _tmp14 =tl .where (r0_mask ,tmp15 ,_tmp14 )\n    tmp14 =tl .sum (_tmp14 ,1 )[:,None ]\n    tmp16 =(ks0 //32 )*(ks1 //32 )*(ks2 //32 )\n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =tmp14 /tmp17 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp18 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .max_pool3d_with_indices .default (arg3_1 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del arg3_1 \n        buf1 =buf0 [0 ]\n        del buf0 \n\n        buf3 =torch .ops .aten .max_pool3d_with_indices .default (buf1 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del buf1 \n        buf4 =buf3 [0 ]\n        del buf3 \n\n        buf6 =torch .ops .aten .max_pool3d_with_indices .default (buf4 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del buf4 \n        buf7 =buf6 [0 ]\n        del buf6 \n\n        buf9 =torch .ops .aten .max_pool3d_with_indices .default (buf7 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del buf7 \n        buf10 =buf9 [0 ]\n        del buf9 \n\n        buf12 =torch .ops .aten .max_pool3d_with_indices .default (buf10 ,[2 ,2 ,2 ],[2 ,2 ,2 ])\n        del buf10 \n        buf13 =buf12 [0 ]\n        del buf12 \n        buf15 =empty_strided_cuda ((),(),torch .float32 )\n        buf16 =buf15 ;del buf15 \n\n        (s0 //32 )*(s1 //32 )*(s2 //32 )\n        get_raw_stream (0 )\n        triton_red_fused_binary_cross_entropy_sigmoid_zeros_like_0 [grid (1 )](buf16 ,buf13 ,64 ,64 ,64 ,1 ,8 ,XBLOCK =1 ,R0_BLOCK =8 ,num_warps =2 ,num_stages =1 )\n        del buf13 \n    return (buf16 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,1 ,64 ,64 ,64 ),(262144 ,262144 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d37cdfdc-3cf9-4d09-8c20-76d165d2b914",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveMaxPool3d', 'BCELoss', 'CosineEmbeddingLoss', 'PoissonNLLLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((5, 5, 5))\n        self.bce_loss = nn.BCELoss()\n        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()\n        self.poisson_nll_loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Apply AdaptiveMaxPool3d to the input\n        x = self.adaptive_max_pool3d(x)\n        \n        # Flatten the output for further processing\n        x = x.view(x.size(0), -1)\n        \n        # Generate random targets for loss functions\n        target_bce = torch.randint(0, 2, (x.size(0), x.size(1)), dtype=torch.float32, device=x.device)\n        target_cosine = torch.randint(0, 2, (x.size(0),), dtype=torch.float32, device=x.device)\n        target_poisson = torch.randint(0, 10, (x.size(0), x.size(1)), dtype=torch.float32, device=x.device)\n        \n        # Apply BCELoss\n        bce_loss = self.bce_loss(torch.sigmoid(x), target_bce)\n        \n        # Apply CosineEmbeddingLoss\n        cosine_loss = self.cosine_embedding_loss(x, x, target_cosine)\n        \n        # Apply PoissonNLLLoss\n        poisson_loss = self.poisson_nll_loss(x, target_poisson)\n        \n        # Return the sum of losses as the output\n        return bce_loss + cosine_loss + poisson_loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 20, 20, 20).cuda()  # Example input shape for AdaptiveMaxPool3d\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_binary_cross_entropy_clamp_min_div_eq_exp_fill_mean_mul_randint_sigmoid_sqrt_sub_sum_where_zeros_like_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,load_seed_offset ,load_seed_offset1 ,ks2 ,load_seed_offset2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp20 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp24 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp34 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp8 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =r0_0 \n        tmp2 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp3 =tl .full ([1 ,1 ],2 ,tl .int64 )\n        tmp4 =triton_helpers .randint64 (tmp0 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp3 )\n        tmp5 =tmp4 .to (tl .float32 )\n        tmp6 =1.0 \n        tmp7 =tmp5 -tmp6 \n        tmp9 =tl .sigmoid (tmp8 )\n        tmp10 =-tmp9 \n        tmp11 =libdevice .log1p (tmp10 )\n        tmp12 =-100.0 \n        tmp13 =triton_helpers .maximum (tmp11 ,tmp12 )\n        tmp14 =tmp7 *tmp13 \n        tmp15 =tl_math .log (tmp9 )\n        tmp16 =triton_helpers .maximum (tmp15 ,tmp12 )\n        tmp17 =tmp5 *tmp16 \n        tmp18 =tmp14 -tmp17 \n        tmp19 =tl .broadcast_to (tmp18 ,[XBLOCK ,R0_BLOCK ])\n        tmp21 =_tmp20 +tmp19 \n        _tmp20 =tl .where (r0_mask ,tmp21 ,_tmp20 )\n        tmp22 =tmp8 *tmp8 \n        tmp23 =tl .broadcast_to (tmp22 ,[XBLOCK ,R0_BLOCK ])\n        tmp25 =_tmp24 +tmp23 \n        _tmp24 =tl .where (r0_mask ,tmp25 ,_tmp24 )\n        tmp26 =tl_math .exp (tmp8 )\n        tmp27 =tl .load (in_ptr0 +load_seed_offset1 )\n        tmp28 =tl .full ([1 ,1 ],10 ,tl .int64 )\n        tmp29 =triton_helpers .randint64 (tmp27 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp28 )\n        tmp30 =tmp29 .to (tl .float32 )\n        tmp31 =tmp30 *tmp8 \n        tmp32 =tmp26 -tmp31 \n        tmp33 =tl .broadcast_to (tmp32 ,[XBLOCK ,R0_BLOCK ])\n        tmp35 =_tmp34 +tmp33 \n        _tmp34 =tl .where (r0_mask ,tmp35 ,_tmp34 )\n    tmp20 =tl .sum (_tmp20 ,1 )[:,None ]\n    tmp24 =tl .sum (_tmp24 ,1 )[:,None ]\n    tmp34 =tl .sum (_tmp34 ,1 )[:,None ]\n    tmp36 =125 *ks2 \n    tmp37 =tmp36 .to (tl .float32 )\n    tmp38 =tmp20 /tmp37 \n    tmp39 =tl .load (in_ptr0 +load_seed_offset2 )\n    tmp40 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp41 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp42 =tl .full ([1 ,1 ],2 ,tl .int64 )\n    tmp43 =triton_helpers .randint64 (tmp39 ,(tmp40 ).to (tl .uint32 ),tmp41 ,tmp42 )\n    tmp44 =tmp43 .to (tl .float32 )\n    tmp45 =1.0 \n    tmp46 =tmp44 ==tmp45 \n    tmp47 =9.999999960041972e-13 \n    tmp48 =tmp24 +tmp47 \n    tmp49 =tmp48 *tmp48 \n    tmp50 =libdevice .sqrt (tmp49 )\n    tmp51 =tmp24 /tmp50 \n    tmp52 =tmp45 -tmp51 \n    tmp53 =0.0 \n    tmp54 =tl .where (tmp46 ,tmp52 ,tmp53 )\n    tmp55 =-1.0 \n    tmp56 =tmp44 ==tmp55 \n    tmp57 =tmp51 -tmp53 \n    tmp58 =triton_helpers .maximum (tmp57 ,tmp53 )\n    tmp59 =tl .where (tmp56 ,tmp58 ,tmp53 )\n    tmp60 =tmp54 +tmp59 \n    tmp61 =tmp60 /tmp45 \n    tmp62 =tmp38 +tmp61 \n    tmp63 =tmp34 /tmp37 \n    tmp64 =tmp62 +tmp63 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp64 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =torch .ops .aten .adaptive_max_pool3d .default (arg4_1 ,[5 ,5 ,5 ])\n        del arg4_1 \n        buf1 =buf0 [0 ]\n        del buf0 \n        buf3 =empty_strided_cuda ((3 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[3 ],out =buf3 )\n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf9 =buf4 ;del buf4 \n\n        125 *s0 \n        get_raw_stream (0 )\n        triton_red_fused_add_binary_cross_entropy_clamp_min_div_eq_exp_fill_mean_mul_randint_sigmoid_sqrt_sub_sum_where_zeros_like_0 [grid (1 )](buf9 ,buf3 ,buf1 ,0 ,2 ,10 ,1 ,1 ,1250 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf1 \n        del buf3 \n    return (buf9 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =20 \n    arg2_1 =20 \n    arg3_1 =20 \n    arg4_1 =rand_strided ((1 ,10 ,20 ,20 ,20 ),(80000 ,8000 ,400 ,20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d3d2a721-41ea-42ee-a160-6d8fa7471b93",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softmax', 'Dropout3d', 'SmoothL1Loss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout3d = nn.Dropout3d(p=0.5)\n        self.softmax = nn.Softmax(dim=1)\n        self.smooth_l1_loss = nn.SmoothL1Loss()\n\n    def forward(self, x):\n        # Apply Dropout3d\n        x = self.dropout3d(x)\n        \n        # Reshape to apply Softmax\n        batch_size = x.size(0)\n        x = x.view(batch_size, -1)  # Flatten all dimensions except batch\n        x = self.softmax(x)\n        \n        # Reshape back to original shape minus the last dimension\n        x = x.view(batch_size, *x.size()[1:])\n        \n        # Compute SmoothL1Loss with a dummy target\n        target = torch.zeros_like(x)\n        loss = self.smooth_l1_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape: (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =12 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp14 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((11 +ks0 *ks1 *ks2 *ks3 )//12 )\n        tmp1 =ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((11 +ks0 *ks1 *ks2 *ks3 )//12 ))%(ks0 *ks1 *ks2 *ks3 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +((((r0_1 +x0 *((11 +ks0 *ks1 *ks2 *ks3 )//12 ))//(ks1 *ks2 *ks3 ))%ks0 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =0.5 \n        tmp6 =tmp4 <tmp5 \n        tmp7 =tmp6 .to (tl .float32 )\n        tmp8 =2.0 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tmp3 *tmp9 \n        tmp11 =tl .full (tmp10 .shape ,float (\"-inf\"),tmp10 .dtype )\n        tmp12 =tl .where (tmp2 ,tmp10 ,tmp11 )\n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =triton_helpers .maximum (_tmp14 ,tmp13 )\n        _tmp14 =tl .where (r0_mask &xmask ,tmp15 ,_tmp14 )\n    tmp14 =triton_helpers .max2 (_tmp14 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_2 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =12 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,float (\"-inf\"))\n    tmp4 =triton_helpers .max2 (tmp3 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp4 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_3 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =12 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp17 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((11 +ks0 *ks1 *ks2 *ks3 )//12 )\n        tmp1 =ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((11 +ks0 *ks1 *ks2 *ks3 )//12 ))%(ks0 *ks1 *ks2 *ks3 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +((((r0_1 +x0 *((11 +ks0 *ks1 *ks2 *ks3 )//12 ))//(ks1 *ks2 *ks3 ))%ks0 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =0.5 \n        tmp6 =tmp4 <tmp5 \n        tmp7 =tmp6 .to (tl .float32 )\n        tmp8 =2.0 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tmp3 *tmp9 \n        tmp11 =tl .load (in_ptr2 +(tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int32 )),tmp2 ,eviction_policy ='evict_last',other =0.0 )\n        tmp12 =tmp10 -tmp11 \n        tmp13 =tl_math .exp (tmp12 )\n        tmp14 =tl .full (tmp13 .shape ,0 ,tmp13 .dtype )\n        tmp15 =tl .where (tmp2 ,tmp13 ,tmp14 )\n        tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ,R0_BLOCK ])\n        tmp18 =_tmp17 +tmp16 \n        _tmp17 =tl .where (r0_mask &xmask ,tmp18 ,_tmp17 )\n    tmp17 =tl .sum (_tmp17 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_4 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =12 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp4 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_smooth_l1_loss_view_zeros_like_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =12 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp29 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((11 +ks0 *ks1 *ks2 *ks3 )//12 )\n        tmp1 =ks0 *ks1 *ks2 *ks3 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(((r0_1 +x0 *((11 +ks0 *ks1 *ks2 *ks3 )//12 ))%(ks0 *ks1 *ks2 *ks3 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +((((r0_1 +x0 *((11 +ks0 *ks1 *ks2 *ks3 )//12 ))//(ks1 *ks2 *ks3 ))%ks0 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =0.5 \n        tmp6 =tmp4 <tmp5 \n        tmp7 =tmp6 .to (tl .float32 )\n        tmp8 =2.0 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tmp3 *tmp9 \n        tmp11 =tl .load (in_ptr2 +(tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int32 )),tmp2 ,eviction_policy ='evict_last',other =0.0 )\n        tmp12 =tmp10 -tmp11 \n        tmp13 =tl_math .exp (tmp12 )\n        tmp14 =tl .load (in_ptr3 +(tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int32 )),tmp2 ,eviction_policy ='evict_last',other =0.0 )\n        tmp15 =tmp13 /tmp14 \n        tmp16 =0.0 \n        tmp17 =tmp15 -tmp16 \n        tmp18 =tl_math .abs (tmp17 )\n        tmp19 =1.0 \n        tmp20 =tmp18 <tmp19 \n        tmp21 =tmp18 *tmp18 \n        tmp22 =tmp21 *tmp5 \n        tmp23 =tmp22 *tmp19 \n        tmp24 =tmp18 -tmp5 \n        tmp25 =tl .where (tmp20 ,tmp23 ,tmp24 )\n        tmp26 =tl .full (tmp25 .shape ,0 ,tmp25 .dtype )\n        tmp27 =tl .where (tmp2 ,tmp25 ,tmp26 )\n        tmp28 =tl .broadcast_to (tmp27 ,[XBLOCK ,R0_BLOCK ])\n        tmp30 =_tmp29 +tmp28 \n        _tmp29 =tl .where (r0_mask &xmask ,tmp30 ,_tmp29 )\n    tmp29 =tl .sum (_tmp29 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp29 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_smooth_l1_loss_view_zeros_like_6 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =12 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =ks0 *ks1 *ks2 *ks3 \n    tmp6 =tmp5 .to (tl .float32 )\n    tmp7 =tmp4 /tmp6 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp7 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf0 ,buf1 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((1 ,1 ,12 ),(12 ,12 ,1 ),torch .float32 )\n\n        (11 +s0 *s1 *s2 *s3 )//12 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_1 [grid (12 )](arg4_1 ,buf1 ,buf2 ,3 ,32 ,32 ,32 ,12 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_2 [grid (1 )](buf2 ,buf3 ,1 ,12 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf4 =buf2 ;del buf2 \n\n        (11 +s0 *s1 *s2 *s3 )//12 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_3 [grid (12 )](arg4_1 ,buf1 ,buf3 ,buf4 ,3 ,32 ,32 ,32 ,12 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_4 [grid (1 )](buf4 ,buf5 ,1 ,12 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf6 =reinterpret_tensor (buf4 ,(12 ,),(1 ,),0 );del buf4 \n\n        (11 +s0 *s1 *s2 *s3 )//12 \n        get_raw_stream (0 )\n        triton_red_fused__softmax_smooth_l1_loss_view_zeros_like_5 [grid (12 )](arg4_1 ,buf1 ,buf3 ,buf5 ,buf6 ,3 ,32 ,32 ,32 ,12 ,8192 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg4_1 \n        del buf1 \n        del buf3 \n        buf7 =reinterpret_tensor (buf5 ,(),(),0 );del buf5 \n        buf8 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_smooth_l1_loss_view_zeros_like_6 [grid (1 )](buf8 ,buf6 ,3 ,32 ,32 ,32 ,1 ,12 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf6 \n    return (buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d45500bd-1c62-4f10-a006-58ae3c2714c3",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['InstanceNorm2d', 'GaussianNLLLoss', 'ConstantPad2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad2d(2, 3.0)  # Padding with value 3.0\n        self.norm1 = nn.InstanceNorm2d(3)    # Instance normalization for 3 channels\n        self.norm2 = nn.InstanceNorm2d(3)    # Repeating InstanceNorm2d\n        self.loss = nn.GaussianNLLLoss()     # Gaussian NLL Loss\n\n    def forward(self, x):\n        # Assume input x is of shape (batch_size, channels, height, width)\n        x = self.pad(x)                      # Apply padding\n        x = self.norm1(x)                    # Apply first instance normalization\n        x = self.norm2(x)                    # Apply second instance normalization\n        \n        # For GaussianNLLLoss, we need to predict mean and variance\n        mean = x.mean(dim=[2, 3], keepdim=True)  # Compute mean over spatial dimensions\n        var = x.var(dim=[2, 3], keepdim=True)    # Compute variance over spatial dimensions\n        \n        # Flatten the output for GaussianNLLLoss\n        mean = mean.view(mean.size(0), -1)\n        var = var.view(var.size(0), -1)\n        \n        # Dummy target for GaussianNLLLoss (same shape as mean)\n        target = torch.zeros_like(mean)\n        \n        # Compute GaussianNLLLoss\n        loss = self.loss(mean, target, var)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Input with 3 channels, 64x64 spatial dimensions\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_constant_pad_nd_view_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp14_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp14_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp14_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index //ks0 \n        r0_1 =(r0_index %ks0 )\n        tmp0 =(-2 )+r0_2 \n        tmp1 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp2 =tmp0 >=tmp1 \n        tmp3 =ks1 \n        tmp4 =tmp0 <tmp3 \n        tmp5 =(-2 )+r0_1 \n        tmp6 =tmp5 >=tmp1 \n        tmp7 =ks2 \n        tmp8 =tmp5 <tmp7 \n        tmp9 =tmp2 &tmp4 \n        tmp10 =tmp9 &tmp6 \n        tmp11 =tmp10 &tmp8 \n        tmp12 =tl .load (in_ptr0 +((-2 )+r0_1 +((-2 )*ks2 )+ks2 *r0_2 +ks1 *ks2 *x0 ),r0_mask &tmp11 &xmask ,eviction_policy ='evict_last',other =3.0 )\n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp14_mean_next ,tmp14_m2_next ,tmp14_weight_next =triton_helpers .welford_reduce (\n        tmp13 ,tmp14_mean ,tmp14_m2 ,tmp14_weight ,roffset ==0 \n        )\n        tmp14_mean =tl .where (r0_mask &xmask ,tmp14_mean_next ,tmp14_mean )\n        tmp14_m2 =tl .where (r0_mask &xmask ,tmp14_m2_next ,tmp14_m2 )\n        tmp14_weight =tl .where (r0_mask &xmask ,tmp14_weight_next ,tmp14_weight )\n    tmp17 ,tmp18 ,tmp19 =triton_helpers .welford (tmp14_mean ,tmp14_m2 ,tmp14_weight ,1 )\n    tmp14 =tmp17 [:,None ]\n    tmp15 =tmp18 [:,None ]\n    tmp16 =tmp19 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp14 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp15 ,xmask )\n    tmp42_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp42_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp42_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index //ks0 \n        r0_1 =(r0_index %ks0 )\n        tmp20 =(-2 )+r0_2 \n        tmp21 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp22 =tmp20 >=tmp21 \n        tmp23 =ks1 \n        tmp24 =tmp20 <tmp23 \n        tmp25 =(-2 )+r0_1 \n        tmp26 =tmp25 >=tmp21 \n        tmp27 =ks2 \n        tmp28 =tmp25 <tmp27 \n        tmp29 =tmp22 &tmp24 \n        tmp30 =tmp29 &tmp26 \n        tmp31 =tmp30 &tmp28 \n        tmp32 =tl .load (in_ptr0 +((-2 )+r0_1 +((-2 )*ks2 )+ks2 *r0_2 +ks1 *ks2 *x0 ),r0_mask &tmp31 &xmask ,eviction_policy ='evict_last',other =3.0 )\n        tmp33 =tmp32 -tmp14 \n        tmp34 =16 +4 *ks1 +4 *ks2 +ks1 *ks2 \n        tmp35 =tmp34 .to (tl .float32 )\n        tmp36 =tmp15 /tmp35 \n        tmp37 =1e-05 \n        tmp38 =tmp36 +tmp37 \n        tmp39 =libdevice .rsqrt (tmp38 )\n        tmp40 =tmp33 *tmp39 \n        tmp41 =tl .broadcast_to (tmp40 ,[XBLOCK ,R0_BLOCK ])\n        tmp42_mean_next ,tmp42_m2_next ,tmp42_weight_next =triton_helpers .welford_reduce (\n        tmp41 ,tmp42_mean ,tmp42_m2 ,tmp42_weight ,roffset ==0 \n        )\n        tmp42_mean =tl .where (r0_mask &xmask ,tmp42_mean_next ,tmp42_mean )\n        tmp42_m2 =tl .where (r0_mask &xmask ,tmp42_m2_next ,tmp42_m2 )\n        tmp42_weight =tl .where (r0_mask &xmask ,tmp42_weight_next ,tmp42_weight )\n    tmp45 ,tmp46 ,tmp47 =triton_helpers .welford (tmp42_mean ,tmp42_m2 ,tmp42_weight ,1 )\n    tmp42 =tmp45 [:,None ]\n    tmp43 =tmp46 [:,None ]\n    tmp44 =tmp47 [:,None ]\n    tl .store (out_ptr2 +(x0 ),tmp42 ,xmask )\n    tl .store (out_ptr3 +(x0 ),tmp43 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_constant_pad_nd_view_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp13 =tl .load (in_ptr1 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr2 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp23 =tl .load (in_ptr3 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp25 =tl .load (in_ptr4 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks3 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks3 )+ks3 *x1 +ks2 *ks3 *x2 ),tmp11 &xmask ,eviction_policy ='evict_last',other =3.0 )\n    tmp14 =tmp12 -tmp13 \n    tmp16 =16 +4 *ks2 +4 *ks3 +ks2 *ks3 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =tmp15 /tmp17 \n    tmp19 =1e-05 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =libdevice .rsqrt (tmp20 )\n    tmp22 =tmp14 *tmp21 \n    tmp24 =tmp22 -tmp23 \n    tmp26 =tmp25 /tmp17 \n    tmp27 =tmp26 +tmp19 \n    tmp28 =libdevice .rsqrt (tmp27 )\n    tmp29 =tmp24 *tmp28 \n    tl .store (out_ptr0 +(x4 ),tmp29 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_mean_var_2 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    tmp4_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp4_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +16 *x0 +4 *ks0 *x0 +4 *ks1 *x0 +ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =_tmp2 +tmp1 \n        _tmp2 =tl .where (r0_mask &xmask ,tmp3 ,_tmp2 )\n        tmp4_mean_next ,tmp4_m2_next ,tmp4_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp4_mean ,tmp4_m2 ,tmp4_weight ,roffset ==0 \n        )\n        tmp4_mean =tl .where (r0_mask &xmask ,tmp4_mean_next ,tmp4_mean )\n        tmp4_m2 =tl .where (r0_mask &xmask ,tmp4_m2_next ,tmp4_m2 )\n        tmp4_weight =tl .where (r0_mask &xmask ,tmp4_weight_next ,tmp4_weight )\n    tmp2 =tl .sum (_tmp2 ,1 )[:,None ]\n    tmp7 ,tmp8 ,tmp9 =triton_helpers .welford (tmp4_mean ,tmp4_m2 ,tmp4_weight ,1 )\n    tmp4 =tmp7 [:,None ]\n    tmp5 =tmp8 [:,None ]\n    tmp6 =tmp9 [:,None ]\n    tmp10 =16 +4 *ks0 +4 *ks1 +ks0 *ks1 \n    tmp11 =tmp10 .to (tl .float32 )\n    tmp12 =tmp2 /tmp11 \n    tmp13 =1.0 \n    tmp14 =tmp11 -tmp13 \n    tmp15 =0.0 \n    tmp16 =triton_helpers .maximum (tmp15 ,tmp14 )\n    tmp17 =tmp5 /tmp16 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp12 ,xmask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(x0 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_zeros_like_3 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    s2 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,3 ,s1 ,s2 ),(3 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        buf0 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n        buf3 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n\n        16 +4 *s1 +4 *s2 +s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_constant_pad_nd_view_0 [grid (3 )](arg2_1 ,buf0 ,buf1 ,buf3 ,buf4 ,68 ,64 ,64 ,3 ,4624 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf6 =empty_strided_cuda ((1 ,3 ,4 +s1 ,4 +s2 ),(48 +12 *s1 +12 *s2 +3 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__native_batch_norm_legit_constant_pad_nd_view_1_xnumel =48 +12 *s1 +12 *s2 +3 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_constant_pad_nd_view_1 [grid (triton_poi_fused__native_batch_norm_legit_constant_pad_nd_view_1_xnumel )](arg2_1 ,buf0 ,buf1 ,buf3 ,buf4 ,buf6 ,68 ,68 ,64 ,64 ,4624 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n        del buf0 \n        buf7 =buf4 ;del buf4 \n        buf11 =buf3 ;del buf3 \n        buf8 =buf7 ;del buf7 \n        buf13 =buf11 ;del buf11 \n\n        16 +4 *s1 +4 *s2 +s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused_mean_var_2 [grid (3 )](buf8 ,buf13 ,buf6 ,64 ,64 ,3 ,4624 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf6 \n        buf9 =reinterpret_tensor (buf1 ,(1 ,3 ),(3 ,1 ),0 );del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_zeros_like_3 [grid (3 )](buf9 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n    return (reinterpret_tensor (buf8 ,(1 ,3 ),(3 ,1 ),0 ),buf9 ,reinterpret_tensor (buf13 ,(1 ,3 ),(3 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d5c1650f-17c2-40d2-86eb-4bd5cab1c7f7",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GaussianNLLLoss', 'Unflatten', 'ZeroPad2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.unflatten = nn.Unflatten(1, (1, 28, 28))  # Assuming input is flattened and needs to be reshaped\n        self.zero_pad = nn.ZeroPad2d(2)  # Padding the input by 2 on each side\n        self.gaussian_nll_loss = nn.GaussianNLLLoss()  # This is a loss function, so it will be used in the forward pass\n\n    def forward(self, x):\n        # Assuming the input is a flattened tensor, we unflatten it\n        x = self.unflatten(x)\n        \n        # Apply zero padding\n        x = self.zero_pad(x)\n        \n        # For the GaussianNLLLoss, we need to predict both mean and variance\n        # Here, we assume the model outputs both mean and variance\n        mean = x.mean(dim=[2, 3], keepdim=True)  # Compute mean over spatial dimensions\n        var = x.var(dim=[2, 3], keepdim=True)    # Compute variance over spatial dimensions\n        \n        # Flatten the output to match the target shape\n        mean = mean.view(mean.size(0), -1)\n        var = var.view(var.size(0), -1)\n        \n        # Assuming the target is provided externally, we compute the loss\n        # For demonstration, we create a dummy target\n        target = torch.randn_like(mean)\n        \n        # Compute the Gaussian NLL Loss\n        loss = self.gaussian_nll_loss(mean, target, var)\n        \n        # Return the loss as the output (this is unusual but fits the module list)\n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 784).cuda()  # Flattened input of size 784 (e.g., 28x28 image)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_constant_pad_nd_mean_var_0 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index //32 \n    r0_0 =(r0_index %32 )\n    tmp0 =(-2 )+r0_1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],28 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+r0_0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =tmp2 &tmp4 \n    tmp9 =tmp8 &tmp6 \n    tmp10 =tmp9 &tmp7 \n    tmp11 =tl .load (in_ptr0 +(tl .broadcast_to ((-58 )+r0_0 +28 *r0_1 ,[R0_BLOCK ])),tmp10 ,other =0.0 )\n    tmp12 =tl .broadcast_to (tmp11 ,[R0_BLOCK ])\n    tmp14 =triton_helpers .promote_to_tensor (tl .sum (tmp12 ,0 ))\n    tmp16 =tl .broadcast_to (tmp12 ,[R0_BLOCK ])\n    tmp18 =triton_helpers .promote_to_tensor (tl .sum (tmp16 ,0 ))\n    tmp19 =tl .full ([1 ],1024 ,tl .int32 )\n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =tmp18 /tmp20 \n    tmp22 =tmp12 -tmp21 \n    tmp23 =tmp22 *tmp22 \n    tmp24 =tl .broadcast_to (tmp23 ,[R0_BLOCK ])\n    tmp26 =triton_helpers .promote_to_tensor (tl .sum (tmp24 ,0 ))\n    tmp27 =1024.0 \n    tmp28 =tmp14 /tmp27 \n    tmp29 =1023.0 \n    tmp30 =tmp26 /tmp29 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp28 ,None )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([1 ],0 ,tl .int32 )),tmp30 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_randn_like_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,784 ),(784 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,1 ,1 ),(1 ,1 ,1 ,1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,1 ,1 ,1 ),(1 ,1 ,1 ,1 ),torch .float32 )\n        buf1 =buf0 ;del buf0 \n        buf7 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_per_fused_constant_pad_nd_mean_var_0 [grid (1 )](buf1 ,buf7 ,arg0_1 ,1 ,1024 ,num_warps =8 ,num_stages =1 )\n        del arg0_1 \n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_randn_like_1 [grid (1 )](buf2 ,buf3 ,0 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf2 \n    return (reinterpret_tensor (buf1 ,(1 ,1 ),(1 ,1 ),0 ),buf3 ,reinterpret_tensor (buf7 ,(1 ,1 ),(1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,784 ),(784 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "d732c123-71a6-40f8-a468-7d10402e9931",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['RNNCell', 'Softshrink', 'LSTM', 'InstanceNorm1d', 'LayerNorm', 'CircularPad1d', 'LazyConvTranspose2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.rnn_cell = nn.RNNCell(input_size=64, hidden_size=128)\n        self.softshrink = nn.Softshrink(lambd=0.5)\n        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n        self.instance_norm = nn.InstanceNorm1d(num_features=64)\n        self.layer_norm = nn.LayerNorm(64)\n        self.circular_pad = nn.CircularPad1d(padding=2)\n        self.lazy_conv_transpose = nn.LazyConvTranspose2d(out_channels=32, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, sequence_length, input_size)\n        batch_size, seq_len, input_size = x.shape\n        \n        # Apply RNNCell\n        hx = torch.zeros(batch_size, 128).to(x.device)\n        rnn_outputs = []\n        for t in range(seq_len):\n            hx = self.rnn_cell(x[:, t, :], hx)\n            rnn_outputs.append(hx)\n        x = torch.stack(rnn_outputs, dim=1)\n        \n        # Apply Softshrink\n        x = self.softshrink(x)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Apply InstanceNorm1d\n        x = x.permute(0, 2, 1)  # (batch_size, features, seq_len)\n        x = self.instance_norm(x)\n        x = x.permute(0, 2, 1)  # (batch_size, seq_len, features)\n        \n        # Apply LayerNorm\n        x = self.layer_norm(x)\n        \n        # Apply CircularPad1d\n        x = x.permute(0, 2, 1)  # (batch_size, features, seq_len)\n        x = self.circular_pad(x)\n        x = x.permute(0, 2, 1)  # (batch_size, seq_len, features)\n        \n        # Reshape for LazyConvTranspose2d\n        x = x.unsqueeze(1)  # (batch_size, 1, seq_len, features)\n        x = self.lazy_conv_transpose(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 64).cuda()  # (batch_size, sequence_length, input_size)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_stack_tanh_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tl .store (in_out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_tanh_tanh_backward_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr3 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tmp8 =tmp7 *tmp7 \n    tmp9 =1.0 \n    tmp10 =tmp9 -tmp8 \n    tl .store (out_ptr0 +(x0 ),tmp7 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_gt_mul_sign_sub_where_3 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1280 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl_math .abs (tmp0 )\n    tmp2 =0.5 \n    tmp3 =tmp1 >tmp2 \n    tmp4 =tl .full ([1 ],0 ,tl .int32 )\n    tmp5 =tmp4 <tmp0 \n    tmp6 =tmp5 .to (tl .int8 )\n    tmp7 =tmp0 <tmp4 \n    tmp8 =tmp7 .to (tl .int8 )\n    tmp9 =tmp6 -tmp8 \n    tmp10 =tmp9 .to (tmp0 .dtype )\n    tmp11 =tmp10 *tmp2 \n    tmp12 =tmp0 -tmp11 \n    tmp13 =0.0 \n    tmp14 =tmp0 *tmp13 \n    tmp15 =tl .where (tmp3 ,tmp12 ,tmp14 )\n    tl .store (out_ptr0 +(x0 ),tmp3 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp15 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ,64 ),(640 ,64 ,1 ))\n    assert_size_stride (primals_2 ,(128 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_3 ,(128 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_4 ,(128 ,),(1 ,))\n    assert_size_stride (primals_5 ,(128 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_0 [grid (128 )](buf0 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf0 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf1 )\n        buf2 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf2 )\n        buf3 =buf1 ;del buf1 \n        buf40 =empty_strided_cuda ((1 ,1280 ),(1280 ,1 ),torch .float32 )\n        buf30 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),0 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_1 [grid (128 )](buf3 ,primals_5 ,buf2 ,primals_4 ,buf30 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf4 =buf2 ;del buf2 \n\n        extern_kernels .mm (buf3 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf4 )\n        buf5 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),64 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf5 )\n        buf6 =buf4 ;del buf4 \n        buf31 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),128 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_1 [grid (128 )](buf6 ,primals_5 ,buf5 ,primals_4 ,buf31 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf7 =buf5 ;del buf5 \n\n        extern_kernels .mm (buf6 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf7 )\n        buf8 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),128 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf8 )\n        buf9 =buf7 ;del buf7 \n        buf32 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),256 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_1 [grid (128 )](buf9 ,primals_5 ,buf8 ,primals_4 ,buf32 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf10 =buf8 ;del buf8 \n\n        extern_kernels .mm (buf9 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf10 )\n        buf11 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),192 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf11 )\n        buf12 =buf10 ;del buf10 \n        buf33 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),384 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_1 [grid (128 )](buf12 ,primals_5 ,buf11 ,primals_4 ,buf33 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf13 =buf11 ;del buf11 \n\n        extern_kernels .mm (buf12 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf13 )\n        buf14 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),256 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf14 )\n        buf15 =buf13 ;del buf13 \n        buf34 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),512 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_1 [grid (128 )](buf15 ,primals_5 ,buf14 ,primals_4 ,buf34 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf16 =buf14 ;del buf14 \n\n        extern_kernels .mm (buf15 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf16 )\n        buf17 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),320 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf17 )\n        buf18 =buf16 ;del buf16 \n        buf35 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),640 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_1 [grid (128 )](buf18 ,primals_5 ,buf17 ,primals_4 ,buf35 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf19 =buf17 ;del buf17 \n\n        extern_kernels .mm (buf18 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf19 )\n        buf20 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),384 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf20 )\n        buf21 =buf19 ;del buf19 \n        buf36 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),768 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_1 [grid (128 )](buf21 ,primals_5 ,buf20 ,primals_4 ,buf36 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf22 =buf20 ;del buf20 \n\n        extern_kernels .mm (buf21 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf22 )\n        buf23 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),448 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf23 )\n        buf24 =buf22 ;del buf22 \n        buf37 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),896 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_1 [grid (128 )](buf24 ,primals_5 ,buf23 ,primals_4 ,buf37 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf25 =buf23 ;del buf23 \n\n        extern_kernels .mm (buf24 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf25 )\n        buf26 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),512 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf26 )\n        buf27 =buf25 ;del buf25 \n        buf38 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),1024 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_stack_tanh_1 [grid (128 )](buf27 ,primals_5 ,buf26 ,primals_4 ,buf38 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf28 =buf26 ;del buf26 \n\n        extern_kernels .mm (buf27 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf28 )\n        buf29 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),576 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf29 )\n        del primals_2 \n        buf39 =reinterpret_tensor (buf40 ,(1 ,128 ),(1280 ,1 ),1152 )\n        buf43 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_tanh_backward_2 [grid (128 )](buf28 ,primals_5 ,buf29 ,primals_4 ,buf39 ,buf43 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf28 \n        del buf29 \n        del primals_4 \n        del primals_5 \n        buf41 =empty_strided_cuda ((1 ,10 ,128 ),(1280 ,128 ,1 ),torch .bool )\n        buf42 =empty_strided_cuda ((1 ,10 ,128 ),(1280 ,128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_gt_mul_sign_sub_where_3 [grid (1280 )](buf40 ,buf41 ,buf42 ,1280 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf30 \n        del buf31 \n        del buf32 \n        del buf33 \n        del buf34 \n        del buf35 \n        del buf36 \n        del buf37 \n        del buf38 \n        del buf39 \n        del buf40 \n    return (buf42 ,buf0 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),0 ),buf3 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),64 ),buf6 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),128 ),buf9 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),192 ),buf12 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),256 ),buf15 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),320 ),buf18 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),384 ),buf21 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),448 ),buf24 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),512 ),buf27 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),576 ),buf41 ,buf43 ,primals_3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ,64 ),(640 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((128 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((128 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "da644d4c-35a4-4760-9a32-5f11c0c7bce3",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['CircularPad2d', 'Hardswish', 'GroupNorm', 'AvgPool1d', 'Softmax']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.circular_pad = nn.CircularPad2d(2)\n        self.hardswish = nn.Hardswish()\n        self.group_norm = nn.GroupNorm(2, 10)  # Assuming 10 channels for GroupNorm\n        self.avg_pool = nn.AvgPool1d(kernel_size=2)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        # Apply CircularPad2d\n        x = self.circular_pad(x)\n        \n        # Apply Hardswish\n        x = self.hardswish(x)\n        \n        # Reshape for GroupNorm (assuming input is 4D: [batch, channels, height, width])\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1)  # Flatten height and width for GroupNorm\n        x = self.group_norm(x)\n        x = x.view(batch_size, channels, height, width)  # Reshape back\n        \n        # Reshape for AvgPool1d (assuming input is 4D: [batch, channels, height, width])\n        x = x.view(batch_size, channels, -1)  # Flatten height and width for AvgPool1d\n        x = self.avg_pool(x)\n        \n        # Reshape for Softmax (assuming input is 3D: [batch, channels, flattened_dim])\n        x = x.view(batch_size, channels, -1)  # Flatten the last dimension\n        x = self.softmax(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks2 )\n    x2 =xindex //ks4 \n    x4 =xindex \n    tmp0 =x0 \n    tmp1 =tl .full ([1 ],2 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =ks1 +x0 \n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 >=tmp4 \n    tmp6 =tl .broadcast_to (2 +ks1 ,[XBLOCK ])\n    tmp7 =tmp3 <tmp6 \n    tmp8 =tmp5 &tmp7 \n    tmp9 =tmp8 &tmp2 \n    tmp10 =x1 \n    tmp11 =tl .full ([1 ],2 ,tl .int64 )\n    tmp12 =tmp10 >=tmp11 \n    tmp13 =tl .broadcast_to (2 +ks3 ,[XBLOCK ])\n    tmp14 =tmp10 <tmp13 \n    tmp15 =tmp12 &tmp14 \n    tmp16 =tmp15 &tmp9 \n    tmp17 =tl .load (in_ptr0 +((-2 )+x0 +((-1 )*ks1 )+ks1 *x1 +ks1 *ks3 *x2 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp18 =tl .load (in_ptr1 +(ks1 +x4 ),tmp9 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =tl .where (tmp15 ,tmp17 ,tmp18 )\n    tmp20 =tl .full (tmp19 .shape ,0.0 ,tmp19 .dtype )\n    tmp21 =tl .where (tmp9 ,tmp19 ,tmp20 )\n    tmp22 =float (\"nan\")\n    tmp23 =tl .where (tmp8 ,tmp21 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tmp0 >=tmp1 \n    tmp27 =2 +ks1 \n    tmp28 =tmp0 <tmp27 \n    tmp29 =tmp26 &tmp28 \n    tmp30 =x1 \n    tmp31 =tl .full ([1 ],2 ,tl .int64 )\n    tmp32 =tmp30 >=tmp31 \n    tmp33 =tl .broadcast_to (2 +ks3 ,[XBLOCK ])\n    tmp34 =tmp30 <tmp33 \n    tmp35 =tmp32 &tmp34 \n    tmp36 =tmp35 &tmp29 \n    tmp37 =tl .load (in_ptr0 +((-2 )+x0 +((-2 )*ks1 )+ks1 *x1 +ks1 *ks3 *x2 ),tmp36 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp38 =tl .load (in_ptr1 +(x4 ),tmp29 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp39 =tl .where (tmp35 ,tmp37 ,tmp38 )\n    tmp40 =tl .full (tmp39 .shape ,0.0 ,tmp39 .dtype )\n    tmp41 =tl .where (tmp29 ,tmp39 ,tmp40 )\n    tmp42 =float (\"nan\")\n    tmp43 =tl .where (tmp29 ,tmp41 ,tmp42 )\n    tmp44 =tl .where (tmp2 ,tmp25 ,tmp43 )\n    tl .store (out_ptr0 +(x4 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x3 =xindex \n    tmp41 =tl .load (in_ptr0 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =2 +ks2 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =x1 +((-1 )*ks2 )\n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .broadcast_to (2 +ks3 ,[XBLOCK ])\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tl .load (in_ptr0 +(x3 +((-1 )*ks3 )),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tl .load (in_ptr0 +(x3 ),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .where (tmp9 ,tmp11 ,tmp12 )\n    tmp14 =tl .full (tmp13 .shape ,0.0 ,tmp13 .dtype )\n    tmp15 =tl .where (tmp6 ,tmp13 ,tmp14 )\n    tmp16 =x0 \n    tmp17 =tl .broadcast_to (2 +ks3 ,[XBLOCK ])\n    tmp18 =tmp16 >=tmp17 \n    tmp19 =tmp18 &tmp2 \n    tmp20 =tl .load (in_ptr0 +(x3 +((-1 )*ks3 )+((-4 )*ks2 )+((-1 )*ks2 *ks3 )),tmp19 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tl .load (in_ptr0 +(x3 +((-4 )*ks2 )+((-1 )*ks2 *ks3 )),tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tl .where (tmp18 ,tmp20 ,tmp21 )\n    tmp23 =tl .where (tmp5 ,tmp15 ,tmp22 )\n    tmp24 =tl .full (tmp23 .shape ,0.0 ,tmp23 .dtype )\n    tmp25 =tl .where (tmp2 ,tmp23 ,tmp24 )\n    tmp26 =tl .full ([1 ],2 ,tl .int64 )\n    tmp27 =tmp0 <tmp26 \n    tmp28 =x0 \n    tmp29 =tl .broadcast_to (2 +ks3 ,[XBLOCK ])\n    tmp30 =tmp28 >=tmp29 \n    tmp31 =tmp30 &tmp27 \n    tmp32 =tl .load (in_ptr0 +(x3 +((-1 )*ks3 )+4 *ks2 +ks2 *ks3 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tl .load (in_ptr0 +(x3 +4 *ks2 +ks2 *ks3 ),tmp27 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp34 =tl .where (tmp30 ,tmp32 ,tmp33 )\n    tmp35 =tl .full (tmp34 .shape ,0.0 ,tmp34 .dtype )\n    tmp36 =tl .where (tmp27 ,tmp34 ,tmp35 )\n    tmp37 =x0 \n    tmp38 =2 +ks3 \n    tmp39 =tmp37 >=tmp38 \n    tmp40 =tl .load (in_ptr0 +(x3 +((-1 )*ks3 )),tmp39 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tl .where (tmp39 ,tmp40 ,tmp41 )\n    tmp43 =tl .where (tmp27 ,tmp36 ,tmp42 )\n    tmp44 =tl .where (tmp2 ,tmp25 ,tmp43 )\n    tl .store (out_ptr0 +(x3 ),tmp44 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_hardswish_2 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =3.0 \n    tmp2 =tmp0 +tmp1 \n    tmp3 =0.0 \n    tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n    tmp5 =6.0 \n    tmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\n    tmp7 =tmp0 *tmp6 \n    tmp8 =0.16666666666666666 \n    tmp9 =tmp7 *tmp8 \n    tl .store (in_out_ptr0 +(x0 ),tmp9 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_hardswish_native_group_norm_view_3 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =(r0_index %ks0 )\n        r0_2 =r0_index //ks0 \n        r0_3 =r0_index \n        tmp0 =tl .load (in_ptr0 +(4 *(r0_1 //ks1 )+16 *r0_2 +80 *x0 +ks3 *(r0_1 //ks1 )+4 *ks2 *r0_2 +4 *ks3 *r0_2 +20 *ks2 *x0 +20 *ks3 *x0 +ks2 *ks3 *r0_2 +5 *ks2 *ks3 *x0 +((r0_1 %ks1 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n        tl .store (out_ptr0 +(r0_3 +80 *x0 +20 *ks2 *x0 +20 *ks3 *x0 +5 *ks2 *ks3 *x0 ),tmp0 ,r0_mask &xmask )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp3 ,xmask )\n    tmp8 =80 +20 *ks2 +20 *ks3 +5 *ks2 *ks3 \n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp3 /tmp9 \n    tmp11 =1e-05 \n    tmp12 =tmp10 +tmp11 \n    tmp13 =libdevice .rsqrt (tmp12 )\n    tl .store (out_ptr3 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_group_norm_4 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x1 //5 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x1 //5 ),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr3 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr4 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 -tmp1 \n    tmp4 =80 +20 *ks1 +20 *ks2 +5 *ks1 *ks2 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tmp7 =1e-05 \n    tmp8 =tmp6 +tmp7 \n    tmp9 =libdevice .rsqrt (tmp8 )\n    tmp10 =tmp2 *tmp9 \n    tmp12 =tmp10 *tmp11 \n    tmp14 =tmp12 +tmp13 \n    tl .store (out_ptr0 +(x2 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_unsqueeze_5 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(4 *(x0 //ks1 )+16 *x1 +ks3 *(x0 //ks1 )+4 *ks2 *x1 +4 *ks3 *x1 +ks2 *ks3 *x1 +((x0 %ks1 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_6 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +16 *r0_1 +4 *ks0 *r0_1 +4 *ks1 *r0_1 +ks0 *ks1 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +16 *r0_1 +4 *ks0 *r0_1 +4 *ks1 *r0_1 +ks0 *ks1 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp2 =tmp1 +tmp0 \n    tmp3 =0.5 \n    tmp4 =tmp2 *tmp3 \n    tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (r0_mask &xmask ,tmp5 ,float (\"-inf\"))\n    tmp8 =triton_helpers .max2 (tmp7 ,1 )[:,None ]\n    tmp9 =tmp4 -tmp8 \n    tmp10 =tl_math .exp (tmp9 )\n    tmp11 =tl .broadcast_to (tmp10 ,[XBLOCK ,R0_BLOCK ])\n    tmp13 =tl .where (r0_mask &xmask ,tmp11 ,0 )\n    tmp14 =tl .sum (tmp13 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp8 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp14 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_7 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +16 *x1 +4 *ks1 *x1 +4 *ks2 *x1 +ks1 *ks2 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +16 *x1 +4 *ks1 *x1 +4 *ks2 *x1 +ks1 *ks2 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp3 =0.5 \n    tmp4 =tmp2 *tmp3 \n    tmp6 =tmp4 -tmp5 \n    tmp7 =tl_math .exp (tmp6 )\n    tmp9 =tmp7 /tmp8 \n    tl .store (out_ptr0 +(x2 ),tmp9 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    s1 =primals_1 \n    s2 =primals_2 \n    assert_size_stride (primals_3 ,(1 ,10 ,s1 ,s2 ),(10 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    assert_size_stride (primals_4 ,(10 ,),(1 ,))\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,10 ,4 +s1 ,4 +s2 ),(160 +40 *s1 +40 *s2 +10 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf1 =empty_strided_cuda ((1 ,10 ,4 +s1 ,4 +s2 ),(160 +40 *s1 +40 *s2 +10 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_0_xnumel =160 +40 *s1 +40 *s2 +10 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (triton_poi_fused_copy_0_xnumel )](primals_3 ,buf0 ,buf1 ,36 ,32 ,36 ,32 ,1296 ,12960 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_3 \n        buf2 =buf0 ;del buf0 \n\n        triton_poi_fused_1_xnumel =160 +40 *s1 +40 *s2 +10 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_1 [grid (triton_poi_fused_1_xnumel )](buf1 ,buf2 ,36 ,36 ,32 ,32 ,12960 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf3 =buf2 ;del buf2 \n\n        triton_poi_fused_hardswish_2_xnumel =160 +40 *s1 +40 *s2 +10 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_hardswish_2 [grid (triton_poi_fused_hardswish_2_xnumel )](buf3 ,12960 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf4 =reinterpret_tensor (buf1 ,(1 ,10 ,16 +4 *s1 +4 *s2 +s1 *s2 ),(160 +40 *s1 +40 *s2 +10 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,1 ),0 );del buf1 \n        buf5 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,2 ,2 ),torch .float32 )\n        buf6 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,2 ,2 ),torch .float32 )\n        buf13 =empty_strided_cuda ((1 ,2 ,1 ,1 ),(2 ,1 ,2 ,2 ),torch .float32 )\n\n        80 +20 *s1 +20 *s2 +5 *s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused_hardswish_native_group_norm_view_3 [grid (2 )](buf3 ,buf4 ,buf5 ,buf6 ,buf13 ,1296 ,36 ,32 ,32 ,2 ,6480 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf8 =reinterpret_tensor (buf3 ,(1 ,10 ,16 +4 *s1 +4 *s2 +s1 *s2 ),(160 +40 *s1 +40 *s2 +10 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,1 ),0 );del buf3 \n\n        triton_poi_fused_native_group_norm_4_xnumel =160 +40 *s1 +40 *s2 +10 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_native_group_norm_4 [grid (triton_poi_fused_native_group_norm_4_xnumel )](buf4 ,buf5 ,buf6 ,primals_4 ,primals_5 ,buf8 ,1296 ,32 ,32 ,12960 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf6 \n        del primals_4 \n        del primals_5 \n        buf9 =empty_strided_cuda ((1 ,10 ,1 ,16 +4 *s1 +4 *s2 +s1 *s2 ),(160 +40 *s1 +40 *s2 +10 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_unsqueeze_5_xnumel =160 +40 *s1 +40 *s2 +10 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_unsqueeze_5 [grid (triton_poi_fused_unsqueeze_5_xnumel )](buf8 ,buf9 ,1296 ,36 ,32 ,32 ,12960 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf8 \n        buf10 =empty_strided_cuda ((1 ,1 ,8 +2 *s1 +2 *s2 +((s1 *s2 )//2 )),(8 +2 *s1 +2 *s2 +((s1 *s2 )//2 ),8 +2 *s1 +2 *s2 +((s1 *s2 )//2 ),1 ),torch .float32 )\n        buf11 =empty_strided_cuda ((1 ,1 ,8 +2 *s1 +2 *s2 +((s1 *s2 )//2 )),(8 +2 *s1 +2 *s2 +((s1 *s2 )//2 ),8 +2 *s1 +2 *s2 +((s1 *s2 )//2 ),1 ),torch .float32 )\n\n        triton_per_fused__softmax_6_xnumel =8 +2 *s1 +2 *s2 +((s1 *s2 )//2 )\n        get_raw_stream (0 )\n        triton_per_fused__softmax_6 [grid (triton_per_fused__softmax_6_xnumel )](buf9 ,buf10 ,buf11 ,32 ,32 ,648 ,10 ,XBLOCK =32 ,num_warps =4 ,num_stages =1 )\n        8 +2 *s1 +2 *s2 +((s1 *s2 )//2 )\n        buf12 =empty_strided_cuda ((1 ,10 ,8 +2 *s1 +2 *s2 +((s1 *s2 )//2 )),(80 +10 *((s1 *s2 )//2 )+20 *s1 +20 *s2 ,8 +2 *s1 +2 *s2 +((s1 *s2 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused__softmax_7_xnumel =80 +10 *((s1 *s2 )//2 )+20 *s1 +20 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__softmax_7 [grid (triton_poi_fused__softmax_7_xnumel )](buf9 ,buf10 ,buf11 ,buf12 ,648 ,32 ,32 ,6480 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf10 \n        del buf11 \n    return (buf12 ,buf4 ,buf9 ,buf12 ,reinterpret_tensor (buf5 ,(1 ,2 ,1 ),(2 ,1 ,1 ),0 ),reinterpret_tensor (buf13 ,(1 ,2 ,1 ),(2 ,1 ,1 ),0 ),s1 ,s2 ,4 +s1 ,4 +s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,8 +2 *s1 +2 *s2 +((s1 *s2 )//2 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =32 \n    primals_2 =32 \n    primals_3 =rand_strided ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "db48588e-f170-45b3-8de9-6886643facb8",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardsigmoid', 'Container', 'MultiMarginLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.container = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.Linear(64, 32),\n            nn.Linear(32, 16)\n        )\n        self.loss = nn.MultiMarginLoss()\n\n    def forward(self, x):\n        # Apply Hardsigmoid activation\n        x = self.hardsigmoid(x)\n        \n        # Flatten the input to fit the container's input shape\n        x = x.view(x.size(0), -1)\n        \n        # Pass through the container (a sequence of Linear layers)\n        x = self.container(x)\n        \n        # Compute the loss (assuming some target is provided)\n        # For demonstration, we'll create a dummy target\n        target = torch.randint(0, 16, (x.size(0),), device=x.device)\n        loss = self.loss(x, target)\n        \n        # Return both the output and the loss\n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 128).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_hardsigmoid_view_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =3.0 \n    tmp2 =tmp0 +tmp1 \n    tmp3 =0.0 \n    tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n    tmp5 =6.0 \n    tmp6 =triton_helpers .minimum (tmp4 ,tmp5 )\n    tmp7 =0.16666666666666666 \n    tmp8 =tmp6 *tmp7 \n    tl .store (out_ptr0 +(x0 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_arange_clamp_min_gather_ge_mean_ne_randint_rsub_scalar_tensor_where_1 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,out_ptr0 ,out_ptr1 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp15 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp0 =tl .load (in_out_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp2 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp3 =tl .full ([1 ,1 ],16 ,tl .int64 )\n    tmp4 =triton_helpers .randint64 (tmp0 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp3 )\n    tmp5 =r0_0 \n    tmp6 =tmp5 !=tmp4 \n    tmp7 =tl .full ([XBLOCK ,R0_BLOCK ],16 ,tl .int32 )\n    tmp8 =tmp4 +tmp7 \n    tmp9 =tmp4 <0 \n    tmp10 =tl .where (tmp9 ,tmp8 ,tmp4 )\n    tl .device_assert ((0 <=tmp10 )&(tmp10 <16 ),\"index out of bounds: 0 <= tmp10 < 16\")\n    tmp12 =tl .load (in_ptr0 +(tmp10 ),None ,eviction_policy ='evict_last')\n    tmp13 =1.0 \n    tmp14 =tmp13 -tmp12 \n    tmp16 =tmp14 +tmp15 \n    tmp17 =0.0 \n    tmp18 =triton_helpers .maximum (tmp16 ,tmp17 )\n    tmp19 =tl .where (tmp6 ,tmp18 ,tmp17 )\n    tmp20 =tl .broadcast_to (tmp19 ,[XBLOCK ,R0_BLOCK ])\n    tmp22 =tl .sum (tmp20 ,1 )[:,None ]\n    tmp23 =tmp16 >=tmp17 \n    tmp24 =16.0 \n    tmp25 =tmp22 /tmp24 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp4 ,None )\n    tl .store (out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp6 ,None )\n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp23 ,None )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp25 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_2 ,(64 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_3 ,(64 ,),(1 ,))\n    assert_size_stride (primals_4 ,(32 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_5 ,(32 ,),(1 ,))\n    assert_size_stride (primals_6 ,(16 ,32 ),(32 ,1 ))\n    assert_size_stride (primals_7 ,(16 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_hardsigmoid_view_0 [grid (128 )](primals_1 ,buf0 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_1 \n        buf1 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_3 ,buf0 ,reinterpret_tensor (primals_2 ,(128 ,64 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf1 )\n        del primals_2 \n        del primals_3 \n        buf2 =empty_strided_cuda ((1 ,32 ),(32 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_5 ,buf1 ,reinterpret_tensor (primals_4 ,(64 ,32 ),(1 ,64 ),0 ),alpha =1 ,beta =1 ,out =buf2 )\n        del primals_5 \n        buf3 =empty_strided_cuda ((1 ,16 ),(16 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_7 ,buf2 ,reinterpret_tensor (primals_6 ,(32 ,16 ),(1 ,32 ),0 ),alpha =1 ,beta =1 ,out =buf3 )\n        del primals_7 \n        buf4 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf4 )\n        buf5 =buf4 ;del buf4 \n        buf6 =empty_strided_cuda ((1 ,16 ),(16 ,1 ),torch .bool )\n        buf7 =empty_strided_cuda ((),(),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,16 ),(16 ,1 ),torch .bool )\n        buf9 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_arange_clamp_min_gather_ge_mean_ne_randint_rsub_scalar_tensor_where_1 [grid (1 )](buf5 ,buf9 ,buf3 ,buf6 ,buf8 ,0 ,1 ,16 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n    return (buf3 ,buf9 ,buf0 ,buf1 ,buf2 ,reinterpret_tensor (buf5 ,(1 ,1 ),(1 ,1 ),0 ),buf6 ,buf8 ,primals_6 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((64 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((32 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((16 ,32 ),(32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "dbbe12c0-925b-4983-bd9c-bf6766a61263",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['KLDivLoss', 'CrossMapLRN2d', 'InstanceNorm2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.instance_norm = nn.InstanceNorm2d(3)\n        self.loss = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, x):\n        # Apply CrossMapLRN2d\n        x = self.lrn(x)\n        \n        # Apply InstanceNorm2d\n        x = self.instance_norm(x)\n        \n        # Reshape the input to match the expected shape for KLDivLoss\n        # Assuming the input is a 4D tensor (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, -1)  # Flatten the spatial dimensions\n        \n        # Create a target tensor with the same shape as the flattened input\n        target = torch.softmax(torch.randn_like(x), dim=1)\n        \n        # Compute KLDivLoss\n        loss = self.loss(F.log_softmax(x, dim=1), target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    r0_numel =4096 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp3 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_randn_like_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =6144 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp4 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =r0_1 +6144 *x0 \n        tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n        tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n        tmp5 =triton_helpers .maximum (_tmp4 ,tmp3 )\n        _tmp4 =tl .where (r0_mask &xmask ,tmp5 ,_tmp4 )\n        tl .store (out_ptr0 +(r0_1 +6144 *x0 ),tmp2 ,r0_mask &xmask )\n    tmp4 =triton_helpers .max2 (_tmp4 ,1 )[:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_2 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =triton_helpers .max2 (tmp1 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp3 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =6144 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp1 =tl .load (in_ptr1 +(0 ))\n    tmp2 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    _tmp6 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +6144 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp3 =tmp0 -tmp2 \n        tmp4 =tl_math .exp (tmp3 )\n        tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n        tmp7 =_tmp6 +tmp5 \n        _tmp6 =tl .where (r0_mask &xmask ,tmp7 ,_tmp6 )\n    tmp6 =tl .sum (_tmp6 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_4 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp3 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =6144 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +6144 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .load (in_ptr1 +((r0_1 +6144 *x0 )//4096 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tl .load (in_ptr2 +((r0_1 +6144 *x0 )//4096 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp4 =4096.0 \n        tmp5 =tmp3 /tmp4 \n        tmp6 =1e-05 \n        tmp7 =tmp5 +tmp6 \n        tmp8 =libdevice .rsqrt (tmp7 )\n        tmp9 =tmp2 *tmp8 \n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =triton_helpers .maximum (_tmp11 ,tmp10 )\n        _tmp11 =tl .where (r0_mask &xmask ,tmp12 ,_tmp11 )\n    tmp11 =triton_helpers .max2 (_tmp11 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax_6 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =6144 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp10 =tl .load (in_ptr3 +(0 ))\n    tmp11 =tl .broadcast_to (tmp10 ,[XBLOCK ,R0_BLOCK ])\n    _tmp15 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_1 +6144 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .load (in_ptr1 +((r0_1 +6144 *x0 )//4096 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tl .load (in_ptr2 +((r0_1 +6144 *x0 )//4096 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp0 -tmp1 \n        tmp4 =4096.0 \n        tmp5 =tmp3 /tmp4 \n        tmp6 =1e-05 \n        tmp7 =tmp5 +tmp6 \n        tmp8 =libdevice .rsqrt (tmp7 )\n        tmp9 =tmp2 *tmp8 \n        tmp12 =tmp9 -tmp11 \n        tmp13 =tl_math .exp (tmp12 )\n        tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n        tmp16 =_tmp15 +tmp14 \n        _tmp15 =tl .where (r0_mask &xmask ,tmp16 ,_tmp15 )\n    tmp15 =tl .sum (_tmp15 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__log_softmax__softmax_mul_sub_sum_xlogy_7 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,in_ptr6 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    r0_numel =6144 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp1 =tl .load (in_ptr0 +(0 ))\n    tmp2 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .load (in_ptr1 +(0 ))\n    tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n    tmp26 =tl .load (in_ptr5 +(0 ))\n    tmp27 =tl .broadcast_to (tmp26 ,[XBLOCK ,R0_BLOCK ])\n    tmp29 =tl .load (in_ptr6 +(0 ))\n    tmp30 =tl .broadcast_to (tmp29 ,[XBLOCK ,R0_BLOCK ])\n    _tmp36 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_out_ptr0 +(r0_1 +6144 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp16 =tl .load (in_ptr2 +(r0_1 +6144 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp17 =tl .load (in_ptr3 +((r0_1 +6144 *x0 )//4096 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp19 =tl .load (in_ptr4 +((r0_1 +6144 *x0 )//4096 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tmp0 -tmp2 \n        tmp4 =tl_math .exp (tmp3 )\n        tmp7 =tmp4 /tmp6 \n        tmp8 =libdevice .isnan (tmp7 ).to (tl .int1 )\n        tmp9 =0.0 \n        tmp10 =tmp7 ==tmp9 \n        tmp11 =tl_math .log (tmp7 )\n        tmp12 =tmp7 *tmp11 \n        tmp13 =tl .where (tmp10 ,tmp9 ,tmp12 )\n        tmp14 =float (\"nan\")\n        tmp15 =tl .where (tmp8 ,tmp14 ,tmp13 )\n        tmp18 =tmp16 -tmp17 \n        tmp20 =4096.0 \n        tmp21 =tmp19 /tmp20 \n        tmp22 =1e-05 \n        tmp23 =tmp21 +tmp22 \n        tmp24 =libdevice .rsqrt (tmp23 )\n        tmp25 =tmp18 *tmp24 \n        tmp28 =tmp25 -tmp27 \n        tmp31 =tl_math .log (tmp30 )\n        tmp32 =tmp28 -tmp31 \n        tmp33 =tmp7 *tmp32 \n        tmp34 =tmp15 -tmp33 \n        tmp35 =tl .broadcast_to (tmp34 ,[XBLOCK ,R0_BLOCK ])\n        tmp37 =_tmp36 +tmp35 \n        _tmp36 =tl .where (r0_mask &xmask ,tmp37 ,_tmp36 )\n    tmp36 =tl .sum (_tmp36 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp36 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_div_sum_8 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =1.0 \n    tmp5 =tmp3 *tmp4 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp5 ,None )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_0 [grid (3 )](arg0_1 ,buf0 ,buf1 ,3 ,4096 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf3 )\n        buf4 =empty_strided_cuda ((1 ,12288 ),(12288 ,1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,1 ,2 ),(2 ,2 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__softmax_randn_like_1 [grid (2 )](buf3 ,buf4 ,buf5 ,0 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf3 \n        buf6 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_2 [grid (1 )](buf5 ,buf6 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf7 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_red_fused__softmax_3 [grid (2 )](buf4 ,buf6 ,buf7 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf8 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_4 [grid (1 )](buf7 ,buf8 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf9 =buf7 ;del buf7 \n\n        get_raw_stream (0 )\n        triton_red_fused__log_softmax_5 [grid (2 )](arg0_1 ,buf0 ,buf1 ,buf9 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf10 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_2 [grid (1 )](buf9 ,buf10 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf11 =buf9 ;del buf9 \n\n        get_raw_stream (0 )\n        triton_red_fused__log_softmax_6 [grid (2 )](arg0_1 ,buf0 ,buf1 ,buf10 ,buf11 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf12 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_4 [grid (1 )](buf11 ,buf12 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf13 =buf4 ;del buf4 \n        buf14 =reinterpret_tensor (buf11 ,(2 ,),(1 ,),0 );del buf11 \n\n        get_raw_stream (0 )\n        triton_red_fused__log_softmax__softmax_mul_sub_sum_xlogy_7 [grid (2 )](buf13 ,buf6 ,buf8 ,arg0_1 ,buf0 ,buf1 ,buf10 ,buf12 ,buf14 ,2 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg0_1 \n        del buf0 \n        del buf1 \n        del buf10 \n        del buf12 \n        del buf13 \n        del buf6 \n        buf15 =reinterpret_tensor (buf8 ,(),(),0 );del buf8 \n        buf16 =buf15 ;del buf15 \n\n        get_raw_stream (0 )\n        triton_per_fused_div_sum_8 [grid (1 )](buf16 ,buf14 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf14 \n    return (buf16 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "de87c063-c6ff-44a9-bd29-0724f3089c25",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ModuleDict', 'InstanceNorm2d', 'ModuleList', 'TripletMarginLoss', 'Softshrink', 'BatchNorm2d', 'BatchNorm3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        \n        # ModuleDict\n        self.module_dict = nn.ModuleDict({\n            'instance_norm2d': nn.InstanceNorm2d(10),\n            'batch_norm2d': nn.BatchNorm2d(10),\n            'batch_norm3d': nn.BatchNorm3d(10),\n            'softshrink': nn.Softshrink(),\n        })\n        \n        # ModuleList\n        self.module_list = nn.ModuleList([\n            nn.InstanceNorm2d(10),\n            nn.BatchNorm2d(10),\n            nn.BatchNorm3d(10),\n            nn.Softshrink(),\n        ])\n        \n        # TripletMarginLoss\n        self.triplet_loss = nn.TripletMarginLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        # Apply InstanceNorm2d from ModuleDict\n        x = self.module_dict['instance_norm2d'](x)\n        \n        # Apply BatchNorm2d from ModuleDict\n        x = self.module_dict['batch_norm2d'](x)\n        \n        # Reshape to 5D for BatchNorm3d\n        x = x.unsqueeze(2)  # Add a depth dimension\n        x = self.module_dict['batch_norm3d'](x)\n        x = x.squeeze(2)  # Remove the depth dimension\n        \n        # Apply Softshrink from ModuleDict\n        x = self.module_dict['softshrink'](x)\n        \n        # Apply InstanceNorm2d from ModuleList\n        x = self.module_list[0](x)\n        \n        # Apply BatchNorm2d from ModuleList\n        x = self.module_list[1](x)\n        \n        # Reshape to 5D for BatchNorm3d\n        x = x.unsqueeze(2)  # Add a depth dimension\n        x = self.module_list[2](x)\n        x = x.squeeze(2)  # Remove the depth dimension\n        \n        # Apply Softshrink from ModuleList\n        x = self.module_list[3](x)\n        \n        # Generate anchor, positive, and negative samples for TripletMarginLoss\n        anchor = x\n        positive = x + torch.randn_like(x) * 0.1\n        negative = x + torch.randn_like(x) * 0.2\n        \n        # Compute TripletMarginLoss\n        loss = self.triplet_loss(anchor, positive, negative)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit__native_batch_norm_legit_functional_abs_gt_mul_sign_sub_where_0 (in_out_ptr0 ,in_out_ptr1 ,in_out_ptr2 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,in_ptr6 ,in_ptr7 ,in_ptr8 ,in_ptr9 ,in_ptr10 ,in_ptr11 ,in_ptr12 ,in_ptr13 ,in_ptr14 ,out_ptr0 ,out_ptr1 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,out_ptr8 ,out_ptr10 ,out_ptr12 ,out_ptr14 ,out_ptr15 ,out_ptr16 ,out_ptr18 ,out_ptr20 ,out_ptr22 ,out_ptr23 ,out_ptr24 ,out_ptr25 ,out_ptr27 ,out_ptr29 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +1024 *x0 ),None )\n    tmp39 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp44 =tl .load (in_ptr2 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp49 =tl .load (in_ptr3 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp51 =tl .load (in_ptr4 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp69 =tl .load (in_ptr5 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp73 =tl .load (in_ptr6 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp78 =tl .load (in_ptr7 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp80 =tl .load (in_ptr8 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp129 =tl .load (in_ptr9 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp133 =tl .load (in_ptr10 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp138 =tl .load (in_ptr11 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp140 =tl .load (in_ptr12 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp158 =tl .load (in_ptr13 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp162 =tl .load (in_ptr14 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp1 =tl .broadcast_to (tmp0 ,[R0_BLOCK ])\n    tmp3 =tl .broadcast_to (tmp1 ,[R0_BLOCK ])\n    tmp5 =triton_helpers .promote_to_tensor (tl .sum (tmp3 ,0 ))\n    tmp6 =tl .full ([1 ],1024 ,tl .int32 )\n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp5 /tmp7 \n    tmp9 =tmp1 -tmp8 \n    tmp10 =tmp9 *tmp9 \n    tmp11 =tl .broadcast_to (tmp10 ,[R0_BLOCK ])\n    tmp13 =triton_helpers .promote_to_tensor (tl .sum (tmp11 ,0 ))\n    tmp14 =1024.0 \n    tmp15 =tmp13 /tmp14 \n    tmp16 =1e-05 \n    tmp17 =tmp15 +tmp16 \n    tmp18 =libdevice .rsqrt (tmp17 )\n    tmp19 =tmp0 -tmp8 \n    tmp20 =tmp19 *tmp18 \n    tmp21 =tl .broadcast_to (tmp20 ,[R0_BLOCK ])\n    tmp23 =tl .broadcast_to (tmp21 ,[R0_BLOCK ])\n    tmp25 =triton_helpers .promote_to_tensor (tl .sum (tmp23 ,0 ))\n    tmp26 =tmp25 /tmp7 \n    tmp27 =tmp21 -tmp26 \n    tmp28 =tmp27 *tmp27 \n    tmp29 =tl .broadcast_to (tmp28 ,[R0_BLOCK ])\n    tmp31 =triton_helpers .promote_to_tensor (tl .sum (tmp29 ,0 ))\n    tmp32 =tmp31 /tmp14 \n    tmp33 =tmp32 +tmp16 \n    tmp34 =libdevice .rsqrt (tmp33 )\n    tmp35 =1.0009775171065494 \n    tmp36 =tmp32 *tmp35 \n    tmp37 =0.1 \n    tmp38 =tmp36 *tmp37 \n    tmp40 =0.9 \n    tmp41 =tmp39 *tmp40 \n    tmp42 =tmp38 +tmp41 \n    tmp43 =tmp26 *tmp37 \n    tmp45 =tmp44 *tmp40 \n    tmp46 =tmp43 +tmp45 \n    tmp47 =tmp20 -tmp26 \n    tmp48 =tmp47 *tmp34 \n    tmp50 =tmp48 *tmp49 \n    tmp52 =tmp50 +tmp51 \n    tmp53 =tl .broadcast_to (tmp52 ,[R0_BLOCK ])\n    tmp55 =tl .broadcast_to (tmp53 ,[R0_BLOCK ])\n    tmp57 =triton_helpers .promote_to_tensor (tl .sum (tmp55 ,0 ))\n    tmp58 =tmp57 /tmp7 \n    tmp59 =tmp53 -tmp58 \n    tmp60 =tmp59 *tmp59 \n    tmp61 =tl .broadcast_to (tmp60 ,[R0_BLOCK ])\n    tmp63 =triton_helpers .promote_to_tensor (tl .sum (tmp61 ,0 ))\n    tmp64 =tmp63 /tmp14 \n    tmp65 =tmp64 +tmp16 \n    tmp66 =libdevice .rsqrt (tmp65 )\n    tmp67 =tmp64 *tmp35 \n    tmp68 =tmp67 *tmp37 \n    tmp70 =tmp69 *tmp40 \n    tmp71 =tmp68 +tmp70 \n    tmp72 =tmp58 *tmp37 \n    tmp74 =tmp73 *tmp40 \n    tmp75 =tmp72 +tmp74 \n    tmp76 =tmp52 -tmp58 \n    tmp77 =tmp76 *tmp66 \n    tmp79 =tmp77 *tmp78 \n    tmp81 =tmp79 +tmp80 \n    tmp82 =tl_math .abs (tmp81 )\n    tmp83 =0.5 \n    tmp84 =tmp82 >tmp83 \n    tmp85 =tl .full ([1 ],0 ,tl .int32 )\n    tmp86 =tmp85 <tmp81 \n    tmp87 =tmp86 .to (tl .int8 )\n    tmp88 =tmp81 <tmp85 \n    tmp89 =tmp88 .to (tl .int8 )\n    tmp90 =tmp87 -tmp89 \n    tmp91 =tmp90 .to (tmp81 .dtype )\n    tmp92 =tmp91 *tmp83 \n    tmp93 =tmp81 -tmp92 \n    tmp94 =0.0 \n    tmp95 =tmp81 *tmp94 \n    tmp96 =tl .where (tmp84 ,tmp93 ,tmp95 )\n    tmp97 =tl .broadcast_to (tmp96 ,[R0_BLOCK ])\n    tmp99 =tl .broadcast_to (tmp97 ,[R0_BLOCK ])\n    tmp101 =triton_helpers .promote_to_tensor (tl .sum (tmp99 ,0 ))\n    tmp102 =tmp101 /tmp7 \n    tmp103 =tmp97 -tmp102 \n    tmp104 =tmp103 *tmp103 \n    tmp105 =tl .broadcast_to (tmp104 ,[R0_BLOCK ])\n    tmp107 =triton_helpers .promote_to_tensor (tl .sum (tmp105 ,0 ))\n    tmp108 =tmp107 /tmp14 \n    tmp109 =tmp108 +tmp16 \n    tmp110 =libdevice .rsqrt (tmp109 )\n    tmp111 =tmp96 -tmp102 \n    tmp112 =tmp111 *tmp110 \n    tmp113 =tl .broadcast_to (tmp112 ,[R0_BLOCK ])\n    tmp115 =tl .broadcast_to (tmp113 ,[R0_BLOCK ])\n    tmp117 =triton_helpers .promote_to_tensor (tl .sum (tmp115 ,0 ))\n    tmp118 =tmp117 /tmp7 \n    tmp119 =tmp113 -tmp118 \n    tmp120 =tmp119 *tmp119 \n    tmp121 =tl .broadcast_to (tmp120 ,[R0_BLOCK ])\n    tmp123 =triton_helpers .promote_to_tensor (tl .sum (tmp121 ,0 ))\n    tmp124 =tmp123 /tmp14 \n    tmp125 =tmp124 +tmp16 \n    tmp126 =libdevice .rsqrt (tmp125 )\n    tmp127 =tmp124 *tmp35 \n    tmp128 =tmp127 *tmp37 \n    tmp130 =tmp129 *tmp40 \n    tmp131 =tmp128 +tmp130 \n    tmp132 =tmp118 *tmp37 \n    tmp134 =tmp133 *tmp40 \n    tmp135 =tmp132 +tmp134 \n    tmp136 =tmp112 -tmp118 \n    tmp137 =tmp136 *tmp126 \n    tmp139 =tmp137 *tmp138 \n    tmp141 =tmp139 +tmp140 \n    tmp142 =tl .broadcast_to (tmp141 ,[R0_BLOCK ])\n    tmp144 =tl .broadcast_to (tmp142 ,[R0_BLOCK ])\n    tmp146 =triton_helpers .promote_to_tensor (tl .sum (tmp144 ,0 ))\n    tmp147 =tmp146 /tmp7 \n    tmp148 =tmp142 -tmp147 \n    tmp149 =tmp148 *tmp148 \n    tmp150 =tl .broadcast_to (tmp149 ,[R0_BLOCK ])\n    tmp152 =triton_helpers .promote_to_tensor (tl .sum (tmp150 ,0 ))\n    tmp153 =tmp152 /tmp14 \n    tmp154 =tmp153 +tmp16 \n    tmp155 =libdevice .rsqrt (tmp154 )\n    tmp156 =tmp153 *tmp35 \n    tmp157 =tmp156 *tmp37 \n    tmp159 =tmp158 *tmp40 \n    tmp160 =tmp157 +tmp159 \n    tmp161 =tmp147 *tmp37 \n    tmp163 =tmp162 *tmp40 \n    tmp164 =tmp161 +tmp163 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp18 ,None )\n    tl .store (out_ptr3 +(x0 ),tmp34 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp42 ,None )\n    tl .store (out_ptr7 +(x0 ),tmp46 ,None )\n    tl .store (out_ptr10 +(x0 ),tmp66 ,None )\n    tl .store (out_ptr12 +(x0 ),tmp71 ,None )\n    tl .store (out_ptr14 +(x0 ),tmp75 ,None )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr2 +(x0 ),tmp110 ,None )\n    tl .store (out_ptr18 +(x0 ),tmp126 ,None )\n    tl .store (out_ptr20 +(x0 ),tmp131 ,None )\n    tl .store (out_ptr22 +(x0 ),tmp135 ,None )\n    tl .store (in_out_ptr1 +(r0_1 +1024 *x0 ),tmp141 ,None )\n    tl .store (out_ptr25 +(x0 ),tmp155 ,None )\n    tl .store (out_ptr27 +(x0 ),tmp160 ,None )\n    tl .store (out_ptr29 +(x0 ),tmp164 ,None )\n    tl .store (out_ptr0 +(x0 ),tmp8 ,None )\n    tl .store (out_ptr1 +(x0 ),tmp26 ,None )\n    tl .store (out_ptr8 +(x0 ),tmp58 ,None )\n    tl .store (out_ptr15 +(x0 ),tmp102 ,None )\n    tl .store (out_ptr16 +(x0 ),tmp118 ,None )\n    tl .store (out_ptr23 +(x0 ),tmp147 ,None )\n    tl .store (out_ptr24 +(x0 ),tmp152 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_abs_add_div_eq_gt_masked_fill_mul_norm_randn_like_scalar_tensor_sign_sub_where_1 (in_out_ptr0 ,in_out_ptr1 ,in_out_ptr2 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,load_seed_offset ,load_seed_offset1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =320 \n    R0_BLOCK :tl .constexpr =32 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    x3 =xindex //32 \n    tmp5 =tl .load (in_out_ptr0 +(r0_1 +32 *x0 ),xmask ,other =0.0 )\n    tmp6 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr2 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr3 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp17 =tl .load (in_ptr4 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_1 +32 *x0 \n    tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =tl .load (in_ptr0 +load_seed_offset1 )\n    tmp4 =tl .randn (tmp3 ,(tmp1 ).to (tl .uint32 ))\n    tmp7 =tmp5 -tmp6 \n    tmp9 =1024.0 \n    tmp10 =tmp8 /tmp9 \n    tmp11 =1e-05 \n    tmp12 =tmp10 +tmp11 \n    tmp13 =libdevice .rsqrt (tmp12 )\n    tmp14 =tmp7 *tmp13 \n    tmp16 =tmp14 *tmp15 \n    tmp18 =tmp16 +tmp17 \n    tmp19 =tl_math .abs (tmp18 )\n    tmp20 =0.5 \n    tmp21 =tmp19 >tmp20 \n    tmp22 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp23 =tmp22 <tmp18 \n    tmp24 =tmp23 .to (tl .int8 )\n    tmp25 =tmp18 <tmp22 \n    tmp26 =tmp25 .to (tl .int8 )\n    tmp27 =tmp24 -tmp26 \n    tmp28 =tmp27 .to (tmp18 .dtype )\n    tmp29 =tmp28 *tmp20 \n    tmp30 =tmp18 -tmp29 \n    tmp31 =0.0 \n    tmp32 =tmp18 *tmp31 \n    tmp33 =tl .where (tmp21 ,tmp30 ,tmp32 )\n    tmp34 =0.1 \n    tmp35 =tmp4 *tmp34 \n    tmp36 =tmp33 +tmp35 \n    tmp37 =tmp33 -tmp36 \n    tmp38 =1e-06 \n    tmp39 =tmp37 +tmp38 \n    tmp40 =tmp39 *tmp39 \n    tmp41 =tl .broadcast_to (tmp40 ,[XBLOCK ,R0_BLOCK ])\n    tmp43 =tl .where (xmask ,tmp41 ,0 )\n    tmp44 =tl .sum (tmp43 ,1 )[:,None ]\n    tmp45 =0.2 \n    tmp46 =tmp2 *tmp45 \n    tmp47 =tmp33 +tmp46 \n    tmp48 =tmp33 -tmp47 \n    tmp49 =tmp48 +tmp38 \n    tmp50 =tmp49 *tmp49 \n    tmp51 =tl .broadcast_to (tmp50 ,[XBLOCK ,R0_BLOCK ])\n    tmp53 =tl .where (xmask ,tmp51 ,0 )\n    tmp54 =tl .sum (tmp53 ,1 )[:,None ]\n    tmp55 =libdevice .sqrt (tmp54 )\n    tmp56 =tmp55 ==tmp31 \n    tmp57 =tmp49 /tmp55 \n    tmp58 =tl .where (tmp56 ,tmp31 ,tmp57 )\n    tmp59 =libdevice .sqrt (tmp44 )\n    tmp60 =tmp59 ==tmp31 \n    tmp61 =tmp39 /tmp59 \n    tmp62 =tl .where (tmp60 ,tmp31 ,tmp61 )\n    tl .store (out_ptr0 +(r0_1 +32 *x0 ),tmp21 ,xmask )\n    tl .store (in_out_ptr1 +(r0_1 +32 *x0 ),tmp58 ,xmask )\n    tl .store (in_out_ptr2 +(r0_1 +32 *x0 ),tmp62 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp44 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp54 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_clamp_min_ge_mean_norm_sub_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    r0_numel =320 \n    R0_BLOCK :tl .constexpr =512 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp4 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =libdevice .sqrt (tmp0 )\n    tmp2 =1.0 \n    tmp3 =tmp1 +tmp2 \n    tmp5 =libdevice .sqrt (tmp4 )\n    tmp6 =tmp3 -tmp5 \n    tmp7 =0.0 \n    tmp8 =triton_helpers .maximum (tmp6 ,tmp7 )\n    tmp9 =tl .broadcast_to (tmp8 ,[R0_BLOCK ])\n    tmp11 =tl .where (r0_mask ,tmp9 ,0 )\n    tmp12 =triton_helpers .promote_to_tensor (tl .sum (tmp11 ,0 ))\n    tmp13 =tmp6 >=tmp7 \n    tmp14 =320.0 \n    tmp15 =tmp12 /tmp14 \n    tl .store (out_ptr0 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp13 ,r0_mask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp15 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_3 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .full ([1 ],1 ,tl .int64 )\n    tmp3 =tmp1 +tmp2 \n    tl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_2 ,(),())\n    assert_size_stride (primals_3 ,(10 ,),(1 ,))\n    assert_size_stride (primals_4 ,(10 ,),(1 ,))\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    assert_size_stride (primals_6 ,(10 ,),(1 ,))\n    assert_size_stride (primals_7 ,(),())\n    assert_size_stride (primals_8 ,(10 ,),(1 ,))\n    assert_size_stride (primals_9 ,(10 ,),(1 ,))\n    assert_size_stride (primals_10 ,(10 ,),(1 ,))\n    assert_size_stride (primals_11 ,(10 ,),(1 ,))\n    assert_size_stride (primals_12 ,(),())\n    assert_size_stride (primals_13 ,(10 ,),(1 ,))\n    assert_size_stride (primals_14 ,(10 ,),(1 ,))\n    assert_size_stride (primals_15 ,(10 ,),(1 ,))\n    assert_size_stride (primals_16 ,(10 ,),(1 ,))\n    assert_size_stride (primals_17 ,(),())\n    assert_size_stride (primals_18 ,(10 ,),(1 ,))\n    assert_size_stride (primals_19 ,(10 ,),(1 ,))\n    assert_size_stride (primals_20 ,(10 ,),(1 ,))\n    assert_size_stride (primals_21 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,10 ,10 ),torch .float32 )\n        buf3 =reinterpret_tensor (buf1 ,(1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),0 );del buf1 \n        buf0 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),torch .float32 )\n        buf9 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,1 ,1 ,1 ),torch .float32 )\n        buf12 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,1 ,1 ,1 ),torch .float32 )\n        buf13 =reinterpret_tensor (buf8 ,(1 ,10 ,1 ,32 ,32 ),(10240 ,1024 ,10240 ,32 ,1 ),0 );del buf8 \n        buf15 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,10 ,10 ),torch .float32 )\n        buf17 =reinterpret_tensor (buf15 ,(1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),0 );del buf15 \n        buf14 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),torch .float32 )\n        buf18 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),torch .float32 )\n        buf21 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),torch .float32 )\n        buf22 =reinterpret_tensor (buf13 ,(1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),0 );del buf13 \n        buf23 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,10 ,10 ,10 ),torch .float32 )\n        buf24 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,10 ,10 ,10 ),torch .float32 )\n        buf26 =empty_strided_cuda ((1 ,10 ,1 ,1 ,1 ),(10 ,1 ,10 ,10 ,10 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit__native_batch_norm_legit_functional_abs_gt_mul_sign_sub_where_0 [grid (10 )](buf3 ,buf22 ,buf17 ,primals_1 ,primals_4 ,primals_3 ,primals_5 ,primals_6 ,primals_9 ,primals_8 ,primals_10 ,primals_11 ,primals_14 ,primals_13 ,primals_15 ,primals_16 ,primals_19 ,primals_18 ,buf0 ,buf4 ,buf7 ,primals_4 ,primals_3 ,buf9 ,buf12 ,primals_9 ,primals_8 ,buf14 ,buf18 ,buf21 ,primals_14 ,primals_13 ,buf23 ,buf24 ,buf26 ,primals_19 ,primals_18 ,10 ,1024 ,num_warps =8 ,num_stages =1 )\n        del primals_13 \n        del primals_14 \n        del primals_18 \n        del primals_19 \n        del primals_3 \n        del primals_4 \n        del primals_8 \n        del primals_9 \n        buf29 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf29 )\n        buf31 =empty_strided_cuda ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),torch .float32 )\n        buf30 =empty_strided_cuda ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),torch .float32 )\n        buf27 =reinterpret_tensor (buf22 ,(1 ,10 ,1 ,32 ,32 ),(10240 ,1024 ,10240 ,32 ,1 ),0 );del buf22 \n        buf28 =empty_strided_cuda ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),torch .bool )\n        buf32 =empty_strided_cuda ((1 ,10 ,32 ),(320 ,32 ,1 ),torch .float32 )\n        buf33 =empty_strided_cuda ((1 ,10 ,32 ),(320 ,32 ,1 ),torch .float32 )\n        buf36 =buf31 ;del buf31 \n        buf37 =buf30 ;del buf30 \n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_abs_add_div_eq_gt_masked_fill_mul_norm_randn_like_scalar_tensor_sign_sub_where_1 [grid (320 )](buf27 ,buf36 ,buf37 ,buf29 ,buf23 ,buf24 ,primals_20 ,primals_21 ,buf28 ,buf32 ,buf33 ,1 ,0 ,320 ,32 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf24 \n        del buf27 \n        del buf29 \n        del primals_21 \n        buf34 =empty_strided_cuda ((),(),torch .float32 )\n        buf35 =empty_strided_cuda ((1 ,10 ,32 ),(320 ,32 ,1 ),torch .bool )\n        buf70 =buf34 ;del buf34 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_clamp_min_ge_mean_norm_sub_2 [grid (1 )](buf70 ,buf32 ,buf33 ,buf35 ,1 ,320 ,num_warps =4 ,num_stages =1 )\n        del buf32 \n        del buf33 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_3 [grid (1 )](primals_2 ,primals_2 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_3 [grid (1 )](primals_7 ,primals_7 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_7 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_3 [grid (1 )](primals_12 ,primals_12 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_12 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_3 [grid (1 )](primals_17 ,primals_17 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_17 \n    return (buf70 ,primals_1 ,primals_5 ,primals_6 ,primals_10 ,primals_11 ,primals_15 ,primals_16 ,primals_20 ,buf0 ,buf3 ,buf4 ,buf7 ,buf9 ,buf12 ,buf14 ,buf17 ,buf18 ,buf21 ,reinterpret_tensor (buf26 ,(10 ,),(1 ,),0 ),buf28 ,buf35 ,buf36 ,buf37 ,reinterpret_tensor (buf23 ,(1 ,10 ,1 ,1 ,1 ),(10 ,1 ,1 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_3 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_8 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_13 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_18 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ded8e6a6-34c0-466e-b0e5-13e35c05cf45",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softmin', 'MultiheadAttention', 'SiLU', 'ParameterDict', 'Mish', 'LazyConvTranspose2d', 'Upsample', 'InstanceNorm3d', 'Tanh', 'ReLU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softmin = nn.Softmin(dim=1)\n        self.multihead_attention = nn.MultiheadAttention(embed_dim=64, num_heads=8)\n        self.silu = nn.SiLU()\n        self.parameter_dict = nn.ParameterDict({\n            'param1': nn.Parameter(torch.randn(64)),\n            'param2': nn.Parameter(torch.randn(64))\n        })\n        self.mish = nn.Mish()\n        self.lazy_conv_transpose2d = nn.LazyConvTranspose2d(out_channels=32, kernel_size=3, stride=2)\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n        self.instance_norm3d = nn.InstanceNorm3d(num_features=32)\n        self.tanh = nn.Tanh()\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # Reshape input to fit MultiheadAttention\n        x = x.view(-1, 64, 64)  # Assuming input is reshaped to (batch_size, seq_len, embed_dim)\n        x, _ = self.multihead_attention(x, x, x)\n        \n        # Apply SiLU activation\n        x = self.silu(x)\n        \n        # Apply Mish activation\n        x = self.mish(x)\n        \n        # Reshape for ConvTranspose2d\n        x = x.view(-1, 64, 8, 8)  # Reshape to (batch_size, channels, height, width)\n        x = self.lazy_conv_transpose2d(x)\n        \n        # Upsample\n        x = self.upsample(x)\n        \n        # Reshape for InstanceNorm3d\n        x = x.unsqueeze(2)  # Add a dimension to make it 5D (batch_size, channels, depth, height, width)\n        x = self.instance_norm3d(x)\n        x = x.squeeze(2)  # Remove the added dimension\n        \n        # Apply Tanh activation\n        x = self.tanh(x)\n        \n        # Apply ReLU activation\n        x = self.relu(x)\n        \n        # Apply Softmin\n        x = x.view(x.size(0), -1)  # Flatten for Softmin\n        x = self.softmin(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64, 64).cuda()  # Input shape (batch_size, seq_len, embed_dim)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clone_0 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =(xindex %64 )\n    x1 =((xindex //64 )%64 )\n    x2 =xindex //4096 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +64 *x2 +192 *x1 ),None )\n    tmp1 =tl .load (in_ptr1 +(x0 +64 *x2 ),None ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (out_ptr0 +(x3 ),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_mul_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),None )\n    tmp1 =0.3535533905932738 \n    tmp2 =tmp0 *tmp1 \n    tl .store (out_ptr0 +(x0 ),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_2 (in_out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =512 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tmp0 -tmp0 \n    tmp2 =tl_math .exp (tmp1 )\n    tmp3 =tmp2 /tmp2 \n    tl .store (in_out_ptr0 +(x0 ),tmp3 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_mish_silu_3 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),None )\n    tmp1 =tl .sigmoid (tmp0 )\n    tmp2 =tmp0 *tmp1 \n    tmp3 =20.0 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tl_math .exp (tmp2 )\n    tmp6 =libdevice .log1p (tmp5 )\n    tmp7 =tl .where (tmp4 ,tmp2 ,tmp6 )\n    tmp8 =libdevice .tanh (tmp7 )\n    tmp9 =tmp2 *tmp8 \n    tl .store (out_ptr0 +(x0 ),tmp9 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_arange_mul_4 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =34 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =x0 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.5 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =tmp3 .to (tl .int32 )\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit__softmax__unsafe_index_convolution_neg_5 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =32 \n    r0_numel =1156 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp10 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp13_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp13_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp13_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index //34 \n        r0_1 =(r0_index %34 )\n        r0_3 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_2 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tl .load (in_ptr0 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .full ([XBLOCK ,R0_BLOCK ],17 ,tl .int32 )\n        tmp2 =tmp0 +tmp1 \n        tmp3 =tmp0 <0 \n        tmp4 =tl .where (tmp3 ,tmp2 ,tmp0 )\n        tmp6 =tmp5 +tmp1 \n        tmp7 =tmp5 <0 \n        tmp8 =tl .where (tmp7 ,tmp6 ,tmp5 )\n        tmp9 =tl .load (in_ptr1 +(tmp8 +17 *tmp4 +289 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp11 =tmp9 +tmp10 \n        tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n        tmp13_mean_next ,tmp13_m2_next ,tmp13_weight_next =triton_helpers .welford_reduce (\n        tmp12 ,tmp13_mean ,tmp13_m2 ,tmp13_weight ,roffset ==0 \n        )\n        tmp13_mean =tl .where (r0_mask &xmask ,tmp13_mean_next ,tmp13_mean )\n        tmp13_m2 =tl .where (r0_mask &xmask ,tmp13_m2_next ,tmp13_m2 )\n        tmp13_weight =tl .where (r0_mask &xmask ,tmp13_weight_next ,tmp13_weight )\n        tl .store (out_ptr0 +(r0_3 +1184 *x0 ),tmp11 ,r0_mask &xmask )\n    tmp16 ,tmp17 ,tmp18 =triton_helpers .welford (tmp13_mean ,tmp13_m2 ,tmp13_weight ,1 )\n    tmp13 =tmp16 [:,None ]\n    tmp14 =tmp17 [:,None ]\n    tmp15 =tmp18 [:,None ]\n    tmp19 =1156.0 \n    tmp20 =tmp14 /tmp19 \n    tmp21 =1e-05 \n    tmp22 =tmp20 +tmp21 \n    tmp23 =libdevice .rsqrt (tmp22 )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp23 ,xmask )\n    tmp26_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp26_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp26_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_3 =r0_index \n        tmp24 =tl .load (out_ptr0 +(r0_3 +1184 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp25 =tl .broadcast_to (tmp24 ,[XBLOCK ,R0_BLOCK ])\n        tmp26_mean_next ,tmp26_m2_next ,tmp26_weight_next =triton_helpers .welford_reduce (\n        tmp25 ,tmp26_mean ,tmp26_m2 ,tmp26_weight ,roffset ==0 \n        )\n        tmp26_mean =tl .where (r0_mask &xmask ,tmp26_mean_next ,tmp26_mean )\n        tmp26_m2 =tl .where (r0_mask &xmask ,tmp26_m2_next ,tmp26_m2 )\n        tmp26_weight =tl .where (r0_mask &xmask ,tmp26_weight_next ,tmp26_weight )\n    tmp29 ,tmp30 ,tmp31 =triton_helpers .welford (tmp26_mean ,tmp26_m2 ,tmp26_weight ,1 )\n    tmp26 =tmp29 [:,None ]\n    tmp27 =tmp30 [:,None ]\n    tmp28 =tmp31 [:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp26 ,xmask )\n    _tmp40 =tl .full ([XBLOCK ,R0_BLOCK ],float (\"-inf\"),tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_3 =r0_index \n        tmp32 =tl .load (out_ptr0 +(r0_3 +1184 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp33 =tmp32 -tmp26 \n        tmp34 =tmp33 *tmp23 \n        tmp35 =libdevice .tanh (tmp34 )\n        tmp36 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp37 =triton_helpers .maximum (tmp36 ,tmp35 )\n        tmp38 =-tmp37 \n        tmp39 =tl .broadcast_to (tmp38 ,[XBLOCK ,R0_BLOCK ])\n        tmp41 =triton_helpers .maximum (_tmp40 ,tmp39 )\n        _tmp40 =tl .where (r0_mask &xmask ,tmp41 ,_tmp40 )\n    tmp40 =triton_helpers .max2 (_tmp40 ,1 )[:,None ]\n    tl .store (out_ptr2 +(x0 ),tmp40 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_neg_6 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =32 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =triton_helpers .max2 (tmp1 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp3 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__softmax_neg_7 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =5 \n    r0_numel =7399 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp18 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +7399 *x0 \n        tmp1 =tl .full ([1 ,1 ],36992 ,tl .int32 )\n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(1184 *((((r0_1 +7399 *x0 )//1156 )%32 ))+(((r0_1 +7399 *x0 )%1156 ))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +((((r0_1 +7399 *x0 )//1156 )%32 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tmp3 -tmp4 \n        tmp6 =tl .load (in_ptr2 +((((r0_1 +7399 *x0 )//1156 )%32 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp7 =tmp5 *tmp6 \n        tmp8 =libdevice .tanh (tmp7 )\n        tmp9 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n        tmp11 =-tmp10 \n        tmp12 =tl .load (in_ptr3 +(tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .int32 )),tmp2 ,eviction_policy ='evict_last',other =0.0 )\n        tmp13 =tmp11 -tmp12 \n        tmp14 =tl_math .exp (tmp13 )\n        tmp15 =tl .full (tmp14 .shape ,0 ,tmp14 .dtype )\n        tmp16 =tl .where (tmp2 ,tmp14 ,tmp15 )\n        tmp17 =tl .broadcast_to (tmp16 ,[XBLOCK ,R0_BLOCK ])\n        tmp19 =_tmp18 +tmp17 \n        _tmp18 =tl .where (r0_mask &xmask ,tmp19 ,_tmp18 )\n    tmp18 =tl .sum (_tmp18 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp18 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_neg_8 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =5 \n    R0_BLOCK :tl .constexpr =8 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp4 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__softmax_neg_9 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =36992 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(1184 *(x0 //1156 )+((x0 %1156 ))),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 //1156 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x0 //1156 ),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr3 +(0 ))\n    tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ])\n    tmp13 =tl .load (in_ptr4 +(0 ))\n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ])\n    tmp2 =tmp0 -tmp1 \n    tmp4 =tmp2 *tmp3 \n    tmp5 =libdevice .tanh (tmp4 )\n    tmp6 =tl .full ([1 ],0 ,tl .int32 )\n    tmp7 =triton_helpers .maximum (tmp6 ,tmp5 )\n    tmp8 =-tmp7 \n    tmp11 =tmp8 -tmp10 \n    tmp12 =tl_math .exp (tmp11 )\n    tmp15 =tmp12 /tmp14 \n    tl .store (out_ptr0 +(x0 ),tmp15 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,64 ,64 ),(4096 ,64 ,1 ))\n    assert_size_stride (primals_2 ,(192 ,),(1 ,))\n    assert_size_stride (primals_3 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_4 ,(64 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_5 ,(64 ,),(1 ,))\n    assert_size_stride (primals_6 ,(64 ,32 ,3 ,3 ),(288 ,9 ,3 ,1 ))\n    assert_size_stride (primals_7 ,(32 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((64 ,192 ),(192 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(64 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_3 ,(64 ,192 ),(1 ,64 ),0 ),out =buf0 )\n        del primals_3 \n        buf1 =empty_strided_cuda ((3 ,1 ,64 ,64 ),(4096 ,1 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_clone_0 [grid (12288 )](buf0 ,primals_2 ,buf1 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        del primals_2 \n        buf2 =empty_strided_cuda ((512 ,1 ,8 ),(8 ,4096 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_mul_1 [grid (4096 )](buf1 ,buf2 ,4096 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((512 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        extern_kernels .bmm (buf2 ,reinterpret_tensor (buf1 ,(512 ,8 ,1 ),(8 ,1 ,0 ),4096 ),out =buf3 )\n        buf4 =buf3 ;del buf3 \n\n        get_raw_stream (0 )\n        triton_poi_fused__softmax_2 [grid (512 )](buf4 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((512 ,1 ,8 ),(8 ,8 ,1 ),torch .float32 )\n\n        extern_kernels .bmm (buf4 ,reinterpret_tensor (buf1 ,(512 ,1 ,8 ),(8 ,0 ,1 ),8192 ),out =buf5 )\n        buf6 =empty_strided_cuda ((64 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_5 ,reinterpret_tensor (buf5 ,(64 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_4 ,(64 ,64 ),(1 ,64 ),0 ),alpha =1 ,beta =1 ,out =buf6 )\n        del primals_5 \n        buf7 =empty_strided_cuda ((1 ,64 ,64 ),(4096 ,64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_mish_silu_3 [grid (4096 )](buf6 ,buf7 ,4096 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n\n        buf8 =extern_kernels .convolution (reinterpret_tensor (buf7 ,(1 ,64 ,8 ,8 ),(0 ,64 ,8 ,1 ),0 ),primals_6 ,stride =(2 ,2 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =True ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf8 ,(1 ,32 ,17 ,17 ),(9248 ,289 ,17 ,1 ))\n        buf9 =empty_strided_cuda ((34 ,),(1 ,),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_arange_mul_4 [grid (34 )](buf9 ,34 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf10 =empty_strided_cuda ((1 ,32 ,34 ,34 ),(37888 ,1184 ,34 ,1 ),torch .float32 )\n        buf12 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,32 ,32 ,32 ),torch .float32 )\n        buf14 =reinterpret_tensor (buf12 ,(1 ,32 ,1 ,1 ,1 ),(32 ,1 ,1 ,1 ,1 ),0 );del buf12 \n        buf11 =empty_strided_cuda ((1 ,32 ,1 ,1 ,1 ),(32 ,1 ,1 ,1 ,1 ),torch .float32 )\n        buf15 =empty_strided_cuda ((1 ,1 ,1 ,32 ,1 ,1 ,1 ),(32 ,32 ,32 ,1 ,32 ,32 ,32 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit__softmax__unsafe_index_convolution_neg_5 [grid (32 )](buf14 ,buf9 ,buf8 ,primals_7 ,buf10 ,buf11 ,buf15 ,32 ,1156 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf8 \n        del primals_7 \n        buf16 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_neg_6 [grid (1 )](buf15 ,buf16 ,1 ,32 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf15 \n        buf17 =empty_strided_cuda ((1 ,1 ,5 ),(5 ,5 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__softmax_neg_7 [grid (5 )](buf10 ,buf11 ,buf14 ,buf16 ,buf17 ,5 ,7399 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf18 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_neg_8 [grid (1 )](buf17 ,buf18 ,1 ,5 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf17 \n        buf19 =empty_strided_cuda ((1 ,36992 ),(36992 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__softmax_neg_9 [grid (36992 )](buf10 ,buf11 ,buf14 ,buf16 ,buf18 ,buf19 ,36992 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf16 \n        del buf18 \n    return (buf19 ,primals_6 ,reinterpret_tensor (primals_1 ,(64 ,64 ),(64 ,1 ),0 ),buf4 ,reinterpret_tensor (buf5 ,(64 ,64 ),(64 ,1 ),0 ),buf6 ,reinterpret_tensor (buf7 ,(1 ,64 ,8 ,8 ),(4096 ,64 ,8 ,1 ),0 ),buf9 ,buf10 ,buf11 ,buf14 ,buf19 ,primals_4 ,reinterpret_tensor (buf1 ,(512 ,8 ,1 ),(8 ,1 ,4096 ),8192 ),reinterpret_tensor (buf2 ,(512 ,8 ,1 ),(8 ,1 ,8 ),0 ),reinterpret_tensor (buf1 ,(512 ,1 ,8 ),(8 ,4096 ,1 ),4096 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,64 ,64 ),(4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((64 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((64 ,32 ,3 ,3 ),(288 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "defdf527-78d5-470b-b5c3-648ee715258e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Dropout1d', 'GELU', 'ConstantPad1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.dropout1 = nn.Dropout1d(p=0.5)\n        self.gelu = nn.GELU()\n        self.pad1 = nn.ConstantPad1d(padding=2, value=0)\n        self.dropout2 = nn.Dropout1d(p=0.5)\n        self.pad2 = nn.ConstantPad1d(padding=1, value=0)\n        self.dropout3 = nn.Dropout1d(p=0.5)\n\n    def forward(self, x):\n        # Ensure the input is at least 1D\n        if x.dim() == 0:\n            x = x.unsqueeze(0)\n        \n        # Reshape to 1D if necessary\n        if x.dim() > 1:\n            x = x.view(x.size(0), -1)\n        \n        x = self.dropout1(x)\n        x = self.gelu(x)\n        x = self.pad1(x)\n        x = self.dropout2(x)\n        x = self.pad2(x)\n        x = self.dropout3(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10).cuda()  # Example input with batch size 1 and 10 features\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp39 =tl .load (in_ptr3 +(0 ))\n    tmp40 =tl .broadcast_to (tmp39 ,[XBLOCK ])\n    tmp0 =(-1 )+x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =4 +ks0 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =(-3 )+x0 \n    tmp7 =tl .full ([1 ],0 ,tl .int64 )\n    tmp8 =tmp6 >=tmp7 \n    tmp9 =tl .broadcast_to (ks0 ,[XBLOCK ])\n    tmp10 =tmp6 <tmp9 \n    tmp11 =tmp8 &tmp10 \n    tmp12 =tmp11 &tmp5 \n    tmp13 =tl .load (in_ptr0 +((-3 )+x0 ),tmp12 &xmask ,other =0.0 )\n    tmp14 =tl .load (in_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp12 ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =0.5 \n    tmp16 =tmp14 <tmp15 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =2.0 \n    tmp19 =tmp17 *tmp18 \n    tmp20 =tmp13 *tmp19 \n    tmp21 =tmp20 *tmp15 \n    tmp22 =0.7071067811865476 \n    tmp23 =tmp20 *tmp22 \n    tmp24 =libdevice .erf (tmp23 )\n    tmp25 =1.0 \n    tmp26 =tmp24 +tmp25 \n    tmp27 =tmp21 *tmp26 \n    tmp28 =tl .full (tmp27 .shape ,0.0 ,tmp27 .dtype )\n    tmp29 =tl .where (tmp12 ,tmp27 ,tmp28 )\n    tmp30 =tl .load (in_ptr2 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp5 ,eviction_policy ='evict_last',other =0.0 )\n    tmp31 =0.5 \n    tmp32 =tmp30 <tmp31 \n    tmp33 =tmp32 .to (tl .float32 )\n    tmp34 =2.0 \n    tmp35 =tmp33 *tmp34 \n    tmp36 =tmp29 *tmp35 \n    tmp37 =tl .full (tmp36 .shape ,0.0 ,tmp36 .dtype )\n    tmp38 =tl .where (tmp5 ,tmp36 ,tmp37 )\n    tmp41 =0.5 \n    tmp42 =tmp40 <tmp41 \n    tmp43 =tmp42 .to (tl .float32 )\n    tmp44 =2.0 \n    tmp45 =tmp43 *tmp44 \n    tmp46 =tmp38 *tmp45 \n    tl .store (out_ptr0 +(x0 ),tmp46 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg1_1 ,(1 ,s0 ),(s0 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((3 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[3 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (1 )](buf0 ,buf1 ,2 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_1 [grid (1 )](buf0 ,buf2 ,1 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (1 )](buf0 ,buf3 ,2 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf4 =empty_strided_cuda ((1 ,1 ,6 +s0 ),(6 +s0 ,6 +s0 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_bernoulli_div_mul_2_xnumel =6 +s0 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_2 [grid (triton_poi_fused__to_copy_bernoulli_div_mul_2_xnumel )](arg1_1 ,buf1 ,buf2 ,buf3 ,buf4 ,10 ,16 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del arg1_1 \n        del buf1 \n        del buf2 \n        del buf3 \n    return (reinterpret_tensor (buf4 ,(1 ,6 +s0 ),(6 +s0 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =rand_strided ((1 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "df85b835-2943-4ce5-9175-0674f0d1b526",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['UpsamplingNearest2d', 'LogSigmoid', 'ReplicationPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.upsample1 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.upsample2 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.pad1 = nn.ReplicationPad3d(padding=(1, 1, 1, 1, 1, 1))\n        self.pad2 = nn.ReplicationPad3d(padding=(1, 1, 1, 1, 1, 1))\n        self.log_sigmoid = nn.LogSigmoid()\n\n    def forward(self, x):\n        # Assuming input is 4D (batch, channels, height, width)\n        x = self.upsample1(x)\n        x = self.pad1(x.unsqueeze(2)).squeeze(2)  # Add and remove a dimension to use ReplicationPad3d\n        x = self.upsample2(x)\n        x = self.pad2(x.unsqueeze(2)).squeeze(2)  # Add and remove a dimension to use ReplicationPad3d\n        x = self.log_sigmoid(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape (batch, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__unsafe_index_replication_pad3d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =((xindex //ks0 )%6 )\n    x1 =((xindex //ks2 )%ks3 )\n    x0 =(xindex %ks2 )\n    x7 =xindex //ks5 \n    x4 =xindex \n    tmp0 =x2 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.5 \n    tmp3 =tmp1 *tmp2 \n    tmp3 .to (tl .int32 )\n    tmp5 =2.0 \n    tmp6 =ks1 \n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp5 *tmp7 \n    tmp9 =tmp5 +tmp8 \n    tmp10 =tmp9 .to (tl .float64 )\n    tmp11 =tl .full ([1 ],2.0 ,tl .float64 )\n    tmp12 =tmp11 *tmp10 \n    tmp13 =tmp10 /tmp12 \n    tmp14 =tmp13 .to (tl .float32 )\n    tmp15 =x1 \n    tmp16 =tmp15 .to (tl .float32 )\n    tmp17 =tmp16 *tmp14 \n    tmp18 =tmp17 .to (tl .int64 )\n    tmp19 =2 +2 *ks1 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =tmp18 <0 \n    tmp22 =tl .where (tmp21 ,tmp20 ,tmp18 )\n    tmp23 =ks4 \n    tmp24 =tmp23 .to (tl .float32 )\n    tmp25 =tmp5 *tmp24 \n    tmp26 =tmp5 +tmp25 \n    tmp27 =tmp26 .to (tl .float64 )\n    tmp28 =tmp11 *tmp27 \n    tmp29 =tmp27 /tmp28 \n    tmp30 =tmp29 .to (tl .float32 )\n    tmp31 =x0 \n    tmp32 =tmp31 .to (tl .float32 )\n    tmp33 =tmp32 *tmp30 \n    tmp34 =tmp33 .to (tl .int64 )\n    tmp35 =2 +2 *ks4 \n    tmp36 =tmp34 +tmp35 \n    tmp37 =tmp34 <0 \n    tmp38 =tl .where (tmp37 ,tmp36 ,tmp34 )\n    tmp39 =tmp6 .to (tl .float64 )\n    tmp40 =tmp11 *tmp39 \n    tmp41 =tmp39 /tmp40 \n    tmp42 =tmp41 .to (tl .float32 )\n    tmp43 =(((-1 )+2 *ks1 )*(((-1 )+2 *ks1 )<=(((0 )*((0 )>=((-1 )+tmp22 ))+((-1 )+tmp22 )*(((-1 )+tmp22 )>(0 )))))+(((0 )*((0 )>=((-1 )+tmp22 ))+((-1 )+tmp22 )*(((-1 )+tmp22 )>(0 ))))*((((0 )*((0 )>=((-1 )+tmp22 ))+((-1 )+tmp22 )*(((-1 )+tmp22 )>(0 ))))<((-1 )+2 *ks1 )))\n    tmp44 =tmp43 .to (tl .float32 )\n    tmp45 =tmp44 *tmp42 \n    tmp46 =tmp45 .to (tl .int64 )\n    tmp47 =tmp46 +tmp6 \n    tmp48 =tmp46 <0 \n    tmp49 =tl .where (tmp48 ,tmp47 ,tmp46 )\n    tmp50 =tmp23 .to (tl .float64 )\n    tmp51 =tmp11 *tmp50 \n    tmp52 =tmp50 /tmp51 \n    tmp53 =tmp52 .to (tl .float32 )\n    tmp54 =(((-1 )+2 *ks4 )*(((-1 )+2 *ks4 )<=(((0 )*((0 )>=((-1 )+tmp38 ))+((-1 )+tmp38 )*(((-1 )+tmp38 )>(0 )))))+(((0 )*((0 )>=((-1 )+tmp38 ))+((-1 )+tmp38 )*(((-1 )+tmp38 )>(0 ))))*((((0 )*((0 )>=((-1 )+tmp38 ))+((-1 )+tmp38 )*(((-1 )+tmp38 )>(0 ))))<((-1 )+2 *ks4 )))\n    tmp55 =tmp54 .to (tl .float32 )\n    tmp56 =tmp55 *tmp53 \n    tmp57 =tmp56 .to (tl .int64 )\n    tmp58 =tmp57 +tmp23 \n    tmp59 =tmp57 <0 \n    tmp60 =tl .where (tmp59 ,tmp58 ,tmp57 )\n    tmp61 =tl .load (in_ptr0 +(tmp60 +ks4 *tmp49 +ks1 *ks4 *x7 ),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x4 ),tmp61 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        16 +16 *s1 +16 *s2 +16 *s1 *s2 \n        4 +4 *s2 \n        4 +4 *s1 \n        96 +96 *s1 +96 *s2 +96 *s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,6 ,4 +4 *s1 ,4 +4 *s2 ),(96 *s0 +96 *s0 *s1 +96 *s0 *s2 +96 *s0 *s1 *s2 ,96 +96 *s1 +96 *s2 +96 *s1 *s2 ,16 +16 *s1 +16 *s2 +16 *s1 *s2 ,4 +4 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused__unsafe_index_replication_pad3d_0_xnumel =96 *s0 +96 *s0 *s1 +96 *s0 *s2 +96 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__unsafe_index_replication_pad3d_0 [grid (triton_poi_fused__unsafe_index_replication_pad3d_0_xnumel )](arg3_1 ,buf0 ,17424 ,32 ,132 ,132 ,32 ,104544 ,313632 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (reinterpret_tensor (buf0 ,(1 ,s0 ,1 ,6 ,4 +4 *s1 ,4 +4 *s2 ),(96 *s0 +96 *s0 *s1 +96 *s0 *s2 +96 *s0 *s1 *s2 ,96 +96 *s1 +96 *s2 +96 *s1 *s2 ,96 +96 *s1 +96 *s2 +96 *s1 *s2 ,16 +16 *s1 +16 *s2 +16 *s1 *s2 ,4 +4 *s2 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e0cdaa16-1c4f-41fb-86b5-44e7671e1259",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ReplicationPad2d', 'GELU', 'CosineSimilarity', 'PoissonNLLLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ReplicationPad2d(2)\n        self.gelu = nn.GELU()\n        self.cosine_sim = nn.CosineSimilarity(dim=1)\n        self.loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Apply ReplicationPad2d\n        x = self.pad(x)\n        \n        # Apply GELU activation\n        x = self.gelu(x)\n        \n        # Reshape x to have two tensors for CosineSimilarity\n        # Assuming the input is of shape (batch_size, channels, height, width)\n        batch_size, channels, height, width = x.shape\n        x1 = x[:, :channels//2, :, :]\n        x2 = x[:, channels//2:, :, :]\n        \n        # Apply CosineSimilarity\n        x = self.cosine_sim(x1, x2)\n        \n        # Reshape x to match the expected input shape for PoissonNLLLoss\n        x = x.view(batch_size, -1)\n        \n        # Generate a random target tensor for PoissonNLLLoss\n        target = torch.randint(0, 10, (batch_size, x.size(1)), device=x.device).float()\n        \n        # Apply PoissonNLLLoss\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 4, 32, 32).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_linalg_vector_norm_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =tl .load (in_ptr0 +(ks2 *(((-1 )+ks1 )*(((-1 )+ks1 )<=(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 )))))+(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))*((((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))<((-1 )+ks1 )))+ks1 *ks2 *r0_2 +(((-1 )+ks2 )*(((-1 )+ks2 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks2 )))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =0.5 \n        tmp2 =tmp0 *tmp1 \n        tmp3 =0.7071067811865476 \n        tmp4 =tmp0 *tmp3 \n        tmp5 =libdevice .erf (tmp4 )\n        tmp6 =1.0 \n        tmp7 =tmp5 +tmp6 \n        tmp8 =tmp2 *tmp7 \n        tmp9 =tmp8 *tmp8 \n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =_tmp11 +tmp10 \n        _tmp11 =tl .where (r0_mask &xmask ,tmp12 ,_tmp11 )\n    tmp11 =tl .sum (_tmp11 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_linalg_vector_norm_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =tl .load (in_ptr0 +(ks3 *(((-1 )+ks2 )*(((-1 )+ks2 )<=(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 )))))+(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))*((((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))<((-1 )+ks2 )))+ks2 *ks3 *r0_2 +ks2 *ks3 *(ks1 //2 )+(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks3 )))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =0.5 \n        tmp2 =tmp0 *tmp1 \n        tmp3 =0.7071067811865476 \n        tmp4 =tmp0 *tmp3 \n        tmp5 =libdevice .erf (tmp4 )\n        tmp6 =1.0 \n        tmp7 =tmp5 +tmp6 \n        tmp8 =tmp2 *tmp7 \n        tmp9 =tmp8 *tmp8 \n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =_tmp11 +tmp10 \n        _tmp11 =tl .where (r0_mask &xmask ,tmp12 ,_tmp11 )\n    tmp11 =tl .sum (_tmp11 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_clamp_min_div_linalg_vector_norm_mul_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =(xindex %ks2 )\n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks4 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 )))))+(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))*((((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *x2 +(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp9 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp14 =tl .load (in_ptr0 +(ks4 *(((-1 )+ks3 )*(((-1 )+ks3 )<=(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 )))))+(((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))*((((0 )*((0 )>=((-2 )+x1 ))+((-2 )+x1 )*(((-2 )+x1 )>(0 ))))<((-1 )+ks3 )))+ks3 *ks4 *x2 +ks3 *ks4 *(ks5 //2 )+(((-1 )+ks4 )*(((-1 )+ks4 )<=(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 )))))+(((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))*((((0 )*((0 )>=((-2 )+x0 ))+((-2 )+x0 )*(((-2 )+x0 )>(0 ))))<((-1 )+ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp20 =tl .load (in_ptr2 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.5 \n    tmp2 =tmp0 *tmp1 \n    tmp3 =0.7071067811865476 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .erf (tmp4 )\n    tmp6 =1.0 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =tmp2 *tmp7 \n    tmp10 =libdevice .sqrt (tmp9 )\n    tmp11 =1e-08 \n    tmp12 =triton_helpers .maximum (tmp10 ,tmp11 )\n    tmp13 =tmp8 /tmp12 \n    tmp15 =tmp14 *tmp1 \n    tmp16 =tmp14 *tmp3 \n    tmp17 =libdevice .erf (tmp16 )\n    tmp18 =tmp17 +tmp6 \n    tmp19 =tmp15 *tmp18 \n    tmp21 =libdevice .sqrt (tmp20 )\n    tmp22 =triton_helpers .maximum (tmp21 ,tmp11 )\n    tmp23 =tmp19 /tmp22 \n    tmp24 =tmp13 *tmp23 \n    tl .store (out_ptr0 +(x4 ),tmp24 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_sum_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp2 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(x0 +16 *r0_1 +4 *ks0 *r0_1 +4 *ks1 *r0_1 +ks0 *ks1 *r0_1 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp3 =_tmp2 +tmp1 \n        _tmp2 =tl .where (r0_mask &xmask ,tmp3 ,_tmp2 )\n    tmp2 =tl .sum (_tmp2 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__to_copy_exp_mean_mul_randint_sub_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,load_seed_offset ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =tl_math .exp (tmp0 )\n        tmp2 =tl .load (in_ptr1 +load_seed_offset )\n        tmp3 =r0_0 \n        tmp4 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp5 =tl .full ([1 ,1 ],10 ,tl .int64 )\n        tmp6 =triton_helpers .randint64 (tmp2 ,(tmp3 ).to (tl .uint32 ),tmp4 ,tmp5 )\n        tmp7 =tmp6 .to (tl .float32 )\n        tmp8 =tmp7 *tmp0 \n        tmp9 =tmp1 -tmp8 \n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =_tmp11 +tmp10 \n        _tmp11 =tl .where (r0_mask ,tmp12 ,_tmp11 )\n    tmp11 =tl .sum (_tmp11 ,1 )[:,None ]\n    tmp13 =16 +4 *ks1 +4 *ks2 +ks1 *ks2 \n    tmp14 =tmp13 .to (tl .float32 )\n    tmp15 =tmp11 /tmp14 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp15 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        buf0 =empty_strided_cuda ((1 ,1 ,4 +s1 ,4 +s2 ),(16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_red_fused_linalg_vector_norm_0_xnumel =16 +4 *s1 +4 *s2 +s1 *s2 \n        s0 //2 \n        get_raw_stream (0 )\n        triton_red_fused_linalg_vector_norm_0 [grid (triton_red_fused_linalg_vector_norm_0_xnumel )](arg3_1 ,buf0 ,36 ,32 ,32 ,1296 ,2 ,XBLOCK =64 ,R0_BLOCK =2 ,num_warps =8 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((1 ,1 ,4 +s1 ,4 +s2 ),(16 +4 *s1 +4 *s2 +s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_red_fused_linalg_vector_norm_1_xnumel =16 +4 *s1 +4 *s2 +s1 *s2 \n        s0 +((-1 )*(s0 //2 ))\n        get_raw_stream (0 )\n        triton_red_fused_linalg_vector_norm_1 [grid (triton_red_fused_linalg_vector_norm_1_xnumel )](arg3_1 ,buf1 ,36 ,4 ,32 ,32 ,1296 ,2 ,XBLOCK =64 ,R0_BLOCK =2 ,num_warps =2 ,num_stages =1 )\n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf2 =empty_strided_cuda ((1 ,s0 //2 ,4 +s1 ,4 +s2 ),(16 *(s0 //2 )+4 *s1 *(s0 //2 )+4 *s2 *(s0 //2 )+s1 *s2 *(s0 //2 ),16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_clamp_min_div_linalg_vector_norm_mul_2_xnumel =16 *(s0 //2 )+4 *s1 *(s0 //2 )+4 *s2 *(s0 //2 )+s1 *s2 *(s0 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_clamp_min_div_linalg_vector_norm_mul_2 [grid (triton_poi_fused_clamp_min_div_linalg_vector_norm_mul_2_xnumel )](arg3_1 ,buf0 ,buf1 ,buf2 ,36 ,36 ,1296 ,32 ,32 ,4 ,2592 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf0 \n        buf3 =reinterpret_tensor (buf1 ,(1 ,4 +s1 ,4 +s2 ),(16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),0 );del buf1 \n\n        triton_red_fused_sum_3_xnumel =16 +4 *s1 +4 *s2 +s1 *s2 \n        s0 //2 \n        get_raw_stream (0 )\n        triton_red_fused_sum_3 [grid (triton_red_fused_sum_3_xnumel )](buf2 ,buf3 ,32 ,32 ,1296 ,2 ,XBLOCK =256 ,R0_BLOCK =2 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        buf4 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf4 )\n        buf5 =empty_strided_cuda ((),(),torch .float32 )\n        buf6 =buf5 ;del buf5 \n\n        16 +4 *s1 +4 *s2 +s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused__to_copy_exp_mean_mul_randint_sub_4 [grid (1 )](buf6 ,buf3 ,buf4 ,0 ,32 ,32 ,1 ,1296 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf3 \n        del buf4 \n    return (buf6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =4 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,4 ,32 ,32 ),(4096 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e0ddd61a-99ef-4cff-a7b8-afe778ae0db4",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LeakyReLU', 'CircularPad1d', 'LSTM', 'AdaptiveMaxPool2d', 'InstanceNorm2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.circular_pad1d = nn.CircularPad1d(2)\n        self.lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.instance_norm2d = nn.InstanceNorm2d(num_features=20)\n        self.adaptive_max_pool2d = nn.AdaptiveMaxPool2d((5, 5))\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.1)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        # First, apply InstanceNorm2d\n        x = self.instance_norm2d(x)\n        \n        # Reshape for LSTM: (batch_size, channels, height, width) -> (batch_size, height, channels * width)\n        batch_size, channels, height, width = x.size()\n        x = x.view(batch_size, height, -1)\n        \n        # Apply CircularPad1d\n        x = self.circular_pad1d(x)\n        \n        # Apply LSTM\n        x, _ = self.lstm(x)\n        \n        # Reshape back to (batch_size, channels, height, width)\n        x = x.view(batch_size, -1, height, width)\n        \n        # Apply AdaptiveMaxPool2d\n        x = self.adaptive_max_pool2d(x)\n        \n        # Apply LeakyReLU\n        x = self.leaky_relu(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_copy_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =x0 \n    tmp1 =2 +ks1 *ks2 \n    tmp2 =tmp0 >=tmp1 \n    tmp3 =x0 +((-1 )*ks1 *ks2 )\n    tmp4 =tl .full ([1 ],2 ,tl .int64 )\n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp5 &tmp2 \n    tmp7 =x0 \n    tmp8 =tl .full ([1 ],2 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tl .broadcast_to (2 +ks1 *ks2 ,[XBLOCK ])\n    tmp11 =tmp7 <tmp10 \n    tmp12 =tmp9 &tmp11 \n    tmp13 =tmp12 &tmp6 \n    tmp14 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *ks2 *x1 ),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =float (\"nan\")\n    tmp16 =tl .where (tmp12 ,tmp14 ,tmp15 )\n    tmp17 =tl .full (tmp16 .shape ,0.0 ,tmp16 .dtype )\n    tmp18 =tl .where (tmp6 ,tmp16 ,tmp17 )\n    tmp19 =tmp3 >=tmp4 \n    tmp20 =tl .broadcast_to (2 +ks1 *ks2 ,[XBLOCK ])\n    tmp21 =tmp3 <tmp20 \n    tmp22 =tmp19 &tmp21 \n    tmp23 =tmp22 &tmp2 \n    tmp24 =tl .load (in_ptr0 +((-2 )+x0 +((-1 )*ks1 *ks2 )+ks1 *ks2 *x1 ),tmp23 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp25 =float (\"nan\")\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tl .where (tmp5 ,tmp18 ,tmp26 )\n    tmp28 =tl .full (tmp27 .shape ,0.0 ,tmp27 .dtype )\n    tmp29 =tl .where (tmp2 ,tmp27 ,tmp28 )\n    tmp30 =tl .full ([1 ],2 ,tl .int64 )\n    tmp31 =tmp0 <tmp30 \n    tmp32 =x0 +ks1 *ks2 \n    tmp33 =tl .full ([1 ],2 ,tl .int64 )\n    tmp34 =tmp32 >=tmp33 \n    tmp35 =tl .broadcast_to (2 +ks1 *ks2 ,[XBLOCK ])\n    tmp36 =tmp32 <tmp35 \n    tmp37 =tmp34 &tmp36 \n    tmp38 =tmp37 &tmp31 \n    tmp39 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *ks2 +ks1 *ks2 *x1 ),tmp38 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp40 =float (\"nan\")\n    tmp41 =tl .where (tmp37 ,tmp39 ,tmp40 )\n    tmp42 =tl .full (tmp41 .shape ,0.0 ,tmp41 .dtype )\n    tmp43 =tl .where (tmp31 ,tmp41 ,tmp42 )\n    tmp44 =tmp0 >=tmp30 \n    tmp45 =tmp0 <tmp1 \n    tmp46 =tmp44 &tmp45 \n    tmp47 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *ks2 *x1 ),tmp46 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp48 =float (\"nan\")\n    tmp49 =tl .where (tmp46 ,tmp47 ,tmp48 )\n    tmp50 =tl .where (tmp31 ,tmp43 ,tmp49 )\n    tmp51 =tl .where (tmp2 ,tmp29 ,tmp50 )\n    tl .store (out_ptr0 +(x2 ),tmp51 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s0 *s2 \n        buf1 =empty_strided_cuda ((1 ,s1 ,4 +s0 *s2 ),(4 *s1 +s0 *s1 *s2 ,4 +s0 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_copy_0_xnumel =4 *s1 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_copy_0 [grid (triton_poi_fused_copy_0_xnumel )](arg3_1 ,buf1 ,324 ,10 ,32 ,10368 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf1 ,s1 ,s2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,10 ,32 ,32 ),(10240 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e0ef221f-e4ba-4ce2-96cb-560ef6553bb2",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['SmoothL1Loss', 'CrossMapLRN2d', 'GELU', 'AdaptiveAvgPool1d', 'ReflectionPad2d', 'TransformerEncoder']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reflection_pad = nn.ReflectionPad2d(2)\n        self.cross_map_lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)\n        self.gelu = nn.GELU()\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool1d(output_size=128)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=128, nhead=8), num_layers=3\n        )\n        self.smooth_l1_loss = nn.SmoothL1Loss()\n\n    def forward(self, x):\n        # Apply ReflectionPad2d\n        x = self.reflection_pad(x)\n        \n        # Apply CrossMapLRN2d\n        x = self.cross_map_lrn(x)\n        \n        # Apply GELU activation\n        x = self.gelu(x)\n        \n        # Reshape for AdaptiveAvgPool1d\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions\n        x = self.adaptive_avg_pool(x)\n        \n        # Apply TransformerEncoder\n        x = x.permute(2, 0, 1)  # Transformer expects (seq_len, batch_size, d_model)\n        x = self.transformer_encoder(x)\n        x = x.permute(1, 2, 0)  # Revert back to (batch_size, d_model, seq_len)\n        \n        # Compute SmoothL1Loss (assuming target is a tensor of zeros for demonstration)\n        target = torch.zeros_like(x)\n        loss = self.smooth_l1_loss(x, target)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_reflection_pad2d_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks4 *(tl .where ((-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+x1 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+x1 )))+2 *ks3 ,(-1 )+ks3 +((-1 )*tl_math .abs (1 +((-1 )*ks3 )+tl_math .abs ((-2 )+x1 )))))+ks3 *ks4 *x2 +(tl .where ((-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+x0 )))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+x0 )))+2 *ks4 ,(-1 )+ks4 +((-1 )*tl_math .abs (1 +((-1 )*ks4 )+tl_math .abs ((-2 )+x0 )))))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x3 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        4 +s1 \n        16 +4 *s1 +4 *s2 +s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ,4 +s2 ),(16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 ,16 +4 *s1 +4 *s2 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_reflection_pad2d_0_xnumel =16 *s0 +4 *s0 *s1 +4 *s0 *s2 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_reflection_pad2d_0 [grid (triton_poi_fused_reflection_pad2d_0_xnumel )](arg3_1 ,buf0 ,36 ,36 ,1296 ,32 ,32 ,3888 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n    return (buf0 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e277e770-e661-4352-8cdf-af2b120b065f",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AvgPool1d', 'TripletMarginLoss', 'Flatten', 'LocalResponseNorm']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.avgpool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n        self.avgpool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n        self.local_response_norm = nn.LocalResponseNorm(size=5)\n        self.flatten = nn.Flatten()\n        self.triplet_margin_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, channels, length)\n        x = self.avgpool1(x)  # Apply first average pooling\n        x = self.local_response_norm(x)  # Apply local response normalization\n        x = self.avgpool2(x)  # Apply second average pooling\n        x = self.flatten(x)  # Flatten the output\n        \n        # For TripletMarginLoss, we need three inputs: anchor, positive, and negative\n        # Here, we use the same input for all three, but in practice, they should be different\n        anchor = x\n        positive = x\n        negative = x\n        \n        # Compute the triplet margin loss\n        loss = self.triplet_margin_loss(anchor, positive, negative)\n        \n        # Return the loss as the output (this is unusual for a model, but it's just an example)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(10, 3, 64).cuda()  # Example input: (batch_size=10, channels=3, length=64)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //ks0 )%ks1 )\n    x0 =(xindex %ks0 )\n    x2 =xindex //ks3 \n    x3 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks2 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +(((-2 )*ks4 )+2 *x0 +ks4 *x1 +ks2 *ks4 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tl .load (in_ptr0 +(1 +((-2 )*ks4 )+2 *x0 +ks4 *x1 +ks2 *ks4 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =tmp7 +tmp6 \n    tmp9 =0.5 \n    tmp10 =tmp8 *tmp9 \n    tmp11 =tmp10 *tmp10 \n    tmp12 =tl .full (tmp11 .shape ,0.0 ,tmp11 .dtype )\n    tmp13 =tl .where (tmp5 ,tmp11 ,tmp12 )\n    tl .store (out_ptr0 +(x3 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_div_mul_pow_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x3 =xindex //ks0 \n    x2 =xindex //ks2 \n    x4 =(xindex %ks2 )\n    x5 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +ks1 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +ks1 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr1 +(x4 +4 *ks0 *x2 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp6 =tl .load (in_ptr1 +(ks0 +x4 +4 *ks0 *x2 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr1 +(x4 +2 *ks0 +4 *ks0 *x2 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr1 +(x4 +3 *ks0 +4 *ks0 *x2 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr1 +(x4 +4 *ks0 +4 *ks0 *x2 +ks0 *ks3 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp3 =0.5 \n    tmp4 =tmp2 *tmp3 \n    tmp7 =tmp6 +tmp5 \n    tmp9 =tmp8 +tmp7 \n    tmp11 =tmp10 +tmp9 \n    tmp13 =tmp12 +tmp11 \n    tmp14 =0.2 \n    tmp15 =tmp13 *tmp14 \n    tmp16 =0.0001 \n    tmp17 =tmp15 *tmp16 \n    tmp18 =1.0 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =0.75 \n    tmp21 =libdevice .pow (tmp19 ,tmp20 )\n    tmp22 =tmp4 /tmp21 \n    tl .store (out_ptr0 +(x5 ),tmp22 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_norm_sub_2 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp10 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(2 *((r0_1 %(ks2 //4 )))+ks0 *(triton_helpers .div_floor_integer (r0_1 ,ks2 //4 ))+ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +(1 +2 *((r0_1 %(ks2 //4 )))+ks0 *(triton_helpers .div_floor_integer (r0_1 ,ks2 //4 ))+ks0 *ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp1 +tmp0 \n        tmp3 =0.5 \n        tmp4 =tmp2 *tmp3 \n        tmp5 =tmp4 -tmp4 \n        tmp6 =1e-06 \n        tmp7 =tmp5 +tmp6 \n        tmp8 =tmp7 *tmp7 \n        tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n        tmp11 =_tmp10 +tmp9 \n        _tmp10 =tl .where (r0_mask &xmask ,tmp11 ,_tmp10 )\n    tmp10 =tl .sum (_tmp10 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_mean_norm_sub_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp10 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =libdevice .sqrt (tmp0 )\n        tmp2 =1.0 \n        tmp3 =tmp1 +tmp2 \n        tmp5 =libdevice .sqrt (tmp4 )\n        tmp6 =tmp3 -tmp5 \n        tmp7 =0.0 \n        tmp8 =triton_helpers .maximum (tmp6 ,tmp7 )\n        tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ,R0_BLOCK ])\n        tmp11 =_tmp10 +tmp9 \n        _tmp10 =tl .where (r0_mask ,tmp11 ,_tmp10 )\n    tmp10 =tl .sum (_tmp10 ,1 )[:,None ]\n    tmp12 =ks0 \n    tmp13 =tmp12 .to (tl .float32 )\n    tmp14 =tmp10 /tmp13 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp14 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(s0 ,s1 ,s2 ),(s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s2 //2 \n        4 +s1 \n        4 *(s2 //2 )+s1 *(s2 //2 )\n        buf0 =empty_strided_cuda ((s0 ,1 ,4 +s1 ,s2 //2 ),(4 *(s2 //2 )+s1 *(s2 //2 ),4 *s0 *(s2 //2 )+s0 *s1 *(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =4 *s0 *(s2 //2 )+s0 *s1 *(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg3_1 ,buf0 ,32 ,7 ,3 ,224 ,64 ,2240 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        s1 *(s2 //2 )\n        buf1 =empty_strided_cuda ((s0 ,s1 ,s2 //2 ),(s1 *(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_add_div_mul_pow_1_xnumel =s0 *s1 *(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_add_div_mul_pow_1 [grid (triton_poi_fused_add_div_mul_pow_1_xnumel )](arg3_1 ,buf0 ,buf1 ,32 ,64 ,96 ,3 ,960 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf0 \n        buf2 =empty_strided_cuda ((s0 ,),(1 ,),torch .float32 )\n        buf3 =empty_strided_cuda ((s0 ,),(1 ,),torch .float32 )\n\n        s1 *(s2 //4 )\n        get_raw_stream (0 )\n        triton_red_fused_add_norm_sub_2 [grid (s0 )](buf1 ,buf2 ,buf3 ,32 ,3 ,64 ,10 ,48 ,XBLOCK =16 ,R0_BLOCK =8 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf5 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_mean_norm_sub_3 [grid (1 )](buf5 ,buf2 ,buf3 ,10 ,1 ,10 ,XBLOCK =1 ,R0_BLOCK =16 ,num_warps =2 ,num_stages =1 )\n        del buf2 \n        del buf3 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =3 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((10 ,3 ,64 ),(192 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e3c1ff50-c2bd-4af2-9a38-0708c0b32675",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ModuleDict', 'L1Loss', 'Tanh', 'AdaptiveAvgPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.module_dict = nn.ModuleDict({\n            'tanh1': nn.Tanh(),\n            'tanh2': nn.Tanh(),\n            'adaptive_avg_pool1d': nn.AdaptiveAvgPool1d(output_size=10),\n        })\n        self.l1_loss = nn.L1Loss()\n\n    def forward(self, x):\n        # Apply the first Tanh activation\n        x = self.module_dict['tanh1'](x)\n        \n        # Reshape the input to fit the AdaptiveAvgPool1d layer\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, sequence_length)\n        \n        # Apply AdaptiveAvgPool1d\n        x = self.module_dict['adaptive_avg_pool1d'](x)\n        \n        # Apply the second Tanh activation\n        x = self.module_dict['tanh2'](x)\n        \n        # Compute L1 loss with a dummy target (for demonstration purposes)\n        dummy_target = torch.zeros_like(x)\n        loss = self.l1_loss(x, dummy_target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with arbitrary shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_tanh_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =libdevice .tanh (tmp0 )\n    tl .store (out_ptr0 +(x0 ),tmp1 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_abs_mean_sub_1 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp4 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =libdevice .tanh (tmp0 )\n        tmp2 =tl_math .abs (tmp1 )\n        tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n        tmp5 =_tmp4 +tmp3 \n        _tmp4 =tl .where (r0_mask ,tmp5 ,_tmp4 )\n    tmp4 =tl .sum (_tmp4 ,1 )[:,None ]\n    tmp6 =10 *ks0 \n    tmp7 =tmp6 .to (tl .float32 )\n    tmp8 =tmp4 /tmp7 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp8 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_tanh_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_tanh_0 [grid (triton_poi_fused_tanh_0_xnumel )](arg3_1 ,buf0 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n\n        buf1 =torch .ops .aten ._adaptive_avg_pool2d .default (reinterpret_tensor (buf0 ,(1 ,s0 ,1 ,s1 *s2 ),(0 ,s1 *s2 ,0 ,1 ),0 ),[1 ,10 ])\n        del buf0 \n        buf2 =buf1 \n        del buf1 \n        buf3 =empty_strided_cuda ((),(),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        10 *s0 \n        get_raw_stream (0 )\n        triton_red_fused_abs_mean_sub_1 [grid (1 )](buf4 ,buf2 ,3 ,1 ,30 ,XBLOCK =1 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n        del buf2 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e52738cf-e1e4-4076-b092-97046a2c0061",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GRU', 'Softmax2d', 'Conv1d', 'Dropout2d', 'Fold', 'NLLLoss2d', 'BatchNorm2d', 'Dropout', 'LazyBatchNorm1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1d = nn.Conv1d(1, 10, kernel_size=5)\n        self.lazy_batch_norm1d = nn.LazyBatchNorm1d()\n        self.gru = nn.GRU(10, 20, batch_first=True)\n        self.dropout = nn.Dropout(0.5)\n        self.batch_norm2d = nn.BatchNorm2d(20)\n        self.dropout2d = nn.Dropout2d(0.5)\n        self.fold = nn.Fold(output_size=(10, 10), kernel_size=(2, 2))\n        self.softmax2d = nn.Softmax2d()\n        self.nll_loss2d = nn.NLLLoss2d()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, sequence_length)\n        x = self.conv1d(x)  # Shape: (batch_size, 10, sequence_length - kernel_size + 1)\n        x = self.lazy_batch_norm1d(x)  # Shape: (batch_size, 10, sequence_length - kernel_size + 1)\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, sequence_length - kernel_size + 1, 10)\n        x, _ = self.gru(x)  # Shape: (batch_size, sequence_length - kernel_size + 1, 20)\n        x = self.dropout(x)  # Shape: (batch_size, sequence_length - kernel_size + 1, 20)\n        x = x.unsqueeze(1)  # Shape: (batch_size, 1, sequence_length - kernel_size + 1, 20)\n        x = self.batch_norm2d(x)  # Shape: (batch_size, 1, sequence_length - kernel_size + 1, 20)\n        x = self.dropout2d(x)  # Shape: (batch_size, 1, sequence_length - kernel_size + 1, 20)\n        x = x.permute(0, 3, 1, 2)  # Shape: (batch_size, 20, 1, sequence_length - kernel_size + 1)\n        x = self.fold(x)  # Shape: (batch_size, 20, 10, 10)\n        x = self.softmax2d(x)  # Shape: (batch_size, 20, 10, 10)\n        # Assuming target is provided externally for NLLLoss2d\n        target = torch.randint(0, 20, (x.size(0), 10, 10)).long().cuda()\n        loss = self.nll_loss2d(x, target)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 100).cuda()  # Example input shape: (batch_size, channels, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_convolution_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    r0_numel =96 \n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(r0_1 +96 *x0 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp26 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp28 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp34 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp39 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (r0_mask &xmask ,tmp3 ,0 )\n    tmp6 =tl .broadcast_to (tmp3 ,[XBLOCK ,R0_BLOCK ])\n    tmp8 =tl .where (r0_mask &xmask ,tmp6 ,0 )\n    tmp9 =tl .sum (tmp8 ,1 )[:,None ]\n    tmp10 =tl .full ([XBLOCK ,1 ],96 ,tl .int32 )\n    tmp11 =tmp10 .to (tl .float32 )\n    tmp12 =tmp9 /tmp11 \n    tmp13 =tmp3 -tmp12 \n    tmp14 =tmp13 *tmp13 \n    tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp17 =tl .where (r0_mask &xmask ,tmp15 ,0 )\n    tmp18 =tl .sum (tmp17 ,1 )[:,None ]\n    tmp19 =tmp2 -tmp12 \n    tmp20 =96.0 \n    tmp21 =tmp18 /tmp20 \n    tmp22 =1e-05 \n    tmp23 =tmp21 +tmp22 \n    tmp24 =libdevice .rsqrt (tmp23 )\n    tmp25 =tmp19 *tmp24 \n    tmp27 =tmp25 *tmp26 \n    tmp29 =tmp27 +tmp28 \n    tmp30 =1.0105263157894737 \n    tmp31 =tmp21 *tmp30 \n    tmp32 =0.1 \n    tmp33 =tmp31 *tmp32 \n    tmp35 =0.9 \n    tmp36 =tmp34 *tmp35 \n    tmp37 =tmp33 +tmp36 \n    tmp38 =tmp12 *tmp32 \n    tmp40 =tmp39 *tmp35 \n    tmp41 =tmp38 +tmp40 \n    tl .store (in_out_ptr0 +(r0_1 +96 *x0 ),tmp2 ,r0_mask &xmask )\n    tl .store (out_ptr2 +(r0_1 +96 *x0 ),tmp29 ,r0_mask &xmask )\n    tl .store (out_ptr3 +(x0 ),tmp24 ,xmask )\n    tl .store (out_ptr5 +(x0 ),tmp37 ,xmask )\n    tl .store (out_ptr7 +(x0 ),tmp41 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_1 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .full ([1 ],1 ,tl .int64 )\n    tmp3 =tmp1 +tmp2 \n    tl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(10 ,1 ,5 ),(5 ,5 ,1 ))\n    assert_size_stride (primals_2 ,(10 ,),(1 ,))\n    assert_size_stride (primals_3 ,(1 ,1 ,100 ),(100 ,100 ,1 ))\n    assert_size_stride (primals_4 ,(),())\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    assert_size_stride (primals_6 ,(10 ,),(1 ,))\n    assert_size_stride (primals_7 ,(10 ,),(1 ,))\n    assert_size_stride (primals_8 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n\n        buf0 =extern_kernels .convolution (primals_3 ,primals_1 ,stride =(1 ,),padding =(0 ,),dilation =(1 ,),transposed =False ,output_padding =(0 ,),groups =1 ,bias =None )\n        assert_size_stride (buf0 ,(1 ,10 ,96 ),(960 ,96 ,1 ))\n        buf1 =buf0 ;del buf0 \n        buf2 =empty_strided_cuda ((1 ,10 ,1 ),(10 ,1 ,10 ),torch .float32 )\n        buf6 =empty_strided_cuda ((1 ,10 ,96 ),(960 ,96 ,1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_convolution_0 [grid (10 )](buf1 ,primals_2 ,primals_7 ,primals_8 ,primals_6 ,primals_5 ,buf2 ,buf6 ,buf5 ,primals_6 ,primals_5 ,10 ,96 ,XBLOCK =8 ,num_warps =8 ,num_stages =1 )\n        del primals_2 \n        del primals_5 \n        del primals_6 \n        del primals_8 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_1 [grid (1 )](primals_4 ,primals_4 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_4 \n    return (reinterpret_tensor (buf6 ,(1 ,96 ,10 ),(960 ,1 ,96 ),0 ),primals_1 ,primals_3 ,primals_7 ,buf1 ,reinterpret_tensor (buf5 ,(10 ,),(1 ,),0 ),reinterpret_tensor (buf2 ,(1 ,10 ,1 ),(10 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((10 ,1 ,5 ),(5 ,5 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((1 ,1 ,100 ),(100 ,100 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e53bc973-2545-4097-8445-d4c83847aebe",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveMaxPool1d', 'Mish', 'ConstantPad1d', 'CosineSimilarity', 'Sigmoid', 'NLLLoss', 'Softshrink', 'LSTMCell', 'RNNBase']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_max_pool = nn.AdaptiveMaxPool1d(output_size=10)\n        self.mish = nn.Mish()\n        self.constant_pad = nn.ConstantPad1d(padding=2, value=0)\n        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n        self.sigmoid = nn.Sigmoid()\n        self.softshrink = nn.Softshrink(lambd=0.5)\n        self.lstm_cell = nn.LSTMCell(input_size=10, hidden_size=20)\n        self.rnn = nn.RNN(input_size=20, hidden_size=30, num_layers=1, batch_first=True)\n        self.nll_loss = nn.NLLLoss()\n\n    def forward(self, x):\n        # Assuming x is of shape (batch_size, sequence_length, features)\n        x = self.constant_pad(x)  # Apply padding\n        x = self.adaptive_max_pool(x)  # Apply adaptive max pooling\n        x = self.mish(x)  # Apply Mish activation\n        x = self.softshrink(x)  # Apply Softshrink\n        \n        # Reshape for LSTM Cell\n        batch_size, seq_len, features = x.size()\n        x = x.view(batch_size * seq_len, -1)\n        \n        # Initialize hidden and cell states for LSTM Cell\n        hx = torch.zeros(batch_size * seq_len, 20).to(x.device)\n        cx = torch.zeros(batch_size * seq_len, 20).to(x.device)\n        \n        # Apply LSTM Cell\n        hx, cx = self.lstm_cell(x, (hx, cx))\n        x = hx.view(batch_size, seq_len, -1)\n        \n        # Apply RNN\n        _, x = self.rnn(x)\n        x = x.squeeze(0)\n        \n        # Apply Sigmoid\n        x = self.sigmoid(x)\n        \n        # Compute Cosine Similarity with a dummy tensor\n        dummy_tensor = torch.ones_like(x)\n        x = self.cosine_similarity(x, dummy_tensor)\n        \n        # Apply NLLLoss (requires log probabilities and target)\n        # For demonstration, we generate a random target\n        target = torch.randint(0, 30, (batch_size,)).to(x.device)\n        x = x.unsqueeze(0)  # Add batch dimension\n        x = self.nll_loss(x, target)\n        \n        return x\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(5, 20, 10).cuda()  # (batch_size, sequence_length, features)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_adaptive_max_pool2d_gt_mish_mul_sign_sub_where_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1000 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %10 )\n    x1 =xindex //10 \n    x2 =xindex \n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =(7 *x0 )//5 \n    tmp4 =(23 +14 *x0 )//10 \n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp2 &tmp5 \n    tmp7 =(-2 )+((7 *x0 )//5 )\n    tmp8 =tl .full ([1 ],0 ,tl .int64 )\n    tmp9 =tmp7 >=tmp8 \n    tmp10 =tl .full ([1 ],10 ,tl .int64 )\n    tmp11 =tmp7 <tmp10 \n    tmp12 =tmp9 &tmp11 \n    tmp13 =tmp12 &tmp6 \n    tmp14 =tl .load (in_ptr0 +((-2 )+10 *x1 +((7 *x0 )//5 )),tmp13 &xmask ,other =0.0 )\n    tmp15 =tl .full (tmp14 .shape ,float (\"-inf\"),tmp14 .dtype )\n    tmp16 =tl .where (tmp6 ,tmp14 ,tmp15 )\n    tmp17 =1 +((7 *x0 )//5 )\n    tmp18 =tmp17 <tmp4 \n    tmp19 =tmp2 &tmp18 \n    tmp20 =(-1 )+((7 *x0 )//5 )\n    tmp21 =tl .full ([1 ],0 ,tl .int64 )\n    tmp22 =tmp20 >=tmp21 \n    tmp23 =tl .full ([1 ],10 ,tl .int64 )\n    tmp24 =tmp20 <tmp23 \n    tmp25 =tmp22 &tmp24 \n    tmp26 =tmp25 &tmp19 \n    tmp27 =tl .load (in_ptr0 +((-1 )+10 *x1 +((7 *x0 )//5 )),tmp26 &xmask ,other =0.0 )\n    tmp28 =tl .full (tmp27 .shape ,float (\"-inf\"),tmp27 .dtype )\n    tmp29 =tl .where (tmp19 ,tmp27 ,tmp28 )\n    tmp30 =triton_helpers .maximum (tmp29 ,tmp16 )\n    tmp31 =2 +((7 *x0 )//5 )\n    tmp32 =tmp31 <tmp4 \n    tmp33 =tmp2 &tmp32 \n    tmp34 =(7 *x0 )//5 \n    tmp35 =tl .full ([1 ],0 ,tl .int64 )\n    tmp36 =tmp34 >=tmp35 \n    tmp37 =tl .full ([1 ],10 ,tl .int64 )\n    tmp38 =tmp34 <tmp37 \n    tmp39 =tmp36 &tmp38 \n    tmp40 =tmp39 &tmp33 \n    tmp41 =tl .load (in_ptr0 +(10 *x1 +((7 *x0 )//5 )),tmp40 &xmask ,other =0.0 )\n    tmp42 =tl .full (tmp41 .shape ,float (\"-inf\"),tmp41 .dtype )\n    tmp43 =tl .where (tmp33 ,tmp41 ,tmp42 )\n    tmp44 =triton_helpers .maximum (tmp43 ,tmp30 )\n    tmp45 =20.0 \n    tmp46 =tmp44 >tmp45 \n    tmp47 =tl_math .exp (tmp44 )\n    tmp48 =libdevice .log1p (tmp47 )\n    tmp49 =tl .where (tmp46 ,tmp44 ,tmp48 )\n    tmp50 =libdevice .tanh (tmp49 )\n    tmp51 =tmp44 *tmp50 \n    tmp52 =tl_math .abs (tmp51 )\n    tmp53 =0.5 \n    tmp54 =tmp52 >tmp53 \n    tmp55 =tl .full ([1 ],0 ,tl .int32 )\n    tmp56 =tmp55 <tmp51 \n    tmp57 =tmp56 .to (tl .int8 )\n    tmp58 =tmp51 <tmp55 \n    tmp59 =tmp58 .to (tl .int8 )\n    tmp60 =tmp57 -tmp59 \n    tmp61 =tmp60 .to (tmp51 .dtype )\n    tmp62 =tmp61 *tmp53 \n    tmp63 =tmp51 -tmp62 \n    tmp64 =0.0 \n    tmp65 =tmp51 *tmp64 \n    tmp66 =tl .where (tmp54 ,tmp63 ,tmp65 )\n    tl .store (in_out_ptr0 +(x2 ),tmp66 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =2000 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(5 ,20 ,10 ),(200 ,10 ,1 ))\n    assert_size_stride (primals_2 ,(80 ,10 ),(10 ,1 ))\n    assert_size_stride (primals_3 ,(80 ,20 ),(20 ,1 ))\n    assert_size_stride (primals_4 ,(80 ,),(1 ,))\n    assert_size_stride (primals_5 ,(80 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((5 ,20 ,1 ,10 ),(200 ,10 ,10 ,1 ),torch .float32 )\n        buf1 =reinterpret_tensor (buf0 ,(5 ,20 ,10 ),(200 ,10 ,1 ),0 );del buf0 \n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_adaptive_max_pool2d_gt_mish_mul_sign_sub_where_0 [grid (1000 )](buf1 ,primals_1 ,1000 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_1 \n        buf2 =empty_strided_cuda ((100 ,20 ),(20 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_1 [grid (2000 )](buf2 ,2000 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((100 ,80 ),(80 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf1 ,(100 ,10 ),(10 ,1 ),0 ),reinterpret_tensor (primals_2 ,(10 ,80 ),(1 ,10 ),0 ),out =buf3 )\n        del primals_2 \n        buf4 =empty_strided_cuda ((100 ,80 ),(80 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf2 ,reinterpret_tensor (primals_3 ,(20 ,80 ),(1 ,20 ),0 ),out =buf4 )\n        del primals_3 \n\n        buf5 =torch .ops .aten ._thnn_fused_lstm_cell .default (buf3 ,buf4 ,buf2 ,primals_4 ,primals_5 )\n        del buf3 \n        del buf4 \n        del primals_4 \n        del primals_5 \n        buf6 =buf5 [0 ]\n        buf7 =buf5 [1 ]\n        buf8 =buf5 [2 ]\n        del buf5 \n    return (reinterpret_tensor (buf6 ,(5 ,20 ,20 ),(400 ,20 ,1 ),0 ),reinterpret_tensor (buf1 ,(100 ,10 ),(10 ,1 ),0 ),buf2 ,buf7 ,buf8 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((5 ,20 ,10 ),(200 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((80 ,10 ),(10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((80 ,20 ),(20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((80 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((80 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e6308b55-3cbe-492e-a1d9-1b19af7a6277",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Tanhshrink', 'Tanh', 'LazyLinear', 'GroupNorm', 'Container', 'Sigmoid', 'ELU']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lazy_linear1 = nn.LazyLinear(128)\n        self.group_norm1 = nn.GroupNorm(8, 128)\n        self.tanhshrink = nn.Tanhshrink()\n        self.tanh = nn.Tanh()\n        self.lazy_linear2 = nn.LazyLinear(64)\n        self.group_norm2 = nn.GroupNorm(4, 64)\n        self.sigmoid = nn.Sigmoid()\n        self.elu = nn.ELU()\n        self.lazy_linear3 = nn.LazyLinear(32)\n        self.container = nn.Sequential(\n            nn.LazyLinear(16),\n            nn.Tanh(),\n            nn.LazyLinear(10)\n        )\n\n    def forward(self, x):\n        # Flatten the input to fit LazyLinear\n        x = x.view(x.size(0), -1)\n        \n        x = self.lazy_linear1(x)\n        x = self.group_norm1(x)\n        x = self.tanhshrink(x)\n        x = self.tanh(x)\n        \n        x = self.lazy_linear2(x)\n        x = self.group_norm2(x)\n        x = self.sigmoid(x)\n        x = self.elu(x)\n        \n        x = self.lazy_linear3(x)\n        x = self.container(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape: (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_native_group_norm_0 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =8 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +16 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp1 ,0 )\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp6 =tl .where (xmask ,tmp4 ,0 )\n    tmp7 =tl .sum (tmp6 ,1 )[:,None ]\n    tmp8 =tl .full ([XBLOCK ,1 ],16 ,tl .int32 )\n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp7 /tmp9 \n    tmp11 =tmp1 -tmp10 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n    tmp15 =tl .where (xmask ,tmp13 ,0 )\n    tmp16 =tl .sum (tmp15 ,1 )[:,None ]\n    tmp17 =16.0 \n    tmp18 =tmp16 /tmp17 \n    tmp19 =1e-05 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =libdevice .rsqrt (tmp20 )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp21 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_group_norm_sub_tanh_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 //16 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x0 //16 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr3 +(x0 ),xmask )\n    tmp7 =tl .load (in_ptr4 +(x0 ),xmask )\n    tmp2 =tmp0 -tmp1 \n    tmp4 =tmp2 *tmp3 \n    tmp6 =tmp4 *tmp5 \n    tmp8 =tmp6 +tmp7 \n    tmp9 =libdevice .tanh (tmp8 )\n    tmp10 =tmp8 -tmp9 \n    tmp11 =libdevice .tanh (tmp10 )\n    tl .store (in_out_ptr0 +(x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_native_group_norm_2 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =4 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +16 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (xmask ,tmp1 ,0 )\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp6 =tl .where (xmask ,tmp4 ,0 )\n    tmp7 =tl .sum (tmp6 ,1 )[:,None ]\n    tmp8 =tl .full ([XBLOCK ,1 ],16 ,tl .int32 )\n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp7 /tmp9 \n    tmp11 =tmp1 -tmp10 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n    tmp15 =tl .where (xmask ,tmp13 ,0 )\n    tmp16 =tl .sum (tmp15 ,1 )[:,None ]\n    tmp17 =16.0 \n    tmp18 =tmp16 /tmp17 \n    tmp19 =1e-05 \n    tmp20 =tmp18 +tmp19 \n    tmp21 =libdevice .rsqrt (tmp20 )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(x0 ),tmp21 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_elu_native_group_norm_sigmoid_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr1 +(x0 //16 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr2 +(x0 //16 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr3 +(x0 ),xmask )\n    tmp7 =tl .load (in_ptr4 +(x0 ),xmask )\n    tmp2 =tmp0 -tmp1 \n    tmp4 =tmp2 *tmp3 \n    tmp6 =tmp4 *tmp5 \n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .sigmoid (tmp8 )\n    tmp10 =0.0 \n    tmp11 =tmp9 >tmp10 \n    tmp12 =1.0 \n    tmp13 =tmp9 *tmp12 \n    tmp14 =libdevice .expm1 (tmp13 )\n    tmp15 =tmp14 *tmp12 \n    tmp16 =tl .where (tmp11 ,tmp13 ,tmp15 )\n    tl .store (in_out_ptr0 +(x0 ),tmp16 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_addmm_tanh_4 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =libdevice .tanh (tmp2 )\n    tl .store (in_out_ptr0 +(x0 ),tmp3 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\n    assert_size_stride (primals_2 ,(128 ,12288 ),(12288 ,1 ))\n    assert_size_stride (primals_3 ,(128 ,),(1 ,))\n    assert_size_stride (primals_4 ,(128 ,),(1 ,))\n    assert_size_stride (primals_5 ,(128 ,),(1 ,))\n    assert_size_stride (primals_6 ,(64 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_7 ,(64 ,),(1 ,))\n    assert_size_stride (primals_8 ,(64 ,),(1 ,))\n    assert_size_stride (primals_9 ,(64 ,),(1 ,))\n    assert_size_stride (primals_10 ,(32 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_11 ,(32 ,),(1 ,))\n    assert_size_stride (primals_12 ,(16 ,32 ),(32 ,1 ))\n    assert_size_stride (primals_13 ,(16 ,),(1 ,))\n    assert_size_stride (primals_14 ,(10 ,16 ),(16 ,1 ))\n    assert_size_stride (primals_15 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_3 ,reinterpret_tensor (primals_1 ,(1 ,12288 ),(12288 ,1 ),0 ),reinterpret_tensor (primals_2 ,(12288 ,128 ),(1 ,12288 ),0 ),alpha =1 ,beta =1 ,out =buf0 )\n        del primals_2 \n        del primals_3 \n        buf1 =empty_strided_cuda ((1 ,8 ,1 ,1 ),(8 ,1 ,1 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,8 ,1 ,1 ),(8 ,1 ,8 ,8 ),torch .float32 )\n        buf4 =reinterpret_tensor (buf2 ,(1 ,8 ,1 ,1 ),(8 ,1 ,1 ,1 ),0 );del buf2 \n\n        get_raw_stream (0 )\n        triton_per_fused_native_group_norm_0 [grid (8 )](buf4 ,buf0 ,buf1 ,8 ,16 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n        buf6 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_poi_fused_native_group_norm_sub_tanh_1 [grid (128 )](buf6 ,buf0 ,buf1 ,buf4 ,primals_4 ,primals_5 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf7 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_7 ,buf6 ,reinterpret_tensor (primals_6 ,(128 ,64 ),(1 ,128 ),0 ),alpha =1 ,beta =1 ,out =buf7 )\n        del primals_7 \n        buf8 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,1 ,1 ),torch .float32 )\n        buf9 =empty_strided_cuda ((1 ,4 ,1 ,1 ),(4 ,1 ,4 ,4 ),torch .float32 )\n        buf11 =reinterpret_tensor (buf9 ,(1 ,4 ,1 ,1 ),(4 ,1 ,1 ,1 ),0 );del buf9 \n\n        get_raw_stream (0 )\n        triton_per_fused_native_group_norm_2 [grid (4 )](buf11 ,buf7 ,buf8 ,4 ,16 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf12 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n        buf13 =buf12 ;del buf12 \n\n        get_raw_stream (0 )\n        triton_poi_fused_elu_native_group_norm_sigmoid_3 [grid (64 )](buf13 ,buf7 ,buf8 ,buf11 ,primals_8 ,primals_9 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf14 =empty_strided_cuda ((1 ,32 ),(32 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_11 ,buf13 ,reinterpret_tensor (primals_10 ,(64 ,32 ),(1 ,64 ),0 ),alpha =1 ,beta =1 ,out =buf14 )\n        del primals_11 \n        buf15 =empty_strided_cuda ((1 ,16 ),(16 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf14 ,reinterpret_tensor (primals_12 ,(32 ,16 ),(1 ,32 ),0 ),out =buf15 )\n        buf16 =buf15 ;del buf15 \n\n        get_raw_stream (0 )\n        triton_poi_fused_addmm_tanh_4 [grid (16 )](buf16 ,primals_13 ,16 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del primals_13 \n        buf17 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_15 ,buf16 ,reinterpret_tensor (primals_14 ,(16 ,10 ),(1 ,16 ),0 ),alpha =1 ,beta =1 ,out =buf17 )\n        del primals_15 \n    return (buf17 ,primals_4 ,primals_5 ,primals_8 ,primals_9 ,reinterpret_tensor (primals_1 ,(1 ,12288 ),(12288 ,1 ),0 ),buf0 ,buf1 ,buf4 ,buf6 ,buf7 ,buf8 ,buf11 ,buf13 ,buf14 ,buf16 ,primals_14 ,primals_12 ,primals_10 ,primals_6 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((128 ,12288 ),(12288 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((64 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((32 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((16 ,32 ),(32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((16 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((10 ,16 ),(16 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e760ce94-ad07-4cc1-b9a5-3079dc86f7b2",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveAvgPool1d', 'Softmin', 'Hardshrink']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool1d(output_size=10)\n        self.softmin = nn.Softmin(dim=1)\n        self.hardshrink = nn.Hardshrink(lambd=0.5)\n\n    def forward(self, x):\n        # Reshape input to have a single channel for AdaptiveAvgPool1d\n        x = x.view(x.size(0), 1, -1)  # Reshape to (batch_size, 1, sequence_length)\n        x = self.adaptive_avg_pool(x)  # Apply AdaptiveAvgPool1d\n        x = x.view(x.size(0), -1)  # Flatten the output\n        x = self.softmin(x)  # Apply Softmin\n        x = self.hardshrink(x)  # Apply Hardshrink\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 100).cuda()  # Example input with shape (batch_size, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__adaptive_avg_pool2d__softmax_abs_le_neg_scalar_tensor_where_0 (in_out_ptr0 ,in_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp1 =tl .load (in_ptr0 +(1 +10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp3 =tl .load (in_ptr0 +(2 +10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp5 =tl .load (in_ptr0 +(3 +10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tl .load (in_ptr0 +(4 +10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp9 =tl .load (in_ptr0 +(5 +10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp11 =tl .load (in_ptr0 +(6 +10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tl .load (in_ptr0 +(7 +10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =tl .load (in_ptr0 +(8 +10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp17 =tl .load (in_ptr0 +(9 +10 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp8 =tmp7 +tmp6 \n    tmp10 =tmp9 +tmp8 \n    tmp12 =tmp11 +tmp10 \n    tmp14 =tmp13 +tmp12 \n    tmp16 =tmp15 +tmp14 \n    tmp18 =tmp17 +tmp16 \n    tmp19 =0.1 \n    tmp20 =tmp18 *tmp19 \n    tmp21 =-tmp20 \n    tmp22 =tl .broadcast_to (tmp21 ,[XBLOCK ,R0_BLOCK ])\n    tmp24 =tl .where (r0_mask ,tmp22 ,float (\"-inf\"))\n    tmp25 =triton_helpers .max2 (tmp24 ,1 )[:,None ]\n    tmp26 =tmp21 -tmp25 \n    tmp27 =tl_math .exp (tmp26 )\n    tmp28 =tl .broadcast_to (tmp27 ,[XBLOCK ,R0_BLOCK ])\n    tmp30 =tl .where (r0_mask ,tmp28 ,0 )\n    tmp31 =tl .sum (tmp30 ,1 )[:,None ]\n    tmp32 =tmp27 /tmp31 \n    tmp33 =tl_math .abs (tmp32 )\n    tmp34 =0.5 \n    tmp35 =tmp33 <=tmp34 \n    tmp36 =0.0 \n    tmp37 =tl .where (tmp35 ,tmp36 ,tmp32 )\n    tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp37 ,r0_mask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,100 ),(100 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,1 ,10 ),(10 ,10 ,10 ,1 ),torch .float32 )\n        buf3 =reinterpret_tensor (buf0 ,(1 ,10 ),(10 ,1 ),0 );del buf0 \n\n        get_raw_stream (0 )\n        triton_per_fused__adaptive_avg_pool2d__softmax_abs_le_neg_scalar_tensor_where_0 [grid (1 )](buf3 ,arg0_1 ,1 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del arg0_1 \n    return (buf3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,100 ),(100 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e83a8739-a0bf-4fba-ad00-729ad50cc695",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['NLLLoss', 'Softmax2d', 'SyncBatchNorm', 'InstanceNorm1d', 'FractionalMaxPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.sync_bn = nn.SyncBatchNorm(10)\n        self.instance_norm = nn.InstanceNorm1d(10)\n        self.frac_max_pool = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))\n        self.softmax = nn.Softmax2d()\n        self.nll_loss = nn.NLLLoss()\n\n    def forward(self, x):\n        # Assuming input is 4D (batch, channels, height, width)\n        x = self.sync_bn(x)\n        x = self.frac_max_pool(x)\n        x = self.softmax(x)\n        \n        # Reshape for InstanceNorm1d\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1)  # Flatten height and width\n        x = self.instance_norm(x)\n        x = x.view(batch_size, channels, height, width)  # Reshape back\n        \n        # Compute NLLLoss (requires log probabilities and target)\n        # For demonstration, we'll create a dummy target\n        target = torch.randint(0, channels, (batch_size, height, width), device=x.device)\n        x = torch.log(x + 1e-10)  # Log probabilities for NLLLoss\n        loss = self.nll_loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 28, 28).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_0 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr3 ,out_ptr5 ,out_ptr7 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    r0_numel =784 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +784 *x0 ),r0_mask ,other =0.0 )\n    tmp24 =tl .load (in_ptr1 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp26 =tl .load (in_ptr2 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp32 =tl .load (in_ptr3 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr4 +(x0 ),None ,eviction_policy ='evict_last')\n    tmp1 =tl .broadcast_to (tmp0 ,[R0_BLOCK ])\n    tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .broadcast_to (tmp1 ,[R0_BLOCK ])\n    tmp6 =tl .where (r0_mask ,tmp4 ,0 )\n    tmp7 =triton_helpers .promote_to_tensor (tl .sum (tmp6 ,0 ))\n    tmp8 =tl .full ([1 ],784 ,tl .int32 )\n    tmp9 =tmp8 .to (tl .float32 )\n    tmp10 =tmp7 /tmp9 \n    tmp11 =tmp1 -tmp10 \n    tmp12 =tmp11 *tmp11 \n    tmp13 =tl .broadcast_to (tmp12 ,[R0_BLOCK ])\n    tmp15 =tl .where (r0_mask ,tmp13 ,0 )\n    tmp16 =triton_helpers .promote_to_tensor (tl .sum (tmp15 ,0 ))\n    tmp17 =tmp0 -tmp10 \n    tmp18 =784.0 \n    tmp19 =tmp16 /tmp18 \n    tmp20 =1e-05 \n    tmp21 =tmp19 +tmp20 \n    tmp22 =libdevice .rsqrt (tmp21 )\n    tmp23 =tmp17 *tmp22 \n    tmp25 =tmp23 *tmp24 \n    tmp27 =tmp25 +tmp26 \n    tmp28 =1.0012770891189575 \n    tmp29 =tmp19 *tmp28 \n    tmp30 =0.1 \n    tmp31 =tmp29 *tmp30 \n    tmp33 =0.9 \n    tmp34 =tmp32 *tmp33 \n    tmp35 =tmp31 +tmp34 \n    tmp36 =tmp10 *tmp30 \n    tmp38 =tmp37 *tmp33 \n    tmp39 =tmp36 +tmp38 \n    tl .store (out_ptr2 +(r0_1 +784 *x0 ),tmp27 ,r0_mask )\n    tl .store (out_ptr3 +(x0 ),tmp22 ,None )\n    tl .store (out_ptr5 +(x0 ),tmp35 ,None )\n    tl .store (out_ptr7 +(x0 ),tmp39 ,None )\n    tl .store (out_ptr0 +(x0 ),tmp10 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_rand_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =20 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_fractional_max_pool2d_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =1960 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex //196 \n    x1 =((xindex //14 )%14 )\n    x0 =(xindex %14 )\n    x4 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp20 =tl .load (in_ptr0 +(1 +2 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =x1 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =tmp2 +tmp0 \n    tmp4 =2.0 \n    tmp5 =tmp3 *tmp4 \n    tmp6 =libdevice .floor (tmp5 )\n    tmp7 =tmp0 *tmp4 \n    tmp8 =libdevice .floor (tmp7 )\n    tmp9 =tmp6 -tmp8 \n    tmp10 =tmp9 .to (tl .int64 )\n    tmp11 =tl .full ([1 ],13 ,tl .int64 )\n    tmp12 =tmp2 <tmp11 \n    tmp13 =tl .full ([1 ],26 ,tl .int64 )\n    tmp14 =tl .where (tmp12 ,tmp10 ,tmp13 )\n    tmp15 =tl .full ([XBLOCK ],28 ,tl .int32 )\n    tmp16 =tmp14 +tmp15 \n    tmp17 =tmp14 <0 \n    tmp18 =tl .where (tmp17 ,tmp16 ,tmp14 )\n    tl .device_assert (((0 <=tmp18 )&(tmp18 <28 ))|~(xmask ),\"index out of bounds: 0 <= tmp18 < 28\")\n    tmp21 =x0 \n    tmp22 =tmp21 .to (tl .float32 )\n    tmp23 =tmp22 +tmp20 \n    tmp24 =tmp23 *tmp4 \n    tmp25 =libdevice .floor (tmp24 )\n    tmp26 =tmp20 *tmp4 \n    tmp27 =libdevice .floor (tmp26 )\n    tmp28 =tmp25 -tmp27 \n    tmp29 =tmp28 .to (tl .int64 )\n    tmp30 =tmp22 <tmp11 \n    tmp31 =tl .where (tmp30 ,tmp29 ,tmp13 )\n    tmp32 =tmp31 +tmp15 \n    tmp33 =tmp31 <0 \n    tmp34 =tl .where (tmp33 ,tmp32 ,tmp31 )\n    tl .device_assert (((0 <=tmp34 )&(tmp34 <28 ))|~(xmask ),\"index out of bounds: 0 <= tmp34 < 28\")\n    tmp36 =tl .load (in_ptr1 +(tmp34 +28 *tmp18 +784 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp37 =tl .load (in_ptr1 +(1 +tmp34 +28 *tmp18 +784 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp38 =triton_helpers .maximum (tmp37 ,tmp36 )\n    tmp39 =tl .load (in_ptr1 +(28 +tmp34 +28 *tmp18 +784 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp40 =triton_helpers .maximum (tmp39 ,tmp38 )\n    tmp41 =tl .load (in_ptr1 +(29 +tmp34 +28 *tmp18 +784 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp42 =triton_helpers .maximum (tmp41 ,tmp40 )\n    tmp43 =tmp37 >tmp36 \n    tmp44 =libdevice .isnan (tmp37 ).to (tl .int1 )\n    tmp45 =tmp43 |tmp44 \n    tmp46 =1 +tmp34 +28 *tmp18 \n    tmp47 =tmp46 .to (tl .int32 )\n    tmp48 =tmp34 +28 *tmp18 \n    tmp49 =tmp48 .to (tl .int32 )\n    tmp50 =tl .where (tmp45 ,tmp47 ,tmp49 )\n    tmp51 =tmp39 >tmp38 \n    tmp52 =libdevice .isnan (tmp39 ).to (tl .int1 )\n    tmp53 =tmp51 |tmp52 \n    tmp54 =28 +tmp34 +28 *tmp18 \n    tmp55 =tmp54 .to (tl .int32 )\n    tmp56 =tl .where (tmp53 ,tmp55 ,tmp50 )\n    tmp57 =tmp41 >tmp40 \n    tmp58 =libdevice .isnan (tmp41 ).to (tl .int1 )\n    tmp59 =tmp57 |tmp58 \n    tmp60 =29 +tmp34 +28 *tmp18 \n    tmp61 =tmp60 .to (tl .int32 )\n    tmp62 =tl .where (tmp59 ,tmp61 ,tmp56 )\n    tl .store (out_ptr0 +(x4 ),tmp42 ,xmask )\n    tl .store (out_ptr1 +(x4 ),tmp62 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__softmax_3 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =196 \n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +196 *r0_1 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask &xmask ,tmp1 ,float (\"-inf\"))\n    tmp4 =triton_helpers .max2 (tmp3 ,1 )[:,None ]\n    tmp5 =tmp0 -tmp4 \n    tmp6 =tl_math .exp (tmp5 )\n    tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n    tmp9 =tl .where (r0_mask &xmask ,tmp7 ,0 )\n    tmp10 =tl .sum (tmp9 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n    tl .store (out_ptr1 +(x0 ),tmp10 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit__softmax_4 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =10 \n    r0_numel =196 \n    R0_BLOCK :tl .constexpr =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(r0_1 +196 *x0 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr0 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp4 =tl .load (in_ptr1 +(r0_1 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n    tmp2 =tmp0 -tmp1 \n    tmp3 =tl_math .exp (tmp2 )\n    tmp5 =tmp3 /tmp4 \n    tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n    tl .where (r0_mask &xmask ,tmp6 ,0 )\n    tmp9 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n    tmp11 =tl .where (r0_mask &xmask ,tmp9 ,0 )\n    tmp12 =tl .sum (tmp11 ,1 )[:,None ]\n    tmp13 =tl .full ([XBLOCK ,1 ],196 ,tl .int32 )\n    tmp14 =tmp13 .to (tl .float32 )\n    tmp15 =tmp12 /tmp14 \n    tmp16 =tmp6 -tmp15 \n    tmp17 =tmp16 *tmp16 \n    tmp18 =tl .broadcast_to (tmp17 ,[XBLOCK ,R0_BLOCK ])\n    tmp20 =tl .where (r0_mask &xmask ,tmp18 ,0 )\n    tmp21 =tl .sum (tmp20 ,1 )[:,None ]\n    tmp22 =196.0 \n    tmp23 =tmp21 /tmp22 \n    tmp24 =1e-05 \n    tmp25 =tmp23 +tmp24 \n    tmp26 =libdevice .rsqrt (tmp25 )\n    tl .store (in_out_ptr0 +(r0_1 +196 *x0 ),tmp5 ,r0_mask &xmask )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(x0 ),tmp26 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_nll_loss2d_forward_randint_5 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,out_ptr0 ,out_ptr2 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =196 \n    R0_BLOCK :tl .constexpr =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_0 \n    tmp2 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp3 =tl .full ([1 ,1 ],10 ,tl .int64 )\n    tmp4 =triton_helpers .randint64 (tmp0 ,(tmp1 ).to (tl .uint32 ),tmp2 ,tmp3 )\n    tmp5 =tl .full ([1 ,1 ],-100 ,tl .int64 )\n    tmp6 =tmp4 !=tmp5 \n    tmp7 =tmp6 .to (tl .int64 )\n    tmp8 =tl .broadcast_to (tmp7 ,[XBLOCK ,R0_BLOCK ])\n    tmp10 =tl .where (r0_mask ,tmp8 ,0 )\n    tmp11 =tl .sum (tmp10 ,1 )[:,None ]\n    tmp12 =tl .where (tmp6 ,tmp4 ,tmp2 )\n    tmp13 =tl .full ([XBLOCK ,R0_BLOCK ],10 ,tl .int32 )\n    tmp14 =tmp12 +tmp13 \n    tmp15 =tmp12 <0 \n    tmp16 =tl .where (tmp15 ,tmp14 ,tmp12 )\n    tl .device_assert (((0 <=tmp16 )&(tmp16 <10 ))|~(r0_mask ),\"index out of bounds: 0 <= tmp16 < 10\")\n    tmp18 =tl .load (in_ptr1 +(r0_0 +196 *tmp16 ),r0_mask ,other =0.0 )\n    tmp19 =tl .load (in_ptr2 +(tmp16 ),r0_mask ,eviction_policy ='evict_last')\n    tmp20 =tmp18 -tmp19 \n    tmp21 =tl .load (in_ptr3 +(tmp16 ),r0_mask ,eviction_policy ='evict_last')\n    tmp22 =tmp20 *tmp21 \n    tmp23 =1e-10 \n    tmp24 =tmp22 +tmp23 \n    tmp25 =tl_math .log (tmp24 )\n    tmp26 =-tmp25 \n    tmp27 =0.0 \n    tmp28 =tl .where (tmp6 ,tmp26 ,tmp27 )\n    tmp29 =tl .broadcast_to (tmp28 ,[XBLOCK ,R0_BLOCK ])\n    tmp31 =tl .where (r0_mask ,tmp29 ,0 )\n    tmp32 =tl .sum (tmp31 ,1 )[:,None ]\n    tmp33 =tmp11 .to (tl .float32 )\n    tmp34 =tmp32 /tmp33 \n    tl .store (out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp4 ,r0_mask )\n    tl .store (out_ptr2 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp33 ,None )\n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp34 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_6 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .full ([1 ],1 ,tl .int64 )\n    tmp3 =tmp1 +tmp2 \n    tl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 =args \n    args .clear ()\n    assert_size_stride (primals_3 ,(1 ,10 ,28 ,28 ),(7840 ,784 ,28 ,1 ))\n    assert_size_stride (primals_4 ,(),())\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    assert_size_stride (primals_6 ,(10 ,),(1 ,))\n    assert_size_stride (primals_7 ,(10 ,),(1 ,))\n    assert_size_stride (primals_8 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,10 ,10 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,10 ,28 ,28 ),(7840 ,784 ,28 ,1 ),torch .float32 )\n        buf3 =empty_strided_cuda ((1 ,10 ,1 ,1 ),(10 ,1 ,10 ,10 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_0 [grid (10 )](primals_3 ,primals_7 ,primals_8 ,primals_6 ,primals_5 ,buf0 ,buf4 ,buf3 ,primals_6 ,primals_5 ,10 ,784 ,num_warps =8 ,num_stages =1 )\n        del primals_5 \n        del primals_6 \n        del primals_7 \n        del primals_8 \n        buf5 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf5 )\n        buf6 =empty_strided_cuda ((1 ,10 ,2 ),(20 ,2 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_rand_1 [grid (20 )](buf5 ,buf6 ,0 ,20 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf7 =empty_strided_cuda ((1 ,10 ,14 ,14 ),(1984 ,196 ,14 ,1 ),torch .float32 )\n        buf8 =empty_strided_cuda ((1 ,10 ,14 ,14 ),(1968 ,196 ,14 ,1 ),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_fractional_max_pool2d_2 [grid (1960 )](buf6 ,buf4 ,buf7 ,buf8 ,1960 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf6 \n        buf9 =empty_strided_cuda ((1 ,1 ,14 ,14 ),(196 ,196 ,14 ,1 ),torch .float32 )\n        buf10 =empty_strided_cuda ((1 ,1 ,14 ,14 ),(196 ,196 ,14 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__softmax_3 [grid (196 )](buf7 ,buf9 ,buf10 ,196 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        buf11 =buf7 ;del buf7 \n        buf12 =empty_strided_cuda ((1 ,10 ,1 ),(10 ,1 ,1 ),torch .float32 )\n        buf13 =empty_strided_cuda ((1 ,10 ,1 ),(10 ,1 ,10 ),torch .float32 )\n        buf15 =reinterpret_tensor (buf13 ,(1 ,10 ,1 ),(10 ,1 ,1 ),0 );del buf13 \n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit__softmax_4 [grid (10 )](buf11 ,buf15 ,buf9 ,buf10 ,buf12 ,10 ,196 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf10 \n        del buf9 \n        buf16 =empty_strided_cuda ((1 ,14 ,14 ),(196 ,14 ,1 ),torch .int64 )\n        buf19 =empty_strided_cuda ((),(),torch .float32 )\n        buf18 =empty_strided_cuda ((),(),torch .float32 )\n        buf28 =buf19 ;del buf19 \n\n        get_raw_stream (0 )\n        triton_per_fused_nll_loss2d_forward_randint_5 [grid (1 )](buf28 ,buf5 ,buf11 ,buf12 ,buf15 ,buf16 ,buf18 ,1 ,1 ,196 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf5 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_6 [grid (1 )](primals_4 ,primals_4 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_4 \n    return (buf28 ,primals_3 ,reinterpret_tensor (buf3 ,(10 ,),(1 ,),0 ),buf4 ,buf8 ,buf11 ,buf12 ,buf15 ,buf16 ,buf18 ,reinterpret_tensor (buf0 ,(1 ,10 ,1 ,1 ),(10 ,1 ,1 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =28 \n    primals_2 =28 \n    primals_3 =rand_strided ((1 ,10 ,28 ,28 ),(7840 ,784 ,28 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "e886c6c2-7762-4c9a-a621-85b697618cc8",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AlphaDropout', 'LPPool1d', 'Mish', 'LazyBatchNorm1d', 'Softplus', 'ZeroPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad = nn.ZeroPad3d(1)\n        self.lp_pool = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.batch_norm = nn.LazyBatchNorm1d()\n        self.mish = nn.Mish()\n        self.softplus = nn.Softplus()\n        self.alpha_dropout = nn.AlphaDropout(p=0.5)\n        \n        # Repeat some modules up to 5 times\n        self.lp_pool2 = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.batch_norm2 = nn.LazyBatchNorm1d()\n        self.mish2 = nn.Mish()\n        self.softplus2 = nn.Softplus()\n        self.alpha_dropout2 = nn.AlphaDropout(p=0.5)\n\n    def forward(self, x):\n        # Assuming input is of arbitrary shape, we first pad it\n        x = self.zero_pad(x)\n        \n        # Reshape to 1D for LPPool1d\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions\n        \n        # Apply LPPool1d\n        x = self.lp_pool(x)\n        \n        # Apply BatchNorm\n        x = self.batch_norm(x)\n        \n        # Apply Mish activation\n        x = self.mish(x)\n        \n        # Apply Softplus activation\n        x = self.softplus(x)\n        \n        # Apply AlphaDropout\n        x = self.alpha_dropout(x)\n        \n        # Repeat some modules\n        x = self.lp_pool2(x)\n        x = self.batch_norm2(x)\n        x = self.mish2(x)\n        x = self.softplus2(x)\n        x = self.alpha_dropout2(x)\n        \n        # Reshape back to original shape (or any desired shape)\n        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input with arbitrary shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =58953 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %19651 )\n    x1 =xindex //19651 \n    tmp0 =(-1 )+(x0 //578 )\n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],32 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+(((x0 //17 )%34 ))\n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =(-1 )+(((2 *x0 )%34 ))\n    tmp9 =tmp8 >=tmp1 \n    tmp10 =tmp8 <tmp3 \n    tmp11 =tmp2 &tmp4 \n    tmp12 =tmp11 &tmp6 \n    tmp13 =tmp12 &tmp7 \n    tmp14 =tmp13 &tmp9 \n    tmp15 =tmp14 &tmp10 \n    tmp16 =tl .load (in_ptr0 +((-1057 )+32 *(((x0 //17 )%34 ))+1024 *(x0 //578 )+32768 *x1 +(((2 *x0 )%34 ))),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp17 =tmp16 *tmp16 \n    tmp18 =(-1 )+((1 +2 *x0 )//1156 )\n    tmp19 =tmp18 >=tmp1 \n    tmp20 =tmp18 <tmp3 \n    tmp21 =(-1 )+((((1 +2 *x0 )//34 )%34 ))\n    tmp22 =tmp21 >=tmp1 \n    tmp23 =tmp21 <tmp3 \n    tmp24 =(-1 )+(((1 +2 *x0 )%34 ))\n    tmp25 =tmp24 >=tmp1 \n    tmp26 =tmp24 <tmp3 \n    tmp27 =tmp19 &tmp20 \n    tmp28 =tmp27 &tmp22 \n    tmp29 =tmp28 &tmp23 \n    tmp30 =tmp29 &tmp25 \n    tmp31 =tmp30 &tmp26 \n    tmp32 =tl .load (in_ptr0 +((-1057 )+32 *((((1 +2 *x0 )//34 )%34 ))+1024 *((1 +2 *x0 )//1156 )+32768 *x1 +(((1 +2 *x0 )%34 ))),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tmp32 *tmp32 \n    tmp34 =tmp33 +tmp17 \n    tmp35 =(-1 )+((1 +x0 )//578 )\n    tmp36 =tmp35 >=tmp1 \n    tmp37 =tmp35 <tmp3 \n    tmp38 =(-1 )+((((1 +x0 )//17 )%34 ))\n    tmp39 =tmp38 >=tmp1 \n    tmp40 =tmp38 <tmp3 \n    tmp41 =(-1 )+(((2 +2 *x0 )%34 ))\n    tmp42 =tmp41 >=tmp1 \n    tmp43 =tmp41 <tmp3 \n    tmp44 =tmp36 &tmp37 \n    tmp45 =tmp44 &tmp39 \n    tmp46 =tmp45 &tmp40 \n    tmp47 =tmp46 &tmp42 \n    tmp48 =tmp47 &tmp43 \n    tmp49 =tl .load (in_ptr0 +((-1057 )+32 *((((1 +x0 )//17 )%34 ))+1024 *((1 +x0 )//578 )+32768 *x1 +(((2 +2 *x0 )%34 ))),tmp48 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp50 =tmp49 *tmp49 \n    tmp51 =tmp50 +tmp34 \n    tmp52 =0.3333333333333333 \n    tmp53 =tmp51 *tmp52 \n    tl .store (out_ptr0 +(x0 +19680 *x1 ),tmp53 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_functional_abs_mul_pow_relu_sign_1 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =9 \n    r0_numel =6551 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %3 )\n    x1 =xindex //3 \n    tmp28_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp28_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp28_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =r0_2 +6551 *x0 \n        tmp1 =tl .full ([1 ,1 ],19651 ,tl .int32 )\n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(r0_2 +6551 *x0 +19680 *x1 ),r0_mask &tmp2 &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp5 =tmp4 <tmp3 \n        tmp6 =tmp5 .to (tl .int8 )\n        tmp7 =tmp3 <tmp4 \n        tmp8 =tmp7 .to (tl .int8 )\n        tmp9 =tmp6 -tmp8 \n        tmp10 =tmp9 .to (tmp3 .dtype )\n        tmp11 =tl_math .abs (tmp3 )\n        tmp12 =triton_helpers .maximum (tmp4 ,tmp11 )\n        tmp13 =tmp10 *tmp12 \n        tmp14 =3.0 \n        tmp15 =tmp13 *tmp14 \n        tmp16 =libdevice .sqrt (tmp15 )\n        tmp17 =tl .full (tmp16 .shape ,0 ,tmp16 .dtype )\n        tmp18 =tl .where (tmp2 ,tmp16 ,tmp17 )\n        tmp19 =0.0 \n        tmp20 =tl .full (tmp19 .shape ,0 ,tmp19 .dtype )\n        tmp21 =tl .where (tmp2 ,tmp19 ,tmp20 )\n        tmp22 =1.0 \n        tmp23 =tl .full (tmp22 .shape ,0 ,tmp22 .dtype )\n        tmp24 =tl .where (tmp2 ,tmp22 ,tmp23 )\n        tmp25 =tl .broadcast_to (tmp18 ,[XBLOCK ,R0_BLOCK ])\n        tmp26 =tl .broadcast_to (tmp21 ,[XBLOCK ,R0_BLOCK ])\n        tmp27 =tl .broadcast_to (tmp24 ,[XBLOCK ,R0_BLOCK ])\n        tmp28_mean_next ,tmp28_m2_next ,tmp28_weight_next =triton_helpers .welford_combine (\n        tmp28_mean ,tmp28_m2 ,tmp28_weight ,\n        tmp25 ,tmp26 ,tmp27 \n        )\n        tmp28_mean =tl .where (r0_mask &xmask ,tmp28_mean_next ,tmp28_mean )\n        tmp28_m2 =tl .where (r0_mask &xmask ,tmp28_m2_next ,tmp28_m2 )\n        tmp28_weight =tl .where (r0_mask &xmask ,tmp28_weight_next ,tmp28_weight )\n    tmp31 ,tmp32 ,tmp33 =triton_helpers .welford (tmp28_mean ,tmp28_m2 ,tmp28_weight ,1 )\n    tmp28 =tmp31 [:,None ]\n    tmp29 =tmp32 [:,None ]\n    tmp30 =tmp33 [:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp28 ,xmask )\n    tl .store (out_ptr1 +(x3 ),tmp29 ,xmask )\n    tl .store (out_ptr2 +(x3 ),tmp30 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_abs_mul_pow_relu_sign_2 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr4 ,out_ptr6 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \n    r0_numel =3 \n    R0_BLOCK :tl .constexpr =4 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +3 *x0 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 +3 *x0 ),r0_mask &xmask ,other =0.0 )\n    tmp2 =tl .load (in_ptr2 +(r0_1 +3 *x0 ),r0_mask &xmask ,other =0.0 )\n    tmp25 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp30 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (r0_mask &xmask ,tmp3 ,0 )\n    tmp8 =tl .where (r0_mask &xmask ,tmp4 ,0 )\n    tmp9 =tl .where (r0_mask &xmask ,tmp5 ,0 )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\n    tmp13 =tmp10 [:,None ]\n    tmp14 =tmp11 [:,None ]\n    tmp15 =tmp12 [:,None ]\n    tmp16 =19651.0 \n    tmp17 =tmp14 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tmp21 =1.0000508905852417 \n    tmp22 =tmp17 *tmp21 \n    tmp23 =0.1 \n    tmp24 =tmp22 *tmp23 \n    tmp26 =0.9 \n    tmp27 =tmp25 *tmp26 \n    tmp28 =tmp24 +tmp27 \n    tmp29 =tmp13 *tmp23 \n    tmp31 =tmp30 *tmp26 \n    tmp32 =tmp29 +tmp31 \n    tl .store (out_ptr2 +(x0 ),tmp20 ,xmask )\n    tl .store (out_ptr4 +(x0 ),tmp28 ,xmask )\n    tl .store (out_ptr6 +(x0 ),tmp32 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_functional__to_copy_abs_add_bernoulli_mish_mul_pow_relu_sign_softplus_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =58953 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %19651 )\n    x1 =xindex //19651 \n    tmp5 =tl .load (in_ptr1 +(x0 +19680 *x1 ),xmask )\n    tmp19 =tl .load (in_ptr2 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr3 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp23 =tl .load (in_ptr4 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp25 =tl .load (in_ptr5 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x2 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tmp6 =tl .full ([1 ],0 ,tl .int32 )\n    tmp7 =tmp6 <tmp5 \n    tmp8 =tmp7 .to (tl .int8 )\n    tmp9 =tmp5 <tmp6 \n    tmp10 =tmp9 .to (tl .int8 )\n    tmp11 =tmp8 -tmp10 \n    tmp12 =tmp11 .to (tmp5 .dtype )\n    tmp13 =tl_math .abs (tmp5 )\n    tmp14 =triton_helpers .maximum (tmp6 ,tmp13 )\n    tmp15 =tmp12 *tmp14 \n    tmp16 =3.0 \n    tmp17 =tmp15 *tmp16 \n    tmp18 =libdevice .sqrt (tmp17 )\n    tmp20 =tmp18 -tmp19 \n    tmp22 =tmp20 *tmp21 \n    tmp24 =tmp22 *tmp23 \n    tmp26 =tmp24 +tmp25 \n    tmp27 =20.0 \n    tmp28 =tmp26 >tmp27 \n    tmp29 =tl_math .exp (tmp26 )\n    tmp30 =libdevice .log1p (tmp29 )\n    tmp31 =tl .where (tmp28 ,tmp26 ,tmp30 )\n    tmp32 =libdevice .tanh (tmp31 )\n    tmp33 =tmp26 *tmp32 \n    tmp34 =1.0 \n    tmp35 =tmp33 *tmp34 \n    tmp36 =tmp35 >tmp27 \n    tmp37 =tl_math .exp (tmp35 )\n    tmp38 =libdevice .log1p (tmp37 )\n    tmp39 =tmp38 *tmp34 \n    tmp40 =tl .where (tmp36 ,tmp33 ,tmp39 )\n    tmp41 =tmp4 .to (tl .float32 )\n    tmp42 =0.8864048946659319 \n    tmp43 =tmp41 *tmp42 \n    tmp44 =tmp40 *tmp43 \n    tmp45 =-1.0 \n    tmp46 =tmp41 +tmp45 \n    tmp47 =1.558387861036063 \n    tmp48 =tmp46 *tmp47 \n    tmp49 =0.7791939305180315 \n    tmp50 =tmp48 +tmp49 \n    tmp51 =tmp44 +tmp50 \n    tmp52 =tmp51 *tmp51 \n    tl .store (out_ptr1 +(x0 +19712 *x1 ),tmp4 ,xmask )\n    tl .store (in_out_ptr0 +(x0 +19680 *x1 ),tmp52 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_4 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =29475 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %9825 )\n    x1 =xindex //9825 \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +19680 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +19680 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 +2 *x0 +19680 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp5 =0.3333333333333333 \n    tmp6 =tmp4 *tmp5 \n    tl .store (out_ptr0 +(x0 +9856 *x1 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_functional_abs_mul_pow_relu_sign_5 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =6 \n    r0_numel =4913 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %2 )\n    x1 =xindex //2 \n    tmp28_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp28_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp28_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =r0_2 +4913 *x0 \n        tmp1 =tl .full ([1 ,1 ],9825 ,tl .int32 )\n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(r0_2 +4913 *x0 +9856 *x1 ),r0_mask &tmp2 &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp4 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp5 =tmp4 <tmp3 \n        tmp6 =tmp5 .to (tl .int8 )\n        tmp7 =tmp3 <tmp4 \n        tmp8 =tmp7 .to (tl .int8 )\n        tmp9 =tmp6 -tmp8 \n        tmp10 =tmp9 .to (tmp3 .dtype )\n        tmp11 =tl_math .abs (tmp3 )\n        tmp12 =triton_helpers .maximum (tmp4 ,tmp11 )\n        tmp13 =tmp10 *tmp12 \n        tmp14 =3.0 \n        tmp15 =tmp13 *tmp14 \n        tmp16 =libdevice .sqrt (tmp15 )\n        tmp17 =tl .full (tmp16 .shape ,0 ,tmp16 .dtype )\n        tmp18 =tl .where (tmp2 ,tmp16 ,tmp17 )\n        tmp19 =0.0 \n        tmp20 =tl .full (tmp19 .shape ,0 ,tmp19 .dtype )\n        tmp21 =tl .where (tmp2 ,tmp19 ,tmp20 )\n        tmp22 =1.0 \n        tmp23 =tl .full (tmp22 .shape ,0 ,tmp22 .dtype )\n        tmp24 =tl .where (tmp2 ,tmp22 ,tmp23 )\n        tmp25 =tl .broadcast_to (tmp18 ,[XBLOCK ,R0_BLOCK ])\n        tmp26 =tl .broadcast_to (tmp21 ,[XBLOCK ,R0_BLOCK ])\n        tmp27 =tl .broadcast_to (tmp24 ,[XBLOCK ,R0_BLOCK ])\n        tmp28_mean_next ,tmp28_m2_next ,tmp28_weight_next =triton_helpers .welford_combine (\n        tmp28_mean ,tmp28_m2 ,tmp28_weight ,\n        tmp25 ,tmp26 ,tmp27 \n        )\n        tmp28_mean =tl .where (r0_mask &xmask ,tmp28_mean_next ,tmp28_mean )\n        tmp28_m2 =tl .where (r0_mask &xmask ,tmp28_m2_next ,tmp28_m2 )\n        tmp28_weight =tl .where (r0_mask &xmask ,tmp28_weight_next ,tmp28_weight )\n    tmp31 ,tmp32 ,tmp33 =triton_helpers .welford (tmp28_mean ,tmp28_m2 ,tmp28_weight ,1 )\n    tmp28 =tmp31 [:,None ]\n    tmp29 =tmp32 [:,None ]\n    tmp30 =tmp33 [:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp28 ,xmask )\n    tl .store (out_ptr1 +(x3 ),tmp29 ,xmask )\n    tl .store (out_ptr2 +(x3 ),tmp30 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__native_batch_norm_legit_functional_abs_mul_pow_relu_sign_6 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr0 ,out_ptr2 ,out_ptr4 ,out_ptr6 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =3 \n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +2 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr1 +(r0_1 +2 *x0 ),xmask ,other =0.0 )\n    tmp2 =tl .load (in_ptr2 +(r0_1 +2 *x0 ),xmask ,other =0.0 )\n    tmp25 =tl .load (in_ptr3 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp30 =tl .load (in_ptr4 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp4 =tl .broadcast_to (tmp1 ,[XBLOCK ,R0_BLOCK ])\n    tmp5 =tl .broadcast_to (tmp2 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .where (xmask ,tmp3 ,0 )\n    tmp8 =tl .where (xmask ,tmp4 ,0 )\n    tmp9 =tl .where (xmask ,tmp5 ,0 )\n    tmp10 ,tmp11 ,tmp12 =triton_helpers .welford (tmp7 ,tmp8 ,tmp9 ,1 )\n    tmp13 =tmp10 [:,None ]\n    tmp14 =tmp11 [:,None ]\n    tmp15 =tmp12 [:,None ]\n    tmp16 =9825.0 \n    tmp17 =tmp14 /tmp16 \n    tmp18 =1e-05 \n    tmp19 =tmp17 +tmp18 \n    tmp20 =libdevice .rsqrt (tmp19 )\n    tmp21 =1.0001017915309447 \n    tmp22 =tmp17 *tmp21 \n    tmp23 =0.1 \n    tmp24 =tmp22 *tmp23 \n    tmp26 =0.9 \n    tmp27 =tmp25 *tmp26 \n    tmp28 =tmp24 +tmp27 \n    tmp29 =tmp13 *tmp23 \n    tmp31 =tmp30 *tmp26 \n    tmp32 =tmp29 +tmp31 \n    tl .store (out_ptr2 +(x0 ),tmp20 ,xmask )\n    tl .store (out_ptr4 +(x0 ),tmp28 ,xmask )\n    tl .store (out_ptr6 +(x0 ),tmp32 ,xmask )\n    tl .store (out_ptr0 +(x0 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__native_batch_norm_legit_functional__to_copy_abs_add_bernoulli_mish_mul_pow_relu_sign_softplus_7 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,out_ptr1 ,out_ptr3 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =29475 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %9825 )\n    x1 =xindex //9825 \n    tmp5 =tl .load (in_ptr1 +(x0 +9856 *x1 ),xmask )\n    tmp19 =tl .load (in_ptr2 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr3 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp23 =tl .load (in_ptr4 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp25 =tl .load (in_ptr5 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x2 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tmp6 =tl .full ([1 ],0 ,tl .int32 )\n    tmp7 =tmp6 <tmp5 \n    tmp8 =tmp7 .to (tl .int8 )\n    tmp9 =tmp5 <tmp6 \n    tmp10 =tmp9 .to (tl .int8 )\n    tmp11 =tmp8 -tmp10 \n    tmp12 =tmp11 .to (tmp5 .dtype )\n    tmp13 =tl_math .abs (tmp5 )\n    tmp14 =triton_helpers .maximum (tmp6 ,tmp13 )\n    tmp15 =tmp12 *tmp14 \n    tmp16 =3.0 \n    tmp17 =tmp15 *tmp16 \n    tmp18 =libdevice .sqrt (tmp17 )\n    tmp20 =tmp18 -tmp19 \n    tmp22 =tmp20 *tmp21 \n    tmp24 =tmp22 *tmp23 \n    tmp26 =tmp24 +tmp25 \n    tmp27 =20.0 \n    tmp28 =tmp26 >tmp27 \n    tmp29 =tl_math .exp (tmp26 )\n    tmp30 =libdevice .log1p (tmp29 )\n    tmp31 =tl .where (tmp28 ,tmp26 ,tmp30 )\n    tmp32 =libdevice .tanh (tmp31 )\n    tmp33 =tmp26 *tmp32 \n    tmp34 =1.0 \n    tmp35 =tmp33 *tmp34 \n    tmp36 =tmp35 >tmp27 \n    tmp37 =tl_math .exp (tmp35 )\n    tmp38 =libdevice .log1p (tmp37 )\n    tmp39 =tmp38 *tmp34 \n    tmp40 =tl .where (tmp36 ,tmp33 ,tmp39 )\n    tmp41 =tmp4 .to (tl .float32 )\n    tmp42 =0.8864048946659319 \n    tmp43 =tmp41 *tmp42 \n    tmp44 =tmp40 *tmp43 \n    tmp45 =-1.0 \n    tmp46 =tmp41 +tmp45 \n    tmp47 =1.558387861036063 \n    tmp48 =tmp46 *tmp47 \n    tmp49 =0.7791939305180315 \n    tmp50 =tmp48 +tmp49 \n    tmp51 =tmp44 +tmp50 \n    tl .store (out_ptr1 +(x0 +9856 *x1 ),tmp4 ,xmask )\n    tl .store (out_ptr3 +(x2 ),tmp51 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_8 (in_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .full ([1 ],1 ,tl .int64 )\n    tmp3 =tmp1 +tmp2 \n    tl .store (out_ptr1 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp3 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ))\n    assert_size_stride (primals_2 ,(),())\n    assert_size_stride (primals_3 ,(3 ,),(1 ,))\n    assert_size_stride (primals_4 ,(3 ,),(1 ,))\n    assert_size_stride (primals_5 ,(3 ,),(1 ,))\n    assert_size_stride (primals_6 ,(3 ,),(1 ,))\n    assert_size_stride (primals_7 ,(),())\n    assert_size_stride (primals_8 ,(3 ,),(1 ,))\n    assert_size_stride (primals_9 ,(3 ,),(1 ,))\n    assert_size_stride (primals_10 ,(3 ,),(1 ,))\n    assert_size_stride (primals_11 ,(3 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,1 ,19651 ),(59040 ,19680 ,19680 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_0 [grid (58953 )](primals_1 ,buf0 ,58953 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_1 \n        buf1 =empty_strided_cuda ((1 ,3 ,1 ,3 ),(9 ,3 ,9 ,1 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,3 ,1 ,3 ),(9 ,3 ,9 ,1 ),torch .float32 )\n        buf3 =empty_strided_cuda ((1 ,3 ,1 ,3 ),(9 ,3 ,9 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_functional_abs_mul_pow_relu_sign_1 [grid (9 )](buf0 ,buf1 ,buf2 ,buf3 ,9 ,6551 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((1 ,3 ,1 ),(3 ,1 ,1 ),torch .float32 )\n        buf7 =empty_strided_cuda ((1 ,3 ,1 ),(3 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_abs_mul_pow_relu_sign_2 [grid (3 )](buf1 ,buf2 ,buf3 ,primals_4 ,primals_3 ,buf4 ,buf7 ,primals_4 ,primals_3 ,3 ,3 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n        del buf2 \n        del buf3 \n        del primals_3 \n        del primals_4 \n        buf9 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf9 )\n        buf11 =empty_strided_cuda ((1 ,3 ,19651 ),(59136 ,19712 ,1 ),torch .bool )\n        buf8 =empty_strided_cuda ((1 ,3 ,19651 ),(59040 ,19680 ,1 ),torch .float32 )\n        buf12 =buf8 ;del buf8 \n\n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_functional__to_copy_abs_add_bernoulli_mish_mul_pow_relu_sign_softplus_3 [grid (58953 )](buf12 ,buf9 ,buf0 ,buf4 ,buf7 ,primals_5 ,primals_6 ,buf11 ,0 ,58953 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf13 =empty_strided_cuda ((1 ,3 ,1 ,9825 ),(29568 ,9856 ,9856 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_4 [grid (29475 )](buf12 ,buf13 ,29475 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf14 =empty_strided_cuda ((1 ,3 ,1 ,2 ),(6 ,2 ,6 ,1 ),torch .float32 )\n        buf15 =empty_strided_cuda ((1 ,3 ,1 ,2 ),(6 ,2 ,6 ,1 ),torch .float32 )\n        buf16 =empty_strided_cuda ((1 ,3 ,1 ,2 ),(6 ,2 ,6 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_functional_abs_mul_pow_relu_sign_5 [grid (6 )](buf13 ,buf14 ,buf15 ,buf16 ,6 ,4913 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf17 =empty_strided_cuda ((1 ,3 ,1 ),(3 ,1 ,1 ),torch .float32 )\n        buf20 =empty_strided_cuda ((1 ,3 ,1 ),(3 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused__native_batch_norm_legit_functional_abs_mul_pow_relu_sign_6 [grid (3 )](buf14 ,buf15 ,buf16 ,primals_9 ,primals_8 ,buf17 ,buf20 ,primals_9 ,primals_8 ,3 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf14 \n        del buf15 \n        del buf16 \n        del primals_8 \n        del primals_9 \n        buf23 =empty_strided_cuda ((1 ,3 ,9825 ),(29568 ,9856 ,1 ),torch .bool )\n        buf40 =empty_strided_cuda ((1 ,3 ,9825 ),(29475 ,9825 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__native_batch_norm_legit_functional__to_copy_abs_add_bernoulli_mish_mul_pow_relu_sign_softplus_7 [grid (29475 )](buf9 ,buf13 ,buf17 ,buf20 ,primals_10 ,primals_11 ,buf23 ,buf40 ,1 ,29475 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf9 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_8 [grid (1 )](primals_2 ,primals_2 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_8 [grid (1 )](primals_7 ,primals_7 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del primals_7 \n    return (reinterpret_tensor (buf40 ,(1 ,29475 ),(29475 ,1 ),0 ),primals_5 ,primals_6 ,primals_10 ,primals_11 ,buf0 ,buf4 ,buf7 ,buf11 ,reinterpret_tensor (buf12 ,(1 ,3 ,1 ,19651 ),(0 ,19680 ,0 ,1 ),0 ),buf13 ,buf17 ,buf20 ,buf23 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_3 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((),(),device ='cuda:0',dtype =torch .int64 )\n    primals_8 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((3 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "eb7a2bd6-2740-4a7d-800d-793cd5519e98",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Tanh', 'ZeroPad1d', 'GRU', 'LPPool3d', 'Module', 'AdaptiveMaxPool2d', 'Flatten']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.zero_pad1d = nn.ZeroPad1d(padding=2)\n        self.gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n        self.lp_pool3d = nn.LPPool3d(norm_type=2, kernel_size=2, stride=2)\n        self.adaptive_max_pool2d = nn.AdaptiveMaxPool2d(output_size=(5, 5))\n        self.flatten = nn.Flatten()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, sequence_length)\n        x = self.zero_pad1d(x)  # Shape: (batch_size, channels, sequence_length + 4)\n        \n        # Reshape for GRU: (batch_size, sequence_length + 4, channels)\n        x = x.permute(0, 2, 1)\n        x, _ = self.gru(x)  # Shape: (batch_size, sequence_length + 4, hidden_size)\n        \n        # Reshape for LPPool3d: (batch_size, hidden_size, sequence_length + 4, 1, 1)\n        x = x.unsqueeze(3).unsqueeze(4)\n        x = self.lp_pool3d(x)  # Shape: (batch_size, hidden_size, (sequence_length + 4)/2, 1, 1)\n        \n        # Reshape for AdaptiveMaxPool2d: (batch_size, hidden_size, (sequence_length + 4)/2, 1)\n        x = x.squeeze(4)\n        x = self.adaptive_max_pool2d(x)  # Shape: (batch_size, hidden_size, 5, 5)\n        \n        x = self.flatten(x)  # Shape: (batch_size, hidden_size * 5 * 5)\n        x = self.tanh(x)  # Shape: (batch_size, hidden_size * 5 * 5)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 64).cuda()  # Example input shape: (batch_size=1, channels=10, sequence_length=64)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =(-2 )+x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *x1 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tl .store (out_ptr0 +(x2 ),tmp6 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,s1 ),(s0 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s1 \n        buf0 =empty_strided_cuda ((1 ,s0 ,4 +s1 ),(4 *s0 +s0 *s1 ,4 +s1 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =4 *s0 +s0 *s1 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](arg2_1 ,buf0 ,68 ,64 ,680 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg2_1 \n    return (reinterpret_tensor (buf0 ,(1 ,4 +s1 ,s0 ),(4 *s0 +s0 *s1 ,1 ,4 +s1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,10 ,64 ),(640 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ed84d218-616c-4585-8f93-8167d1896d5e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Softsign', 'ConstantPad2d', 'TripletMarginLoss', 'FeatureAlphaDropout', 'MaxPool2d', 'AdaptiveMaxPool2d', 'LogSigmoid', 'Container', 'LazyConvTranspose2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad2d(2, 3.0)\n        self.conv_transpose1 = nn.LazyConvTranspose2d(out_channels=32, kernel_size=3)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.adaptive_max_pool = nn.AdaptiveMaxPool2d((5, 5))\n        self.dropout = nn.FeatureAlphaDropout(p=0.5)\n        self.conv_transpose2 = nn.LazyConvTranspose2d(out_channels=64, kernel_size=3)\n        self.softsign = nn.Softsign()\n        self.log_sigmoid = nn.LogSigmoid()\n        self.container = nn.Sequential(\n            nn.LazyConvTranspose2d(out_channels=128, kernel_size=3),\n            nn.Softsign(),\n            nn.LazyConvTranspose2d(out_channels=256, kernel_size=3),\n            nn.LogSigmoid()\n        )\n        self.triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.conv_transpose1(x)\n        x = self.max_pool(x)\n        x = self.adaptive_max_pool(x)\n        x = self.dropout(x)\n        x = self.conv_transpose2(x)\n        x = self.softsign(x)\n        x = self.log_sigmoid(x)\n        x = self.container(x)\n        \n        # For TripletMarginLoss, we need three inputs: anchor, positive, and negative\n        # Here, we use the same input for simplicity, but in practice, they should be different\n        anchor = x\n        positive = x\n        negative = x\n        loss = self.triplet_loss(anchor, positive, negative)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =13872 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =((xindex //68 )%68 )\n    x0 =(xindex %68 )\n    x2 =xindex //4624 \n    x4 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],64 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-2 )+x0 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =tmp2 &tmp4 \n    tmp9 =tmp8 &tmp6 \n    tmp10 =tmp9 &tmp7 \n    tmp11 =tl .load (in_ptr0 +((-130 )+x0 +64 *x1 +4096 *x2 ),tmp10 &xmask ,other =3.0 )\n    tl .store (out_ptr0 +(x4 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_1 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =156800 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //4900 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_2 (in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =39200 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %35 )\n    x3 =xindex //35 \n    x2 =xindex //1225 \n    x4 =(xindex %1225 )\n    tmp0 =tl .load (in_ptr0 +(2 *x0 +140 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +140 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(70 +2 *x0 +140 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(71 +2 *x0 +140 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp6 =triton_helpers .maximum (tmp5 ,tmp4 )\n    tmp7 =tmp1 >tmp0 \n    tmp8 =tl .full ([1 ],1 ,tl .int8 )\n    tmp9 =tl .full ([1 ],0 ,tl .int8 )\n    tmp10 =tl .where (tmp7 ,tmp8 ,tmp9 )\n    tmp11 =tmp3 >tmp2 \n    tmp12 =tl .full ([1 ],2 ,tl .int8 )\n    tmp13 =tl .where (tmp11 ,tmp12 ,tmp10 )\n    tmp14 =tmp5 >tmp4 \n    tmp15 =tl .full ([1 ],3 ,tl .int8 )\n    tmp16 =tl .where (tmp14 ,tmp15 ,tmp13 )\n    tl .store (out_ptr0 +(x4 +1248 *x2 ),tmp6 ,xmask )\n    tl .store (out_ptr1 +(x4 +1280 *x2 ),tmp16 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_3 (in_ptr0 ,out_ptr1 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_mul_4 (in_out_ptr0 ,in_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =800 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //25 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last').to (tl .int1 )\n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =0.8864048946659319 \n    tmp4 =tmp2 *tmp3 \n    tmp5 =tmp0 *tmp4 \n    tmp6 =-1.0 \n    tmp7 =tmp2 +tmp6 \n    tmp8 =1.558387861036063 \n    tmp9 =tmp7 *tmp8 \n    tmp10 =0.7791939305180315 \n    tmp11 =tmp9 +tmp10 \n    tmp12 =tmp5 +tmp11 \n    tl .store (in_out_ptr0 +(x2 ),tmp12 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_convolution_div_log_sigmoid_forward_5 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =3136 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //49 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl_math .abs (tmp2 )\n    tmp4 =1.0 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 /tmp5 \n    tmp7 =0.0 \n    tmp8 =triton_helpers .minimum (tmp7 ,tmp6 )\n    tmp9 =tl_math .abs (tmp6 )\n    tmp10 =-tmp9 \n    tmp11 =tl_math .exp (tmp10 )\n    tmp12 =libdevice .log1p (tmp11 )\n    tmp13 =tmp8 -tmp12 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n    tl .store (out_ptr0 +(x2 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_add_convolution_div_6 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =10368 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //81 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl_math .abs (tmp2 )\n    tmp4 =1.0 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 /tmp5 \n    tl .store (in_out_ptr0 +(x2 ),tmp2 ,xmask )\n    tl .store (out_ptr0 +(x2 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_convolution_div_eq_log_sigmoid_forward_masked_fill_norm_scalar_tensor_sub_7 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =2816 \n    r0_numel =11 \n    R0_BLOCK :tl .constexpr =16 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_2 =r0_index \n    x3 =xindex \n    x1 =xindex //11 \n    tmp0 =tl .load (in_out_ptr0 +(r0_2 +11 *x3 ),r0_mask &xmask ,other =0.0 )\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =0.0 \n    tmp4 =triton_helpers .minimum (tmp3 ,tmp2 )\n    tmp5 =tl_math .abs (tmp2 )\n    tmp6 =-tmp5 \n    tmp7 =tl_math .exp (tmp6 )\n    tmp8 =libdevice .log1p (tmp7 )\n    tmp9 =tmp4 -tmp8 \n    tmp10 =tmp9 -tmp9 \n    tmp11 =1e-06 \n    tmp12 =tmp10 +tmp11 \n    tmp13 =tmp12 *tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[XBLOCK ,R0_BLOCK ])\n    tmp16 =tl .where (r0_mask &xmask ,tmp14 ,0 )\n    tmp17 =tl .sum (tmp16 ,1 )[:,None ]\n    tmp18 =libdevice .sqrt (tmp17 )\n    tmp19 =tmp18 ==tmp3 \n    tmp20 =tmp12 /tmp18 \n    tmp21 =tl .where (tmp19 ,tmp3 ,tmp20 )\n    tl .store (in_out_ptr0 +(r0_2 +11 *x3 ),tmp2 ,r0_mask &xmask )\n    tl .store (out_ptr1 +(r0_2 +11 *x3 ),tmp21 ,r0_mask &xmask )\n    tl .store (out_ptr0 +(x3 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_ge_mean_norm_sub_8 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    r0_numel =2816 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp8 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =libdevice .sqrt (tmp0 )\n        tmp2 =1.0 \n        tmp3 =tmp1 +tmp2 \n        tmp4 =tmp3 -tmp1 \n        tmp5 =0.0 \n        tmp6 =triton_helpers .maximum (tmp4 ,tmp5 )\n        tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n        tmp9 =_tmp8 +tmp7 \n        _tmp8 =tl .where (r0_mask ,tmp9 ,_tmp8 )\n        tmp10 =tmp4 >=tmp5 \n        tl .store (out_ptr0 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp10 ,r0_mask )\n    tmp8 =tl .sum (_tmp8 ,1 )[:,None ]\n    tmp11 =2816.0 \n    tmp12 =tmp8 /tmp11 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp12 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\n    assert_size_stride (primals_2 ,(3 ,32 ,3 ,3 ),(288 ,9 ,3 ,1 ))\n    assert_size_stride (primals_3 ,(32 ,),(1 ,))\n    assert_size_stride (primals_4 ,(32 ,64 ,3 ,3 ),(576 ,9 ,3 ,1 ))\n    assert_size_stride (primals_5 ,(64 ,),(1 ,))\n    assert_size_stride (primals_6 ,(64 ,128 ,3 ,3 ),(1152 ,9 ,3 ,1 ))\n    assert_size_stride (primals_7 ,(128 ,),(1 ,))\n    assert_size_stride (primals_8 ,(128 ,256 ,3 ,3 ),(2304 ,9 ,3 ,1 ))\n    assert_size_stride (primals_9 ,(256 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,68 ,68 ),(13872 ,4624 ,68 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (13872 )](primals_1 ,buf0 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_1 \n\n        buf1 =extern_kernels .convolution (buf0 ,primals_2 ,stride =(1 ,1 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =True ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf1 ,(1 ,32 ,70 ,70 ),(156800 ,4900 ,70 ,1 ))\n        buf2 =buf1 ;del buf1 \n\n        get_raw_stream (0 )\n        triton_poi_fused_convolution_1 [grid (156800 )](buf2 ,primals_3 ,156800 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del primals_3 \n        buf3 =empty_strided_cuda ((1 ,32 ,35 ,35 ),(39936 ,1248 ,35 ,1 ),torch .float32 )\n        buf4 =empty_strided_cuda ((1 ,32 ,35 ,35 ),(40960 ,1280 ,35 ,1 ),torch .int8 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_2 [grid (39200 )](buf2 ,buf3 ,buf4 ,39200 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        buf5 =torch .ops .aten .max_pool2d_with_indices .default (buf3 ,[7 ,7 ])\n        buf6 =buf5 [0 ]\n        buf7 =buf5 [1 ]\n        del buf5 \n        buf8 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf8 )\n        buf10 =empty_strided_cuda ((1 ,32 ,1 ,1 ),(32 ,1 ,1 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_3 [grid (32 )](buf8 ,buf10 ,0 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del buf8 \n        buf11 =buf6 ;del buf6 \n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_mul_4 [grid (800 )](buf11 ,buf10 ,800 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        buf12 =extern_kernels .convolution (buf11 ,primals_4 ,stride =(1 ,1 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =True ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf12 ,(1 ,64 ,7 ,7 ),(3136 ,49 ,7 ,1 ))\n        buf13 =buf12 ;del buf12 \n        buf14 =empty_strided_cuda ((1 ,64 ,7 ,7 ),(3136 ,49 ,7 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_convolution_div_log_sigmoid_forward_5 [grid (3136 )](buf13 ,primals_5 ,buf14 ,3136 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_5 \n\n        buf15 =extern_kernels .convolution (buf14 ,primals_6 ,stride =(1 ,1 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =True ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf15 ,(1 ,128 ,9 ,9 ),(10368 ,81 ,9 ,1 ))\n        buf16 =buf15 ;del buf15 \n        buf17 =empty_strided_cuda ((1 ,128 ,9 ,9 ),(10368 ,81 ,9 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_abs_add_convolution_div_6 [grid (10368 )](buf16 ,primals_7 ,buf17 ,10368 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_7 \n\n        buf18 =extern_kernels .convolution (buf17 ,primals_8 ,stride =(1 ,1 ),padding =(0 ,0 ),dilation =(1 ,1 ),transposed =True ,output_padding =(0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf18 ,(1 ,256 ,11 ,11 ),(30976 ,121 ,11 ,1 ))\n        buf19 =buf18 ;del buf18 \n        buf20 =empty_strided_cuda ((1 ,256 ,11 ),(2816 ,11 ,1 ),torch .float32 )\n        buf23 =empty_strided_cuda ((1 ,256 ,11 ,11 ),(30976 ,121 ,11 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_convolution_div_eq_log_sigmoid_forward_masked_fill_norm_scalar_tensor_sub_7 [grid (2816 )](buf19 ,primals_9 ,buf20 ,buf23 ,2816 ,11 ,XBLOCK =8 ,num_warps =2 ,num_stages =1 )\n        del primals_9 \n        buf21 =empty_strided_cuda ((),(),torch .float32 )\n        buf22 =empty_strided_cuda ((1 ,256 ,11 ),(2816 ,11 ,1 ),torch .bool )\n        buf24 =buf21 ;del buf21 \n\n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_ge_mean_norm_sub_8 [grid (1 )](buf24 ,buf20 ,buf22 ,1 ,2816 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del buf20 \n    return (buf24 ,primals_2 ,primals_4 ,primals_6 ,primals_8 ,buf0 ,buf2 ,buf3 ,buf4 ,buf7 ,buf10 ,buf11 ,buf13 ,buf14 ,buf16 ,buf17 ,buf19 ,buf22 ,buf23 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((3 ,32 ,3 ,3 ),(288 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((32 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((32 ,64 ,3 ,3 ),(576 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((64 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((64 ,128 ,3 ,3 ),(1152 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((128 ,256 ,3 ,3 ),(2304 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ed8cbee8-5138-4cca-aa02-85cc6db0dd42",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['RNNCell', 'UpsamplingNearest2d', 'AdaptiveAvgPool3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.rnn_cell1 = nn.RNNCell(input_size=64, hidden_size=128)\n        self.rnn_cell2 = nn.RNNCell(input_size=128, hidden_size=256)\n        self.upsample = nn.UpsamplingNearest2d(scale_factor=2)\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, sequence_length, input_size)\n        batch_size, sequence_length, input_size = x.size()\n        \n        # Initialize hidden states for RNN cells\n        h1 = torch.zeros(batch_size, 128).to(x.device)\n        h2 = torch.zeros(batch_size, 256).to(x.device)\n        \n        # Process sequence through RNN cells\n        for t in range(sequence_length):\n            h1 = self.rnn_cell1(x[:, t, :], h1)\n            h2 = self.rnn_cell2(h1, h2)\n        \n        # Reshape and upsample\n        x = h2.view(batch_size, 16, 16)  # Reshape to 2D\n        x = x.unsqueeze(1)  # Add channel dimension\n        x = self.upsample(x)  # Upsample to (batch_size, 1, 32, 32)\n        \n        # Reshape and apply adaptive average pooling\n        x = x.unsqueeze(1)  # Add depth dimension\n        x = self.adaptive_avg_pool(x)  # Pool to (batch_size, 1, 1, 1, 1)\n        \n        # Flatten output\n        x = x.view(batch_size, -1)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 64).cuda()  # (batch_size, sequence_length, input_size)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =256 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_tanh_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =128 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tl .store (in_out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_tanh_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =256 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tl .store (in_out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_add_arange_mul_4 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =x0 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.5 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =tmp3 .to (tl .int32 )\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_mean_5 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index //32 \n    r0_0 =(r0_index %32 )\n    tmp0 =tl .load (in_ptr0 +(r0_1 ),None ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(r0_0 ),None ,eviction_policy ='evict_last')\n    tmp1 =tl .full ([R0_BLOCK ],16 ,tl .int32 )\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tmp0 <0 \n    tmp4 =tl .where (tmp3 ,tmp2 ,tmp0 )\n    tmp6 =tmp5 +tmp1 \n    tmp7 =tmp5 <0 \n    tmp8 =tl .where (tmp7 ,tmp6 ,tmp5 )\n    tmp9 =tl .load (in_ptr1 +(tmp8 +16 *tmp4 ),None ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr2 +(tmp8 +16 *tmp4 ),None ,eviction_policy ='evict_last')\n    tmp11 =tmp9 +tmp10 \n    tmp12 =tl .load (in_ptr3 +(tmp8 +16 *tmp4 ),None ,eviction_policy ='evict_last')\n    tmp13 =tl .load (in_ptr4 +(tmp8 +16 *tmp4 ),None ,eviction_policy ='evict_last')\n    tmp14 =tmp12 +tmp13 \n    tmp15 =tmp11 +tmp14 \n    tmp16 =libdevice .tanh (tmp15 )\n    tmp17 =tl .broadcast_to (tmp16 ,[R0_BLOCK ])\n    tmp19 =triton_helpers .promote_to_tensor (tl .sum (tmp17 ,0 ))\n    tmp20 =1024.0 \n    tmp21 =tmp19 /tmp20 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([1 ],0 ,tl .int32 )),tmp21 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_tanh_tanh_backward_6 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =256 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tmp8 =tmp7 *tmp7 \n    tmp9 =1.0 \n    tmp10 =tmp9 -tmp8 \n    tl .store (in_out_ptr0 +(x0 ),tmp10 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ,64 ),(640 ,64 ,1 ))\n    assert_size_stride (primals_2 ,(128 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_3 ,(128 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_4 ,(128 ,),(1 ,))\n    assert_size_stride (primals_5 ,(128 ,),(1 ,))\n    assert_size_stride (primals_6 ,(256 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_7 ,(256 ,256 ),(256 ,1 ))\n    assert_size_stride (primals_8 ,(256 ,),(1 ,))\n    assert_size_stride (primals_9 ,(256 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_0 [grid (128 )](buf0 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_1 [grid (256 )](buf1 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf0 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),0 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf3 )\n        buf4 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf4 ,primals_5 ,buf3 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf1 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf5 )\n        buf6 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf4 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf6 )\n        buf7 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (256 )](buf7 ,primals_9 ,buf6 ,primals_8 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf8 =buf3 ;del buf3 \n\n        extern_kernels .mm (buf4 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf8 )\n        buf9 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),64 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf9 )\n        buf10 =buf8 ;del buf8 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf10 ,primals_5 ,buf9 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf11 =buf6 ;del buf6 \n\n        extern_kernels .mm (buf7 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf11 )\n        buf12 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf10 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf12 )\n        buf13 =buf11 ;del buf11 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (256 )](buf13 ,primals_9 ,buf12 ,primals_8 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf14 =buf9 ;del buf9 \n\n        extern_kernels .mm (buf10 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf14 )\n        buf15 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),128 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf15 )\n        buf16 =buf14 ;del buf14 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf16 ,primals_5 ,buf15 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf17 =buf12 ;del buf12 \n\n        extern_kernels .mm (buf13 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf17 )\n        buf18 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf16 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf18 )\n        buf19 =buf17 ;del buf17 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (256 )](buf19 ,primals_9 ,buf18 ,primals_8 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf20 =buf15 ;del buf15 \n\n        extern_kernels .mm (buf16 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf20 )\n        buf21 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),192 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf21 )\n        buf22 =buf20 ;del buf20 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf22 ,primals_5 ,buf21 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf23 =buf18 ;del buf18 \n\n        extern_kernels .mm (buf19 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf23 )\n        buf24 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf22 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf24 )\n        buf25 =buf23 ;del buf23 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (256 )](buf25 ,primals_9 ,buf24 ,primals_8 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf26 =buf21 ;del buf21 \n\n        extern_kernels .mm (buf22 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf26 )\n        buf27 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),256 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf27 )\n        buf28 =buf26 ;del buf26 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf28 ,primals_5 ,buf27 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf29 =buf24 ;del buf24 \n\n        extern_kernels .mm (buf25 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf29 )\n        buf30 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf28 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf30 )\n        buf31 =buf29 ;del buf29 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (256 )](buf31 ,primals_9 ,buf30 ,primals_8 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf32 =buf27 ;del buf27 \n\n        extern_kernels .mm (buf28 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf32 )\n        buf33 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),320 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf33 )\n        buf34 =buf32 ;del buf32 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf34 ,primals_5 ,buf33 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf35 =buf30 ;del buf30 \n\n        extern_kernels .mm (buf31 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf35 )\n        buf36 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf34 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf36 )\n        buf37 =buf35 ;del buf35 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (256 )](buf37 ,primals_9 ,buf36 ,primals_8 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf38 =buf33 ;del buf33 \n\n        extern_kernels .mm (buf34 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf38 )\n        buf39 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),384 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf39 )\n        buf40 =buf38 ;del buf38 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf40 ,primals_5 ,buf39 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf41 =buf36 ;del buf36 \n\n        extern_kernels .mm (buf37 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf41 )\n        buf42 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf40 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf42 )\n        buf43 =buf41 ;del buf41 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (256 )](buf43 ,primals_9 ,buf42 ,primals_8 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf44 =buf39 ;del buf39 \n\n        extern_kernels .mm (buf40 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf44 )\n        buf45 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),448 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf45 )\n        buf46 =buf44 ;del buf44 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf46 ,primals_5 ,buf45 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf47 =buf42 ;del buf42 \n\n        extern_kernels .mm (buf43 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf47 )\n        buf48 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf46 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf48 )\n        buf49 =buf47 ;del buf47 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (256 )](buf49 ,primals_9 ,buf48 ,primals_8 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf50 =buf45 ;del buf45 \n\n        extern_kernels .mm (buf46 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf50 )\n        buf51 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),512 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf51 )\n        buf52 =buf50 ;del buf50 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf52 ,primals_5 ,buf51 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf53 =buf48 ;del buf48 \n\n        extern_kernels .mm (buf49 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf53 )\n        buf54 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf52 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf54 )\n        buf55 =buf53 ;del buf53 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (256 )](buf55 ,primals_9 ,buf54 ,primals_8 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf56 =buf51 ;del buf51 \n\n        extern_kernels .mm (buf52 ,reinterpret_tensor (primals_3 ,(128 ,128 ),(1 ,128 ),0 ),out =buf56 )\n        buf57 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,64 ),(64 ,1 ),576 ),reinterpret_tensor (primals_2 ,(64 ,128 ),(1 ,64 ),0 ),out =buf57 )\n        del primals_2 \n        buf58 =buf56 ;del buf56 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (128 )](buf58 ,primals_5 ,buf57 ,primals_4 ,128 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf57 \n        del primals_4 \n        del primals_5 \n        buf59 =buf54 ;del buf54 \n\n        extern_kernels .mm (buf55 ,reinterpret_tensor (primals_7 ,(256 ,256 ),(1 ,256 ),0 ),out =buf59 )\n        buf60 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf58 ,reinterpret_tensor (primals_6 ,(128 ,256 ),(1 ,128 ),0 ),out =buf60 )\n        buf61 =empty_strided_cuda ((32 ,),(1 ,),torch .int64 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_add_arange_mul_4 [grid (32 )](buf61 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        buf62 =empty_strided_cuda ((1 ,1 ,1 ,1 ,1 ),(1 ,1 ,1 ,1 ,1 ),torch .float32 )\n        buf63 =buf62 ;del buf62 \n\n        get_raw_stream (0 )\n        triton_per_fused_mean_5 [grid (1 )](buf63 ,buf61 ,buf59 ,primals_9 ,buf60 ,primals_8 ,1 ,1024 ,num_warps =8 ,num_stages =1 )\n        buf64 =buf59 ;del buf59 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_tanh_backward_6 [grid (256 )](buf64 ,primals_9 ,buf60 ,primals_8 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf60 \n        del primals_8 \n        del primals_9 \n    return (reinterpret_tensor (buf63 ,(1 ,1 ),(1 ,1 ),0 ),buf0 ,buf1 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),0 ),buf4 ,buf7 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),64 ),buf10 ,buf13 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),128 ),buf16 ,buf19 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),192 ),buf22 ,buf25 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),256 ),buf28 ,buf31 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),320 ),buf34 ,buf37 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),384 ),buf40 ,buf43 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),448 ),buf46 ,buf49 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),512 ),buf52 ,buf55 ,reinterpret_tensor (primals_1 ,(1 ,64 ),(640 ,1 ),576 ),buf58 ,buf61 ,buf64 ,primals_6 ,primals_7 ,primals_3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ,64 ),(640 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((128 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((128 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((256 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((256 ,256 ),(256 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "eebd98d3-3b50-4865-af2f-61bbe470ddf1",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['LocalResponseNorm', 'ELU', 'MaxPool2d', 'SmoothL1Loss', 'AdaptiveAvgPool2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.local_response_norm = nn.LocalResponseNorm(size=5)\n        self.elu = nn.ELU(alpha=1.0)\n        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.smooth_l1_loss = nn.SmoothL1Loss()\n        self.adaptive_avg_pool2d = nn.AdaptiveAvgPool2d((1, 1))\n\n    def forward(self, x):\n        # Apply LocalResponseNorm\n        x = self.local_response_norm(x)\n        \n        # Apply ELU activation\n        x = self.elu(x)\n        \n        # Apply MaxPool2d\n        x = self.max_pool2d(x)\n        \n        # Apply AdaptiveAvgPool2d to reduce spatial dimensions to 1x1\n        x = self.adaptive_avg_pool2d(x)\n        \n        # Flatten the tensor for the loss calculation\n        x = x.view(x.size(0), -1)\n        \n        # Create a dummy target tensor for SmoothL1Loss\n        target = torch.zeros_like(x)\n        \n        # Apply SmoothL1Loss\n        loss = self.smooth_l1_loss(x, target)\n        \n        # Return the loss as the output\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Assuming 3 input channels\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_avg_pool3d_constant_pad_nd_div_elu_mul_pow_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp48 =tl .load (in_ptr0 +(x2 ),xmask )\n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +(x2 +((-2 )*ks2 *ks3 )),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tmp6 *tmp6 \n    tmp8 =tl .full (tmp7 .shape ,0.0 ,tmp7 .dtype )\n    tmp9 =tl .where (tmp5 ,tmp7 ,tmp8 )\n    tmp10 =(-1 )+x1 \n    tmp11 =tmp10 >=tmp1 \n    tmp12 =tmp10 <tmp3 \n    tmp13 =tmp11 &tmp12 \n    tmp14 =tl .load (in_ptr0 +(x2 +((-1 )*ks2 *ks3 )),tmp13 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp15 =tmp14 *tmp14 \n    tmp16 =tl .full (tmp15 .shape ,0.0 ,tmp15 .dtype )\n    tmp17 =tl .where (tmp13 ,tmp15 ,tmp16 )\n    tmp18 =tmp17 +tmp9 \n    tmp19 =x1 \n    tmp20 =tmp19 >=tmp1 \n    tmp21 =tmp19 <tmp3 \n    tmp22 =tmp20 &tmp21 \n    tmp23 =tl .load (in_ptr0 +(x2 ),tmp22 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp24 =tmp23 *tmp23 \n    tmp25 =tl .full (tmp24 .shape ,0.0 ,tmp24 .dtype )\n    tmp26 =tl .where (tmp22 ,tmp24 ,tmp25 )\n    tmp27 =tmp26 +tmp18 \n    tmp28 =1 +x1 \n    tmp29 =tmp28 >=tmp1 \n    tmp30 =tmp28 <tmp3 \n    tmp31 =tmp29 &tmp30 \n    tmp32 =tl .load (in_ptr0 +(ks0 +x2 ),tmp31 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp33 =tmp32 *tmp32 \n    tmp34 =tl .full (tmp33 .shape ,0.0 ,tmp33 .dtype )\n    tmp35 =tl .where (tmp31 ,tmp33 ,tmp34 )\n    tmp36 =tmp35 +tmp27 \n    tmp37 =2 +x1 \n    tmp38 =tmp37 >=tmp1 \n    tmp39 =tmp37 <tmp3 \n    tmp40 =tmp38 &tmp39 \n    tmp41 =tl .load (in_ptr0 +(x2 +2 *ks2 *ks3 ),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tmp41 *tmp41 \n    tmp43 =tl .full (tmp42 .shape ,0.0 ,tmp42 .dtype )\n    tmp44 =tl .where (tmp40 ,tmp42 ,tmp43 )\n    tmp45 =tmp44 +tmp36 \n    tmp46 =0.2 \n    tmp47 =tmp45 *tmp46 \n    tmp49 =0.0001 \n    tmp50 =tmp47 *tmp49 \n    tmp51 =1.0 \n    tmp52 =tmp50 +tmp51 \n    tmp53 =0.75 \n    tmp54 =libdevice .pow (tmp52 ,tmp53 )\n    tmp55 =tmp48 /tmp54 \n    tmp56 =0.0 \n    tmp57 =tmp55 >tmp56 \n    tmp58 =tmp55 *tmp51 \n    tmp59 =libdevice .expm1 (tmp58 )\n    tmp60 =tmp59 *tmp51 \n    tmp61 =tl .where (tmp57 ,tmp58 ,tmp60 )\n    tl .store (in_out_ptr0 +(x2 ),tmp61 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_div_elu_max_pool2d_with_indices_mean_mul_pow_1 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =(xindex %8 )\n    x1 =xindex //8 \n    _tmp13 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    x3 =xindex \n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_2 =r0_index \n        tmp0 =r0_2 +x0 *(triton_helpers .div_floor_integer (7 +(ks0 //2 )*(ks1 //2 ),8 ))\n        tmp1 =(ks0 //2 )*(ks1 //2 )\n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(2 *(((r0_2 +x0 *(triton_helpers .div_floor_integer (7 +(ks0 //2 )*(ks1 //2 ),8 )))%(ks1 //2 )))+2 *ks1 *((((r0_2 +x0 *(triton_helpers .div_floor_integer (7 +(ks0 //2 )*(ks1 //2 ),8 )))//(ks1 //2 ))%(ks0 //2 )))+ks0 *ks1 *x1 ),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr0 +(1 +2 *(((r0_2 +x0 *(triton_helpers .div_floor_integer (7 +(ks0 //2 )*(ks1 //2 ),8 )))%(ks1 //2 )))+2 *ks1 *((((r0_2 +x0 *(triton_helpers .div_floor_integer (7 +(ks0 //2 )*(ks1 //2 ),8 )))//(ks1 //2 ))%(ks0 //2 )))+ks0 *ks1 *x1 ),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =triton_helpers .maximum (tmp4 ,tmp3 )\n        tmp6 =tl .load (in_ptr0 +(ks1 +2 *(((r0_2 +x0 *(triton_helpers .div_floor_integer (7 +(ks0 //2 )*(ks1 //2 ),8 )))%(ks1 //2 )))+2 *ks1 *((((r0_2 +x0 *(triton_helpers .div_floor_integer (7 +(ks0 //2 )*(ks1 //2 ),8 )))//(ks1 //2 ))%(ks0 //2 )))+ks0 *ks1 *x1 ),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp7 =triton_helpers .maximum (tmp6 ,tmp5 )\n        tmp8 =tl .load (in_ptr0 +(1 +ks1 +2 *(((r0_2 +x0 *(triton_helpers .div_floor_integer (7 +(ks0 //2 )*(ks1 //2 ),8 )))%(ks1 //2 )))+2 *ks1 *((((r0_2 +x0 *(triton_helpers .div_floor_integer (7 +(ks0 //2 )*(ks1 //2 ),8 )))//(ks1 //2 ))%(ks0 //2 )))+ks0 *ks1 *x1 ),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp9 =triton_helpers .maximum (tmp8 ,tmp7 )\n        tmp10 =tl .full (tmp9 .shape ,0 ,tmp9 .dtype )\n        tmp11 =tl .where (tmp2 ,tmp9 ,tmp10 )\n        tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n        tmp14 =_tmp13 +tmp12 \n        _tmp13 =tl .where (r0_mask &xmask ,tmp14 ,_tmp13 )\n    tmp13 =tl .sum (_tmp13 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x3 ),tmp13 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_div_elu_max_pool2d_with_indices_mean_mul_pow_2 (in_ptr0 ,out_ptr0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =8 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_1 =r0_index \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(r0_1 +8 *x0 ),xmask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (xmask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_smooth_l1_loss_3 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp14 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =(ks0 //2 )*(ks1 //2 )\n        tmp2 =tmp1 .to (tl .float32 )\n        tmp3 =tmp0 /tmp2 \n        tmp4 =tl_math .abs (tmp3 )\n        tmp5 =1.0 \n        tmp6 =tmp4 <tmp5 \n        tmp7 =tmp4 *tmp4 \n        tmp8 =0.5 \n        tmp9 =tmp7 *tmp8 \n        tmp10 =tmp9 *tmp5 \n        tmp11 =tmp4 -tmp8 \n        tmp12 =tl .where (tmp6 ,tmp10 ,tmp11 )\n        tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ,R0_BLOCK ])\n        tmp15 =_tmp14 +tmp13 \n        _tmp14 =tl .where (r0_mask ,tmp15 ,_tmp14 )\n    tmp14 =tl .sum (_tmp14 ,1 )[:,None ]\n    tmp16 =ks2 \n    tmp17 =tmp16 .to (tl .float32 )\n    tmp18 =tmp14 /tmp17 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp18 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        s1 *s2 \n        buf0 =empty_strided_cuda ((1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),torch .float32 )\n        buf1 =reinterpret_tensor (buf0 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ),0 );del buf0 \n\n        triton_poi_fused_add_avg_pool3d_constant_pad_nd_div_elu_mul_pow_0_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_add_avg_pool3d_constant_pad_nd_div_elu_mul_pow_0 [grid (triton_poi_fused_add_avg_pool3d_constant_pad_nd_div_elu_mul_pow_0_xnumel )](buf1 ,arg3_1 ,4096 ,3 ,64 ,64 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf2 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,8 ),(8 *s0 ,8 ,8 *s0 ,8 *s0 ,1 ),torch .float32 )\n\n        triton_red_fused_add_div_elu_max_pool2d_with_indices_mean_mul_pow_1_xnumel =8 *s0 \n        (7 +(s1 //2 )*(s2 //2 ))//8 \n        get_raw_stream (0 )\n        triton_red_fused_add_div_elu_max_pool2d_with_indices_mean_mul_pow_1 [grid (triton_red_fused_add_div_elu_max_pool2d_with_indices_mean_mul_pow_1_xnumel )](buf1 ,buf2 ,64 ,64 ,24 ,128 ,XBLOCK =32 ,R0_BLOCK =8 ,num_warps =2 ,num_stages =1 )\n        del buf1 \n        buf3 =empty_strided_cuda ((1 ,s0 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_div_elu_max_pool2d_with_indices_mean_mul_pow_2 [grid (s0 )](buf2 ,buf3 ,3 ,8 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf2 \n        buf4 =empty_strided_cuda ((),(),torch .float32 )\n        buf5 =buf4 ;del buf4 \n\n        get_raw_stream (0 )\n        triton_red_fused_smooth_l1_loss_3 [grid (1 )](buf5 ,buf3 ,64 ,64 ,3 ,1 ,3 ,XBLOCK =1 ,R0_BLOCK =4 ,num_warps =2 ,num_stages =1 )\n        del buf3 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ef6bee61-c372-4c07-90ae-bc68bd398e12",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveAvgPool2d', 'InstanceNorm2d', 'UpsamplingBilinear2d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((16, 16))\n        self.instance_norm1 = nn.InstanceNorm2d(3)\n        self.instance_norm2 = nn.InstanceNorm2d(3)\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.adaptive_avg_pool2 = nn.AdaptiveAvgPool2d((32, 32))\n        self.instance_norm3 = nn.InstanceNorm2d(3)\n        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, x):\n        x = self.adaptive_avg_pool(x)\n        x = self.instance_norm1(x)\n        x = self.upsample(x)\n        x = self.instance_norm2(x)\n        x = self.adaptive_avg_pool2(x)\n        x = self.instance_norm3(x)\n        x = self.upsample2(x)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Assuming 3 channels for InstanceNorm2d\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__adaptive_avg_pool2d__native_batch_norm_legit_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    r0_numel =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp34_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp34_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp34_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =(r0_index %16 )\n        r0_2 =r0_index //16 \n        r0_3 =r0_index \n        tmp0 =tl .load (in_ptr0 +(4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +(1 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tl .load (in_ptr0 +(2 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tl .load (in_ptr0 +(3 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp7 =tl .load (in_ptr0 +(64 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp9 =tl .load (in_ptr0 +(65 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp11 =tl .load (in_ptr0 +(66 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp13 =tl .load (in_ptr0 +(67 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp15 =tl .load (in_ptr0 +(128 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp17 =tl .load (in_ptr0 +(129 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp19 =tl .load (in_ptr0 +(130 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp21 =tl .load (in_ptr0 +(131 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp23 =tl .load (in_ptr0 +(192 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp25 =tl .load (in_ptr0 +(193 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp27 =tl .load (in_ptr0 +(194 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp29 =tl .load (in_ptr0 +(195 +4 *r0_1 +256 *r0_2 +4096 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp2 =tmp1 +tmp0 \n        tmp4 =tmp3 +tmp2 \n        tmp6 =tmp5 +tmp4 \n        tmp8 =tmp7 +tmp6 \n        tmp10 =tmp9 +tmp8 \n        tmp12 =tmp11 +tmp10 \n        tmp14 =tmp13 +tmp12 \n        tmp16 =tmp15 +tmp14 \n        tmp18 =tmp17 +tmp16 \n        tmp20 =tmp19 +tmp18 \n        tmp22 =tmp21 +tmp20 \n        tmp24 =tmp23 +tmp22 \n        tmp26 =tmp25 +tmp24 \n        tmp28 =tmp27 +tmp26 \n        tmp30 =tmp29 +tmp28 \n        tmp31 =0.0625 \n        tmp32 =tmp30 *tmp31 \n        tmp33 =tl .broadcast_to (tmp32 ,[XBLOCK ,R0_BLOCK ])\n        tmp34_mean_next ,tmp34_m2_next ,tmp34_weight_next =triton_helpers .welford_reduce (\n        tmp33 ,tmp34_mean ,tmp34_m2 ,tmp34_weight ,roffset ==0 \n        )\n        tmp34_mean =tl .where (r0_mask &xmask ,tmp34_mean_next ,tmp34_mean )\n        tmp34_m2 =tl .where (r0_mask &xmask ,tmp34_m2_next ,tmp34_m2 )\n        tmp34_weight =tl .where (r0_mask &xmask ,tmp34_weight_next ,tmp34_weight )\n        tl .store (out_ptr0 +(r0_3 +256 *x0 ),tmp32 ,r0_mask &xmask )\n    tmp37 ,tmp38 ,tmp39 =triton_helpers .welford (tmp34_mean ,tmp34_m2 ,tmp34_weight ,1 )\n    tmp34 =tmp37 [:,None ]\n    tmp35 =tmp38 [:,None ]\n    tmp36 =tmp39 [:,None ]\n    tl .store (out_ptr1 +(x0 ),tmp34 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp35 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__adaptive_avg_pool2d__native_batch_norm_legit__to_copy__unsafe_index_add_arange_clamp_mul_sub_1 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr2 ,out_ptr3 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =3 \n    r0_numel =1024 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp19 =tl .load (in_ptr1 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr2 +(x0 ),xmask ,eviction_policy ='evict_last')\n    tmp56_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp56_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp56_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_2 =r0_index //32 \n        r0_1 =(r0_index %32 )\n        r0_3 =r0_index \n        tmp0 =r0_2 \n        tmp1 =tmp0 .to (tl .float32 )\n        tmp2 =0.4838709677419355 \n        tmp3 =tmp1 *tmp2 \n        tmp4 =0.0 \n        tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n        tmp6 =tmp5 .to (tl .int32 )\n        tmp7 =tl .full ([1 ,1 ],1 ,tl .int64 )\n        tmp8 =tmp6 +tmp7 \n        tmp9 =tl .full ([1 ,1 ],15 ,tl .int64 )\n        tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n        tmp11 =r0_1 \n        tmp12 =tmp11 .to (tl .float32 )\n        tmp13 =tmp12 *tmp2 \n        tmp14 =triton_helpers .maximum (tmp13 ,tmp4 )\n        tmp15 =tmp14 .to (tl .int32 )\n        tmp16 =tmp15 +tmp7 \n        tmp17 =triton_helpers .minimum (tmp16 ,tmp9 )\n        tmp18 =tl .load (in_ptr0 +(tmp17 +16 *tmp10 +256 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp20 =tmp18 -tmp19 \n        tmp22 =256.0 \n        tmp23 =tmp21 /tmp22 \n        tmp24 =1e-05 \n        tmp25 =tmp23 +tmp24 \n        tmp26 =libdevice .rsqrt (tmp25 )\n        tmp27 =tmp20 *tmp26 \n        tmp28 =tl .load (in_ptr0 +(tmp15 +16 *tmp10 +256 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp29 =tmp28 -tmp19 \n        tmp30 =tmp29 *tmp26 \n        tmp31 =tmp27 -tmp30 \n        tmp32 =tmp15 .to (tl .float32 )\n        tmp33 =tmp14 -tmp32 \n        tmp34 =triton_helpers .maximum (tmp33 ,tmp4 )\n        tmp35 =1.0 \n        tmp36 =triton_helpers .minimum (tmp34 ,tmp35 )\n        tmp37 =tmp31 *tmp36 \n        tmp38 =tmp30 +tmp37 \n        tmp39 =tl .load (in_ptr0 +(tmp17 +16 *tmp6 +256 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp40 =tmp39 -tmp19 \n        tmp41 =tmp40 *tmp26 \n        tmp42 =tl .load (in_ptr0 +(tmp15 +16 *tmp6 +256 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last')\n        tmp43 =tmp42 -tmp19 \n        tmp44 =tmp43 *tmp26 \n        tmp45 =tmp41 -tmp44 \n        tmp46 =tmp45 *tmp36 \n        tmp47 =tmp44 +tmp46 \n        tmp48 =tmp38 -tmp47 \n        tmp49 =tmp6 .to (tl .float32 )\n        tmp50 =tmp5 -tmp49 \n        tmp51 =triton_helpers .maximum (tmp50 ,tmp4 )\n        tmp52 =triton_helpers .minimum (tmp51 ,tmp35 )\n        tmp53 =tmp48 *tmp52 \n        tmp54 =tmp47 +tmp53 \n        tmp55 =tl .broadcast_to (tmp54 ,[XBLOCK ,R0_BLOCK ])\n        tmp56_mean_next ,tmp56_m2_next ,tmp56_weight_next =triton_helpers .welford_reduce (\n        tmp55 ,tmp56_mean ,tmp56_m2 ,tmp56_weight ,roffset ==0 \n        )\n        tmp56_mean =tl .where (r0_mask &xmask ,tmp56_mean_next ,tmp56_mean )\n        tmp56_m2 =tl .where (r0_mask &xmask ,tmp56_m2_next ,tmp56_m2 )\n        tmp56_weight =tl .where (r0_mask &xmask ,tmp56_weight_next ,tmp56_weight )\n        tl .store (in_out_ptr0 +(r0_3 +1024 *x0 ),tmp38 ,r0_mask &xmask )\n        tl .store (in_out_ptr1 +(r0_3 +1024 *x0 ),tmp47 ,r0_mask &xmask )\n    tmp59 ,tmp60 ,tmp61 =triton_helpers .welford (tmp56_mean ,tmp56_m2 ,tmp56_weight ,1 )\n    tmp56 =tmp59 [:,None ]\n    tmp57 =tmp60 [:,None ]\n    tmp58 =tmp61 [:,None ]\n    tmp87_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp87_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp87_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_3 =r0_index \n        r0_2 =r0_index //32 \n        tmp62 =tl .load (in_out_ptr1 +(r0_3 +1024 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp63 =tl .load (in_out_ptr0 +(r0_3 +1024 *x0 ),r0_mask &xmask ,eviction_policy ='evict_first',other =0.0 )\n        tmp64 =tmp63 -tmp62 \n        tmp65 =r0_2 \n        tmp66 =tmp65 .to (tl .float32 )\n        tmp67 =0.4838709677419355 \n        tmp68 =tmp66 *tmp67 \n        tmp69 =0.0 \n        tmp70 =triton_helpers .maximum (tmp68 ,tmp69 )\n        tmp71 =tmp70 .to (tl .int32 )\n        tmp72 =tmp71 .to (tl .float32 )\n        tmp73 =tmp70 -tmp72 \n        tmp74 =triton_helpers .maximum (tmp73 ,tmp69 )\n        tmp75 =1.0 \n        tmp76 =triton_helpers .minimum (tmp74 ,tmp75 )\n        tmp77 =tmp64 *tmp76 \n        tmp78 =tmp62 +tmp77 \n        tmp79 =tmp78 -tmp56 \n        tmp80 =1024.0 \n        tmp81 =tmp57 /tmp80 \n        tmp82 =1e-05 \n        tmp83 =tmp81 +tmp82 \n        tmp84 =libdevice .rsqrt (tmp83 )\n        tmp85 =tmp79 *tmp84 \n        tmp86 =tl .broadcast_to (tmp85 ,[XBLOCK ,R0_BLOCK ])\n        tmp87_mean_next ,tmp87_m2_next ,tmp87_weight_next =triton_helpers .welford_reduce (\n        tmp86 ,tmp87_mean ,tmp87_m2 ,tmp87_weight ,roffset ==0 \n        )\n        tmp87_mean =tl .where (r0_mask &xmask ,tmp87_mean_next ,tmp87_mean )\n        tmp87_m2 =tl .where (r0_mask &xmask ,tmp87_m2_next ,tmp87_m2 )\n        tmp87_weight =tl .where (r0_mask &xmask ,tmp87_weight_next ,tmp87_weight )\n        tl .store (in_out_ptr1 +(r0_3 +1024 *x0 ),tmp85 ,r0_mask &xmask )\n    tmp90 ,tmp91 ,tmp92 =triton_helpers .welford (tmp87_mean ,tmp87_m2 ,tmp87_weight ,1 )\n    tmp87 =tmp90 [:,None ]\n    tmp88 =tmp91 [:,None ]\n    tmp89 =tmp92 [:,None ]\n    tl .store (out_ptr2 +(x0 ),tmp87 ,xmask )\n    tl .store (out_ptr3 +(x0 ),tmp88 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d__native_batch_norm_legit__to_copy__unsafe_index_add_arange_clamp_mul_sub_2 (in_out_ptr1 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    x1 =((xindex //64 )%64 )\n    x0 =(xindex %64 )\n    x2 =xindex //4096 \n    x4 =xindex \n    tmp19 =tl .load (in_ptr1 +(x2 ),None ,eviction_policy ='evict_last')\n    tmp21 =tl .load (in_ptr2 +(x2 ),None ,eviction_policy ='evict_last')\n    tmp0 =x1 \n    tmp1 =tmp0 .to (tl .float32 )\n    tmp2 =0.49206349206349204 \n    tmp3 =tmp1 *tmp2 \n    tmp4 =0.0 \n    tmp5 =triton_helpers .maximum (tmp3 ,tmp4 )\n    tmp6 =tmp5 .to (tl .int32 )\n    tmp7 =tl .full ([1 ],1 ,tl .int64 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],31 ,tl .int64 )\n    tmp10 =triton_helpers .minimum (tmp8 ,tmp9 )\n    tmp11 =x0 \n    tmp12 =tmp11 .to (tl .float32 )\n    tmp13 =tmp12 *tmp2 \n    tmp14 =triton_helpers .maximum (tmp13 ,tmp4 )\n    tmp15 =tmp14 .to (tl .int32 )\n    tmp16 =tmp15 +tmp7 \n    tmp17 =triton_helpers .minimum (tmp16 ,tmp9 )\n    tmp18 =tl .load (in_ptr0 +(tmp17 +32 *tmp10 +1024 *x2 ),None ,eviction_policy ='evict_last')\n    tmp20 =tmp18 -tmp19 \n    tmp22 =1024.0 \n    tmp23 =tmp21 /tmp22 \n    tmp24 =1e-05 \n    tmp25 =tmp23 +tmp24 \n    tmp26 =libdevice .rsqrt (tmp25 )\n    tmp27 =tmp20 *tmp26 \n    tmp28 =tl .load (in_ptr0 +(tmp15 +32 *tmp10 +1024 *x2 ),None ,eviction_policy ='evict_last')\n    tmp29 =tmp28 -tmp19 \n    tmp30 =tmp29 *tmp26 \n    tmp31 =tmp27 -tmp30 \n    tmp32 =tmp15 .to (tl .float32 )\n    tmp33 =tmp14 -tmp32 \n    tmp34 =triton_helpers .maximum (tmp33 ,tmp4 )\n    tmp35 =1.0 \n    tmp36 =triton_helpers .minimum (tmp34 ,tmp35 )\n    tmp37 =tmp31 *tmp36 \n    tmp38 =tmp30 +tmp37 \n    tmp39 =tl .load (in_ptr0 +(tmp17 +32 *tmp6 +1024 *x2 ),None ,eviction_policy ='evict_last')\n    tmp40 =tmp39 -tmp19 \n    tmp41 =tmp40 *tmp26 \n    tmp42 =tl .load (in_ptr0 +(tmp15 +32 *tmp6 +1024 *x2 ),None ,eviction_policy ='evict_last')\n    tmp43 =tmp42 -tmp19 \n    tmp44 =tmp43 *tmp26 \n    tmp45 =tmp41 -tmp44 \n    tmp46 =tmp45 *tmp36 \n    tmp47 =tmp44 +tmp46 \n    tmp48 =tmp38 -tmp47 \n    tmp49 =tmp6 .to (tl .float32 )\n    tmp50 =tmp5 -tmp49 \n    tmp51 =triton_helpers .maximum (tmp50 ,tmp4 )\n    tmp52 =triton_helpers .minimum (tmp51 ,tmp35 )\n    tmp53 =tmp48 *tmp52 \n    tmp54 =tmp47 +tmp53 \n    tl .store (in_out_ptr1 +(x4 ),tmp54 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    assert_size_stride (arg2_1 ,(1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,3 ,16 ,16 ),(768 ,256 ,16 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n        buf2 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__adaptive_avg_pool2d__native_batch_norm_legit_0 [grid (3 )](arg2_1 ,buf0 ,buf1 ,buf2 ,3 ,256 ,XBLOCK =1 ,R0_BLOCK =256 ,num_warps =2 ,num_stages =1 )\n        del arg2_1 \n        buf4 =empty_strided_cuda ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),torch .float32 )\n        buf5 =buf4 ;del buf4 \n        buf6 =buf5 ;del buf5 \n        buf7 =empty_strided_cuda ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),torch .float32 )\n        buf8 =buf7 ;del buf7 \n        buf12 =buf8 ;del buf8 \n        buf13 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n        buf14 =empty_strided_cuda ((1 ,3 ,1 ,1 ),(3 ,1 ,3 ,3 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_red_fused__adaptive_avg_pool2d__native_batch_norm_legit__to_copy__unsafe_index_add_arange_clamp_mul_sub_1 [grid (3 )](buf6 ,buf12 ,buf0 ,buf1 ,buf2 ,buf13 ,buf14 ,3 ,1024 ,XBLOCK =1 ,R0_BLOCK =1024 ,num_warps =8 ,num_stages =1 )\n        del buf0 \n        del buf1 \n        del buf2 \n        del buf6 \n        buf19 =empty_strided_cuda ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),torch .float32 )\n        buf20 =buf19 ;del buf19 \n        buf21 =buf20 ;del buf20 \n\n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d__native_batch_norm_legit__to_copy__unsafe_index_add_arange_clamp_mul_sub_2 [grid (12288 )](buf21 ,buf12 ,buf13 ,buf14 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf12 \n        del buf13 \n        del buf14 \n    return (buf21 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =64 \n    arg1_1 =64 \n    arg2_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f24ca04a-7b46-4fa8-993c-4a7c66ffc48e",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxUnpool2d', 'AvgPool2d', 'GRU', 'GELU', 'LPPool1d', 'Flatten', 'Dropout']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.gru = nn.GRU(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n        self.gelu = nn.GELU()\n        self.lp_pool = nn.LPPool1d(norm_type=2, kernel_size=2, stride=2)\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        # Assuming x is a 4D tensor (batch, channels, height, width)\n        # Apply MaxUnpool2d (requires indices from a previous MaxPool2d operation)\n        # Since we don't have a MaxPool2d in the list, we'll simulate it\n        pool_output, indices = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool(pool_output, indices)\n        \n        # Apply AvgPool2d\n        x = self.avg_pool(x)\n        \n        # Reshape for GRU (assuming we want to treat the spatial dimensions as sequence)\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, -1).transpose(1, 2)  # (batch, seq_len, features)\n        \n        # Apply GRU\n        x, _ = self.gru(x)\n        \n        # Apply GELU\n        x = self.gelu(x)\n        \n        # Reshape for LPPool1d\n        x = x.transpose(1, 2)  # (batch, features, seq_len)\n        \n        # Apply LPPool1d\n        x = self.lp_pool(x)\n        \n        # Flatten the output\n        x = self.flatten(x)\n        \n        # Apply Dropout\n        x = self.dropout(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input: batch_size=1, channels=3, height=64, width=64\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_max_unpool2d_1 (in_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp7 =tl .load (in_ptr0 +(ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp12 =tl .load (in_ptr0 +(1 +ks4 +2 *x0 +2 *ks4 *x1 +ks3 *ks4 *x2 ),xmask ,eviction_policy ='evict_last')\n    tmp35 =tl .load (in_ptr0 +(2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp36 =tl .load (in_ptr0 +(1 +2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp38 =tl .load (in_ptr0 +(ks4 +2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp40 =tl .load (in_ptr0 +(1 +ks4 +2 *((x3 %ks0 ))+2 *ks4 *(((x3 //ks0 )%ks1 ))+ks3 *ks4 *(x3 //ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 >tmp0 \n    tmp3 =tl .full ([1 ],1 ,tl .int8 )\n    tmp4 =tl .full ([1 ],0 ,tl .int8 )\n    tmp5 =tl .where (tmp2 ,tmp3 ,tmp4 )\n    tmp6 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp8 =tmp7 >tmp6 \n    tmp9 =tl .full ([1 ],2 ,tl .int8 )\n    tmp10 =tl .where (tmp8 ,tmp9 ,tmp5 )\n    tmp11 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp13 =tmp12 >tmp11 \n    tmp14 =tl .full ([1 ],3 ,tl .int8 )\n    tmp15 =tl .where (tmp13 ,tmp14 ,tmp10 )\n    triton_helpers .maximum (tmp12 ,tmp11 )\n    tmp17 =tl .full ([1 ],2 ,tl .int32 )\n    tmp18 =tl .where ((tmp15 <0 )!=(tmp17 <0 ),tl .where (tmp15 %tmp17 !=0 ,tmp15 //tmp17 -1 ,tmp15 //tmp17 ),tmp15 //tmp17 )\n    tmp19 =tmp18 *tmp17 \n    tmp20 =tmp15 -tmp19 \n    tmp21 =2 *x1 \n    tmp22 =tmp21 +tmp18 \n    tmp23 =2 *x0 \n    tmp24 =tmp23 +tmp20 \n    tmp25 =ks4 \n    tmp26 =tmp22 *tmp25 \n    tmp27 =tmp26 +tmp24 \n    tmp28 =4 *ks0 *ks1 *x2 \n    tmp29 =tmp27 +tmp28 \n    tmp30 =4 *ks0 *ks1 *ks5 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =tmp29 <0 \n    tmp33 =tl .where (tmp32 ,tmp31 ,tmp29 )\n    tl .device_assert (((0 <=tmp33 )&(tmp33 <4 *ks5 *(ks3 //2 )*(ks4 //2 )))|~(xmask ),\"index out of bounds: 0 <= tmp33 < 4*ks5*(ks3 // 2)*(ks4 // 2)\")\n    tmp37 =triton_helpers .maximum (tmp36 ,tmp35 )\n    tmp39 =triton_helpers .maximum (tmp38 ,tmp37 )\n    tmp41 =triton_helpers .maximum (tmp40 ,tmp39 )\n    tl .store (out_ptr1 +(tl .broadcast_to ((tmp33 %(4 *ks0 *ks1 *ks5 )),[XBLOCK ])),tmp41 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_avg_pool2d_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks0 *((((x0 +2 *ks0 *x1 )//ks0 )%(2 *ks1 )))+4 *ks0 *ks1 *((((x0 +2 *ks0 *x1 +2 *ks0 *ks1 *x2 )//(2 *ks0 *ks1 ))%ks3 ))),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(2 *ks0 *((((1 +2 *x0 +4 *ks0 *x1 )//(2 *ks0 ))%(2 *ks1 )))+4 *ks0 *ks1 *((((1 +2 *x0 +4 *ks0 *x1 +4 *ks0 *ks1 *x2 )//(4 *ks0 *ks1 ))%ks3 ))+(((1 +2 *x0 )%(2 *ks0 )))),xmask ,eviction_policy ='evict_last')\n    tmp3 =tl .load (in_ptr0 +(2 *x0 +2 *ks0 *((((ks0 +x0 +2 *ks0 *x1 )//ks0 )%(2 *ks1 )))+4 *ks0 *ks1 *((((ks0 +x0 +2 *ks0 *x1 +2 *ks0 *ks1 *x2 )//(2 *ks0 *ks1 ))%ks3 ))),xmask ,eviction_policy ='evict_last')\n    tmp5 =tl .load (in_ptr0 +(2 *ks0 *((((1 +2 *ks0 +2 *x0 +4 *ks0 *x1 )//(2 *ks0 ))%(2 *ks1 )))+4 *ks0 *ks1 *((((1 +2 *ks0 +2 *x0 +4 *ks0 *x1 +4 *ks0 *ks1 *x2 )//(4 *ks0 *ks1 ))%ks3 ))+(((1 +2 *x0 )%(2 *ks0 )))),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp1 +tmp0 \n    tmp4 =tmp3 +tmp2 \n    tmp6 =tmp5 +tmp4 \n    tmp7 =0.25 \n    tmp8 =tmp6 *tmp7 \n    tl .store (out_ptr0 +(x3 ),tmp8 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,s0 ,2 *(s1 //2 ),2 *(s2 //2 )),(4 *s0 *(s1 //2 )*(s2 //2 ),4 *(s1 //2 )*(s2 //2 ),2 *(s2 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_0_xnumel =4 *s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_0 [grid (triton_poi_fused_max_unpool2d_0_xnumel )](buf1 ,12288 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        s2 //2 \n        s1 //2 \n        (s1 //2 )*(s2 //2 )\n\n        triton_poi_fused_max_pool2d_with_indices_max_unpool2d_1_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_max_unpool2d_1 [grid (triton_poi_fused_max_pool2d_with_indices_max_unpool2d_1_xnumel )](arg3_1 ,buf1 ,32 ,32 ,1024 ,64 ,64 ,3 ,3072 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf3 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ),(s0 *(s1 //2 )*(s2 //2 ),(s1 //2 )*(s2 //2 ),s2 //2 ,1 ),torch .float32 )\n\n        triton_poi_fused_avg_pool2d_2_xnumel =s0 *(s1 //2 )*(s2 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused_avg_pool2d_2 [grid (triton_poi_fused_avg_pool2d_2_xnumel )](buf1 ,buf3 ,32 ,32 ,1024 ,3 ,3072 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n    return (reinterpret_tensor (buf3 ,(1 ,(s1 //2 )*(s2 //2 ),s0 ),(s0 *(s1 //2 )*(s2 //2 ),1 ,(s1 //2 )*(s2 //2 )),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f2986b54-91fb-42b7-b8ef-2acf501599f7",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['RNNCell', 'TransformerEncoder', 'ZeroPad3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.rnn_cell1 = nn.RNNCell(input_size=128, hidden_size=256)\n        self.rnn_cell2 = nn.RNNCell(input_size=256, hidden_size=512)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=512, nhead=8), num_layers=3\n        )\n        self.zero_pad3d = nn.ZeroPad3d(padding=(1, 1, 1, 1, 1, 1))\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, sequence_length, input_size)\n        batch_size, sequence_length, input_size = x.size()\n        \n        # Initialize hidden states for RNN cells\n        h1 = torch.zeros(batch_size, 256).to(x.device)\n        h2 = torch.zeros(batch_size, 512).to(x.device)\n        \n        # Process the sequence through RNN cells\n        for t in range(sequence_length):\n            h1 = self.rnn_cell1(x[:, t, :], h1)\n            h2 = self.rnn_cell2(h1, h2)\n        \n        # Reshape for TransformerEncoder\n        h2 = h2.unsqueeze(0)  # Add sequence dimension\n        h2 = self.transformer_encoder(h2)\n        \n        # Reshape for ZeroPad3d\n        h2 = h2.unsqueeze(1).unsqueeze(1)  # Add dummy spatial dimensions\n        h2 = self.zero_pad3d(h2)\n        \n        # Flatten the output\n        output = h2.view(batch_size, -1)\n        \n        return output\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 128).cuda()  # (batch_size, sequence_length, input_size)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_0 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =256 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =512 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_tanh_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =256 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tl .store (in_out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_addmm_tanh_3 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =512 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_out_ptr0 +(x0 ),xmask )\n    tmp1 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp3 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp4 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp2 =tmp0 +tmp1 \n    tmp5 =tmp3 +tmp4 \n    tmp6 =tmp2 +tmp5 \n    tmp7 =libdevice .tanh (tmp6 )\n    tl .store (in_out_ptr0 +(x0 ),tmp7 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,out_ptr1 ,out_ptr4 ,out_ptr5 ,load_seed_offset ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =512 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp5 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp7 =tl .load (in_out_ptr0 +(r0_0 ),None )\n    tmp8 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp34 =tl .load (in_ptr3 +(r0_0 ),None )\n    tmp36 =tl .load (in_ptr4 +(r0_0 ),None )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[R0_BLOCK ])\n    tmp16 =tl .broadcast_to (tmp14 ,[R0_BLOCK ])\n    tmp18 =triton_helpers .promote_to_tensor (tl .sum (tmp16 ,0 ))\n    tmp19 =tl .full ([1 ],512 ,tl .int32 )\n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =tmp18 /tmp20 \n    tmp22 =tmp14 -tmp21 \n    tmp23 =tmp22 *tmp22 \n    tmp24 =tl .broadcast_to (tmp23 ,[R0_BLOCK ])\n    tmp26 =triton_helpers .promote_to_tensor (tl .sum (tmp24 ,0 ))\n    tmp27 =tmp13 -tmp21 \n    tmp28 =512.0 \n    tmp29 =tmp26 /tmp28 \n    tmp30 =1e-05 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =libdevice .rsqrt (tmp31 )\n    tmp33 =tmp27 *tmp32 \n    tmp35 =tmp33 *tmp34 \n    tmp37 =tmp35 +tmp36 \n    tmp38 =0.001953125 \n    tmp39 =tmp32 *tmp38 \n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp4 ,None )\n    tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp33 ,None )\n    tl .store (out_ptr4 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp37 ,None )\n    tl .store (out_ptr5 +(tl .full ([1 ],0 ,tl .int32 )),tmp39 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_threshold_backward_5 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =2048 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp6 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp7 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.0 \n    tmp15 =tmp10 <=tmp14 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr3 +(x0 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_native_dropout_relu_threshold_backward_6 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =2048 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp6 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp7 =tl .load (in_ptr2 +(x0 ),xmask )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp8 =tmp6 +tmp7 \n    tmp9 =tl .full ([1 ],0 ,tl .int32 )\n    tmp10 =triton_helpers .maximum (tmp9 ,tmp8 )\n    tmp11 =tmp5 *tmp10 \n    tmp12 =1.1111111111111112 \n    tmp13 =tmp11 *tmp12 \n    tmp14 =0.0 \n    tmp15 =tmp10 <=tmp14 \n    tl .store (out_ptr1 +(x0 ),tmp4 ,xmask )\n    tl .store (out_ptr2 +(x0 ),tmp13 ,xmask )\n    tl .store (out_ptr3 +(x0 ),tmp15 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_7 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr1 ,out_ptr4 ,load_seed_offset ,xnumel ,r0_numel ):\n    XBLOCK :tl .constexpr =1 \n    R0_BLOCK :tl .constexpr =512 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    tl .full ([1 ],xoffset ,tl .int32 )\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[:]\n    tl .full ([R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp5 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp7 =tl .load (in_out_ptr0 +(r0_0 ),None )\n    tmp8 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =r0_0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tmp3 =0.1 \n    tmp4 =tmp2 >tmp3 \n    tmp6 =tmp4 .to (tl .float32 )\n    tmp9 =tmp7 +tmp8 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =1.1111111111111112 \n    tmp12 =tmp10 *tmp11 \n    tmp13 =tmp5 +tmp12 \n    tmp14 =tl .broadcast_to (tmp13 ,[R0_BLOCK ])\n    tmp16 =tl .broadcast_to (tmp14 ,[R0_BLOCK ])\n    tmp18 =triton_helpers .promote_to_tensor (tl .sum (tmp16 ,0 ))\n    tmp19 =tl .full ([1 ],512 ,tl .int32 )\n    tmp20 =tmp19 .to (tl .float32 )\n    tmp21 =tmp18 /tmp20 \n    tmp22 =tmp14 -tmp21 \n    tmp23 =tmp22 *tmp22 \n    tmp24 =tl .broadcast_to (tmp23 ,[R0_BLOCK ])\n    tmp26 =triton_helpers .promote_to_tensor (tl .sum (tmp24 ,0 ))\n    tmp27 =tmp13 -tmp21 \n    tmp28 =512.0 \n    tmp29 =tmp26 /tmp28 \n    tmp30 =1e-05 \n    tmp31 =tmp29 +tmp30 \n    tmp32 =libdevice .rsqrt (tmp31 )\n    tmp33 =tmp27 *tmp32 \n    tmp34 =0.001953125 \n    tmp35 =tmp32 *tmp34 \n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp4 ,None )\n    tl .store (in_out_ptr0 +(tl .broadcast_to (r0_0 ,[R0_BLOCK ])),tmp33 ,None )\n    tl .store (out_ptr4 +(tl .full ([1 ],0 ,tl .int32 )),tmp35 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_8 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =4626 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex //1542 \n    x1 =((xindex //514 )%3 )\n    x0 =(xindex %514 )\n    x5 =xindex \n    tmp0 =(-1 )+x2 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =tl .full ([1 ],1 ,tl .int64 )\n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+x1 \n    tmp6 =tmp5 >=tmp1 \n    tmp7 =tmp5 <tmp3 \n    tmp8 =(-1 )+x0 \n    tmp9 =tmp8 >=tmp1 \n    tmp10 =tl .full ([1 ],512 ,tl .int64 )\n    tmp11 =tmp8 <tmp10 \n    tmp12 =tmp2 &tmp4 \n    tmp13 =tmp12 &tmp6 \n    tmp14 =tmp13 &tmp7 \n    tmp15 =tmp14 &tmp9 \n    tmp16 =tmp15 &tmp11 \n    tmp17 =tl .load (in_ptr0 +((-1 )+x0 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp18 =tl .load (in_ptr1 +((-1 )+x0 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp19 =tmp17 *tmp18 \n    tmp20 =tl .load (in_ptr2 +((-1 )+x0 ),tmp16 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp21 =tmp19 +tmp20 \n    tmp22 =tl .full (tmp21 .shape ,0.0 ,tmp21 .dtype )\n    tmp23 =tl .where (tmp16 ,tmp21 ,tmp22 )\n    tl .store (out_ptr0 +(x5 ),tmp23 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 ,primals_27 ,primals_28 ,primals_29 ,primals_30 ,primals_31 ,primals_32 ,primals_33 ,primals_34 ,primals_35 ,primals_36 ,primals_37 ,primals_38 ,primals_39 ,primals_40 ,primals_41 ,primals_42 ,primals_43 ,primals_44 ,primals_45 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,10 ,128 ),(1280 ,128 ,1 ))\n    assert_size_stride (primals_2 ,(256 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_3 ,(256 ,256 ),(256 ,1 ))\n    assert_size_stride (primals_4 ,(256 ,),(1 ,))\n    assert_size_stride (primals_5 ,(256 ,),(1 ,))\n    assert_size_stride (primals_6 ,(512 ,256 ),(256 ,1 ))\n    assert_size_stride (primals_7 ,(512 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_8 ,(512 ,),(1 ,))\n    assert_size_stride (primals_9 ,(512 ,),(1 ,))\n    assert_size_stride (primals_10 ,(1536 ,),(1 ,))\n    assert_size_stride (primals_11 ,(1536 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_12 ,(512 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_13 ,(512 ,),(1 ,))\n    assert_size_stride (primals_14 ,(512 ,),(1 ,))\n    assert_size_stride (primals_15 ,(512 ,),(1 ,))\n    assert_size_stride (primals_16 ,(2048 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_17 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_18 ,(512 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_19 ,(512 ,),(1 ,))\n    assert_size_stride (primals_20 ,(512 ,),(1 ,))\n    assert_size_stride (primals_21 ,(512 ,),(1 ,))\n    assert_size_stride (primals_22 ,(1536 ,),(1 ,))\n    assert_size_stride (primals_23 ,(1536 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_24 ,(512 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_25 ,(512 ,),(1 ,))\n    assert_size_stride (primals_26 ,(512 ,),(1 ,))\n    assert_size_stride (primals_27 ,(512 ,),(1 ,))\n    assert_size_stride (primals_28 ,(2048 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_29 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_30 ,(512 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_31 ,(512 ,),(1 ,))\n    assert_size_stride (primals_32 ,(512 ,),(1 ,))\n    assert_size_stride (primals_33 ,(512 ,),(1 ,))\n    assert_size_stride (primals_34 ,(1536 ,),(1 ,))\n    assert_size_stride (primals_35 ,(1536 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_36 ,(512 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_37 ,(512 ,),(1 ,))\n    assert_size_stride (primals_38 ,(512 ,),(1 ,))\n    assert_size_stride (primals_39 ,(512 ,),(1 ,))\n    assert_size_stride (primals_40 ,(2048 ,512 ),(512 ,1 ))\n    assert_size_stride (primals_41 ,(2048 ,),(1 ,))\n    assert_size_stride (primals_42 ,(512 ,2048 ),(2048 ,1 ))\n    assert_size_stride (primals_43 ,(512 ,),(1 ,))\n    assert_size_stride (primals_44 ,(512 ,),(1 ,))\n    assert_size_stride (primals_45 ,(512 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_0 [grid (256 )](buf0 ,256 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf1 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_1 [grid (512 )](buf1 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf0 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),0 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf3 )\n        buf4 =buf2 ;del buf2 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf4 ,primals_5 ,buf3 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf5 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf1 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf5 )\n        buf6 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf4 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf6 )\n        buf7 =buf5 ;del buf5 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf7 ,primals_9 ,buf6 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf8 =buf3 ;del buf3 \n\n        extern_kernels .mm (buf4 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf8 )\n        buf9 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),128 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf9 )\n        buf10 =buf8 ;del buf8 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf10 ,primals_5 ,buf9 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf11 =buf6 ;del buf6 \n\n        extern_kernels .mm (buf7 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf11 )\n        buf12 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf10 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf12 )\n        buf13 =buf11 ;del buf11 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf13 ,primals_9 ,buf12 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf14 =buf9 ;del buf9 \n\n        extern_kernels .mm (buf10 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf14 )\n        buf15 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),256 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf15 )\n        buf16 =buf14 ;del buf14 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf16 ,primals_5 ,buf15 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf17 =buf12 ;del buf12 \n\n        extern_kernels .mm (buf13 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf17 )\n        buf18 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf16 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf18 )\n        buf19 =buf17 ;del buf17 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf19 ,primals_9 ,buf18 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf20 =buf15 ;del buf15 \n\n        extern_kernels .mm (buf16 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf20 )\n        buf21 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),384 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf21 )\n        buf22 =buf20 ;del buf20 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf22 ,primals_5 ,buf21 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf23 =buf18 ;del buf18 \n\n        extern_kernels .mm (buf19 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf23 )\n        buf24 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf22 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf24 )\n        buf25 =buf23 ;del buf23 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf25 ,primals_9 ,buf24 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf26 =buf21 ;del buf21 \n\n        extern_kernels .mm (buf22 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf26 )\n        buf27 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),512 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf27 )\n        buf28 =buf26 ;del buf26 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf28 ,primals_5 ,buf27 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf29 =buf24 ;del buf24 \n\n        extern_kernels .mm (buf25 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf29 )\n        buf30 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf28 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf30 )\n        buf31 =buf29 ;del buf29 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf31 ,primals_9 ,buf30 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf32 =buf27 ;del buf27 \n\n        extern_kernels .mm (buf28 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf32 )\n        buf33 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),640 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf33 )\n        buf34 =buf32 ;del buf32 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf34 ,primals_5 ,buf33 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf35 =buf30 ;del buf30 \n\n        extern_kernels .mm (buf31 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf35 )\n        buf36 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf34 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf36 )\n        buf37 =buf35 ;del buf35 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf37 ,primals_9 ,buf36 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf38 =buf33 ;del buf33 \n\n        extern_kernels .mm (buf34 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf38 )\n        buf39 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),768 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf39 )\n        buf40 =buf38 ;del buf38 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf40 ,primals_5 ,buf39 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf41 =buf36 ;del buf36 \n\n        extern_kernels .mm (buf37 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf41 )\n        buf42 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf40 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf42 )\n        buf43 =buf41 ;del buf41 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf43 ,primals_9 ,buf42 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf44 =buf39 ;del buf39 \n\n        extern_kernels .mm (buf40 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf44 )\n        buf45 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),896 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf45 )\n        buf46 =buf44 ;del buf44 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf46 ,primals_5 ,buf45 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf47 =buf42 ;del buf42 \n\n        extern_kernels .mm (buf43 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf47 )\n        buf48 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf46 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf48 )\n        buf49 =buf47 ;del buf47 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf49 ,primals_9 ,buf48 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf50 =buf45 ;del buf45 \n\n        extern_kernels .mm (buf46 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf50 )\n        buf51 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),1024 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf51 )\n        buf52 =buf50 ;del buf50 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf52 ,primals_5 ,buf51 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        buf53 =buf48 ;del buf48 \n\n        extern_kernels .mm (buf49 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf53 )\n        buf54 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf52 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf54 )\n        buf55 =buf53 ;del buf53 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf55 ,primals_9 ,buf54 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf56 =buf51 ;del buf51 \n\n        extern_kernels .mm (buf52 ,reinterpret_tensor (primals_3 ,(256 ,256 ),(1 ,256 ),0 ),out =buf56 )\n        buf57 =empty_strided_cuda ((1 ,256 ),(256 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (primals_1 ,(1 ,128 ),(128 ,1 ),1152 ),reinterpret_tensor (primals_2 ,(128 ,256 ),(1 ,128 ),0 ),out =buf57 )\n        del primals_2 \n        buf58 =buf56 ;del buf56 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_2 [grid (256 )](buf58 ,primals_5 ,buf57 ,primals_4 ,256 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf57 \n        del primals_4 \n        del primals_5 \n        buf59 =buf54 ;del buf54 \n\n        extern_kernels .mm (buf55 ,reinterpret_tensor (primals_7 ,(512 ,512 ),(1 ,512 ),0 ),out =buf59 )\n        buf60 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf58 ,reinterpret_tensor (primals_6 ,(256 ,512 ),(1 ,256 ),0 ),out =buf60 )\n        buf61 =buf59 ;del buf59 \n\n        get_raw_stream (0 )\n        triton_poi_fused_add_addmm_tanh_3 [grid (512 )](buf61 ,primals_9 ,buf60 ,primals_8 ,512 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_8 \n        del primals_9 \n        buf62 =empty_strided_cuda ((1 ,1536 ),(1536 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_10 ,buf61 ,reinterpret_tensor (primals_11 ,(512 ,1536 ),(1 ,512 ),0 ),alpha =1 ,beta =1 ,out =buf62 )\n        del primals_10 \n\n        buf63 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf62 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),0 ),reinterpret_tensor (buf62 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),512 ),reinterpret_tensor (buf62 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),1024 ),None ,True ,0.1 )\n        buf64 =buf63 [0 ]\n        buf65 =buf63 [1 ]\n        buf66 =buf63 [2 ]\n        buf67 =buf63 [3 ]\n        del buf63 \n        buf68 =buf60 ;del buf60 \n\n        extern_kernels .mm (reinterpret_tensor (buf64 ,(1 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_12 ,(512 ,512 ),(1 ,512 ),0 ),out =buf68 )\n        buf69 =empty_strided_cuda ((9 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[9 ],out =buf69 )\n        buf71 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .bool )\n        buf75 =reinterpret_tensor (buf68 ,(1 ,1 ,512 ),(512 ,512 ,1 ),0 );del buf68 \n        buf76 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .float32 )\n        buf149 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 [grid (1 )](buf75 ,buf69 ,buf61 ,primals_13 ,primals_14 ,primals_15 ,buf71 ,buf76 ,buf149 ,6 ,1 ,512 ,num_warps =4 ,num_stages =1 )\n        del primals_13 \n        del primals_15 \n        buf77 =empty_strided_cuda ((1 ,2048 ),(2048 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf76 ,(1 ,512 ),(0 ,1 ),0 ),reinterpret_tensor (primals_16 ,(512 ,2048 ),(1 ,512 ),0 ),out =buf77 )\n        buf79 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n        buf80 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .float32 )\n        buf143 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_5 [grid (2048 )](buf69 ,buf77 ,primals_17 ,buf79 ,buf80 ,buf143 ,1 ,2048 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_17 \n        buf81 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf80 ,(1 ,2048 ),(0 ,1 ),0 ),reinterpret_tensor (primals_18 ,(2048 ,512 ),(1 ,2048 ),0 ),out =buf81 )\n        buf83 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .bool )\n        buf87 =reinterpret_tensor (buf81 ,(1 ,1 ,512 ),(512 ,512 ,1 ),0 );del buf81 \n        buf88 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .float32 )\n        buf148 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 [grid (1 )](buf87 ,buf69 ,buf76 ,primals_19 ,primals_20 ,primals_21 ,buf83 ,buf88 ,buf148 ,6 ,1 ,512 ,num_warps =4 ,num_stages =1 )\n        del primals_19 \n        del primals_21 \n        buf89 =empty_strided_cuda ((1 ,1536 ),(1536 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_22 ,reinterpret_tensor (buf88 ,(1 ,512 ),(0 ,1 ),0 ),reinterpret_tensor (primals_23 ,(512 ,1536 ),(1 ,512 ),0 ),alpha =1 ,beta =1 ,out =buf89 )\n        del primals_22 \n\n        buf90 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf89 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),0 ),reinterpret_tensor (buf89 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),512 ),reinterpret_tensor (buf89 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),1024 ),None ,True ,0.1 )\n        buf91 =buf90 [0 ]\n        buf92 =buf90 [1 ]\n        buf93 =buf90 [2 ]\n        buf94 =buf90 [3 ]\n        del buf90 \n        buf95 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf91 ,(1 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_24 ,(512 ,512 ),(1 ,512 ),0 ),out =buf95 )\n        buf97 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .bool )\n        buf101 =reinterpret_tensor (buf95 ,(1 ,1 ,512 ),(512 ,512 ,1 ),0 );del buf95 \n        buf102 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .float32 )\n        buf147 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 [grid (1 )](buf101 ,buf69 ,buf88 ,primals_25 ,primals_26 ,primals_27 ,buf97 ,buf102 ,buf147 ,6 ,1 ,512 ,num_warps =4 ,num_stages =1 )\n        del primals_25 \n        del primals_27 \n        buf103 =buf77 ;del buf77 \n\n        extern_kernels .mm (reinterpret_tensor (buf102 ,(1 ,512 ),(0 ,1 ),0 ),reinterpret_tensor (primals_28 ,(512 ,2048 ),(1 ,512 ),0 ),out =buf103 )\n        buf105 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n        buf106 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .float32 )\n        buf142 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_6 [grid (2048 )](buf69 ,buf103 ,primals_29 ,buf105 ,buf106 ,buf142 ,7 ,2048 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del primals_29 \n        buf107 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf106 ,(1 ,2048 ),(0 ,1 ),0 ),reinterpret_tensor (primals_30 ,(2048 ,512 ),(1 ,2048 ),0 ),out =buf107 )\n        buf109 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .bool )\n        buf113 =reinterpret_tensor (buf107 ,(1 ,1 ,512 ),(512 ,512 ,1 ),0 );del buf107 \n        buf114 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .float32 )\n        buf146 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 [grid (1 )](buf113 ,buf69 ,buf102 ,primals_31 ,primals_32 ,primals_33 ,buf109 ,buf114 ,buf146 ,6 ,1 ,512 ,num_warps =4 ,num_stages =1 )\n        del primals_31 \n        del primals_33 \n        buf115 =empty_strided_cuda ((1 ,1536 ),(1536 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_34 ,reinterpret_tensor (buf114 ,(1 ,512 ),(0 ,1 ),0 ),reinterpret_tensor (primals_35 ,(512 ,1536 ),(1 ,512 ),0 ),alpha =1 ,beta =1 ,out =buf115 )\n        del primals_34 \n\n        buf116 =torch .ops .aten ._scaled_dot_product_efficient_attention .default (reinterpret_tensor (buf115 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),0 ),reinterpret_tensor (buf115 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),512 ),reinterpret_tensor (buf115 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),1024 ),None ,True ,0.1 )\n        buf117 =buf116 [0 ]\n        buf118 =buf116 [1 ]\n        buf119 =buf116 [2 ]\n        buf120 =buf116 [3 ]\n        del buf116 \n        buf121 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf117 ,(1 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (primals_36 ,(512 ,512 ),(1 ,512 ),0 ),out =buf121 )\n        buf123 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .bool )\n        buf127 =reinterpret_tensor (buf121 ,(1 ,1 ,512 ),(512 ,512 ,1 ),0 );del buf121 \n        buf128 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .float32 )\n        buf145 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_4 [grid (1 )](buf127 ,buf69 ,buf114 ,primals_37 ,primals_38 ,primals_39 ,buf123 ,buf128 ,buf145 ,6 ,1 ,512 ,num_warps =4 ,num_stages =1 )\n        del primals_37 \n        del primals_39 \n        buf129 =buf103 ;del buf103 \n\n        extern_kernels .mm (reinterpret_tensor (buf128 ,(1 ,512 ),(0 ,1 ),0 ),reinterpret_tensor (primals_40 ,(512 ,2048 ),(1 ,512 ),0 ),out =buf129 )\n        buf131 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n        buf132 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .float32 )\n        buf141 =empty_strided_cuda ((1 ,1 ,2048 ),(2048 ,2048 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_poi_fused_native_dropout_relu_threshold_backward_6 [grid (2048 )](buf69 ,buf129 ,primals_41 ,buf131 ,buf132 ,buf141 ,7 ,2048 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf129 \n        del primals_41 \n        buf133 =empty_strided_cuda ((1 ,512 ),(512 ,1 ),torch .float32 )\n\n        extern_kernels .mm (reinterpret_tensor (buf132 ,(1 ,2048 ),(0 ,1 ),0 ),reinterpret_tensor (primals_42 ,(2048 ,512 ),(1 ,2048 ),0 ),out =buf133 )\n        buf135 =empty_strided_cuda ((1 ,1 ,512 ),(512 ,512 ,1 ),torch .bool )\n        buf139 =reinterpret_tensor (buf133 ,(1 ,1 ,512 ),(512 ,512 ,1 ),0 );del buf133 \n        buf144 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_7 [grid (1 )](buf139 ,buf69 ,buf128 ,primals_43 ,buf135 ,buf144 ,8 ,1 ,512 ,num_warps =4 ,num_stages =1 )\n        del buf69 \n        del primals_43 \n        buf140 =empty_strided_cuda ((1 ,1 ,3 ,3 ,514 ),(4626 ,1 ,1542 ,514 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_8 [grid (4626 )](buf139 ,primals_44 ,primals_45 ,buf140 ,4626 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_45 \n    return (reinterpret_tensor (buf140 ,(1 ,4626 ),(4626 ,1 ),0 ),primals_14 ,primals_20 ,primals_26 ,primals_32 ,primals_38 ,primals_44 ,buf0 ,buf1 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),0 ),buf4 ,buf7 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),128 ),buf10 ,buf13 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),256 ),buf16 ,buf19 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),384 ),buf22 ,buf25 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),512 ),buf28 ,buf31 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),640 ),buf34 ,buf37 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),768 ),buf40 ,buf43 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),896 ),buf46 ,buf49 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),1024 ),buf52 ,buf55 ,reinterpret_tensor (primals_1 ,(1 ,128 ),(1280 ,1 ),1152 ),buf58 ,buf61 ,reinterpret_tensor (buf62 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),0 ),reinterpret_tensor (buf62 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),512 ),reinterpret_tensor (buf62 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),1024 ),buf64 ,buf65 ,buf66 ,buf67 ,buf71 ,buf75 ,reinterpret_tensor (buf76 ,(1 ,512 ),(512 ,1 ),0 ),buf79 ,reinterpret_tensor (buf80 ,(1 ,2048 ),(2048 ,1 ),0 ),buf83 ,buf87 ,reinterpret_tensor (buf88 ,(1 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (buf89 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),0 ),reinterpret_tensor (buf89 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),512 ),reinterpret_tensor (buf89 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),1024 ),buf91 ,buf92 ,buf93 ,buf94 ,buf97 ,buf101 ,reinterpret_tensor (buf102 ,(1 ,512 ),(512 ,1 ),0 ),buf105 ,reinterpret_tensor (buf106 ,(1 ,2048 ),(2048 ,1 ),0 ),buf109 ,buf113 ,reinterpret_tensor (buf114 ,(1 ,512 ),(512 ,1 ),0 ),reinterpret_tensor (buf115 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),0 ),reinterpret_tensor (buf115 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),512 ),reinterpret_tensor (buf115 ,(1 ,8 ,1 ,64 ),(512 ,64 ,1536 ,1 ),1024 ),buf117 ,buf118 ,buf119 ,buf120 ,buf123 ,buf127 ,reinterpret_tensor (buf128 ,(1 ,512 ),(512 ,1 ),0 ),buf131 ,reinterpret_tensor (buf132 ,(1 ,2048 ),(2048 ,1 ),0 ),buf135 ,buf139 ,buf144 ,primals_42 ,buf141 ,primals_40 ,buf145 ,primals_36 ,primals_35 ,buf146 ,primals_30 ,buf142 ,primals_28 ,buf147 ,primals_24 ,primals_23 ,buf148 ,primals_18 ,buf143 ,primals_16 ,buf149 ,primals_12 ,primals_11 ,primals_6 ,primals_7 ,primals_3 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,10 ,128 ),(1280 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((256 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((256 ,256 ),(256 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((256 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((512 ,256 ),(256 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((512 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((1536 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((1536 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((512 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((2048 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_18 =rand_strided ((512 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_22 =rand_strided ((1536 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_23 =rand_strided ((1536 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_24 =rand_strided ((512 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_25 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_26 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_27 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_28 =rand_strided ((2048 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_29 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_30 =rand_strided ((512 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_31 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_32 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_33 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_34 =rand_strided ((1536 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_35 =rand_strided ((1536 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_36 =rand_strided ((512 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_37 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_38 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_39 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_40 =rand_strided ((2048 ,512 ),(512 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_41 =rand_strided ((2048 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_42 =rand_strided ((512 ,2048 ),(2048 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_43 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_44 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_45 =rand_strided ((512 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ,primals_26 ,primals_27 ,primals_28 ,primals_29 ,primals_30 ,primals_31 ,primals_32 ,primals_33 ,primals_34 ,primals_35 ,primals_36 ,primals_37 ,primals_38 ,primals_39 ,primals_40 ,primals_41 ,primals_42 ,primals_43 ,primals_44 ,primals_45 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f2e6ba6e-f364-451a-b88a-669c2be27669",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool1d', 'Softplus', 'HuberLoss', 'LocalResponseNorm']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.softplus = nn.Softplus()\n        self.local_response_norm = nn.LocalResponseNorm(size=5)\n        self.huber_loss = nn.HuberLoss()\n\n    def forward(self, x):\n        # Ensure the input is at least 3D (batch, channels, length)\n        if x.dim() == 2:\n            x = x.unsqueeze(1)  # Add channel dimension if missing\n        \n        # Apply MaxPool1d twice\n        x = self.maxpool1(x)\n        x = self.maxpool2(x)\n        \n        # Apply LocalResponseNorm\n        x = self.local_response_norm(x)\n        \n        # Apply Softplus\n        x = self.softplus(x)\n        \n        # Compute HuberLoss (assuming we have a target tensor for demonstration)\n        # For simplicity, let's assume the target is a tensor of zeros with the same shape as x\n        target = torch.zeros_like(x)\n        loss = self.huber_loss(x, target)\n        \n        # Return the loss as the output (since HuberLoss is a loss function)\n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 64).cuda()  # Example input: (batch_size, channels, length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_1 (in_ptr0 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x1 =xindex //16 \n    x2 =xindex \n    tmp0 =(-2 )+x1 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks0 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-64 )+2 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp7 =tl .load (in_ptr0 +((-63 )+2 *x2 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =triton_helpers .maximum (tmp7 ,tmp6 )\n    tmp9 =tmp8 *tmp8 \n    tmp10 =tl .full (tmp9 .shape ,0.0 ,tmp9 .dtype )\n    tmp11 =tl .where (tmp5 ,tmp9 ,tmp10 )\n    tl .store (out_ptr0 +(x2 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_div_huber_loss_mul_pow_softplus_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp37 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(2 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .load (in_ptr0 +(1 +2 *r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp3 =tl .load (in_ptr1 +(r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +(16 +r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp6 =tl .load (in_ptr1 +(32 +r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp8 =tl .load (in_ptr1 +(48 +r0_0 ),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp10 =tl .load (in_ptr1 +(64 +r0_0 ),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n        tmp5 =tmp4 +tmp3 \n        tmp7 =tmp6 +tmp5 \n        tmp9 =tmp8 +tmp7 \n        tmp11 =tmp10 +tmp9 \n        tmp12 =0.2 \n        tmp13 =tmp11 *tmp12 \n        tmp14 =0.0001 \n        tmp15 =tmp13 *tmp14 \n        tmp16 =1.0 \n        tmp17 =tmp15 +tmp16 \n        tmp18 =0.75 \n        tmp19 =libdevice .pow (tmp17 ,tmp18 )\n        tmp20 =tmp2 /tmp19 \n        tmp21 =tmp20 *tmp16 \n        tmp22 =20.0 \n        tmp23 =tmp21 >tmp22 \n        tmp24 =tl_math .exp (tmp21 )\n        tmp25 =libdevice .log1p (tmp24 )\n        tmp26 =tmp25 *tmp16 \n        tmp27 =tl .where (tmp23 ,tmp20 ,tmp26 )\n        tmp28 =tl_math .abs (tmp27 )\n        tmp29 =tmp28 <tmp16 \n        tmp30 =0.5 \n        tmp31 =tmp28 *tmp30 \n        tmp32 =tmp31 *tmp28 \n        tmp33 =tmp28 -tmp30 \n        tmp34 =tmp33 *tmp16 \n        tmp35 =tl .where (tmp29 ,tmp32 ,tmp34 )\n        tmp36 =tl .broadcast_to (tmp35 ,[XBLOCK ,R0_BLOCK ])\n        tmp38 =_tmp37 +tmp36 \n        _tmp37 =tl .where (r0_mask ,tmp38 ,_tmp37 )\n    tmp37 =tl .sum (_tmp37 ,1 )[:,None ]\n    tmp39 =16 *ks0 \n    tmp40 =tmp39 .to (tl .float32 )\n    tmp41 =tmp37 /tmp40 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp41 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg1_1 ,(1 ,s0 ,64 ),(64 *s0 ,64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,1 ,32 ),(32 *s0 ,32 ,32 *s0 ,1 ),torch .float32 )\n\n        triton_poi_fused_max_pool2d_with_indices_0_xnumel =32 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_0 [grid (triton_poi_fused_max_pool2d_with_indices_0_xnumel )](arg1_1 ,buf0 ,320 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg1_1 \n        buf1 =empty_strided_cuda ((1 ,1 ,4 +s0 ,16 ),(64 +16 *s0 ,64 +16 *s0 ,16 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_1_xnumel =64 +16 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_1 [grid (triton_poi_fused_constant_pad_nd_1_xnumel )](buf0 ,buf1 ,10 ,224 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        buf3 =empty_strided_cuda ((),(),torch .float32 )\n        buf4 =buf3 ;del buf3 \n\n        16 *s0 \n        get_raw_stream (0 )\n        triton_red_fused_add_div_huber_loss_mul_pow_softplus_2 [grid (1 )](buf4 ,buf0 ,buf1 ,10 ,1 ,160 ,XBLOCK =1 ,R0_BLOCK =256 ,num_warps =2 ,num_stages =1 )\n        del buf0 \n        del buf1 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =10 \n    arg1_1 =rand_strided ((1 ,10 ,64 ),(640 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f472131d-062d-4406-a504-1d242521d02a",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ParameterList', 'InstanceNorm1d', 'Unflatten', 'PairwiseDistance', 'ReflectionPad3d', 'Hardsigmoid']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.param_list = nn.ParameterList([nn.Parameter(torch.randn(10)) for _ in range(5)])\n        self.instance_norm = nn.InstanceNorm1d(10)\n        self.unflatten = nn.Unflatten(1, (2, 5))\n        self.pairwise_distance = nn.PairwiseDistance(p=2)\n        self.reflection_pad = nn.ReflectionPad3d(1)\n        self.hardsigmoid = nn.Hardsigmoid()\n\n    def forward(self, x):\n        # Apply ReflectionPad3d\n        x = self.reflection_pad(x)\n        \n        # Flatten the input to apply InstanceNorm1d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, -1)\n        x = self.instance_norm(x)\n        \n        # Unflatten the input\n        x = self.unflatten(x)\n        \n        # Apply PairwiseDistance\n        x = x.view(x.size(0), -1)  # Flatten to (batch_size, -1)\n        x = self.pairwise_distance(x[:, :5], x[:, 5:10])\n        \n        # Apply Hardsigmoid\n        x = self.hardsigmoid(x)\n        \n        # Use ParameterList\n        for param in self.param_list:\n            x = x + param[0]  # Add the first element of each parameter to x\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 10, 10, 10).cuda()  # Arbitrary input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__native_batch_norm_legit_0 (in_ptr0 ,out_ptr2 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =10 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    tmp2_mean =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_m2 =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    tmp2_weight =tl .zeros ([XBLOCK ,R0_BLOCK ],tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(ks2 *(tl .where ((-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+(((r0_1 //(2 +ks2 ))%(2 +ks1 ))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+(((r0_1 //(2 +ks2 ))%(2 +ks1 ))))))+2 *ks1 ,(-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+(((r0_1 //(2 +ks2 ))%(2 +ks1 ))))))))+ks1 *ks2 *(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-1 )+(r0_1 //(4 +2 *ks1 +2 *ks2 +ks1 *ks2 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-1 )+(r0_1 //(4 +2 *ks1 +2 *ks2 +ks1 *ks2 )))))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-1 )+(r0_1 //(4 +2 *ks1 +2 *ks2 +ks1 *ks2 )))))))+ks0 *ks1 *ks2 *x0 +(tl .where ((-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+((r0_1 %(2 +ks2 ))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+((r0_1 %(2 +ks2 ))))))+2 *ks2 ,(-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+((r0_1 %(2 +ks2 ))))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n        tmp2_mean_next ,tmp2_m2_next ,tmp2_weight_next =triton_helpers .welford_reduce (\n        tmp1 ,tmp2_mean ,tmp2_m2 ,tmp2_weight ,roffset ==0 \n        )\n        tmp2_mean =tl .where (r0_mask &xmask ,tmp2_mean_next ,tmp2_mean )\n        tmp2_m2 =tl .where (r0_mask &xmask ,tmp2_m2_next ,tmp2_m2 )\n        tmp2_weight =tl .where (r0_mask &xmask ,tmp2_weight_next ,tmp2_weight )\n    tmp5 ,tmp6 ,tmp7 =triton_helpers .welford (tmp2_mean ,tmp2_m2 ,tmp2_weight ,1 )\n    tmp2 =tmp5 [:,None ]\n    tmp3 =tmp6 [:,None ]\n    tmp4 =tmp7 [:,None ]\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        r0_1 =r0_index \n        tmp8 =tl .load (in_ptr0 +(ks2 *(tl .where ((-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+(((r0_1 //(2 +ks2 ))%(2 +ks1 ))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+(((r0_1 //(2 +ks2 ))%(2 +ks1 ))))))+2 *ks1 ,(-1 )+ks1 +((-1 )*tl_math .abs (1 +((-1 )*ks1 )+tl_math .abs ((-1 )+(((r0_1 //(2 +ks2 ))%(2 +ks1 ))))))))+ks1 *ks2 *(tl .where ((-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-1 )+(r0_1 //(4 +2 *ks1 +2 *ks2 +ks1 *ks2 )))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-1 )+(r0_1 //(4 +2 *ks1 +2 *ks2 +ks1 *ks2 )))))+2 *ks0 ,(-1 )+ks0 +((-1 )*tl_math .abs (1 +((-1 )*ks0 )+tl_math .abs ((-1 )+(r0_1 //(4 +2 *ks1 +2 *ks2 +ks1 *ks2 )))))))+ks0 *ks1 *ks2 *x0 +(tl .where ((-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+((r0_1 %(2 +ks2 ))))))<0 ,(-1 )+((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+((r0_1 %(2 +ks2 ))))))+2 *ks2 ,(-1 )+ks2 +((-1 )*tl_math .abs (1 +((-1 )*ks2 )+tl_math .abs ((-1 )+((r0_1 %(2 +ks2 ))))))))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp9 =tmp8 -tmp2 \n        tmp10 =8 +4 *ks0 +4 *ks1 +4 *ks2 +2 *ks0 *ks1 +2 *ks0 *ks2 +2 *ks1 *ks2 +ks0 *ks1 *ks2 \n        tmp11 =tmp10 .to (tl .float32 )\n        tmp12 =tmp3 /tmp11 \n        tmp13 =1e-05 \n        tmp14 =tmp12 +tmp13 \n        tmp15 =libdevice .rsqrt (tmp14 )\n        tmp16 =tmp9 *tmp15 \n        tl .store (out_ptr2 +(r0_1 +8 *x0 +4 *ks0 *x0 +4 *ks1 *x0 +4 *ks2 *x0 +2 *ks0 *ks1 *x0 +2 *ks0 *ks2 *x0 +2 *ks1 *ks2 *x0 +ks0 *ks1 *ks2 *x0 ),tmp16 ,r0_mask &xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_hardsigmoid_norm_sub_1 (in_ptr0 ,in_ptr1 ,in_ptr2 ,in_ptr3 ,in_ptr4 ,in_ptr5 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp2 =tl .load (in_ptr0 +(5 ))\n    tmp3 =tl .broadcast_to (tmp2 ,[XBLOCK ])\n    tmp8 =tl .load (in_ptr0 +(1 ))\n    tmp9 =tl .broadcast_to (tmp8 ,[XBLOCK ])\n    tmp10 =tl .load (in_ptr0 +(6 ))\n    tmp11 =tl .broadcast_to (tmp10 ,[XBLOCK ])\n    tmp16 =tl .load (in_ptr0 +(2 ))\n    tmp17 =tl .broadcast_to (tmp16 ,[XBLOCK ])\n    tmp18 =tl .load (in_ptr0 +(7 ))\n    tmp19 =tl .broadcast_to (tmp18 ,[XBLOCK ])\n    tmp24 =tl .load (in_ptr0 +(3 ))\n    tmp25 =tl .broadcast_to (tmp24 ,[XBLOCK ])\n    tmp26 =tl .load (in_ptr0 +(8 ))\n    tmp27 =tl .broadcast_to (tmp26 ,[XBLOCK ])\n    tmp32 =tl .load (in_ptr0 +(4 ))\n    tmp33 =tl .broadcast_to (tmp32 ,[XBLOCK ])\n    tmp34 =tl .load (in_ptr0 +(9 ))\n    tmp35 =tl .broadcast_to (tmp34 ,[XBLOCK ])\n    tmp49 =tl .load (in_ptr1 +(0 ))\n    tmp50 =tl .broadcast_to (tmp49 ,[XBLOCK ])\n    tmp52 =tl .load (in_ptr2 +(0 ))\n    tmp53 =tl .broadcast_to (tmp52 ,[XBLOCK ])\n    tmp55 =tl .load (in_ptr3 +(0 ))\n    tmp56 =tl .broadcast_to (tmp55 ,[XBLOCK ])\n    tmp58 =tl .load (in_ptr4 +(0 ))\n    tmp59 =tl .broadcast_to (tmp58 ,[XBLOCK ])\n    tmp61 =tl .load (in_ptr5 +(0 ))\n    tmp62 =tl .broadcast_to (tmp61 ,[XBLOCK ])\n    tmp4 =tmp1 -tmp3 \n    tmp5 =1e-06 \n    tmp6 =tmp4 +tmp5 \n    tmp7 =tmp6 *tmp6 \n    tmp12 =tmp9 -tmp11 \n    tmp13 =tmp12 +tmp5 \n    tmp14 =tmp13 *tmp13 \n    tmp15 =tmp7 +tmp14 \n    tmp20 =tmp17 -tmp19 \n    tmp21 =tmp20 +tmp5 \n    tmp22 =tmp21 *tmp21 \n    tmp23 =tmp15 +tmp22 \n    tmp28 =tmp25 -tmp27 \n    tmp29 =tmp28 +tmp5 \n    tmp30 =tmp29 *tmp29 \n    tmp31 =tmp23 +tmp30 \n    tmp36 =tmp33 -tmp35 \n    tmp37 =tmp36 +tmp5 \n    tmp38 =tmp37 *tmp37 \n    tmp39 =tmp31 +tmp38 \n    tmp40 =libdevice .sqrt (tmp39 )\n    tmp41 =3.0 \n    tmp42 =tmp40 +tmp41 \n    tmp43 =0.0 \n    tmp44 =triton_helpers .maximum (tmp42 ,tmp43 )\n    tmp45 =6.0 \n    tmp46 =triton_helpers .minimum (tmp44 ,tmp45 )\n    tmp47 =0.16666666666666666 \n    tmp48 =tmp46 *tmp47 \n    tmp51 =tmp48 +tmp50 \n    tmp54 =tmp51 +tmp53 \n    tmp57 =tmp54 +tmp56 \n    tmp60 =tmp57 +tmp59 \n    tmp63 =tmp60 +tmp62 \n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp63 ,None )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 =args \n    args .clear ()\n    s1 =primals_1 \n    s2 =primals_2 \n    s3 =primals_3 \n    assert_size_stride (primals_4 ,(1 ,10 ,s1 ,s2 ,s3 ),(10 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    assert_size_stride (primals_5 ,(10 ,),(1 ,))\n    assert_size_stride (primals_6 ,(10 ,),(1 ,))\n    assert_size_stride (primals_7 ,(10 ,),(1 ,))\n    assert_size_stride (primals_8 ,(10 ,),(1 ,))\n    assert_size_stride (primals_9 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf3 =empty_strided_cuda ((1 ,10 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ),(80 +40 *s1 +40 *s2 +40 *s3 +20 *s1 *s2 +20 *s1 *s3 +20 *s2 *s3 +10 *s1 *s2 *s3 ,8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 ,1 ),torch .float32 )\n\n        8 +4 *s1 +4 *s2 +4 *s3 +2 *s1 *s2 +2 *s1 *s3 +2 *s2 *s3 +s1 *s2 *s3 \n        get_raw_stream (0 )\n        triton_red_fused__native_batch_norm_legit_0 [grid (10 )](primals_4 ,buf3 ,10 ,10 ,10 ,10 ,1728 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del primals_4 \n        buf4 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_hardsigmoid_norm_sub_1 [grid (1 )](buf3 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,buf4 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf3 \n        del primals_5 \n        del primals_6 \n        del primals_7 \n        del primals_8 \n        del primals_9 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =10 \n    primals_2 =10 \n    primals_3 =10 \n    primals_4 =rand_strided ((1 ,10 ,10 ,10 ,10 ),(10000 ,1000 ,100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f4f1ee0b-336b-4990-a2ef-a4a8c00922b9",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ELU', 'Hardshrink', 'Softshrink', 'MaxPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.elu1 = nn.ELU(alpha=1.0)\n        self.hardshrink1 = nn.Hardshrink(lambd=0.5)\n        self.softshrink1 = nn.Softshrink(lambd=0.5)\n        self.maxpool1d1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.elu2 = nn.ELU(alpha=1.0)\n        self.hardshrink2 = nn.Hardshrink(lambd=0.5)\n        self.softshrink2 = nn.Softshrink(lambd=0.5)\n        self.maxpool1d2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        # Ensure the input is at least 1D\n        if x.dim() == 0:\n            x = x.unsqueeze(0)\n        \n        # Reshape input to be compatible with MaxPool1d\n        if x.dim() == 1:\n            x = x.unsqueeze(0).unsqueeze(0)  # Shape: (batch_size, channels, length)\n        elif x.dim() == 2:\n            x = x.unsqueeze(1)  # Shape: (batch_size, channels, length)\n        elif x.dim() > 3:\n            x = x.view(x.size(0), -1, x.size(-1))  # Flatten all dimensions except the last one\n        \n        x = self.elu1(x)\n        x = self.hardshrink1(x)\n        x = self.softshrink1(x)\n        x = self.maxpool1d1(x)\n        x = self.elu2(x)\n        x = self.hardshrink2(x)\n        x = self.softshrink2(x)\n        x = self.maxpool1d2(x)\n        \n        # Flatten the output\n        x = x.view(x.size(0), -1)\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 64).cuda()  # Example input with shape (batch_size, length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =32 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp25 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =1.0 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .expm1 (tmp4 )\n    tmp6 =tmp5 *tmp3 \n    tmp7 =tl .where (tmp2 ,tmp4 ,tmp6 )\n    tmp8 =tl_math .abs (tmp7 )\n    tmp9 =0.5 \n    tmp10 =tmp8 <=tmp9 \n    tmp11 =tl .where (tmp10 ,tmp1 ,tmp7 )\n    tmp12 =tl_math .abs (tmp11 )\n    tmp13 =tmp12 >tmp9 \n    tmp14 =tl .full ([1 ],0 ,tl .int32 )\n    tmp15 =tmp14 <tmp11 \n    tmp16 =tmp15 .to (tl .int8 )\n    tmp17 =tmp11 <tmp14 \n    tmp18 =tmp17 .to (tl .int8 )\n    tmp19 =tmp16 -tmp18 \n    tmp20 =tmp19 .to (tmp11 .dtype )\n    tmp21 =tmp20 *tmp9 \n    tmp22 =tmp11 -tmp21 \n    tmp23 =tmp11 *tmp1 \n    tmp24 =tl .where (tmp13 ,tmp22 ,tmp23 )\n    tmp26 =tmp25 >tmp1 \n    tmp27 =tmp25 *tmp3 \n    tmp28 =libdevice .expm1 (tmp27 )\n    tmp29 =tmp28 *tmp3 \n    tmp30 =tl .where (tmp26 ,tmp27 ,tmp29 )\n    tmp31 =tl_math .abs (tmp30 )\n    tmp32 =tmp31 <=tmp9 \n    tmp33 =tl .where (tmp32 ,tmp1 ,tmp30 )\n    tmp34 =tl_math .abs (tmp33 )\n    tmp35 =tmp34 >tmp9 \n    tmp36 =tmp14 <tmp33 \n    tmp37 =tmp36 .to (tl .int8 )\n    tmp38 =tmp33 <tmp14 \n    tmp39 =tmp38 .to (tl .int8 )\n    tmp40 =tmp37 -tmp39 \n    tmp41 =tmp40 .to (tmp33 .dtype )\n    tmp42 =tmp41 *tmp9 \n    tmp43 =tmp33 -tmp42 \n    tmp44 =tmp33 *tmp1 \n    tmp45 =tl .where (tmp35 ,tmp43 ,tmp44 )\n    tmp46 =triton_helpers .maximum (tmp45 ,tmp24 )\n    tl .store (out_ptr0 +(x0 ),tmp46 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =16 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp25 =tl .load (in_ptr0 +(1 +2 *x0 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =1.0 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .expm1 (tmp4 )\n    tmp6 =tmp5 *tmp3 \n    tmp7 =tl .where (tmp2 ,tmp4 ,tmp6 )\n    tmp8 =tl_math .abs (tmp7 )\n    tmp9 =0.5 \n    tmp10 =tmp8 <=tmp9 \n    tmp11 =tl .where (tmp10 ,tmp1 ,tmp7 )\n    tmp12 =tl_math .abs (tmp11 )\n    tmp13 =tmp12 >tmp9 \n    tmp14 =tl .full ([1 ],0 ,tl .int32 )\n    tmp15 =tmp14 <tmp11 \n    tmp16 =tmp15 .to (tl .int8 )\n    tmp17 =tmp11 <tmp14 \n    tmp18 =tmp17 .to (tl .int8 )\n    tmp19 =tmp16 -tmp18 \n    tmp20 =tmp19 .to (tmp11 .dtype )\n    tmp21 =tmp20 *tmp9 \n    tmp22 =tmp11 -tmp21 \n    tmp23 =tmp11 *tmp1 \n    tmp24 =tl .where (tmp13 ,tmp22 ,tmp23 )\n    tmp26 =tmp25 >tmp1 \n    tmp27 =tmp25 *tmp3 \n    tmp28 =libdevice .expm1 (tmp27 )\n    tmp29 =tmp28 *tmp3 \n    tmp30 =tl .where (tmp26 ,tmp27 ,tmp29 )\n    tmp31 =tl_math .abs (tmp30 )\n    tmp32 =tmp31 <=tmp9 \n    tmp33 =tl .where (tmp32 ,tmp1 ,tmp30 )\n    tmp34 =tl_math .abs (tmp33 )\n    tmp35 =tmp34 >tmp9 \n    tmp36 =tmp14 <tmp33 \n    tmp37 =tmp36 .to (tl .int8 )\n    tmp38 =tmp33 <tmp14 \n    tmp39 =tmp38 .to (tl .int8 )\n    tmp40 =tmp37 -tmp39 \n    tmp41 =tmp40 .to (tmp33 .dtype )\n    tmp42 =tmp41 *tmp9 \n    tmp43 =tmp33 -tmp42 \n    tmp44 =tmp33 *tmp1 \n    tmp45 =tl .where (tmp35 ,tmp43 ,tmp44 )\n    tmp46 =triton_helpers .maximum (tmp45 ,tmp24 )\n    tl .store (out_ptr0 +(x0 ),tmp46 ,xmask )\n\ndef call (args ):\n    arg0_1 ,=args \n    args .clear ()\n    assert_size_stride (arg0_1 ,(1 ,64 ),(64 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ,1 ,32 ),(32 ,32 ,32 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_0 [grid (32 )](arg0_1 ,buf0 ,32 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del arg0_1 \n        buf1 =empty_strided_cuda ((1 ,1 ,1 ,16 ),(16 ,16 ,16 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_1 [grid (16 )](buf0 ,buf1 ,16 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n    return (reinterpret_tensor (buf1 ,(1 ,16 ),(16 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =rand_strided ((1 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f6142f2c-0e2e-46d5-95dc-243e060a1fb2",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['TripletMarginWithDistanceLoss', 'Softshrink', 'ReLU6', 'AdaptiveMaxPool1d', 'Unflatten']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softshrink = nn.Softshrink(lambd=0.5)\n        self.relu6 = nn.ReLU6()\n        self.adaptive_max_pool1d = nn.AdaptiveMaxPool1d(output_size=10)\n        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(2, 5))\n        self.triplet_loss = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: F.pairwise_distance(x, y, p=2))\n\n    def forward(self, x):\n        # Apply Softshrink\n        x = self.softshrink(x)\n        \n        # Apply ReLU6\n        x = self.relu6(x)\n        \n        # Reshape for AdaptiveMaxPool1d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, sequence_length)\n        x = self.adaptive_max_pool1d(x)\n        \n        # Unflatten the output\n        x = self.unflatten(x)\n        \n        # Dummy anchor, positive, and negative for TripletMarginWithDistanceLoss\n        anchor = x[:, 0, :]  # Use the first part of the unflattened output as anchor\n        positive = x[:, 1, :]  # Use the second part of the unflattened output as positive\n        negative = torch.randn_like(anchor)  # Random negative example\n        \n        # Compute triplet loss\n        loss = self.triplet_loss(anchor, positive, negative)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 10, 20).cuda()  # Example input shape (batch_size, channels, sequence_length)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_norm_randn_like_sub_0 (in_ptr0 ,in_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,ks1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =5 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp75 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp80 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp3 =tl .load (in_ptr1 +(2 *r0_1 +ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp22 =tl .load (in_ptr1 +(1 +2 *r0_1 +ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp38 =tl .load (in_ptr1 +(2 *r0_1 +5 *ks1 +ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp53 =tl .load (in_ptr1 +(1 +2 *r0_1 +5 *ks1 +ks1 *x0 ),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp0 =tl .load (in_ptr0 +load_seed_offset )\n        tmp1 =r0_1 +x0 *(ks1 //2 )\n        tmp2 =tl .randn (tmp0 ,(tmp1 ).to (tl .uint32 ))\n        tmp4 =tl_math .abs (tmp3 )\n        tmp5 =0.5 \n        tmp6 =tmp4 >tmp5 \n        tmp7 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp8 =tmp7 <tmp3 \n        tmp9 =tmp8 .to (tl .int8 )\n        tmp10 =tmp3 <tmp7 \n        tmp11 =tmp10 .to (tl .int8 )\n        tmp12 =tmp9 -tmp11 \n        tmp13 =tmp12 .to (tmp3 .dtype )\n        tmp14 =tmp13 *tmp5 \n        tmp15 =tmp3 -tmp14 \n        tmp16 =0.0 \n        tmp17 =tmp3 *tmp16 \n        tmp18 =tl .where (tmp6 ,tmp15 ,tmp17 )\n        tmp19 =triton_helpers .maximum (tmp18 ,tmp16 )\n        tmp20 =6.0 \n        tmp21 =triton_helpers .minimum (tmp19 ,tmp20 )\n        tmp23 =tl_math .abs (tmp22 )\n        tmp24 =tmp23 >tmp5 \n        tmp25 =tmp7 <tmp22 \n        tmp26 =tmp25 .to (tl .int8 )\n        tmp27 =tmp22 <tmp7 \n        tmp28 =tmp27 .to (tl .int8 )\n        tmp29 =tmp26 -tmp28 \n        tmp30 =tmp29 .to (tmp22 .dtype )\n        tmp31 =tmp30 *tmp5 \n        tmp32 =tmp22 -tmp31 \n        tmp33 =tmp22 *tmp16 \n        tmp34 =tl .where (tmp24 ,tmp32 ,tmp33 )\n        tmp35 =triton_helpers .maximum (tmp34 ,tmp16 )\n        tmp36 =triton_helpers .minimum (tmp35 ,tmp20 )\n        tmp37 =triton_helpers .maximum (tmp36 ,tmp21 )\n        tmp39 =tl_math .abs (tmp38 )\n        tmp40 =tmp39 >tmp5 \n        tmp41 =tmp7 <tmp38 \n        tmp42 =tmp41 .to (tl .int8 )\n        tmp43 =tmp38 <tmp7 \n        tmp44 =tmp43 .to (tl .int8 )\n        tmp45 =tmp42 -tmp44 \n        tmp46 =tmp45 .to (tmp38 .dtype )\n        tmp47 =tmp46 *tmp5 \n        tmp48 =tmp38 -tmp47 \n        tmp49 =tmp38 *tmp16 \n        tmp50 =tl .where (tmp40 ,tmp48 ,tmp49 )\n        tmp51 =triton_helpers .maximum (tmp50 ,tmp16 )\n        tmp52 =triton_helpers .minimum (tmp51 ,tmp20 )\n        tmp54 =tl_math .abs (tmp53 )\n        tmp55 =tmp54 >tmp5 \n        tmp56 =tmp7 <tmp53 \n        tmp57 =tmp56 .to (tl .int8 )\n        tmp58 =tmp53 <tmp7 \n        tmp59 =tmp58 .to (tl .int8 )\n        tmp60 =tmp57 -tmp59 \n        tmp61 =tmp60 .to (tmp53 .dtype )\n        tmp62 =tmp61 *tmp5 \n        tmp63 =tmp53 -tmp62 \n        tmp64 =tmp53 *tmp16 \n        tmp65 =tl .where (tmp55 ,tmp63 ,tmp64 )\n        tmp66 =triton_helpers .maximum (tmp65 ,tmp16 )\n        tmp67 =triton_helpers .minimum (tmp66 ,tmp20 )\n        tmp68 =triton_helpers .maximum (tmp67 ,tmp52 )\n        tmp69 =tmp37 -tmp68 \n        tmp70 =tmp37 -tmp2 \n        tmp71 =1e-06 \n        tmp72 =tmp70 +tmp71 \n        tmp73 =tmp72 *tmp72 \n        tmp74 =tl .broadcast_to (tmp73 ,[XBLOCK ,R0_BLOCK ])\n        tmp76 =_tmp75 +tmp74 \n        _tmp75 =tl .where (r0_mask &xmask ,tmp76 ,_tmp75 )\n        tmp77 =tmp69 +tmp71 \n        tmp78 =tmp77 *tmp77 \n        tmp79 =tl .broadcast_to (tmp78 ,[XBLOCK ,R0_BLOCK ])\n        tmp81 =_tmp80 +tmp79 \n        _tmp80 =tl .where (r0_mask &xmask ,tmp81 ,_tmp80 )\n    tmp75 =tl .sum (_tmp75 ,1 )[:,None ]\n    tmp80 =tl .sum (_tmp80 ,1 )[:,None ]\n    tl .store (out_ptr2 +(x0 ),tmp75 ,xmask )\n    tl .store (out_ptr3 +(x0 ),tmp80 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_clamp_min_mean_norm_sub_1 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp5 =tl .load (in_ptr1 +(0 ))\n    tmp6 =tl .broadcast_to (tmp5 ,[XBLOCK ])\n    tmp11 =tl .load (in_ptr0 +(1 ))\n    tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ])\n    tmp15 =tl .load (in_ptr1 +(1 ))\n    tmp16 =tl .broadcast_to (tmp15 ,[XBLOCK ])\n    tmp21 =tl .load (in_ptr0 +(2 ))\n    tmp22 =tl .broadcast_to (tmp21 ,[XBLOCK ])\n    tmp25 =tl .load (in_ptr1 +(2 ))\n    tmp26 =tl .broadcast_to (tmp25 ,[XBLOCK ])\n    tmp31 =tl .load (in_ptr0 +(3 ))\n    tmp32 =tl .broadcast_to (tmp31 ,[XBLOCK ])\n    tmp35 =tl .load (in_ptr1 +(3 ))\n    tmp36 =tl .broadcast_to (tmp35 ,[XBLOCK ])\n    tmp41 =tl .load (in_ptr0 +(4 ))\n    tmp42 =tl .broadcast_to (tmp41 ,[XBLOCK ])\n    tmp45 =tl .load (in_ptr1 +(4 ))\n    tmp46 =tl .broadcast_to (tmp45 ,[XBLOCK ])\n    tmp2 =libdevice .sqrt (tmp1 )\n    tmp3 =1.0 \n    tmp4 =tmp2 +tmp3 \n    tmp7 =libdevice .sqrt (tmp6 )\n    tmp8 =tmp4 -tmp7 \n    tmp9 =0.0 \n    tmp10 =triton_helpers .maximum (tmp8 ,tmp9 )\n    tmp13 =libdevice .sqrt (tmp12 )\n    tmp14 =tmp13 +tmp3 \n    tmp17 =libdevice .sqrt (tmp16 )\n    tmp18 =tmp14 -tmp17 \n    tmp19 =triton_helpers .maximum (tmp18 ,tmp9 )\n    tmp20 =tmp10 +tmp19 \n    tmp23 =libdevice .sqrt (tmp22 )\n    tmp24 =tmp23 +tmp3 \n    tmp27 =libdevice .sqrt (tmp26 )\n    tmp28 =tmp24 -tmp27 \n    tmp29 =triton_helpers .maximum (tmp28 ,tmp9 )\n    tmp30 =tmp20 +tmp29 \n    tmp33 =libdevice .sqrt (tmp32 )\n    tmp34 =tmp33 +tmp3 \n    tmp37 =libdevice .sqrt (tmp36 )\n    tmp38 =tmp34 -tmp37 \n    tmp39 =triton_helpers .maximum (tmp38 ,tmp9 )\n    tmp40 =tmp30 +tmp39 \n    tmp43 =libdevice .sqrt (tmp42 )\n    tmp44 =tmp43 +tmp3 \n    tmp47 =libdevice .sqrt (tmp46 )\n    tmp48 =tmp44 -tmp47 \n    tmp49 =triton_helpers .maximum (tmp48 ,tmp9 )\n    tmp50 =tmp40 +tmp49 \n    tmp51 =5.0 \n    tmp52 =tmp50 /tmp51 \n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp52 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 =args \n    args .clear ()\n    s1 =arg0_1 \n    assert_size_stride (arg1_1 ,(1 ,10 ,s1 ),(10 *s1 ,s1 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf2 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf2 )\n        buf4 =empty_strided_cuda ((1 ,5 ),(5 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,5 ),(5 ,1 ),torch .float32 )\n\n        s1 //2 \n        get_raw_stream (0 )\n        triton_red_fused_add_norm_randn_like_sub_0 [grid (5 )](buf2 ,arg1_1 ,buf4 ,buf1 ,0 ,20 ,5 ,10 ,XBLOCK =1 ,R0_BLOCK =16 ,num_warps =2 ,num_stages =1 )\n        del arg1_1 \n        del buf2 \n        buf5 =empty_strided_cuda ((),(),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_add_clamp_min_mean_norm_sub_1 [grid (1 )](buf1 ,buf4 ,buf5 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf1 \n        del buf4 \n    return (buf5 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =20 \n    arg1_1 =rand_strided ((1 ,10 ,20 ),(200 ,20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f6942f2b-01d2-46c8-a105-6f073d528772",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Tanhshrink', 'CosineSimilarity', 'CrossEntropyLoss', 'Hardsigmoid', 'CosineEmbeddingLoss', 'ModuleDict']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.tanhshrink = nn.Tanhshrink()\n        self.hardsigmoid = nn.Hardsigmoid()\n        self.module_dict = nn.ModuleDict({\n            'cosine_sim': nn.CosineSimilarity(dim=1),\n            'cosine_embedding_loss': nn.CosineEmbeddingLoss(),\n            'cross_entropy_loss': nn.CrossEntropyLoss()\n        })\n        \n    def forward(self, x):\n        # Apply Tanhshrink and Hardsigmoid\n        x = self.tanhshrink(x)\n        x = self.hardsigmoid(x)\n        \n        # Reshape for CosineSimilarity\n        x_reshaped = x.view(x.size(0), -1)\n        x1 = x_reshaped[:, :x_reshaped.size(1)//2]\n        x2 = x_reshaped[:, x_reshaped.size(1)//2:]\n        \n        # Apply CosineSimilarity\n        cosine_sim = self.module_dict['cosine_sim'](x1, x2)\n        \n        # Dummy target for CosineEmbeddingLoss\n        target = torch.ones(x.size(0)).to(x.device)\n        \n        # Apply CosineEmbeddingLoss\n        cosine_embedding_loss = self.module_dict['cosine_embedding_loss'](x1, x2, target)\n        \n        # Dummy target for CrossEntropyLoss\n        dummy_logits = torch.randn(x.size(0), 10).to(x.device)\n        dummy_labels = torch.randint(0, 10, (x.size(0),)).to(x.device)\n        \n        # Apply CrossEntropyLoss\n        cross_entropy_loss = self.module_dict['cross_entropy_loss'](dummy_logits, dummy_labels)\n        \n        # Return the cosine similarity and the losses\n        return cosine_sim, cosine_embedding_loss, cross_entropy_loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nfrom ctypes import c_void_p ,c_long ,c_int \nimport torch \nimport math \nimport random \nimport os \nimport tempfile \nfrom math import inf ,nan \nfrom cmath import nanj \nfrom torch ._inductor .hooks import run_intermediate_hooks \nfrom torch ._inductor .utils import maybe_profile \nfrom torch ._inductor .codegen .memory_planning import _align as align \nfrom torch import device ,empty_strided \nfrom torch ._inductor .async_compile import AsyncCompile \nfrom torch ._inductor .select_algorithm import extern_kernels \nfrom torch ._inductor .codegen .multi_kernel import MultiKernelCall \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\nsplit_scan_grid ,\ngrid_combo_kernels ,\nstart_graph ,\nend_graph ,\ncooperative_reduction_grid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_linalg_vector_norm_mul_sum_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =1 \n    rnumel =r0_numel \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    rbase =r0_base \n    _tmp13 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        rindex =r0_index \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +(r0_0 +((ks0 *ks1 *ks2 )//2 )),r0_mask ,eviction_policy ='evict_first',other =0.0 )\n        tmp1 =libdevice .tanh (tmp0 )\n        tmp2 =tmp0 -tmp1 \n        tmp3 =3.0 \n        tmp4 =tmp2 +tmp3 \n        tmp5 =0.0 \n        tmp6 =triton_helpers .maximum (tmp4 ,tmp5 )\n        tmp7 =6.0 \n        tmp8 =triton_helpers .minimum (tmp6 ,tmp7 )\n        tmp9 =0.16666666666666666 \n        tmp10 =tmp8 *tmp9 \n        tmp11 =tmp10 *tmp10 \n        tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n        tmp14 =_tmp13 +tmp12 \n        _tmp13 =tl .where (r0_mask ,tmp14 ,_tmp13 )\n    tmp13 =tl .sum (_tmp13 ,1 )[:,None ]\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp13 ,None )\n    tl .store (out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp13 ,None )\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_div_eq_fill_linalg_vector_norm_mean_mul_sqrt_sub_sum_where_zeros_like_1 (in_out_ptr0 ,in_out_ptr1 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =1 \n    rnumel =r0_numel \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    rbase =r0_base \n    _tmp13 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        rindex =r0_index \n        r0_0 =r0_index \n        tmp0 =tl .load (in_ptr0 +((r0_0 %(ks0 *ks1 *ks2 ))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =libdevice .tanh (tmp0 )\n        tmp2 =tmp0 -tmp1 \n        tmp3 =3.0 \n        tmp4 =tmp2 +tmp3 \n        tmp5 =0.0 \n        tmp6 =triton_helpers .maximum (tmp4 ,tmp5 )\n        tmp7 =6.0 \n        tmp8 =triton_helpers .minimum (tmp6 ,tmp7 )\n        tmp9 =0.16666666666666666 \n        tmp10 =tmp8 *tmp9 \n        tmp11 =tmp10 *tmp10 \n        tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,R0_BLOCK ])\n        tmp14 =_tmp13 +tmp12 \n        _tmp13 =tl .where (r0_mask ,tmp14 ,_tmp13 )\n    tmp13 =tl .sum (_tmp13 ,1 )[:,None ]\n    tmp37 =tl .load (in_ptr1 +(0 ))\n    tmp38 =tl .broadcast_to (tmp37 ,[XBLOCK ,R0_BLOCK ])\n    _tmp44 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp48 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    _tmp52 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        roffset =r0_offset \n        rindex =r0_index \n        r0_0 =r0_index \n        tmp15 =tl .load (in_ptr0 +((r0_0 %(ks0 *ks1 *ks2 ))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp30 =tl .load (in_ptr0 +(((r0_0 +((ks0 *ks1 *ks2 )//2 ))%(ks0 *ks1 *ks2 ))),r0_mask ,eviction_policy ='evict_last',other =0.0 )\n        tmp16 =libdevice .tanh (tmp15 )\n        tmp17 =tmp15 -tmp16 \n        tmp18 =3.0 \n        tmp19 =tmp17 +tmp18 \n        tmp20 =0.0 \n        tmp21 =triton_helpers .maximum (tmp19 ,tmp20 )\n        tmp22 =6.0 \n        tmp23 =triton_helpers .minimum (tmp21 ,tmp22 )\n        tmp24 =0.16666666666666666 \n        tmp25 =tmp23 *tmp24 \n        tmp26 =libdevice .sqrt (tmp13 )\n        tmp27 =1e-08 \n        tmp28 =triton_helpers .maximum (tmp26 ,tmp27 )\n        tmp29 =tmp25 /tmp28 \n        tmp31 =libdevice .tanh (tmp30 )\n        tmp32 =tmp30 -tmp31 \n        tmp33 =tmp32 +tmp18 \n        tmp34 =triton_helpers .maximum (tmp33 ,tmp20 )\n        tmp35 =triton_helpers .minimum (tmp34 ,tmp22 )\n        tmp36 =tmp35 *tmp24 \n        tmp39 =libdevice .sqrt (tmp38 )\n        tmp40 =triton_helpers .maximum (tmp39 ,tmp27 )\n        tmp41 =tmp36 /tmp40 \n        tmp42 =tmp29 *tmp41 \n        tmp43 =tl .broadcast_to (tmp42 ,[XBLOCK ,R0_BLOCK ])\n        tmp45 =_tmp44 +tmp43 \n        _tmp44 =tl .where (r0_mask ,tmp45 ,_tmp44 )\n        tmp46 =tmp25 *tmp36 \n        tmp47 =tl .broadcast_to (tmp46 ,[XBLOCK ,R0_BLOCK ])\n        tmp49 =_tmp48 +tmp47 \n        _tmp48 =tl .where (r0_mask ,tmp49 ,_tmp48 )\n        tmp50 =tmp25 *tmp25 \n        tmp51 =tl .broadcast_to (tmp50 ,[XBLOCK ,R0_BLOCK ])\n        tmp53 =_tmp52 +tmp51 \n        _tmp52 =tl .where (r0_mask ,tmp53 ,_tmp52 )\n    tmp44 =tl .sum (_tmp44 ,1 )[:,None ]\n    tmp48 =tl .sum (_tmp48 ,1 )[:,None ]\n    tmp52 =tl .sum (_tmp52 ,1 )[:,None ]\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp44 ,None )\n    tmp56 =tl .load (in_ptr2 +(0 ))\n    tmp57 =tl .broadcast_to (tmp56 ,[XBLOCK ,1 ])\n    tmp54 =9.999999960041972e-13 \n    tmp55 =tmp52 +tmp54 \n    tmp58 =tmp57 +tmp54 \n    tmp59 =tmp55 *tmp58 \n    tmp60 =libdevice .sqrt (tmp59 )\n    tmp61 =tmp48 /tmp60 \n    tmp62 =1.0 \n    tmp63 =tmp62 -tmp61 \n    tmp64 =tl .full ([1 ,1 ],True ,tl .int1 )\n    tmp65 =0.0 \n    tmp66 =tl .where (tmp64 ,tmp63 ,tmp65 )\n    tmp67 =tmp61 -tmp65 \n    tmp68 =triton_helpers .maximum (tmp67 ,tmp65 )\n    tmp69 =tl .full ([1 ,1 ],False ,tl .int1 )\n    tmp70 =tl .where (tmp69 ,tmp68 ,tmp65 )\n    tmp71 =tmp66 +tmp70 \n    tmp72 =tmp71 /tmp62 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp72 ,None )\n\nextern \"C\"void kernel (const int64_t *in_ptr0 ,\nint64_t *out_ptr0 )\n{\n{\n{\n{\nauto tmp0 =in_ptr0 [static_cast <int64_t >(1 L )];\nauto tmp1 =static_cast <int32_t >(0 );\nauto tmp2 =static_cast <int64_t >(0 );\nauto tmp3 =static_cast <int64_t >(10 );\nauto tmp4 =randint64_cpu (tmp0 ,tmp1 ,tmp2 ,tmp3 );\nout_ptr0 [static_cast <int64_t >(0 L )]=tmp4 ;\n}\n}\n}\n}\n''')\n\n#include \"/tmp/torchinductor_sahanp/3b/c3bi5gk6mslf6u4iaqafhxm64z6u65e3eain4xlary5blqnvv6xx.h\"\nextern \"C\"  void kernel(const int64_t* in_ptr0,\n                       float* out_ptr0)\n{\n    {\n        #pragma GCC ivdep\n        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(10L); x0+=static_cast<int64_t>(1L))\n        {\n            {\n                {\n                    auto tmp0 = in_ptr0[static_cast<int64_t>(0L)];\n                    auto tmp1 = x0;\n                    auto tmp2 = c10::convert<int32_t>(tmp1);\n                    auto tmp3 = randn_cpu(tmp0, tmp2);\n                    out_ptr0[static_cast<int64_t>(x0)] = tmp3;\n                }\n            }\n        }\n    }\n}\n''')\n\nimport triton \nimport triton .language as tl \nfrom triton .compiler .compiler import AttrsDescriptor \n\nfrom torch ._inductor .runtime import triton_helpers ,triton_heuristics \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \nfrom torch ._inductor .runtime .hints import AutotuneHint ,ReductionHint ,TileHint ,DeviceProperties \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused__log_softmax_nll_loss_forward_4 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    xnumel =1 \n    r0_numel =10 \n    R0_BLOCK :tl .constexpr =16 \n    rnumel =r0_numel \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_offset =0 \n    r0_mask =r0_index <r0_numel \n    roffset =r0_offset \n    rindex =r0_index \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp11 =tl .load (in_ptr1 +(0 ))\n    tmp12 =tl .broadcast_to (tmp11 ,[XBLOCK ,1 ])\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,float (\"-inf\"))\n    tmp4 =triton_helpers .max2 (tmp3 ,1 )[:,None ]\n    tmp5 =tmp0 -tmp4 \n    tmp6 =tl_math .exp (tmp5 )\n    tmp7 =tl .broadcast_to (tmp6 ,[XBLOCK ,R0_BLOCK ])\n    tmp9 =tl .where (r0_mask ,tmp7 ,0 )\n    tmp10 =tl .sum (tmp9 ,1 )[:,None ]\n    tmp13 =tl .full ([1 ,1 ],-100 ,tl .int64 )\n    tmp14 =tmp12 !=tmp13 \n    tmp15 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp16 =tl .where (tmp14 ,tmp12 ,tmp15 )\n    tmp17 =tl .full ([XBLOCK ,1 ],10 ,tl .int32 )\n    tmp18 =tmp16 +tmp17 \n    tmp19 =tmp16 <0 \n    tmp20 =tl .where (tmp19 ,tmp18 ,tmp16 )\n    tl .device_assert ((0 <=tmp20 )&(tmp20 <10 ),\"index out of bounds: 0 <= tmp20 < 10\")\n    tmp22 =tl .load (in_ptr0 +(tmp20 ),None ,eviction_policy ='evict_last')\n    tmp23 =tmp22 -tmp4 \n    tmp24 =tl_math .log (tmp10 )\n    tmp25 =tmp23 -tmp24 \n    tmp26 =-tmp25 \n    tmp27 =0.0 \n    tmp28 =tl .where (tmp14 ,tmp26 ,tmp27 )\n    tmp29 =tmp14 .to (tl .int64 )\n    tmp30 =tmp29 .to (tl .float32 )\n    tmp31 =tmp28 /tmp30 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp31 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf5 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n\n        triton_red_fused_linalg_vector_norm_mul_sum_0_r0_numel =((-1 )*((s0 *s1 *s2 )//2 ))+s0 *s1 *s2 \n        stream0 =get_raw_stream (0 )\n        triton_red_fused_linalg_vector_norm_mul_sum_0 [grid (1 )](arg3_1 ,buf1 ,buf5 ,3 ,64 ,64 ,1 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf0 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf2 =reinterpret_tensor (buf0 ,(1 ,),(1 ,),0 );del buf0 \n        buf3 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf13 =reinterpret_tensor (buf3 ,(),(),0 );del buf3 \n\n        triton_red_fused_add_clamp_min_div_eq_fill_linalg_vector_norm_mean_mul_sqrt_sub_sum_where_zeros_like_1_r0_numel =(s0 *s1 *s2 )//2 \n        stream0 =get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_div_eq_fill_linalg_vector_norm_mean_mul_sqrt_sub_sum_where_zeros_like_1 [grid (1 )](buf2 ,buf13 ,arg3_1 ,buf1 ,buf5 ,3 ,64 ,64 ,1 ,6144 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n    buf6 =empty_strided_cpu ((2 ,),(1 ,),torch .int64 )\n\n    aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf6 )\n    buf7 =empty_strided_cpu ((1 ,),(1 ,),torch .int64 )\n    cpp_fused_randint_2 (buf6 ,buf7 )\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf8 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n        buf8 .copy_ (buf7 ,False )\n        del buf7 \n    buf9 =empty_strided_cpu ((1 ,10 ),(10 ,1 ),torch .float32 )\n    cpp_fused_randn_3 (buf6 ,buf9 )\n    del buf6 \n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf10 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n        buf10 .copy_ (buf9 ,False )\n        del buf9 \n        buf11 =reinterpret_tensor (buf5 ,(1 ,1 ),(1 ,1 ),0 );del buf5 \n        buf14 =reinterpret_tensor (buf11 ,(),(),0 );del buf11 \n\n        stream0 =get_raw_stream (0 )\n        triton_per_fused__log_softmax_nll_loss_forward_4 [grid (1 )](buf14 ,buf10 ,buf8 ,1 ,10 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf10 \n        del buf8 \n    return (buf2 ,buf13 ,buf14 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f6c4d740-e541-4ac3-917a-c1dadf53be70",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AdaptiveAvgPool1d', 'ReLU', 'ConstantPad1d', 'ModuleList', 'PoissonNLLLoss']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad1d(padding=2, value=0)\n        self.pool = nn.AdaptiveAvgPool1d(output_size=10)\n        self.relu = nn.ReLU()\n        self.module_list = nn.ModuleList([nn.ReLU() for _ in range(3)])  # Repeating ReLU up to 5 times\n        self.loss = nn.PoissonNLLLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, length)\n        x = self.pad(x)  # Apply padding\n        x = self.pool(x)  # Apply adaptive average pooling\n        x = self.relu(x)  # Apply ReLU\n        \n        # Apply ReLU from ModuleList\n        for module in self.module_list:\n            x = module(x)\n        \n        # Compute PoissonNLLLoss (assuming target is the same as input for simplicity)\n        target = x.detach()  # Detach to avoid backprop through target\n        loss = self.loss(x, target)\n        \n        return loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 20).cuda()  # Example input shape (batch_size=1, channels=3, length=20)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused__adaptive_avg_pool2d_exp_mean_mul_relu_sub_0 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    _tmp84 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_0 =(r0_index %10 )\n        r0_1 =r0_index //10 \n        tmp0 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp1 =tl .full ([1 ,1 ],1 ,tl .int64 )\n        tmp2 =tmp0 <tmp1 \n        tmp3 =(12 *r0_0 )//5 \n        tmp4 =(33 +24 *r0_0 )//10 \n        tmp5 =tmp3 <tmp4 \n        tmp6 =tmp2 &tmp5 \n        tmp7 =tl .broadcast_to ((-2 )+((12 *r0_0 )//5 ),[XBLOCK ,R0_BLOCK ])\n        tmp8 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp9 =tmp7 >=tmp8 \n        tmp10 =tl .full ([1 ,1 ],20 ,tl .int64 )\n        tmp11 =tmp7 <tmp10 \n        tmp12 =tmp9 &tmp11 \n        tmp13 =tmp12 &tmp6 \n        tmp14 =tl .load (in_ptr0 +(tl .broadcast_to ((-2 )+20 *r0_1 +((12 *r0_0 )//5 ),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp13 ,eviction_policy ='evict_last',other =0.0 )\n        tmp15 =tl .full (tmp14 .shape ,0.0 ,tmp14 .dtype )\n        tmp16 =tl .where (tmp6 ,tmp14 ,tmp15 )\n        tmp17 =1 +((12 *r0_0 )//5 )\n        tmp18 =tmp17 <tmp4 \n        tmp19 =tmp2 &tmp18 \n        tmp20 =tl .broadcast_to ((-1 )+((12 *r0_0 )//5 ),[XBLOCK ,R0_BLOCK ])\n        tmp21 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp22 =tmp20 >=tmp21 \n        tmp23 =tl .full ([1 ,1 ],20 ,tl .int64 )\n        tmp24 =tmp20 <tmp23 \n        tmp25 =tmp22 &tmp24 \n        tmp26 =tmp25 &tmp19 \n        tmp27 =tl .load (in_ptr0 +(tl .broadcast_to ((-1 )+20 *r0_1 +((12 *r0_0 )//5 ),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp26 ,eviction_policy ='evict_last',other =0.0 )\n        tmp28 =tl .full (tmp27 .shape ,0.0 ,tmp27 .dtype )\n        tmp29 =tl .where (tmp19 ,tmp27 ,tmp28 )\n        tmp30 =tmp29 +tmp16 \n        tmp31 =2 +((12 *r0_0 )//5 )\n        tmp32 =tmp31 <tmp4 \n        tmp33 =tmp2 &tmp32 \n        tmp34 =tl .broadcast_to ((12 *r0_0 )//5 ,[XBLOCK ,R0_BLOCK ])\n        tmp35 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp36 =tmp34 >=tmp35 \n        tmp37 =tl .full ([1 ,1 ],20 ,tl .int64 )\n        tmp38 =tmp34 <tmp37 \n        tmp39 =tmp36 &tmp38 \n        tmp40 =tmp39 &tmp33 \n        tmp41 =tl .load (in_ptr0 +(tl .broadcast_to (20 *r0_1 +((12 *r0_0 )//5 ),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp40 ,eviction_policy ='evict_last',other =0.0 )\n        tmp42 =tl .full (tmp41 .shape ,0.0 ,tmp41 .dtype )\n        tmp43 =tl .where (tmp33 ,tmp41 ,tmp42 )\n        tmp44 =tmp43 +tmp30 \n        tmp45 =3 +((12 *r0_0 )//5 )\n        tmp46 =tmp45 <tmp4 \n        tmp47 =tmp2 &tmp46 \n        tmp48 =tl .broadcast_to (1 +((12 *r0_0 )//5 ),[XBLOCK ,R0_BLOCK ])\n        tmp49 =tl .full ([1 ,1 ],0 ,tl .int64 )\n        tmp50 =tmp48 >=tmp49 \n        tmp51 =tl .full ([1 ,1 ],20 ,tl .int64 )\n        tmp52 =tmp48 <tmp51 \n        tmp53 =tmp50 &tmp52 \n        tmp54 =tmp53 &tmp47 \n        tmp55 =tl .load (in_ptr0 +(tl .broadcast_to (1 +20 *r0_1 +((12 *r0_0 )//5 ),[XBLOCK ,R0_BLOCK ])),r0_mask &tmp54 ,eviction_policy ='evict_last',other =0.0 )\n        tmp56 =tl .full (tmp55 .shape ,0.0 ,tmp55 .dtype )\n        tmp57 =tl .where (tmp47 ,tmp55 ,tmp56 )\n        tmp58 =tmp57 +tmp44 \n        tmp59 =1.0 \n        tmp60 =tl .full (tmp59 .shape ,0.0 ,tmp59 .dtype )\n        tmp61 =tl .where (tmp6 ,tmp59 ,tmp60 )\n        tmp62 =1.0 \n        tmp63 =tl .full (tmp62 .shape ,0.0 ,tmp62 .dtype )\n        tmp64 =tl .where (tmp19 ,tmp62 ,tmp63 )\n        tmp65 =tmp64 +tmp61 \n        tmp66 =1.0 \n        tmp67 =tl .full (tmp66 .shape ,0.0 ,tmp66 .dtype )\n        tmp68 =tl .where (tmp33 ,tmp66 ,tmp67 )\n        tmp69 =tmp68 +tmp65 \n        tmp70 =1.0 \n        tmp71 =tl .full (tmp70 .shape ,0.0 ,tmp70 .dtype )\n        tmp72 =tl .where (tmp47 ,tmp70 ,tmp71 )\n        tmp73 =tmp72 +tmp69 \n        tmp74 =tmp58 /tmp73 \n        tmp75 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp76 =triton_helpers .maximum (tmp75 ,tmp74 )\n        tmp77 =triton_helpers .maximum (tmp75 ,tmp76 )\n        tmp78 =triton_helpers .maximum (tmp75 ,tmp77 )\n        tmp79 =triton_helpers .maximum (tmp75 ,tmp78 )\n        tmp80 =tl_math .exp (tmp79 )\n        tmp81 =tmp79 *tmp79 \n        tmp82 =tmp80 -tmp81 \n        tmp83 =tl .broadcast_to (tmp82 ,[XBLOCK ,R0_BLOCK ])\n        tmp85 =_tmp84 +tmp83 \n        _tmp84 =tl .where (r0_mask ,tmp85 ,_tmp84 )\n    tmp84 =tl .sum (_tmp84 ,1 )[:,None ]\n    tmp86 =10 *ks0 \n    tmp87 =tmp86 .to (tl .float32 )\n    tmp88 =tmp84 /tmp87 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp88 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg2_1 ,(1 ,s0 ,20 ),(20 *s0 ,20 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((),(),torch .float32 )\n        buf2 =buf1 ;del buf1 \n\n        10 *s0 \n        get_raw_stream (0 )\n        triton_red_fused__adaptive_avg_pool2d_exp_mean_mul_relu_sub_0 [grid (1 )](buf2 ,arg2_1 ,3 ,1 ,30 ,XBLOCK =1 ,R0_BLOCK =32 ,num_warps =2 ,num_stages =1 )\n        del arg2_1 \n    return (buf2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =20 \n    arg2_1 =rand_strided ((1 ,3 ,20 ),(60 ,20 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f72bb6e0-be87-41f6-8fbc-2b667f38270b",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['AlphaDropout', 'LPPool1d', 'GRU', 'CELU', 'MaxUnpool1d', 'ConstantPad2d', 'AdaptiveAvgPool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.alpha_dropout = nn.AlphaDropout(p=0.5)\n        self.lp_pool1d = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)\n        self.gru = nn.GRU(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n        self.celu = nn.CELU(alpha=1.0)\n        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)\n        self.constant_pad2d = nn.ConstantPad2d(padding=(1, 1, 1, 1), value=0)\n        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=10)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, height, width)\n        x = self.constant_pad2d(x)  # Apply ConstantPad2d\n        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, height * width)\n        x = self.lp_pool1d(x)  # Apply LPPool1d\n        x = self.alpha_dropout(x)  # Apply AlphaDropout\n        x = x.permute(0, 2, 1)  # Reshape to (batch_size, seq_len, features) for GRU\n        x, _ = self.gru(x)  # Apply GRU\n        x = self.celu(x)  # Apply CELU\n        x = x.permute(0, 2, 1)  # Reshape back to (batch_size, features, seq_len)\n        x = self.max_unpool1d(x, indices=torch.arange(x.size(2)).unsqueeze(0).repeat(x.size(0), 1))  # Apply MaxUnpool1d\n        x = self.adaptive_avg_pool1d(x)  # Apply AdaptiveAvgPool1d\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape (batch_size=1, channels=3, height=64, width=64)\n    return [x]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n\n# Example usage:\n# model = Model().cuda()\n# inputs = get_inputs()\n# output = model(*inputs)\n# print(output.shape)\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_abs_add_avg_pool2d_bernoulli_mul_pow_relu_sign_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp53 =tl .load (in_ptr1 +(x0 +x1 +ks1 *x1 +ks2 *x1 +x1 *((1 +ks1 *ks2 )//2 )),xmask ,eviction_policy ='evict_last')\n    tmp0 =(-1 )+((((2 *x0 )//(2 +ks2 ))%(2 +ks1 )))\n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =(-1 )+(((2 *x0 )%(2 +ks2 )))\n    tmp6 =tmp5 >=tmp1 \n    tmp7 =ks2 \n    tmp8 =tmp5 <tmp7 \n    tmp9 =tmp2 &tmp4 \n    tmp10 =tmp9 &tmp6 \n    tmp11 =tmp10 &tmp8 \n    tmp12 =tl .load (in_ptr0 +((-1 )+((-1 )*ks2 )+ks2 *((((2 *x0 )//(2 +ks2 ))%(2 +ks1 )))+ks1 *ks2 *x1 +(((2 *x0 )%(2 +ks2 )))),tmp11 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp13 =tmp12 *tmp12 \n    tmp14 =(-1 )+((((1 +2 *x0 )//(2 +ks2 ))%(2 +ks1 )))\n    tmp15 =tmp14 >=tmp1 \n    tmp16 =tmp14 <tmp3 \n    tmp17 =(-1 )+(((1 +2 *x0 )%(2 +ks2 )))\n    tmp18 =tmp17 >=tmp1 \n    tmp19 =tmp17 <tmp7 \n    tmp20 =tmp15 &tmp16 \n    tmp21 =tmp20 &tmp18 \n    tmp22 =tmp21 &tmp19 \n    tmp23 =tl .load (in_ptr0 +((-1 )+((-1 )*ks2 )+ks2 *((((1 +2 *x0 )//(2 +ks2 ))%(2 +ks1 )))+ks1 *ks2 *x1 +(((1 +2 *x0 )%(2 +ks2 )))),tmp22 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp24 =tmp23 *tmp23 \n    tmp25 =tmp24 +tmp13 \n    tmp26 =(-1 )+((((2 +2 *x0 )//(2 +ks2 ))%(2 +ks1 )))\n    tmp27 =tmp26 >=tmp1 \n    tmp28 =tmp26 <tmp3 \n    tmp29 =(-1 )+(((2 +2 *x0 )%(2 +ks2 )))\n    tmp30 =tmp29 >=tmp1 \n    tmp31 =tmp29 <tmp7 \n    tmp32 =tmp27 &tmp28 \n    tmp33 =tmp32 &tmp30 \n    tmp34 =tmp33 &tmp31 \n    tmp35 =tl .load (in_ptr0 +((-1 )+((-1 )*ks2 )+ks2 *((((2 +2 *x0 )//(2 +ks2 ))%(2 +ks1 )))+ks1 *ks2 *x1 +(((2 +2 *x0 )%(2 +ks2 )))),tmp34 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp36 =tmp35 *tmp35 \n    tmp37 =tmp36 +tmp25 \n    tmp38 =0.3333333333333333 \n    tmp39 =tmp37 *tmp38 \n    tmp40 =tl .full ([1 ],0 ,tl .int32 )\n    tmp41 =tmp40 <tmp39 \n    tmp42 =tmp41 .to (tl .int8 )\n    tmp43 =tmp39 <tmp40 \n    tmp44 =tmp43 .to (tl .int8 )\n    tmp45 =tmp42 -tmp44 \n    tmp46 =tmp45 .to (tmp39 .dtype )\n    tmp47 =tl_math .abs (tmp39 )\n    tmp48 =triton_helpers .maximum (tmp40 ,tmp47 )\n    tmp49 =tmp46 *tmp48 \n    tmp50 =3.0 \n    tmp51 =tmp49 *tmp50 \n    tmp52 =libdevice .sqrt (tmp51 )\n    tmp54 =0.5 \n    tmp55 =tmp53 <tmp54 \n    tmp56 =tmp55 .to (tl .float32 )\n    tmp57 =0.8864048946659319 \n    tmp58 =tmp56 *tmp57 \n    tmp59 =tmp52 *tmp58 \n    tmp60 =-1.0 \n    tmp61 =tmp56 +tmp60 \n    tmp62 =1.558387861036063 \n    tmp63 =tmp61 *tmp62 \n    tmp64 =0.7791939305180315 \n    tmp65 =tmp63 +tmp64 \n    tmp66 =tmp59 +tmp65 \n    tl .store (in_out_ptr0 +(x2 ),tmp66 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf1 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,1 +s1 +s2 +((1 +s1 *s2 )//2 )),(s0 +s0 *s1 +s0 *s2 +s0 *((1 +s1 *s2 )//2 ),1 +s1 +s2 +((1 +s1 *s2 )//2 ),1 ),torch .float32 )\n\n        triton_poi_fused_bernoulli_0_xnumel =s0 +s0 *s1 +s0 *s2 +s0 *((1 +s1 *s2 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (triton_poi_fused_bernoulli_0_xnumel )](buf1 ,buf2 ,0 ,6531 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del buf1 \n        s1 +s2 +((3 +s1 *s2 )//2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,1 ,s1 +s2 +((3 +s1 *s2 )//2 )),(s0 *s1 +s0 *s2 +s0 *((3 +s1 *s2 )//2 ),s1 +s2 +((3 +s1 *s2 )//2 ),s1 +s2 +((3 +s1 *s2 )//2 ),1 ),torch .float32 )\n        buf3 =reinterpret_tensor (buf0 ,(1 ,s0 ,s1 +s2 +((3 +s1 *s2 )//2 )),(s0 *s1 +s0 *s2 +s0 *((3 +s1 *s2 )//2 ),s1 +s2 +((3 +s1 *s2 )//2 ),1 ),0 );del buf0 \n\n        triton_poi_fused__to_copy_abs_add_avg_pool2d_bernoulli_mul_pow_relu_sign_1_xnumel =s0 *s1 +s0 *s2 +s0 *((3 +s1 *s2 )//2 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_abs_add_avg_pool2d_bernoulli_mul_pow_relu_sign_1 [grid (triton_poi_fused__to_copy_abs_add_avg_pool2d_bernoulli_mul_pow_relu_sign_1_xnumel )](buf3 ,arg3_1 ,buf2 ,2177 ,64 ,64 ,6531 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf2 \n    return (reinterpret_tensor (buf3 ,(1 ,s1 +s2 +((3 +s1 *s2 )//2 ),s0 ),(s0 +s0 *s1 +s0 *s2 +s0 *((1 +s1 *s2 )//2 ),1 ,s1 +s2 +((3 +s1 *s2 )//2 )),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f7ad9916-7224-41d9-8a2d-cb52c153fa75",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['MaxPool1d', 'Softshrink', 'ReflectionPad1d', 'AdaptiveAvgPool1d', 'Dropout3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.maxpool1d_1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.softshrink_1 = nn.Softshrink(lambd=0.5)\n        self.reflectionpad1d_1 = nn.ReflectionPad1d(padding=2)\n        self.adaptiveavgpool1d_1 = nn.AdaptiveAvgPool1d(output_size=10)\n        self.dropout3d_1 = nn.Dropout3d(p=0.5)\n        self.maxpool1d_2 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.softshrink_2 = nn.Softshrink(lambd=0.5)\n        self.reflectionpad1d_2 = nn.ReflectionPad1d(padding=2)\n        self.adaptiveavgpool1d_2 = nn.AdaptiveAvgPool1d(output_size=5)\n        self.dropout3d_2 = nn.Dropout3d(p=0.5)\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, length)\n        x = self.maxpool1d_1(x)\n        x = self.softshrink_1(x)\n        x = self.reflectionpad1d_1(x)\n        x = self.adaptiveavgpool1d_1(x)\n        x = x.unsqueeze(2)  # Add a dimension to make it 3D for Dropout3d\n        x = self.dropout3d_1(x)\n        x = x.squeeze(2)  # Remove the added dimension\n        x = self.maxpool1d_2(x)\n        x = self.softshrink_2(x)\n        x = self.reflectionpad1d_2(x)\n        x = self.adaptiveavgpool1d_2(x)\n        x = x.unsqueeze(2)  # Add a dimension to make it 3D for Dropout3d\n        x = self.dropout3d_2(x)\n        x = x.squeeze(2)  # Remove the added dimension\n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 128).cuda()  # Example input shape (batch_size=1, channels=3, length=128)\n    return [x]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_gt_mul_reflection_pad1d_sign_sub_where_0 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %68 )\n    x1 =xindex //68 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(126 +((-2 )*tl_math .abs ((-63 )+tl_math .abs ((-2 )+x0 )))+128 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(127 +((-2 )*tl_math .abs ((-63 )+tl_math .abs ((-2 )+x0 )))+128 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =triton_helpers .maximum (tmp1 ,tmp0 )\n    tmp3 =tl_math .abs (tmp2 )\n    tmp4 =0.5 \n    tmp5 =tmp3 >tmp4 \n    tmp6 =tl .full ([1 ],0 ,tl .int32 )\n    tmp7 =tmp6 <tmp2 \n    tmp8 =tmp7 .to (tl .int8 )\n    tmp9 =tmp2 <tmp6 \n    tmp10 =tmp9 .to (tl .int8 )\n    tmp11 =tmp8 -tmp10 \n    tmp12 =tmp11 .to (tmp2 .dtype )\n    tmp13 =tmp12 *tmp4 \n    tmp14 =tmp2 -tmp13 \n    tmp15 =0.0 \n    tmp16 =tmp2 *tmp15 \n    tmp17 =tl .where (tmp5 ,tmp14 ,tmp16 )\n    tl .store (out_ptr0 +(x2 ),tmp17 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__adaptive_avg_pool2d_1 (in_ptr0 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %10 )\n    x1 =xindex //10 \n    x2 =xindex \n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =(34 *x0 )//5 \n    tmp4 =(77 +68 *x0 )//10 \n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp2 &tmp5 \n    tmp7 =tl .load (in_ptr0 +(68 *x1 +((34 *x0 )//5 )),tmp6 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp8 =1 +((34 *x0 )//5 )\n    tmp9 =tmp8 <tmp4 \n    tmp10 =tmp2 &tmp9 \n    tmp11 =tl .load (in_ptr0 +(1 +68 *x1 +((34 *x0 )//5 )),tmp10 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp12 =tmp11 +tmp7 \n    tmp13 =2 +((34 *x0 )//5 )\n    tmp14 =tmp13 <tmp4 \n    tmp15 =tmp2 &tmp14 \n    tmp16 =tl .load (in_ptr0 +(2 +68 *x1 +((34 *x0 )//5 )),tmp15 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp17 =tmp16 +tmp12 \n    tmp18 =3 +((34 *x0 )//5 )\n    tmp19 =tmp18 <tmp4 \n    tmp20 =tmp2 &tmp19 \n    tmp21 =tl .load (in_ptr0 +(3 +68 *x1 +((34 *x0 )//5 )),tmp20 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp22 =tmp21 +tmp17 \n    tmp23 =4 +((34 *x0 )//5 )\n    tmp24 =tmp23 <tmp4 \n    tmp25 =tmp2 &tmp24 \n    tmp26 =tl .load (in_ptr0 +(4 +68 *x1 +((34 *x0 )//5 )),tmp25 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp27 =tmp26 +tmp22 \n    tmp28 =5 +((34 *x0 )//5 )\n    tmp29 =tmp28 <tmp4 \n    tmp30 =tmp2 &tmp29 \n    tmp31 =tl .load (in_ptr0 +(5 +68 *x1 +((34 *x0 )//5 )),tmp30 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp32 =tmp31 +tmp27 \n    tmp33 =6 +((34 *x0 )//5 )\n    tmp34 =tmp33 <tmp4 \n    tmp35 =tmp2 &tmp34 \n    tmp36 =tl .load (in_ptr0 +(6 +68 *x1 +((34 *x0 )//5 )),tmp35 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp37 =tmp36 +tmp32 \n    tmp38 =7 +((34 *x0 )//5 )\n    tmp39 =tmp38 <tmp4 \n    tmp40 =tmp2 &tmp39 \n    tmp41 =tl .load (in_ptr0 +(7 +68 *x1 +((34 *x0 )//5 )),tmp40 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tmp42 =tmp41 +tmp37 \n    tmp43 =1.0 \n    tmp44 =tl .full (tmp43 .shape ,0.0 ,tmp43 .dtype )\n    tmp45 =tl .where (tmp6 ,tmp43 ,tmp44 )\n    tmp46 =1.0 \n    tmp47 =tl .full (tmp46 .shape ,0.0 ,tmp46 .dtype )\n    tmp48 =tl .where (tmp10 ,tmp46 ,tmp47 )\n    tmp49 =tmp48 +tmp45 \n    tmp50 =1.0 \n    tmp51 =tl .full (tmp50 .shape ,0.0 ,tmp50 .dtype )\n    tmp52 =tl .where (tmp15 ,tmp50 ,tmp51 )\n    tmp53 =tmp52 +tmp49 \n    tmp54 =1.0 \n    tmp55 =tl .full (tmp54 .shape ,0.0 ,tmp54 .dtype )\n    tmp56 =tl .where (tmp20 ,tmp54 ,tmp55 )\n    tmp57 =tmp56 +tmp53 \n    tmp58 =1.0 \n    tmp59 =tl .full (tmp58 .shape ,0.0 ,tmp58 .dtype )\n    tmp60 =tl .where (tmp25 ,tmp58 ,tmp59 )\n    tmp61 =tmp60 +tmp57 \n    tmp62 =1.0 \n    tmp63 =tl .full (tmp62 .shape ,0.0 ,tmp62 .dtype )\n    tmp64 =tl .where (tmp30 ,tmp62 ,tmp63 )\n    tmp65 =tmp64 +tmp61 \n    tmp66 =1.0 \n    tmp67 =tl .full (tmp66 .shape ,0.0 ,tmp66 .dtype )\n    tmp68 =tl .where (tmp35 ,tmp66 ,tmp67 )\n    tmp69 =tmp68 +tmp65 \n    tmp70 =1.0 \n    tmp71 =tl .full (tmp70 .shape ,0.0 ,tmp70 .dtype )\n    tmp72 =tl .where (tmp40 ,tmp70 ,tmp71 )\n    tmp73 =tmp72 +tmp69 \n    tmp74 =tmp42 /tmp73 \n    tl .store (out_ptr0 +(x2 ),tmp74 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_2 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_abs_gt_mul_reflection_pad1d_sign_sub_where_3 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %9 )\n    x1 =xindex //9 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(8 +((-2 )*tl_math .abs ((-4 )+tl_math .abs ((-2 )+x0 )))+10 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(0 ))\n    tmp2 =tl .broadcast_to (tmp1 ,[XBLOCK ])\n    tmp9 =tl .load (in_ptr0 +(9 +((-2 )*tl_math .abs ((-4 )+tl_math .abs ((-2 )+x0 )))+10 *x1 ),xmask ,eviction_policy ='evict_last')\n    tmp3 =0.5 \n    tmp4 =tmp2 <tmp3 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =2.0 \n    tmp7 =tmp5 *tmp6 \n    tmp8 =tmp0 *tmp7 \n    tmp10 =tmp9 *tmp7 \n    tmp11 =triton_helpers .maximum (tmp10 ,tmp8 )\n    tmp12 =tl_math .abs (tmp11 )\n    tmp13 =tmp12 >tmp3 \n    tmp14 =tl .full ([1 ],0 ,tl .int32 )\n    tmp15 =tmp14 <tmp11 \n    tmp16 =tmp15 .to (tl .int8 )\n    tmp17 =tmp11 <tmp14 \n    tmp18 =tmp17 .to (tl .int8 )\n    tmp19 =tmp16 -tmp18 \n    tmp20 =tmp19 .to (tmp11 .dtype )\n    tmp21 =tmp20 *tmp3 \n    tmp22 =tmp11 -tmp21 \n    tmp23 =0.0 \n    tmp24 =tmp11 *tmp23 \n    tmp25 =tl .where (tmp13 ,tmp22 ,tmp24 )\n    tl .store (out_ptr0 +(x2 ),tmp25 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_4 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_5 (in_ptr0 ,in_ptr1 ,out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %5 )\n    x1 =xindex //5 \n    x2 =xindex \n    tmp30 =tl .load (in_ptr1 +(0 ))\n    tmp31 =tl .broadcast_to (tmp30 ,[XBLOCK ])\n    tmp0 =tl .full ([1 ],0 ,tl .int64 )\n    tmp1 =tl .full ([1 ],1 ,tl .int64 )\n    tmp2 =tmp0 <tmp1 \n    tmp3 =(9 *x0 )//5 \n    tmp4 =(13 +9 *x0 )//5 \n    tmp5 =tmp3 <tmp4 \n    tmp6 =tmp2 &tmp5 \n    tmp7 =tl .load (in_ptr0 +(9 *x1 +((9 *x0 )//5 )),tmp6 &xmask ,other =0.0 )\n    tmp8 =1 +((9 *x0 )//5 )\n    tmp9 =tmp8 <tmp4 \n    tmp10 =tmp2 &tmp9 \n    tmp11 =tl .load (in_ptr0 +(1 +9 *x1 +((9 *x0 )//5 )),tmp10 &xmask ,other =0.0 )\n    tmp12 =tmp11 +tmp7 \n    tmp13 =2 +((9 *x0 )//5 )\n    tmp14 =tmp13 <tmp4 \n    tmp15 =tmp2 &tmp14 \n    tmp16 =tl .load (in_ptr0 +(2 +9 *x1 +((9 *x0 )//5 )),tmp15 &xmask ,other =0.0 )\n    tmp17 =tmp16 +tmp12 \n    tmp18 =1.0 \n    tmp19 =tl .full (tmp18 .shape ,0.0 ,tmp18 .dtype )\n    tmp20 =tl .where (tmp6 ,tmp18 ,tmp19 )\n    tmp21 =1.0 \n    tmp22 =tl .full (tmp21 .shape ,0.0 ,tmp21 .dtype )\n    tmp23 =tl .where (tmp10 ,tmp21 ,tmp22 )\n    tmp24 =tmp23 +tmp20 \n    tmp25 =1.0 \n    tmp26 =tl .full (tmp25 .shape ,0.0 ,tmp25 .dtype )\n    tmp27 =tl .where (tmp15 ,tmp25 ,tmp26 )\n    tmp28 =tmp27 +tmp24 \n    tmp29 =tmp17 /tmp28 \n    tmp32 =0.5 \n    tmp33 =tmp31 <tmp32 \n    tmp34 =tmp33 .to (tl .float32 )\n    tmp35 =2.0 \n    tmp36 =tmp34 *tmp35 \n    tmp37 =tmp29 *tmp36 \n    tl .store (out_ptr0 +(x2 ),tmp37 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    assert_size_stride (arg1_1 ,(1 ,s0 ,128 ),(128 *s0 ,128 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,68 ),(68 *s0 ,68 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_gt_mul_reflection_pad1d_sign_sub_where_0_xnumel =68 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_gt_mul_reflection_pad1d_sign_sub_where_0 [grid (triton_poi_fused_abs_gt_mul_reflection_pad1d_sign_sub_where_0_xnumel )](arg1_1 ,buf0 ,204 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg1_1 \n        buf1 =empty_strided_cuda ((1 ,s0 ,1 ,10 ),(10 *s0 ,10 ,10 ,1 ),torch .float32 )\n\n        triton_poi_fused__adaptive_avg_pool2d_1_xnumel =10 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused__adaptive_avg_pool2d_1 [grid (triton_poi_fused__adaptive_avg_pool2d_1_xnumel )](buf0 ,buf1 ,30 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf2 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf2 )\n        buf3 =empty_strided_cuda ((1 ,1 ,1 ,1 ,1 ),(1 ,1 ,1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_2 [grid (1 )](buf2 ,buf3 ,0 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        buf4 =empty_strided_cuda ((1 ,s0 ,9 ),(9 *s0 ,9 ,1 ),torch .float32 )\n\n        triton_poi_fused_abs_gt_mul_reflection_pad1d_sign_sub_where_3_xnumel =9 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused_abs_gt_mul_reflection_pad1d_sign_sub_where_3 [grid (triton_poi_fused_abs_gt_mul_reflection_pad1d_sign_sub_where_3_xnumel )](buf1 ,buf3 ,buf4 ,27 ,XBLOCK =32 ,num_warps =1 ,num_stages =1 )\n        del buf1 \n        buf5 =buf3 ;del buf3 \n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_4 [grid (1 )](buf2 ,buf5 ,1 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf2 \n        buf6 =empty_strided_cuda ((1 ,1 ,s0 ,1 ,5 ),(5 *s0 ,5 *s0 ,5 ,5 ,1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_bernoulli_div_mul_5_xnumel =5 *s0 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_5 [grid (triton_poi_fused__to_copy_bernoulli_div_mul_5_xnumel )](buf4 ,buf5 ,buf6 ,15 ,XBLOCK =16 ,num_warps =1 ,num_stages =1 )\n        del buf4 \n        del buf5 \n    return (reinterpret_tensor (buf6 ,(1 ,s0 ,5 ),(5 *s0 ,5 ,1 ),0 ),)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =rand_strided ((1 ,3 ,128 ),(384 ,128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f874db18-448f-4c45-bf98-d43877b51a58",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['PairwiseDistance', 'ConstantPad1d', 'MultiMarginLoss', 'ConvTranspose3d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad1d = nn.ConstantPad1d(padding=2, value=0)\n        self.conv_transpose3d_1 = nn.ConvTranspose3d(1, 10, kernel_size=3, stride=2)\n        self.conv_transpose3d_2 = nn.ConvTranspose3d(10, 20, kernel_size=3, stride=2)\n        self.pairwise_distance = nn.PairwiseDistance(p=2)\n        self.multi_margin_loss = nn.MultiMarginLoss()\n\n    def forward(self, x):\n        # Assuming input x is of shape (batch_size, channels, depth, height, width)\n        # Apply padding to the input\n        x = self.pad1d(x)\n        \n        # Apply ConvTranspose3d layers\n        x = F.relu(self.conv_transpose3d_1(x))\n        x = F.relu(self.conv_transpose3d_2(x))\n        \n        # Reshape the output to fit PairwiseDistance input requirements\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x1 = x[:, :x.size(1)//2]  # Split the tensor into two halves\n        x2 = x[:, x.size(1)//2:]\n        \n        # Compute PairwiseDistance\n        x = self.pairwise_distance(x1, x2)\n        \n        # Compute MultiMarginLoss (assuming target is a dummy tensor)\n        target = torch.randint(0, 1, (x.size(0),), device=x.device)\n        loss = self.multi_margin_loss(x.unsqueeze(1), target)\n        \n        return loss\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 1, 10, 10, 10).cuda()  # Example input shape\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_constant_pad_nd_0 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =(-2 )+x0 \n    tmp1 =tl .full ([1 ],0 ,tl .int64 )\n    tmp2 =tmp0 >=tmp1 \n    tmp3 =ks1 \n    tmp4 =tmp0 <tmp3 \n    tmp5 =tmp2 &tmp4 \n    tmp6 =tl .load (in_ptr0 +((-2 )+x0 +ks1 *x1 ),tmp5 &xmask ,eviction_policy ='evict_last',other =0.0 )\n    tl .store (out_ptr0 +(x2 ),tmp6 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_relu_1 (in_out_ptr0 ,in_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_out_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr0 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tl .store (in_out_ptr0 +(x2 ),tmp4 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_norm_sub_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =134 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp19 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )\n        tmp1 =1710 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 \n        tmp2 =tmp0 <tmp1 \n        tmp3 =tl .load (in_ptr0 +(19 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(19 +4 *ks2 ))%(3 +4 *ks1 )))+57 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+171 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+4 *ks2 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(19 +4 *ks2 ))%(3 +4 *ks1 )))+12 *ks2 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+36 *ks2 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+76 *ks1 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+228 *ks0 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+228 *ks1 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+16 *ks1 *ks2 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+48 *ks0 *ks2 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+48 *ks1 *ks2 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+304 *ks0 *ks1 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+64 *ks0 *ks1 *ks2 *((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+(((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))%(19 +4 *ks2 )))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp4 =tl .load (in_ptr1 +((((r0_1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 ))//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp5 =tmp3 +tmp4 \n        tmp6 =tl .full ([1 ,1 ],0 ,tl .int32 )\n        tmp7 =triton_helpers .maximum (tmp6 ,tmp5 )\n        tmp8 =tl .load (in_ptr0 +(19 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(19 +4 *ks2 ))%(3 +4 *ks1 )))+57 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+171 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+4 *ks2 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(19 +4 *ks2 ))%(3 +4 *ks1 )))+12 *ks2 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+36 *ks2 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+76 *ks1 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+228 *ks0 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+228 *ks1 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+16 *ks1 *ks2 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+48 *ks0 *ks2 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+48 *ks1 *ks2 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+304 *ks0 *ks1 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+64 *ks0 *ks1 *ks2 *((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+(((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )%(19 +4 *ks2 )))),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp9 =tl .load (in_ptr1 +((((1710 +r0_1 +360 *ks2 +2280 *ks0 +2280 *ks1 +x0 *((1843 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//134 )+480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 )),r0_mask &tmp2 &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp10 =tmp8 +tmp9 \n        tmp11 =triton_helpers .maximum (tmp6 ,tmp10 )\n        tmp12 =tmp7 -tmp11 \n        tmp13 =1e-06 \n        tmp14 =tmp12 +tmp13 \n        tmp15 =tmp14 *tmp14 \n        tmp16 =tl .full (tmp15 .shape ,0 ,tmp15 .dtype )\n        tmp17 =tl .where (tmp2 ,tmp15 ,tmp16 )\n        tmp18 =tl .broadcast_to (tmp17 ,[XBLOCK ,R0_BLOCK ])\n        tmp20 =_tmp19 +tmp18 \n        _tmp19 =tl .where (r0_mask &xmask ,tmp20 ,_tmp19 )\n    tmp19 =tl .sum (_tmp19 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp19 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_arange_clamp_min_gather_ge_mean_ne_norm_randint_rsub_scalar_tensor_sub_where_3 (in_out_ptr0 ,in_ptr0 ,out_ptr0 ,out_ptr1 ,out_ptr2 ,out_ptr3 ,load_seed_offset ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    r0_numel =134 \n    R0_BLOCK :tl .constexpr =256 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    r0_mask =r0_index <r0_numel \n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),r0_mask ,other =0.0 )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .where (r0_mask ,tmp1 ,0 )\n    tmp4 =tl .sum (tmp3 ,1 )[:,None ]\n    tmp5 =tl .load (in_out_ptr0 +load_seed_offset )\n    tmp6 =tl .full ([1 ,1 ],0 ,tl .int32 )\n    tmp7 =tl .full ([1 ,1 ],0 ,tl .int64 )\n    tmp8 =tl .full ([1 ,1 ],1 ,tl .int64 )\n    tmp9 =triton_helpers .randint64 (tmp5 ,(tmp6 ).to (tl .uint32 ),tmp7 ,tmp8 )\n    tmp10 =tmp7 !=tmp9 \n    tmp11 =tl .full ([XBLOCK ,1 ],1 ,tl .int32 )\n    tmp12 =tmp9 +tmp11 \n    tmp13 =tmp9 <0 \n    tmp14 =tl .where (tmp13 ,tmp12 ,tmp9 )\n    tl .device_assert ((0 <=tmp14 )&(tmp14 <1 ),\"index out of bounds: 0 <= tmp14 < 1\")\n    tmp16 =libdevice .sqrt (tmp4 )\n    tmp17 =1.0 \n    tmp18 =tmp17 -tmp16 \n    tmp19 =tmp18 +tmp16 \n    tmp20 =0.0 \n    tmp21 =triton_helpers .maximum (tmp19 ,tmp20 )\n    tmp22 =tl .where (tmp10 ,tmp21 ,tmp20 )\n    tmp23 =tmp22 /tmp17 \n    tmp24 =tmp19 >=tmp20 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp9 ,None )\n    tl .store (out_ptr1 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp10 ,None )\n    tl .store (out_ptr2 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp23 ,None )\n    tl .store (out_ptr3 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp24 ,None )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp4 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_add_div_eq_masked_fill_scalar_tensor_sub_4 (in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(0 ))\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ])\n    tmp5 =tl .load (in_ptr1 +(19 *(((x0 //(19 +4 *ks2 ))%(3 +4 *ks1 )))+57 *(((x0 //(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+171 *(((x0 //(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+4 *ks2 *(((x0 //(19 +4 *ks2 ))%(3 +4 *ks1 )))+12 *ks2 *(((x0 //(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+36 *ks2 *(((x0 //(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+76 *ks1 *(((x0 //(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+228 *ks0 *(((x0 //(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+228 *ks1 *(((x0 //(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+16 *ks1 *ks2 *(((x0 //(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+48 *ks0 *ks2 *(((x0 //(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+48 *ks1 *ks2 *(((x0 //(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+304 *ks0 *ks1 *(((x0 //(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+64 *ks0 *ks1 *ks2 *(((x0 //(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 ))+((x0 %(19 +4 *ks2 )))),xmask ,eviction_policy ='evict_last')\n    tmp6 =tl .load (in_ptr2 +(((x0 //(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))%20 )),xmask ,eviction_policy ='evict_last')\n    tmp10 =tl .load (in_ptr1 +(19 *((((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(19 +4 *ks2 ))%(3 +4 *ks1 )))+57 *((((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+171 *((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))+4 *ks2 *((((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(19 +4 *ks2 ))%(3 +4 *ks1 )))+12 *ks2 *((((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+36 *ks2 *((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))+76 *ks1 *((((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+228 *ks0 *((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))+228 *ks1 *((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))+16 *ks1 *ks2 *((((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(57 +12 *ks2 +76 *ks1 +16 *ks1 *ks2 ))%(3 +4 *ks0 )))+48 *ks0 *ks2 *((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))+48 *ks1 *ks2 *((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))+304 *ks0 *ks1 *((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))+64 *ks0 *ks1 *ks2 *((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 ))+(((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )%(19 +4 *ks2 )))),xmask ,eviction_policy ='evict_last')\n    tmp11 =tl .load (in_ptr2 +((1710 +x0 +360 *ks2 +2280 *ks0 +2280 *ks1 +480 *ks0 *ks2 +480 *ks1 *ks2 +3040 *ks0 *ks1 +640 *ks0 *ks1 *ks2 )//(171 +36 *ks2 +228 *ks0 +228 *ks1 +48 *ks0 *ks2 +48 *ks1 *ks2 +304 *ks0 *ks1 +64 *ks0 *ks1 *ks2 )),xmask ,eviction_policy ='evict_last')\n    tmp2 =libdevice .sqrt (tmp1 )\n    tmp3 =0.0 \n    tmp4 =tmp2 ==tmp3 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =tl .full ([1 ],0 ,tl .int32 )\n    tmp9 =triton_helpers .maximum (tmp8 ,tmp7 )\n    tmp12 =tmp10 +tmp11 \n    tmp13 =triton_helpers .maximum (tmp8 ,tmp12 )\n    tmp14 =tmp9 -tmp13 \n    tmp15 =1e-06 \n    tmp16 =tmp14 +tmp15 \n    tmp17 =tmp16 /tmp2 \n    tmp18 =tl .where (tmp4 ,tmp3 ,tmp17 )\n    tl .store (out_ptr0 +(x0 ),tmp18 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_convolution_relu_threshold_backward_5 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x1 =xindex //ks0 \n    tmp0 =tl .load (in_ptr0 +(x2 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =tl .load (in_ptr1 +(x1 ),xmask ,eviction_policy ='evict_last')\n    tmp2 =tmp0 +tmp1 \n    tmp3 =tl .full ([1 ],0 ,tl .int32 )\n    tmp4 =triton_helpers .maximum (tmp3 ,tmp2 )\n    tmp5 =0.0 \n    tmp6 =tmp4 <=tmp5 \n    tl .store (out_ptr0 +(x2 ),tmp6 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 =args \n    args .clear ()\n    s0 =primals_1 \n    s1 =primals_2 \n    s2 =primals_3 \n    assert_size_stride (primals_4 ,(1 ,1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    assert_size_stride (primals_5 ,(1 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_6 ,(10 ,),(1 ,))\n    assert_size_stride (primals_7 ,(10 ,20 ,3 ,3 ,3 ),(540 ,27 ,9 ,3 ,1 ))\n    assert_size_stride (primals_8 ,(20 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        4 +s2 \n        buf0 =empty_strided_cuda ((1 ,1 ,s0 ,s1 ,4 +s2 ),(4 *s0 *s1 +s0 *s1 *s2 ,4 *s0 *s1 +s0 *s1 *s2 ,4 *s1 +s1 *s2 ,4 +s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_constant_pad_nd_0_xnumel =4 *s0 *s1 +s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_constant_pad_nd_0 [grid (triton_poi_fused_constant_pad_nd_0_xnumel )](primals_4 ,buf0 ,14 ,10 ,1400 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del primals_4 \n\n        buf1 =extern_kernels .convolution (buf0 ,primals_5 ,stride =(2 ,2 ,2 ),padding =(0 ,0 ,0 ),dilation =(1 ,1 ,1 ),transposed =True ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf1 ,(1 ,10 ,1 +2 *s0 ,1 +2 *s1 ,9 +2 *s2 ),(90 +20 *s2 +180 *s0 +180 *s1 +40 *s0 *s2 +40 *s1 *s2 +360 *s0 *s1 +80 *s0 *s1 *s2 ,9 +2 *s2 +18 *s0 +18 *s1 +4 *s0 *s2 +4 *s1 *s2 +36 *s0 *s1 +8 *s0 *s1 *s2 ,9 +2 *s2 +18 *s1 +4 *s1 *s2 ,9 +2 *s2 ,1 ))\n        9 +2 *s2 +18 *s0 +18 *s1 +4 *s0 *s2 +4 *s1 *s2 +36 *s0 *s1 +8 *s0 *s1 *s2 \n        buf2 =buf1 ;del buf1 \n\n        triton_poi_fused_convolution_relu_1_xnumel =90 +20 *s2 +180 *s0 +180 *s1 +40 *s0 *s2 +40 *s1 *s2 +360 *s0 *s1 +80 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_convolution_relu_1 [grid (triton_poi_fused_convolution_relu_1_xnumel )](buf2 ,primals_6 ,12789 ,127890 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del primals_6 \n\n        buf3 =extern_kernels .convolution (buf2 ,primals_7 ,stride =(2 ,2 ,2 ),padding =(0 ,0 ,0 ),dilation =(1 ,1 ,1 ),transposed =True ,output_padding =(0 ,0 ,0 ),groups =1 ,bias =None )\n        assert_size_stride (buf3 ,(1 ,20 ,3 +4 *s0 ,3 +4 *s1 ,19 +4 *s2 ),(3420 +720 *s2 +4560 *s0 +4560 *s1 +960 *s0 *s2 +960 *s1 *s2 +6080 *s0 *s1 +1280 *s0 *s1 *s2 ,171 +36 *s2 +228 *s0 +228 *s1 +48 *s0 *s2 +48 *s1 *s2 +304 *s0 *s1 +64 *s0 *s1 *s2 ,57 +12 *s2 +76 *s1 +16 *s1 *s2 ,19 +4 *s2 ,1 ))\n        buf4 =empty_strided_cuda ((1 ,134 ),(134 ,1 ),torch .float32 )\n\n        (1843 +360 *s2 +2280 *s0 +2280 *s1 +480 *s0 *s2 +480 *s1 *s2 +3040 *s0 *s1 +640 *s0 *s1 *s2 )//134 \n        get_raw_stream (0 )\n        triton_red_fused_add_norm_sub_2 [grid (134 )](buf3 ,primals_8 ,buf4 ,10 ,10 ,10 ,134 ,8142 ,XBLOCK =1 ,R0_BLOCK =1024 ,num_warps =16 ,num_stages =1 )\n        buf6 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf6 )\n        buf5 =empty_strided_cuda ((1 ,),(1 ,),torch .float32 )\n        buf7 =buf6 ;del buf6 \n        buf8 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .bool )\n        buf11 =empty_strided_cuda ((),(),torch .float32 )\n        buf12 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .bool )\n\n        get_raw_stream (0 )\n        triton_per_fused_add_arange_clamp_min_gather_ge_mean_ne_norm_randint_rsub_scalar_tensor_sub_where_3 [grid (1 )](buf7 ,buf4 ,buf5 ,buf8 ,buf11 ,buf12 ,0 ,1 ,134 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf4 \n        buf9 =empty_strided_cuda ((1 ,1710 +360 *s2 +2280 *s0 +2280 *s1 +480 *s0 *s2 +480 *s1 *s2 +3040 *s0 *s1 +640 *s0 *s1 *s2 ),(1710 +360 *s2 +2280 *s0 +2280 *s1 +480 *s0 *s2 +480 *s1 *s2 +3040 *s0 *s1 +640 *s0 *s1 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_add_div_eq_masked_fill_scalar_tensor_sub_4_xnumel =1710 +360 *s2 +2280 *s0 +2280 *s1 +480 *s0 *s2 +480 *s1 *s2 +3040 *s0 *s1 +640 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_add_div_eq_masked_fill_scalar_tensor_sub_4 [grid (triton_poi_fused_add_div_eq_masked_fill_scalar_tensor_sub_4_xnumel )](buf5 ,buf3 ,primals_8 ,buf9 ,10 ,10 ,10 ,1090910 ,XBLOCK =512 ,num_warps =8 ,num_stages =1 )\n        del buf5 \n        171 +36 *s2 +228 *s0 +228 *s1 +48 *s0 *s2 +48 *s1 *s2 +304 *s0 *s1 +64 *s0 *s1 *s2 \n        buf10 =empty_strided_cuda ((1 ,20 ,3 +4 *s0 ,3 +4 *s1 ,19 +4 *s2 ),(3420 +720 *s2 +4560 *s0 +4560 *s1 +960 *s0 *s2 +960 *s1 *s2 +6080 *s0 *s1 +1280 *s0 *s1 *s2 ,171 +36 *s2 +228 *s0 +228 *s1 +48 *s0 *s2 +48 *s1 *s2 +304 *s0 *s1 +64 *s0 *s1 *s2 ,57 +12 *s2 +76 *s1 +16 *s1 *s2 ,19 +4 *s2 ,1 ),torch .bool )\n\n        triton_poi_fused_convolution_relu_threshold_backward_5_xnumel =3420 +720 *s2 +4560 *s0 +4560 *s1 +960 *s0 *s2 +960 *s1 *s2 +6080 *s0 *s1 +1280 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_convolution_relu_threshold_backward_5 [grid (triton_poi_fused_convolution_relu_threshold_backward_5_xnumel )](buf3 ,primals_8 ,buf10 ,109091 ,2181820 ,XBLOCK =1024 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n        del primals_8 \n    return (buf11 ,primals_5 ,primals_7 ,buf0 ,buf2 ,reinterpret_tensor (buf7 ,(1 ,1 ),(1 ,1 ),0 ),buf8 ,buf12 ,buf9 ,buf10 ,s0 ,s1 ,s2 ,3 +4 *s0 ,3 +4 *s1 ,19 +4 *s2 ,3420 +720 *s2 +4560 *s0 +4560 *s1 +960 *s0 *s2 +960 *s1 *s2 +6080 *s0 *s1 +1280 *s0 *s1 *s2 ,1710 +360 *s2 +2280 *s0 +2280 *s1 +480 *s0 *s2 +480 *s1 *s2 +3040 *s0 *s1 +640 *s0 *s1 *s2 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =10 \n    primals_2 =10 \n    primals_3 =10 \n    primals_4 =rand_strided ((1 ,1 ,10 ,10 ,10 ),(1000 ,1000 ,100 ,10 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((1 ,10 ,3 ,3 ,3 ),(270 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((10 ,20 ,3 ,3 ,3 ),(540 ,27 ,9 ,3 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((20 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "f9d8eeb0-938d-4a54-9bae-4b10d10e129d",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['GELU', 'ConstantPad2d', 'Identity', 'HingeEmbeddingLoss', 'Upsample', 'MaxUnpool1d']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.pad = nn.ConstantPad2d(1, 0.5)\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)\n        self.gelu = nn.GELU()\n        self.identity = nn.Identity()\n        self.hinge_loss = nn.HingeEmbeddingLoss()\n\n    def forward(self, x):\n        # Apply ConstantPad2d\n        x = self.pad(x)\n        \n        # Apply Upsample\n        x = self.upsample(x)\n        \n        # Reshape for MaxUnpool1d\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels * height, width)\n        \n        # Apply MaxUnpool1d (requires indices from a previous MaxPool1d)\n        # For simplicity, we assume the input is already pooled and indices are available\n        # Here, we create dummy indices for demonstration\n        pool_output, indices = F.max_pool1d(x, kernel_size=2, stride=2, return_indices=True)\n        x = self.max_unpool1d(pool_output, indices)\n        \n        # Reshape back to original dimensions\n        x = x.view(batch_size, channels, height, width)\n        \n        # Apply GELU\n        x = self.gelu(x)\n        \n        # Apply Identity\n        x = self.identity(x)\n        \n        # Apply HingeEmbeddingLoss (requires a target, which we create here for demonstration)\n        target = torch.ones_like(x)\n        loss = self.hinge_loss(x, target)\n        \n        # Return the loss as part of the output (for demonstration purposes)\n        return x, loss\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32).cuda()\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_pool2d_with_indices_0 (in_ptr0 ,out_ptr0 ,out_ptr1 ,ks0 ,ks1 ,ks2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x2 =xindex \n    x0 =(xindex %ks2 )\n    x1 =xindex //ks2 \n    tmp0 =2.0 \n    tmp1 =ks0 \n    tmp2 =tmp1 .to (tl .float32 )\n    tmp3 =tmp0 +tmp2 \n    tmp4 =tmp3 .to (tl .float64 )\n    tmp5 =tl .full ([1 ],2.0 ,tl .float64 )\n    tmp6 =tmp5 *tmp4 \n    tmp7 =tmp4 /tmp6 \n    tmp8 =tmp7 .to (tl .float32 )\n    tmp9 =((x2 //(2 +ks1 ))%(4 +2 *ks0 ))\n    tmp10 =tmp9 .to (tl .float32 )\n    tmp11 =tmp10 *tmp8 \n    tmp12 =tmp11 .to (tl .int64 )\n    tmp13 =2 +ks0 \n    tmp14 =tmp12 +tmp13 \n    tmp15 =tmp12 <0 \n    tmp16 =tl .where (tmp15 ,tmp14 ,tmp12 )\n    tmp17 =ks1 \n    tmp18 =tmp17 .to (tl .float32 )\n    tmp19 =tmp0 +tmp18 \n    tmp20 =tmp19 .to (tl .float64 )\n    tmp21 =tmp5 *tmp20 \n    tmp22 =tmp20 /tmp21 \n    tmp23 =tmp22 .to (tl .float32 )\n    tmp24 =2 *x0 \n    tmp25 =tmp24 .to (tl .float32 )\n    tmp26 =tmp25 *tmp23 \n    tmp27 =tmp26 .to (tl .int64 )\n    tmp28 =ks2 \n    tmp29 =tmp27 +tmp28 \n    tmp30 =tmp27 <0 \n    tmp31 =tl .where (tmp30 ,tmp29 ,tmp27 )\n    tmp32 =(-1 )+tmp16 \n    tmp33 =tmp32 .to (tl .int32 )\n    tmp34 =tl .full ([1 ],0 ,tl .int64 )\n    tmp35 =tmp33 >=tmp34 \n    tmp36 =tmp33 <tmp1 \n    tmp37 =(-1 )+tmp31 \n    tmp38 =tmp37 .to (tl .int32 )\n    tmp39 =tmp38 >=tmp34 \n    tmp40 =tmp38 <tmp17 \n    tmp41 =tmp35 &tmp36 \n    tmp42 =tmp41 &tmp39 \n    tmp43 =tmp42 &tmp40 \n    tmp44 =tl .load (in_ptr0 +((-1 )+tmp31 +((-1 )*ks1 )+ks1 *tmp16 +ks0 *ks1 *(x1 //(4 +2 *ks0 ))),tmp43 &xmask ,eviction_policy ='evict_last',other =0.5 )\n    tmp45 =((x2 //ks2 )%(4 +2 *ks0 ))\n    tmp46 =tmp45 .to (tl .float32 )\n    tmp47 =tmp46 *tmp8 \n    tmp48 =tmp47 .to (tl .int64 )\n    tmp49 =tmp48 +tmp13 \n    tmp50 =tmp48 <0 \n    tmp51 =tl .where (tmp50 ,tmp49 ,tmp48 )\n    tmp52 =1 +2 *x0 \n    tmp53 =tmp52 .to (tl .float32 )\n    tmp54 =tmp53 *tmp23 \n    tmp55 =tmp54 .to (tl .int64 )\n    tmp56 =tmp55 +tmp28 \n    tmp57 =tmp55 <0 \n    tmp58 =tl .where (tmp57 ,tmp56 ,tmp55 )\n    tmp59 =(-1 )+tmp51 \n    tmp60 =tmp59 .to (tl .int32 )\n    tmp61 =tmp60 >=tmp34 \n    tmp62 =tmp60 <tmp1 \n    tmp63 =(-1 )+tmp58 \n    tmp64 =tmp63 .to (tl .int32 )\n    tmp65 =tmp64 >=tmp34 \n    tmp66 =tmp64 <tmp17 \n    tmp67 =tmp61 &tmp62 \n    tmp68 =tmp67 &tmp65 \n    tmp69 =tmp68 &tmp66 \n    tmp70 =tl .load (in_ptr0 +((-1 )+tmp58 +((-1 )*ks1 )+ks1 *tmp51 +ks0 *ks1 *(x1 //(4 +2 *ks0 ))),tmp69 &xmask ,eviction_policy ='evict_last',other =0.5 )\n    tmp71 =triton_helpers .maximum (tmp70 ,tmp44 )\n    tmp72 =tmp67 &tmp39 \n    tmp73 =tmp72 &tmp40 \n    tmp74 =tl .load (in_ptr0 +((-1 )+tmp31 +((-1 )*ks1 )+ks1 *tmp51 +ks0 *ks1 *(x1 //(4 +2 *ks0 ))),tmp73 &xmask ,eviction_policy ='evict_last',other =0.5 )\n    tmp75 =tmp70 >tmp74 \n    tmp76 =tl .full ([1 ],1 ,tl .int8 )\n    tmp77 =tl .full ([1 ],0 ,tl .int8 )\n    tmp78 =tl .where (tmp75 ,tmp76 ,tmp77 )\n    triton_helpers .maximum (tmp70 ,tmp74 )\n    tl .store (out_ptr0 +(x2 ),tmp71 ,xmask )\n    tl .store (out_ptr1 +(x2 ),tmp78 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_max_unpool2d_2 (in_ptr0 ,in_ptr1 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *(x0 //ks0 )+ks1 *(x0 //ks0 )+((x0 %ks0 ))),xmask ,eviction_policy ='evict_last')\n    tmp19 =tl .load (in_ptr1 +(x0 ),xmask )\n    tmp1 =tl .full ([1 ],2 ,tl .int32 )\n    tmp2 =tl .where ((tmp0 <0 )!=(tmp1 <0 ),tl .where (tmp0 %tmp1 !=0 ,tmp0 //tmp1 -1 ,tmp0 //tmp1 ),tmp0 //tmp1 )\n    tmp3 =tmp2 *tmp1 \n    tmp4 =tmp0 -tmp3 \n    tmp5 =tl .full ([1 ],0 ,tl .int64 )\n    tmp6 =tmp5 +tmp2 \n    tmp7 =2 *((x0 %ks0 ))\n    tmp8 =tmp7 +tmp4 \n    tmp9 =4 +2 *ks1 \n    tmp10 =tmp6 *tmp9 \n    tmp11 =tmp10 +tmp8 \n    tmp12 =4 *(x0 //ks0 )+2 *ks1 *(x0 //ks0 )\n    tmp13 =tmp11 +tmp12 \n    tmp14 =16 *ks2 +8 *ks1 *ks2 +8 *ks2 *ks3 +4 *ks1 *ks2 *ks3 \n    tmp15 =tmp13 +tmp14 \n    tmp16 =tmp13 <0 \n    tmp17 =tl .where (tmp16 ,tmp15 ,tmp13 )\n    tl .device_assert (((0 <=tmp17 )&(tmp17 <16 *ks2 +8 *ks1 *ks2 +8 *ks2 *ks3 +4 *ks1 *ks2 *ks3 ))|~(xmask ),\"index out of bounds: 0 <= tmp17 < 16*ks2 + 8*ks1*ks2 + 8*ks2*ks3 + 4*ks1*ks2*ks3\")\n    tl .store (out_ptr0 +(tl .broadcast_to (4 *(((tmp17 //(4 +2 *ks1 ))%(4 *ks2 +2 *ks2 *ks3 )))+2 *ks1 *(((tmp17 //(4 +2 *ks1 ))%(4 *ks2 +2 *ks2 *ks3 )))+((tmp17 %(4 +2 *ks1 ))),[XBLOCK ])),tmp19 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_gelu_3 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =xindex //ks2 \n    x3 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 +4 *(((x1 +4 *x2 +2 *ks4 *x2 )%(4 *ks3 +2 *ks3 *ks4 )))+2 *ks5 *(((x1 +4 *x2 +2 *ks4 *x2 )%(4 *ks3 +2 *ks3 *ks4 )))),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.5 \n    tmp2 =tmp0 *tmp1 \n    tmp3 =0.7071067811865476 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .erf (tmp4 )\n    tmp6 =1.0 \n    tmp7 =tmp5 +tmp6 \n    tmp8 =tmp2 *tmp7 \n    tl .store (out_ptr0 +(x3 ),tmp8 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_red_fused_add_clamp_min_fill_mean_ne_sub_where_zeros_like_4 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ,R0_BLOCK :tl .constexpr ):\n    xnumel =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    xmask =xindex <xnumel \n    r0_base =tl .arange (0 ,R0_BLOCK )[None ,:]\n    x0 =xindex \n    _tmp11 =tl .full ([XBLOCK ,R0_BLOCK ],0 ,tl .float32 )\n    for r0_offset in range (0 ,r0_numel ,R0_BLOCK ):\n        r0_index =r0_offset +r0_base \n        r0_mask =r0_index <r0_numel \n        r0_1 =r0_index \n        tmp0 =tl .load (in_ptr0 +(4 *((((r0_1 +8 *ks2 *x0 +4 *ks2 *ks3 *x0 +4 *ks2 *ks4 *x0 +2 *ks2 *ks3 *ks4 *x0 )//ks0 )%ks1 ))+16 *((((r0_1 +8 *ks2 *x0 +4 *ks2 *ks3 *x0 +4 *ks2 *ks4 *x0 +2 *ks2 *ks3 *ks4 *x0 )//(16 +8 *ks3 +8 *ks4 +4 *ks3 *ks4 ))%ks2 ))+2 *ks4 *((((r0_1 +8 *ks2 *x0 +4 *ks2 *ks3 *x0 +4 *ks2 *ks4 *x0 +2 *ks2 *ks3 *ks4 *x0 )//ks0 )%ks1 ))+8 *ks3 *((((r0_1 +8 *ks2 *x0 +4 *ks2 *ks3 *x0 +4 *ks2 *ks4 *x0 +2 *ks2 *ks3 *ks4 *x0 )//(16 +8 *ks3 +8 *ks4 +4 *ks3 *ks4 ))%ks2 ))+8 *ks4 *((((r0_1 +8 *ks2 *x0 +4 *ks2 *ks3 *x0 +4 *ks2 *ks4 *x0 +2 *ks2 *ks3 *ks4 *x0 )//(16 +8 *ks3 +8 *ks4 +4 *ks3 *ks4 ))%ks2 ))+4 *ks3 *ks4 *((((r0_1 +8 *ks2 *x0 +4 *ks2 *ks3 *x0 +4 *ks2 *ks4 *x0 +2 *ks2 *ks3 *ks4 *x0 )//(16 +8 *ks3 +8 *ks4 +4 *ks3 *ks4 ))%ks2 ))+((r0_1 %ks0 ))),r0_mask &xmask ,eviction_policy ='evict_last',other =0.0 )\n        tmp1 =1.0 \n        tmp2 =tmp1 -tmp0 \n        tmp3 =0.0 \n        tmp4 =triton_helpers .maximum (tmp2 ,tmp3 )\n        tmp5 =tl .full ([1 ,1 ],False ,tl .int1 )\n        tmp6 =tl .where (tmp5 ,tmp4 ,tmp3 )\n        tmp7 =tl .full ([1 ,1 ],True ,tl .int1 )\n        tmp8 =tl .where (tmp7 ,tmp0 ,tmp3 )\n        tmp9 =tmp6 +tmp8 \n        tmp10 =tl .broadcast_to (tmp9 ,[XBLOCK ,R0_BLOCK ])\n        tmp12 =_tmp11 +tmp10 \n        _tmp11 =tl .where (r0_mask &xmask ,tmp12 ,_tmp11 )\n    tmp11 =tl .sum (_tmp11 ,1 )[:,None ]\n    tl .store (out_ptr0 +(x0 ),tmp11 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_add_clamp_min_fill_mean_ne_sub_where_zeros_like_5 (in_out_ptr0 ,in_ptr0 ,ks0 ,ks1 ,ks2 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =2 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp1 =tl .broadcast_to (tmp0 ,[XBLOCK ,R0_BLOCK ])\n    tmp3 =tl .sum (tmp1 ,1 )[:,None ]\n    tmp4 =16 *ks0 +8 *ks0 *ks1 +8 *ks0 *ks2 +4 *ks0 *ks1 *ks2 \n    tmp5 =tmp4 .to (tl .float32 )\n    tmp6 =tmp3 /tmp5 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp6 ,None )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        2 +s2 \n        buf0 =empty_strided_cuda ((1 ,4 *s0 +2 *s0 *s1 ,1 ,2 +s2 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +2 *s0 *s1 *s2 ,2 +s2 ,2 +s2 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,4 *s0 +2 *s0 *s1 ,1 ,2 +s2 ),(8 *s0 +4 *s0 *s1 +4 *s0 *s2 +2 *s0 *s1 *s2 ,2 +s2 ,8 *s0 +4 *s0 *s1 +4 *s0 *s2 +2 *s0 *s1 *s2 ,1 ),torch .int8 )\n\n        triton_poi_fused_max_pool2d_with_indices_0_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +2 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_max_pool2d_with_indices_0 [grid (triton_poi_fused_max_pool2d_with_indices_0_xnumel )](arg3_1 ,buf0 ,buf1 ,32 ,32 ,34 ,6936 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        buf2 =empty_strided_cuda ((1 ,4 *s0 +2 *s0 *s1 ,4 +2 *s2 ,1 ),(16 *s0 +8 *s0 *s1 +8 *s0 *s2 +4 *s0 *s1 *s2 ,4 +2 *s2 ,1 ,1 ),torch .float32 )\n\n        triton_poi_fused_max_unpool2d_1_xnumel =16 *s0 +8 *s0 *s1 +8 *s0 *s2 +4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_1 [grid (triton_poi_fused_max_unpool2d_1_xnumel )](buf2 ,13872 ,XBLOCK =256 ,num_warps =4 ,num_stages =1 )\n\n        triton_poi_fused_max_unpool2d_2_xnumel =8 *s0 +4 *s0 *s1 +4 *s0 *s2 +2 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_max_unpool2d_2 [grid (triton_poi_fused_max_unpool2d_2_xnumel )](buf1 ,buf0 ,buf2 ,34 ,32 ,3 ,32 ,6936 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf0 \n        del buf1 \n        4 +2 *s2 \n        4 +2 *s1 \n        16 +8 *s1 +8 *s2 +4 *s1 *s2 \n        buf4 =empty_strided_cuda ((1 ,s0 ,4 +2 *s1 ,4 +2 *s2 ),(16 *s0 +8 *s0 *s1 +8 *s0 *s2 +4 *s0 *s1 *s2 ,16 +8 *s1 +8 *s2 +4 *s1 *s2 ,4 +2 *s2 ,1 ),torch .float32 )\n\n        triton_poi_fused_gelu_3_xnumel =16 *s0 +8 *s0 *s1 +8 *s0 *s2 +4 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused_gelu_3 [grid (triton_poi_fused_gelu_3_xnumel )](buf2 ,buf4 ,68 ,68 ,4624 ,3 ,32 ,32 ,13872 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf2 \n        buf5 =empty_strided_cuda ((2 ,),(1 ,),torch .float32 )\n\n        8 *s0 +4 *s0 *s1 +4 *s0 *s2 +2 *s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_red_fused_add_clamp_min_fill_mean_ne_sub_where_zeros_like_4 [grid (2 )](buf4 ,buf5 ,68 ,68 ,3 ,32 ,32 ,2 ,6936 ,XBLOCK =1 ,R0_BLOCK =2048 ,num_warps =16 ,num_stages =1 )\n        buf6 =empty_strided_cuda ((),(),torch .float32 )\n        buf7 =buf6 ;del buf6 \n\n        get_raw_stream (0 )\n        triton_per_fused_add_clamp_min_fill_mean_ne_sub_where_zeros_like_5 [grid (1 )](buf7 ,buf5 ,3 ,32 ,32 ,1 ,2 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del buf5 \n    return (buf4 ,buf7 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =rand_strided ((1 ,3 ,32 ,32 ),(3072 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "faf7d5b8-19f5-4375-b519-d97961d244f0",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Hardtanh', 'RMSNorm', 'GRUCell']\nimport torch\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    def __init__(self, input_size=128, hidden_size=64) -> None:\n        super().__init__()\n        self.hardtanh = nn.Hardtanh()\n        self.rms_norm = nn.LayerNorm(input_size)  # Using LayerNorm as a placeholder for RMSNorm\n        self.gru_cell1 = nn.GRUCell(input_size, hidden_size)\n        self.gru_cell2 = nn.GRUCell(hidden_size, hidden_size)\n        self.gru_cell3 = nn.GRUCell(hidden_size, hidden_size)\n        self.gru_cell4 = nn.GRUCell(hidden_size, hidden_size)\n        self.gru_cell5 = nn.GRUCell(hidden_size, hidden_size)\n        self.fc = nn.Linear(hidden_size, 10)\n\n    def forward(self, x):\n        # Flatten the input to (batch_size, input_size)\n        x = x.view(x.size(0), -1)\n        \n        # Apply Hardtanh\n        x = self.hardtanh(x)\n        \n        # Apply RMSNorm\n        x = self.rms_norm(x)\n        \n        # Initialize hidden state for GRUCell\n        hx = torch.zeros(x.size(0), 64).to(x.device)\n        \n        # Apply GRUCell multiple times\n        hx = self.gru_cell1(x, hx)\n        hx = self.gru_cell2(hx, hx)\n        hx = self.gru_cell3(hx, hx)\n        hx = self.gru_cell4(hx, hx)\n        hx = self.gru_cell5(hx, hx)\n        \n        # Apply final linear layer\n        x = self.fc(hx)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 128).cuda()  # Assuming input size is 128\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n",
        "triton_code": "\nimport torch \nfrom torch ._inductor .select_algorithm import extern_kernels \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_per_fused_hardtanh_native_layer_norm_0 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,out_ptr0 ,out_ptr1 ,xnumel ,r0_numel ,XBLOCK :tl .constexpr ):\n    R0_BLOCK :tl .constexpr =128 \n    RBLOCK :tl .constexpr =R0_BLOCK \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:,None ]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_index =tl .arange (0 ,R0_BLOCK )[None ,:]\n    tl .full ([XBLOCK ,R0_BLOCK ],True ,tl .int1 )\n    r0_0 =r0_index \n    tmp0 =tl .load (in_ptr0 +(r0_0 ),None )\n    tmp25 =tl .load (in_ptr1 +(r0_0 ),None )\n    tmp27 =tl .load (in_ptr2 +(r0_0 ),None )\n    tmp1 =-1.0 \n    tmp2 =triton_helpers .maximum (tmp0 ,tmp1 )\n    tmp3 =1.0 \n    tmp4 =triton_helpers .minimum (tmp2 ,tmp3 )\n    tmp5 =tl .broadcast_to (tmp4 ,[XBLOCK ,R0_BLOCK ])\n    tmp7 =tl .broadcast_to (tmp5 ,[XBLOCK ,R0_BLOCK ])\n    tmp9 =tl .sum (tmp7 ,1 )[:,None ]\n    tmp10 =tl .full ([XBLOCK ,1 ],128 ,tl .int32 )\n    tmp11 =tmp10 .to (tl .float32 )\n    tmp12 =tmp9 /tmp11 \n    tmp13 =tmp5 -tmp12 \n    tmp14 =tmp13 *tmp13 \n    tmp15 =tl .broadcast_to (tmp14 ,[XBLOCK ,R0_BLOCK ])\n    tmp17 =tl .sum (tmp15 ,1 )[:,None ]\n    tmp18 =128.0 \n    tmp19 =tmp17 /tmp18 \n    tmp20 =1e-05 \n    tmp21 =tmp19 +tmp20 \n    tmp22 =libdevice .rsqrt (tmp21 )\n    tmp23 =tmp4 -tmp12 \n    tmp24 =tmp23 *tmp22 \n    tmp26 =tmp24 *tmp25 \n    tmp28 =tmp26 +tmp27 \n    tl .debug_barrier ()\n    tl .store (in_out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp22 ,None )\n    tl .store (out_ptr1 +(tl .broadcast_to (r0_0 ,[XBLOCK ,R0_BLOCK ])),tmp28 ,None )\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ,1 ],0 ,tl .int32 )),tmp12 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_1 (out_ptr0 ,xnumel ,XBLOCK :tl .constexpr ):\n    xnumel =64 \n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =0.0 \n    tl .store (out_ptr0 +(x0 ),tmp0 ,xmask )\n\ndef call (args ):\n    primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 =args \n    args .clear ()\n    assert_size_stride (primals_1 ,(1 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_2 ,(128 ,),(1 ,))\n    assert_size_stride (primals_3 ,(128 ,),(1 ,))\n    assert_size_stride (primals_4 ,(192 ,128 ),(128 ,1 ))\n    assert_size_stride (primals_5 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_6 ,(192 ,),(1 ,))\n    assert_size_stride (primals_7 ,(192 ,),(1 ,))\n    assert_size_stride (primals_8 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_9 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_10 ,(192 ,),(1 ,))\n    assert_size_stride (primals_11 ,(192 ,),(1 ,))\n    assert_size_stride (primals_12 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_13 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_14 ,(192 ,),(1 ,))\n    assert_size_stride (primals_15 ,(192 ,),(1 ,))\n    assert_size_stride (primals_16 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_17 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_18 ,(192 ,),(1 ,))\n    assert_size_stride (primals_19 ,(192 ,),(1 ,))\n    assert_size_stride (primals_20 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_21 ,(192 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_22 ,(192 ,),(1 ,))\n    assert_size_stride (primals_23 ,(192 ,),(1 ,))\n    assert_size_stride (primals_24 ,(10 ,64 ),(64 ,1 ))\n    assert_size_stride (primals_25 ,(10 ,),(1 ,))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf1 =empty_strided_cuda ((1 ,1 ),(1 ,1 ),torch .float32 )\n        buf3 =buf1 ;del buf1 \n        buf4 =empty_strided_cuda ((1 ,128 ),(128 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_per_fused_hardtanh_native_layer_norm_0 [grid (1 )](buf3 ,primals_1 ,primals_2 ,primals_3 ,buf0 ,buf4 ,1 ,128 ,XBLOCK =1 ,num_warps =2 ,num_stages =1 )\n        del primals_2 \n        del primals_3 \n        buf5 =empty_strided_cuda ((1 ,64 ),(64 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_1 [grid (64 )](buf5 ,64 ,XBLOCK =64 ,num_warps =1 ,num_stages =1 )\n        buf6 =empty_strided_cuda ((1 ,192 ),(192 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf4 ,reinterpret_tensor (primals_4 ,(128 ,192 ),(1 ,128 ),0 ),out =buf6 )\n        buf7 =empty_strided_cuda ((1 ,192 ),(192 ,1 ),torch .float32 )\n\n        extern_kernels .mm (buf5 ,reinterpret_tensor (primals_5 ,(64 ,192 ),(1 ,64 ),0 ),out =buf7 )\n        del primals_5 \n\n        buf8 =torch .ops .aten ._thnn_fused_gru_cell .default (buf6 ,buf7 ,buf5 ,primals_6 ,primals_7 )\n        del primals_6 \n        del primals_7 \n        buf9 =buf8 [0 ]\n        buf10 =buf8 [1 ]\n        del buf8 \n        buf11 =buf7 ;del buf7 \n\n        extern_kernels .mm (buf9 ,reinterpret_tensor (primals_8 ,(64 ,192 ),(1 ,64 ),0 ),out =buf11 )\n        buf12 =buf6 ;del buf6 \n\n        extern_kernels .mm (buf9 ,reinterpret_tensor (primals_9 ,(64 ,192 ),(1 ,64 ),0 ),out =buf12 )\n\n        buf13 =torch .ops .aten ._thnn_fused_gru_cell .default (buf11 ,buf12 ,buf9 ,primals_10 ,primals_11 )\n        del primals_10 \n        del primals_11 \n        buf14 =buf13 [0 ]\n        buf15 =buf13 [1 ]\n        del buf13 \n        buf16 =buf12 ;del buf12 \n\n        extern_kernels .mm (buf14 ,reinterpret_tensor (primals_12 ,(64 ,192 ),(1 ,64 ),0 ),out =buf16 )\n        buf17 =buf11 ;del buf11 \n\n        extern_kernels .mm (buf14 ,reinterpret_tensor (primals_13 ,(64 ,192 ),(1 ,64 ),0 ),out =buf17 )\n\n        buf18 =torch .ops .aten ._thnn_fused_gru_cell .default (buf16 ,buf17 ,buf14 ,primals_14 ,primals_15 )\n        del primals_14 \n        del primals_15 \n        buf19 =buf18 [0 ]\n        buf20 =buf18 [1 ]\n        del buf18 \n        buf21 =buf17 ;del buf17 \n\n        extern_kernels .mm (buf19 ,reinterpret_tensor (primals_16 ,(64 ,192 ),(1 ,64 ),0 ),out =buf21 )\n        buf22 =buf16 ;del buf16 \n\n        extern_kernels .mm (buf19 ,reinterpret_tensor (primals_17 ,(64 ,192 ),(1 ,64 ),0 ),out =buf22 )\n\n        buf23 =torch .ops .aten ._thnn_fused_gru_cell .default (buf21 ,buf22 ,buf19 ,primals_18 ,primals_19 )\n        del primals_18 \n        del primals_19 \n        buf24 =buf23 [0 ]\n        buf25 =buf23 [1 ]\n        del buf23 \n        buf26 =buf22 ;del buf22 \n\n        extern_kernels .mm (buf24 ,reinterpret_tensor (primals_20 ,(64 ,192 ),(1 ,64 ),0 ),out =buf26 )\n        buf27 =buf21 ;del buf21 \n\n        extern_kernels .mm (buf24 ,reinterpret_tensor (primals_21 ,(64 ,192 ),(1 ,64 ),0 ),out =buf27 )\n\n        buf28 =torch .ops .aten ._thnn_fused_gru_cell .default (buf26 ,buf27 ,buf24 ,primals_22 ,primals_23 )\n        del buf26 \n        del buf27 \n        del primals_22 \n        del primals_23 \n        buf29 =buf28 [0 ]\n        buf30 =buf28 [1 ]\n        del buf28 \n        buf31 =empty_strided_cuda ((1 ,10 ),(10 ,1 ),torch .float32 )\n\n        extern_kernels .addmm (primals_25 ,buf29 ,reinterpret_tensor (primals_24 ,(64 ,10 ),(1 ,64 ),0 ),alpha =1 ,beta =1 ,out =buf31 )\n        del primals_25 \n    return (buf31 ,primals_1 ,buf0 ,buf3 ,buf4 ,buf5 ,buf9 ,buf10 ,buf14 ,buf15 ,buf19 ,buf20 ,buf24 ,buf25 ,buf29 ,buf30 ,primals_24 ,primals_21 ,primals_20 ,primals_17 ,primals_16 ,primals_13 ,primals_12 ,primals_9 ,primals_8 ,primals_4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    primals_1 =rand_strided ((1 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_2 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_3 =rand_strided ((128 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_4 =rand_strided ((192 ,128 ),(128 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_5 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_6 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_7 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_8 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_9 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_10 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_11 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_12 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_13 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_14 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_15 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_16 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_17 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_18 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_19 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_20 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_21 =rand_strided ((192 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_22 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_23 =rand_strided ((192 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    primals_24 =rand_strided ((10 ,64 ),(64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    primals_25 =rand_strided ((10 ,),(1 ,),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([primals_1 ,primals_2 ,primals_3 ,primals_4 ,primals_5 ,primals_6 ,primals_7 ,primals_8 ,primals_9 ,primals_10 ,primals_11 ,primals_12 ,primals_13 ,primals_14 ,primals_15 ,primals_16 ,primals_17 ,primals_18 ,primals_19 ,primals_20 ,primals_21 ,primals_22 ,primals_23 ,primals_24 ,primals_25 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "fbedf465-8ed4-4cba-a7b5-ca13514c169b",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['ELU', 'AvgPool3d', 'GRU', 'RNNCellBase', 'PairwiseDistance', 'Dropout3d', 'LogSoftmax']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.elu1 = nn.ELU()\n        self.avg_pool3d = nn.AvgPool3d(kernel_size=2, stride=2)\n        self.gru = nn.GRU(input_size=128, hidden_size=256, num_layers=2, batch_first=True)\n        self.rnn_cell = nn.RNNCell(input_size=256, hidden_size=128)\n        self.pairwise_distance = nn.PairwiseDistance()\n        self.dropout3d = nn.Dropout3d(p=0.5)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        # Assume input x is of shape (batch_size, channels, depth, height, width)\n        x = self.elu1(x)  # Apply ELU activation\n        x = self.avg_pool3d(x)  # Apply 3D average pooling\n        x = self.dropout3d(x)  # Apply 3D dropout\n        \n        # Reshape for GRU: (batch_size, seq_len, features)\n        batch_size, channels, depth, height, width = x.shape\n        x = x.view(batch_size, depth, -1)  # Flatten spatial dimensions\n        \n        # Apply GRU\n        x, _ = self.gru(x)\n        \n        # Apply RNNCell to each time step\n        hx = torch.zeros(batch_size, 128).to(x.device)\n        for t in range(x.size(1)):\n            hx = self.rnn_cell(x[:, t, :], hx)\n        \n        # Compute pairwise distance between the last hidden state and itself\n        x = self.pairwise_distance(hx, hx)\n        \n        # Reshape and apply log softmax\n        x = x.unsqueeze(1)  # Add a dimension for log softmax\n        x = self.log_softmax(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape: (batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =x0 \n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(x0 ),tmp2 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_avg_pool3d_bernoulli_div_elu_mul_1 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,ks6 ,ks7 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =((xindex //ks0 )%ks1 )\n    x2 =((xindex //ks2 )%ks3 )\n    x3 =xindex //ks4 \n    x5 =xindex \n    tmp0 =tl .load (in_ptr0 +(2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp8 =tl .load (in_ptr0 +(1 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp15 =tl .load (in_ptr0 +(ks7 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp22 =tl .load (in_ptr0 +(1 +ks7 +2 *x0 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp29 =tl .load (in_ptr0 +(2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp36 =tl .load (in_ptr0 +(1 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp43 =tl .load (in_ptr0 +(ks7 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp50 =tl .load (in_ptr0 +(1 +ks7 +2 *x0 +ks6 *ks7 +2 *ks7 *x1 +2 *ks6 *ks7 *x2 +ks5 *ks6 *ks7 *x3 ),xmask ,eviction_policy ='evict_last')\n    tmp59 =tl .load (in_ptr1 +(x3 ),xmask ,eviction_policy ='evict_last')\n    tmp1 =0.0 \n    tmp2 =tmp0 >tmp1 \n    tmp3 =1.0 \n    tmp4 =tmp0 *tmp3 \n    tmp5 =libdevice .expm1 (tmp4 )\n    tmp6 =tmp5 *tmp3 \n    tmp7 =tl .where (tmp2 ,tmp4 ,tmp6 )\n    tmp9 =tmp8 >tmp1 \n    tmp10 =tmp8 *tmp3 \n    tmp11 =libdevice .expm1 (tmp10 )\n    tmp12 =tmp11 *tmp3 \n    tmp13 =tl .where (tmp9 ,tmp10 ,tmp12 )\n    tmp14 =tmp13 +tmp7 \n    tmp16 =tmp15 >tmp1 \n    tmp17 =tmp15 *tmp3 \n    tmp18 =libdevice .expm1 (tmp17 )\n    tmp19 =tmp18 *tmp3 \n    tmp20 =tl .where (tmp16 ,tmp17 ,tmp19 )\n    tmp21 =tmp20 +tmp14 \n    tmp23 =tmp22 >tmp1 \n    tmp24 =tmp22 *tmp3 \n    tmp25 =libdevice .expm1 (tmp24 )\n    tmp26 =tmp25 *tmp3 \n    tmp27 =tl .where (tmp23 ,tmp24 ,tmp26 )\n    tmp28 =tmp27 +tmp21 \n    tmp30 =tmp29 >tmp1 \n    tmp31 =tmp29 *tmp3 \n    tmp32 =libdevice .expm1 (tmp31 )\n    tmp33 =tmp32 *tmp3 \n    tmp34 =tl .where (tmp30 ,tmp31 ,tmp33 )\n    tmp35 =tmp34 +tmp28 \n    tmp37 =tmp36 >tmp1 \n    tmp38 =tmp36 *tmp3 \n    tmp39 =libdevice .expm1 (tmp38 )\n    tmp40 =tmp39 *tmp3 \n    tmp41 =tl .where (tmp37 ,tmp38 ,tmp40 )\n    tmp42 =tmp41 +tmp35 \n    tmp44 =tmp43 >tmp1 \n    tmp45 =tmp43 *tmp3 \n    tmp46 =libdevice .expm1 (tmp45 )\n    tmp47 =tmp46 *tmp3 \n    tmp48 =tl .where (tmp44 ,tmp45 ,tmp47 )\n    tmp49 =tmp48 +tmp42 \n    tmp51 =tmp50 >tmp1 \n    tmp52 =tmp50 *tmp3 \n    tmp53 =libdevice .expm1 (tmp52 )\n    tmp54 =tmp53 *tmp3 \n    tmp55 =tl .where (tmp51 ,tmp52 ,tmp54 )\n    tmp56 =tmp55 +tmp49 \n    tmp57 =0.125 \n    tmp58 =tmp56 *tmp57 \n    tmp60 =0.5 \n    tmp61 =tmp59 <tmp60 \n    tmp62 =tmp61 .to (tl .float32 )\n    tmp63 =2.0 \n    tmp64 =tmp62 *tmp63 \n    tmp65 =tmp58 *tmp64 \n    tl .store (in_out_ptr0 +(x5 ),tmp65 ,xmask )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_mul_view_2 (in_ptr0 ,out_ptr0 ,ks0 ,ks1 ,ks2 ,ks3 ,ks4 ,ks5 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =(xindex %ks0 )\n    x1 =xindex //ks0 \n    x2 =xindex \n    tmp0 =tl .load (in_ptr0 +(ks1 *ks2 *((((x0 +ks1 *ks2 *ks5 *x1 )//ks3 )%(ks4 *ks5 )))+((x0 %ks3 ))),xmask ,eviction_policy ='evict_last')\n    tl .store (out_ptr0 +(x2 ),tmp0 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    s3 =arg3_1 \n    assert_size_stride (arg4_1 ,(1 ,s0 ,s1 ,s2 ,s3 ),(s0 *s1 *s2 *s3 ,s1 *s2 *s3 ,s2 *s3 ,s3 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf1 =empty_strided_cuda ((1 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[1 ],out =buf1 )\n        buf2 =empty_strided_cuda ((1 ,s0 ,1 ,1 ,1 ),(s0 ,1 ,s0 ,s0 ,s0 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (s0 )](buf1 ,buf2 ,0 ,3 ,XBLOCK =4 ,num_warps =1 ,num_stages =1 )\n        del buf1 \n        s3 //2 \n        s2 //2 \n        (s2 //2 )*(s3 //2 )\n        s1 //2 \n        (s1 //2 )*(s2 //2 )*(s3 //2 )\n        buf0 =empty_strided_cuda ((1 ,s0 ,s1 //2 ,s2 //2 ,s3 //2 ),(s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),(s1 //2 )*(s2 //2 )*(s3 //2 ),(s2 //2 )*(s3 //2 ),s3 //2 ,1 ),torch .float32 )\n        buf3 =buf0 ;del buf0 \n\n        triton_poi_fused__to_copy_avg_pool3d_bernoulli_div_elu_mul_1_xnumel =s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_avg_pool3d_bernoulli_div_elu_mul_1 [grid (triton_poi_fused__to_copy_avg_pool3d_bernoulli_div_elu_mul_1_xnumel )](buf3 ,arg4_1 ,buf2 ,16 ,16 ,256 ,16 ,4096 ,32 ,32 ,32 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg4_1 \n        del buf2 \n        s0 *(s2 //2 )*(s3 //2 )\n        buf4 =empty_strided_cuda ((1 ,s1 //2 ,s0 *(s2 //2 )*(s3 //2 )),(s0 *(s1 //2 )*(s2 //2 )*(s3 //2 ),s0 *(s2 //2 )*(s3 //2 ),1 ),torch .float32 )\n\n        triton_poi_fused__to_copy_bernoulli_div_mul_view_2_xnumel =s0 *(s1 //2 )*(s2 //2 )*(s3 //2 )\n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_mul_view_2 [grid (triton_poi_fused__to_copy_bernoulli_div_mul_view_2_xnumel )](buf3 ,buf4 ,768 ,16 ,16 ,256 ,16 ,3 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del buf3 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =32 \n    arg2_1 =32 \n    arg3_1 =32 \n    arg4_1 =rand_strided ((1 ,3 ,32 ,32 ,32 ),(98304 ,32768 ,1024 ,32 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ,arg4_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    },
    {
        "uuid": "ff53d986-d015-4b7f-9bff-333225d41678",
        "pytorch_code": "\n# This is a random torch model generated by the following modules: ['Tanhshrink', 'Hardswish', 'Dropout1d', 'LogSigmoid']\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.tanhshrink1 = nn.Tanhshrink()\n        self.hardswish1 = nn.Hardswish()\n        self.dropout1d1 = nn.Dropout1d(p=0.5)\n        self.logsigmoid1 = nn.LogSigmoid()\n        self.tanhshrink2 = nn.Tanhshrink()\n        self.hardswish2 = nn.Hardswish()\n        self.dropout1d2 = nn.Dropout1d(p=0.5)\n        self.logsigmoid2 = nn.LogSigmoid()\n\n    def forward(self, x):\n        # Apply the first sequence of modules\n        x = self.tanhshrink1(x)\n        x = self.hardswish1(x)\n        x = x.view(x.size(0), -1)  # Flatten the input for Dropout1d\n        x = self.dropout1d1(x)\n        x = self.logsigmoid1(x)\n        \n        # Apply the second sequence of modules\n        x = self.tanhshrink2(x)\n        x = self.hardswish2(x)\n        x = x.view(x.size(0), -1)  # Flatten the input for Dropout1d\n        x = self.dropout1d2(x)\n        x = self.logsigmoid2(x)\n        \n        return x\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with shape (batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n",
        "triton_code": "\nimport torch \nimport triton \nimport triton .language as tl \nfrom torch ._inductor .runtime .triton_heuristics import (\ngrid ,\n)\nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \nfrom torch ._C import _cuda_getCurrentRawStream as get_raw_stream \n\naten =torch .ops .aten \ninductor_ops =torch .ops .inductor \n_quantized =torch .ops ._quantized \nassert_size_stride =torch ._C ._dynamo .guards .assert_size_stride \nempty_strided_cpu =torch ._C ._dynamo .guards ._empty_strided_cpu \nempty_strided_cuda =torch ._C ._dynamo .guards ._empty_strided_cuda \nempty_strided_xpu =torch ._C ._dynamo .guards ._empty_strided_xpu \nreinterpret_tensor =torch ._C ._dynamo .guards ._reinterpret_tensor \nalloc_from_pool =torch .ops .inductor ._alloc_from_pool \n\nempty_strided_p2p =torch ._C ._distributed_c10d ._SymmetricMemory .empty_strided_p2p \n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_0 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused_bernoulli_1 (in_ptr0 ,out_ptr0 ,load_seed_offset ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    tl .full ([XBLOCK ],True ,tl .int1 )\n    tmp0 =tl .load (in_ptr0 +load_seed_offset )\n    tmp1 =tl .full ([1 ],0 ,tl .int32 )\n    tmp2 =tl .rand (tmp0 ,(tmp1 ).to (tl .uint32 ))\n    tl .store (out_ptr0 +(tl .full ([XBLOCK ],0 ,tl .int32 )),tmp2 ,None )\n\nimport triton \nimport triton .language as tl \n\nfrom torch ._inductor .runtime import triton_helpers \nfrom torch ._inductor .runtime .triton_helpers import libdevice ,math as tl_math \ntriton_helpers .set_driver_to_gpu ()\n\n@triton .jit \ndef triton_poi_fused__to_copy_bernoulli_div_log_sigmoid_forward_mul_2 (in_out_ptr0 ,in_ptr0 ,in_ptr1 ,in_ptr2 ,xnumel ,XBLOCK :tl .constexpr ):\n    xoffset =tl .program_id (0 )*XBLOCK \n    xindex =xoffset +tl .arange (0 ,XBLOCK )[:]\n    xmask =xindex <xnumel \n    x0 =xindex \n    tmp0 =tl .load (in_ptr0 +(x0 ),xmask )\n    tmp12 =tl .load (in_ptr1 +(0 ))\n    tmp13 =tl .broadcast_to (tmp12 ,[XBLOCK ])\n    tmp33 =tl .load (in_ptr2 +(0 ))\n    tmp34 =tl .broadcast_to (tmp33 ,[XBLOCK ])\n    tmp1 =libdevice .tanh (tmp0 )\n    tmp2 =tmp0 -tmp1 \n    tmp3 =3.0 \n    tmp4 =tmp2 +tmp3 \n    tmp5 =0.0 \n    tmp6 =triton_helpers .maximum (tmp4 ,tmp5 )\n    tmp7 =6.0 \n    tmp8 =triton_helpers .minimum (tmp6 ,tmp7 )\n    tmp9 =tmp2 *tmp8 \n    tmp10 =0.16666666666666666 \n    tmp11 =tmp9 *tmp10 \n    tmp14 =0.5 \n    tmp15 =tmp13 <tmp14 \n    tmp16 =tmp15 .to (tl .float32 )\n    tmp17 =2.0 \n    tmp18 =tmp16 *tmp17 \n    tmp19 =tmp11 *tmp18 \n    tmp20 =triton_helpers .minimum (tmp5 ,tmp19 )\n    tmp21 =tl_math .abs (tmp19 )\n    tmp22 =-tmp21 \n    tmp23 =tl_math .exp (tmp22 )\n    tmp24 =libdevice .log1p (tmp23 )\n    tmp25 =tmp20 -tmp24 \n    tmp26 =libdevice .tanh (tmp25 )\n    tmp27 =tmp25 -tmp26 \n    tmp28 =tmp27 +tmp3 \n    tmp29 =triton_helpers .maximum (tmp28 ,tmp5 )\n    tmp30 =triton_helpers .minimum (tmp29 ,tmp7 )\n    tmp31 =tmp27 *tmp30 \n    tmp32 =tmp31 *tmp10 \n    tmp35 =tmp34 <tmp14 \n    tmp36 =tmp35 .to (tl .float32 )\n    tmp37 =tmp36 *tmp17 \n    tmp38 =tmp32 *tmp37 \n    tmp39 =triton_helpers .minimum (tmp5 ,tmp38 )\n    tmp40 =tl_math .abs (tmp38 )\n    tmp41 =-tmp40 \n    tmp42 =tl_math .exp (tmp41 )\n    tmp43 =libdevice .log1p (tmp42 )\n    tmp44 =tmp39 -tmp43 \n    tl .store (in_out_ptr0 +(x0 ),tmp44 ,xmask )\n\ndef call (args ):\n    arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 =args \n    args .clear ()\n    s0 =arg0_1 \n    s1 =arg1_1 \n    s2 =arg2_1 \n    assert_size_stride (arg3_1 ,(1 ,s0 ,s1 ,s2 ),(s0 *s1 *s2 ,s1 *s2 ,s2 ,1 ))\n    with torch .cuda ._DeviceGuard (0 ):\n        torch .cuda .set_device (0 )\n        buf0 =empty_strided_cuda ((2 ,),(1 ,),torch .int64 )\n\n        aten .randint .low_out (-9223372036854775808 ,9223372036854775807 ,[2 ],out =buf0 )\n        buf1 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_0 [grid (1 )](buf0 ,buf1 ,0 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        buf2 =empty_strided_cuda ((1 ,1 ,1 ),(1 ,1 ,1 ),torch .float32 )\n\n        get_raw_stream (0 )\n        triton_poi_fused_bernoulli_1 [grid (1 )](buf0 ,buf2 ,1 ,1 ,XBLOCK =1 ,num_warps =1 ,num_stages =1 )\n        del buf0 \n        buf3 =empty_strided_cuda ((1 ,1 ,s0 *s1 *s2 ),(s0 *s1 *s2 ,s0 *s1 *s2 ,1 ),torch .float32 )\n        buf4 =reinterpret_tensor (buf3 ,(1 ,s0 *s1 *s2 ),(s0 *s1 *s2 ,1 ),0 );del buf3 \n\n        triton_poi_fused__to_copy_bernoulli_div_log_sigmoid_forward_mul_2_xnumel =s0 *s1 *s2 \n        get_raw_stream (0 )\n        triton_poi_fused__to_copy_bernoulli_div_log_sigmoid_forward_mul_2 [grid (triton_poi_fused__to_copy_bernoulli_div_log_sigmoid_forward_mul_2_xnumel )](buf4 ,arg3_1 ,buf1 ,buf2 ,12288 ,XBLOCK =128 ,num_warps =4 ,num_stages =1 )\n        del arg3_1 \n        del buf1 \n        del buf2 \n    return (buf4 ,)\n\ndef benchmark_compiled_module (times =10 ,repeat =10 ):\n    from torch ._dynamo .testing import rand_strided \n    from torch ._inductor .utils import print_performance \n    arg0_1 =3 \n    arg1_1 =64 \n    arg2_1 =64 \n    arg3_1 =rand_strided ((1 ,3 ,64 ,64 ),(12288 ,4096 ,64 ,1 ),device ='cuda:0',dtype =torch .float32 )\n    fn =lambda :call ([arg0_1 ,arg1_1 ,arg2_1 ,arg3_1 ])\n    return print_performance (fn ,times =times ,repeat =repeat )\n\nif __name__ ==\"__main__\":\n    from torch ._inductor .wrapper_benchmark import compiled_module_main \n    compiled_module_main ('None',benchmark_compiled_module )\n"
    }
]