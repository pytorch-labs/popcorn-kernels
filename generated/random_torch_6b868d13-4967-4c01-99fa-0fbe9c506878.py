
# This is a random torch model generated by the following modules: ['Flatten', 'Tanh', 'Dropout1d', 'TransformerDecoder', 'Softsign', 'SyncBatchNorm', 'TransformerDecoderLayer', 'Sigmoid']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.flatten = nn.Flatten()
        self.tanh = nn.Tanh()
        self.dropout1d = nn.Dropout1d(p=0.5)
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=64, nhead=8),
            num_layers=3
        )
        self.softsign = nn.Softsign()
        self.sync_batch_norm = nn.SyncBatchNorm(64)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Flatten the input
        x = self.flatten(x)
        
        # Apply Tanh activation
        x = self.tanh(x)
        
        # Reshape for Dropout1d
        x = x.unsqueeze(1)  # Add a dimension for Dropout1d
        x = self.dropout1d(x)
        x = x.squeeze(1)  # Remove the added dimension
        
        # Reshape for TransformerDecoder
        x = x.view(-1, 8, 8)  # Reshape to (batch_size, seq_len, d_model)
        
        # Apply TransformerDecoder
        memory = torch.randn_like(x)  # Random memory for TransformerDecoder
        x = self.transformer_decoder(x, memory)
        
        # Apply Softsign activation
        x = self.softsign(x)
        
        # Reshape for SyncBatchNorm
        x = x.view(-1, 64)  # Reshape to (batch_size, channels)
        x = self.sync_batch_norm(x)
        
        # Apply Sigmoid activation
        x = self.sigmoid(x)
        
        return x

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

