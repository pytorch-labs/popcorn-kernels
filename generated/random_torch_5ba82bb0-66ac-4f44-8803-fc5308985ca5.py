
# This is a random torch model generated by the following modules: ['Fold', 'UpsamplingBilinear2d', 'MSELoss', 'L1Loss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.fold1 = nn.Fold(output_size=(64, 64), kernel_size=(3, 3), stride=(1, 1))
        self.fold2 = nn.Fold(output_size=(32, 32), kernel_size=(3, 3), stride=(1, 1))
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.mse_loss = nn.MSELoss()
        self.l1_loss = nn.L1Loss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, height, width)
        batch_size, channels, height, width = x.shape
        
        # Apply first fold operation
        x = x.view(batch_size, -1)  # Flatten the input for Fold
        x = self.fold1(x)
        
        # Apply first upsampling
        x = self.upsample1(x)
        
        # Apply second fold operation
        x = x.view(batch_size, -1)  # Flatten the input for Fold
        x = self.fold2(x)
        
        # Apply second upsampling
        x = self.upsample2(x)
        
        # Compute MSE loss with a dummy target
        dummy_target = torch.zeros_like(x)
        mse_loss = self.mse_loss(x, dummy_target)
        
        # Compute L1 loss with a dummy target
        l1_loss = self.l1_loss(x, dummy_target)
        
        # Return the sum of the losses as the output
        return mse_loss + l1_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

