
# This is a random torch model generated by the following modules: ['Dropout1d', 'HingeEmbeddingLoss', 'NLLLoss', 'ConvTranspose3d', 'NLLLoss2d', 'SyncBatchNorm', 'AdaptiveAvgPool3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(1, 10, kernel_size=3, stride=2, padding=1)
        self.sync_batch_norm = nn.SyncBatchNorm(10)
        self.dropout1d = nn.Dropout1d(p=0.5)
        self.adaptive_avg_pool3d = nn.AdaptiveAvgPool3d((5, 5, 5))
        self.nll_loss = nn.NLLLoss()
        self.nll_loss2d = nn.NLLLoss2d()
        self.hinge_embedding_loss = nn.HingeEmbeddingLoss()

    def forward(self, x):
        # Apply ConvTranspose3d
        x = self.conv_transpose3d(x)
        
        # Apply SyncBatchNorm
        x = self.sync_batch_norm(x)
        
        # Reshape for Dropout1d
        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions
        x = self.dropout1d(x)
        
        # Reshape back to 3D for AdaptiveAvgPool3d
        x = x.view(x.size(0), x.size(1), 5, 5, 5)  # Reshape to (batch_size, channels, 5, 5, 5)
        x = self.adaptive_avg_pool3d(x)
        
        # Compute NLLLoss (requires log probabilities and target)
        log_probs = F.log_softmax(x, dim=1)
        target = torch.randint(0, 10, (x.size(0), *x.size()[2:])).to(x.device)
        nll_loss = self.nll_loss(log_probs, target)
        
        # Compute NLLLoss2d (requires log probabilities and target)
        log_probs_2d = F.log_softmax(x[:, :, :, :, 0], dim=1)  # Take first slice along depth
        target_2d = torch.randint(0, 10, (x.size(0), *x.size()[2:4])).to(x.device)
        nll_loss2d = self.nll_loss2d(log_probs_2d, target_2d)
        
        # Compute HingeEmbeddingLoss (requires input and target)
        hinge_input = x.view(x.size(0), -1)  # Flatten for hinge embedding
        hinge_target = torch.ones(x.size(0)).to(x.device)  # Arbitrary target
        hinge_loss = self.hinge_embedding_loss(hinge_input, hinge_target)
        
        # Return the losses as a tuple
        return nll_loss, nll_loss2d, hinge_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 10, 10, 10).cuda()  # Arbitrary input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
