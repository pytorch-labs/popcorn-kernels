
# This is a random torch model generated by the following modules: ['Embedding', 'MultiLabelSoftMarginLoss', 'AdaptiveMaxPool3d', 'LSTMCell', 'Conv3d', 'Dropout1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.conv3d_1 = nn.Conv3d(1, 32, kernel_size=3, stride=1, padding=1)  # Conv3d layer
        self.conv3d_2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)  # Conv3d layer
        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((8, 8, 8))  # AdaptiveMaxPool3d layer
        self.dropout1d = nn.Dropout1d(p=0.5)  # Dropout1d layer
        self.lstm_cell = nn.LSTMCell(64 * 8 * 8 * 8, 256)  # LSTMCell layer
        self.fc = nn.Linear(256, 10)  # Fully connected layer
        self.loss = nn.MultiLabelSoftMarginLoss()  # MultiLabelSoftMarginLoss layer

    def forward(self, x):
        # Assume x is a tensor of arbitrary shape
        x = self.embedding(x)  # Apply embedding
        x = x.unsqueeze(1)  # Add a channel dimension for Conv3d
        x = F.relu(self.conv3d_1(x))  # Apply Conv3d and ReLU
        x = F.relu(self.conv3d_2(x))  # Apply Conv3d and ReLU
        x = self.adaptive_max_pool3d(x)  # Apply AdaptiveMaxPool3d
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = self.dropout1d(x)  # Apply Dropout1d
        hx = torch.zeros(x.size(0), 256).to(x.device)  # Initialize hidden state for LSTMCell
        cx = torch.zeros(x.size(0), 256).to(x.device)  # Initialize cell state for LSTMCell
        x, _ = self.lstm_cell(x, (hx, cx))  # Apply LSTMCell
        x = self.fc(x)  # Apply fully connected layer
        return x

    def compute_loss(self, output, target):
        return self.loss(output, target)  # Compute loss using MultiLabelSoftMarginLoss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (1, 32)).cuda()  # Random input tensor for embedding
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

