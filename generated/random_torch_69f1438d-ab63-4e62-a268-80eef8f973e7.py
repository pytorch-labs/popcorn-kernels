
# This is a random torch model generated by the following modules: ['Fold', 'Unflatten', 'MaxPool3d', 'CTCLoss', 'RNNCell', 'BatchNorm3d', 'Dropout1d', 'ReflectionPad1d', 'LazyBatchNorm3d', 'BatchNorm1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.unflatten = nn.Unflatten(1, (1, 64))
        self.maxpool3d = nn.MaxPool3d(kernel_size=(2, 2, 2))
        self.batchnorm3d = nn.BatchNorm3d(1)
        self.lazybatchnorm3d = nn.LazyBatchNorm3d()
        self.dropout1d = nn.Dropout1d(p=0.5)
        self.reflectionpad1d = nn.ReflectionPad1d(2)
        self.rnncell = nn.RNNCell(64, 128)
        self.batchnorm1d = nn.BatchNorm1d(128)
        self.fold = nn.Fold(output_size=(8, 8), kernel_size=(2, 2))
        self.ctcloss = nn.CTCLoss()

    def forward(self, x):
        # Unflatten the input to add a channel dimension
        x = self.unflatten(x)
        
        # Apply 3D max pooling
        x = self.maxpool3d(x)
        
        # Apply 3D batch normalization
        x = self.batchnorm3d(x)
        
        # Apply lazy 3D batch normalization
        x = self.lazybatchnorm3d(x)
        
        # Apply 1D dropout
        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions
        x = self.dropout1d(x)
        
        # Apply 1D reflection padding
        x = self.reflectionpad1d(x)
        
        # Apply RNN cell
        x = x.permute(0, 2, 1)  # Swap dimensions for RNNCell
        hx = torch.zeros(x.size(0), 128).to(x.device)
        x = self.rnncell(x, hx)
        
        # Apply 1D batch normalization
        x = self.batchnorm1d(x)
        
        # Fold the tensor
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.fold(x)
        
        # Compute CTC loss (dummy target for demonstration)
        target = torch.randint(1, 10, (x.size(0), 5), dtype=torch.long).to(x.device)
        input_lengths = torch.full((x.size(0),), x.size(1), dtype=torch.long).to(x.device)
        target_lengths = torch.randint(1, 6, (x.size(0),), dtype=torch.long).to(x.device)
        loss = self.ctcloss(x, target, input_lengths, target_lengths)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

