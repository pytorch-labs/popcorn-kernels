
# This is a random torch model generated by the following modules: ['Bilinear', 'Tanh', 'LPPool2d', 'ConstantPad1d', 'AdaptiveMaxPool3d', 'SyncBatchNorm', 'ConvTranspose2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.bilinear1 = nn.Bilinear(10, 20, 30)
        self.tanh = nn.Tanh()
        self.lppool2d = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)
        self.constant_pad1d = nn.ConstantPad1d(padding=2, value=0)
        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d(output_size=(5, 5, 5))
        self.sync_batch_norm = nn.SyncBatchNorm(5)
        self.conv_transpose2d = nn.ConvTranspose2d(in_channels=5, out_channels=10, kernel_size=3, stride=2, padding=1)

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, height, width)
        # Reshape and pad to fit the Bilinear layer
        x = x.view(x.size(0), -1)  # Flatten the input
        x = self.bilinear1(x, x)  # Apply Bilinear layer
        x = self.tanh(x)  # Apply Tanh activation
        x = x.view(x.size(0), 10, 20, 30)  # Reshape for LPPool2d
        x = self.lppool2d(x)  # Apply LPPool2d
        x = x.view(x.size(0), -1)  # Flatten for ConstantPad1d
        x = self.constant_pad1d(x)  # Apply ConstantPad1d
        x = x.view(x.size(0), 5, 10, 10, 10)  # Reshape for AdaptiveMaxPool3d
        x = self.adaptive_max_pool3d(x)  # Apply AdaptiveMaxPool3d
        x = x.view(x.size(0), 5, 5, 5, 5)  # Reshape for SyncBatchNorm
        x = self.sync_batch_norm(x)  # Apply SyncBatchNorm
        x = x.view(x.size(0), 5, 5, 5)  # Reshape for ConvTranspose2d
        x = self.conv_transpose2d(x)  # Apply ConvTranspose2d
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 20, 30).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
