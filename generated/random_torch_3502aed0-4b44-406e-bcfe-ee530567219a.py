
# This is a random torch model generated by the following modules: ['ConstantPad2d', 'Flatten', 'MaxPool2d', 'ZeroPad3d', 'ConvTranspose1d', 'LazyConvTranspose3d', 'MaxUnpool3d', 'ModuleList', 'Upsample', 'Module']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.pad2d = nn.ConstantPad2d(2, 3.0)
        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.zeropad3d = nn.ZeroPad3d(1)
        self.convtranspose1d = nn.ConvTranspose1d(1, 10, kernel_size=5, stride=2)
        self.lazyconvtranspose3d = nn.LazyConvTranspose3d(20, kernel_size=3, stride=1)
        self.maxunpool3d = nn.MaxUnpool3d(kernel_size=2, stride=2)
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        
        # Using ModuleList to hold multiple layers
        self.module_list = nn.ModuleList([
            nn.ConvTranspose1d(10, 20, kernel_size=3, stride=1),
            nn.ConvTranspose1d(20, 30, kernel_size=3, stride=1),
            nn.ConvTranspose1d(30, 40, kernel_size=3, stride=1),
        ])
        
        self.flatten = nn.Flatten()

    def forward(self, x):
        # Apply ConstantPad2d
        x = self.pad2d(x)
        
        # Apply MaxPool2d
        x = self.maxpool2d(x)
        
        # Reshape for ZeroPad3d
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.zeropad3d(x)
        
        # Reshape for ConvTranspose1d
        x = x.squeeze(2)  # Remove height dimension
        x = self.convtranspose1d(x)
        
        # Reshape for LazyConvTranspose3d
        x = x.unsqueeze(2).unsqueeze(3)  # Add height and depth dimensions
        x = self.lazyconvtranspose3d(x)
        
        # Apply MaxUnpool3d
        indices = torch.arange(0, x.numel(), dtype=torch.long, device=x.device).view(x.shape)
        x = self.maxunpool3d(x, indices)
        
        # Apply Upsample
        x = self.upsample(x)
        
        # Apply ModuleList layers
        for layer in self.module_list:
            x = layer(x)
        
        # Flatten the output
        x = self.flatten(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
