
# This is a random torch model generated by the following modules: ['BatchNorm3d', 'BCEWithLogitsLoss', 'LazyBatchNorm3d', 'SyncBatchNorm', 'ConstantPad2d', 'ParameterDict', 'CTCLoss', 'GELU', 'CrossEntropyLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.bn1 = nn.BatchNorm3d(10)
        self.lazy_bn = nn.LazyBatchNorm3d()
        self.sync_bn = nn.SyncBatchNorm(20)
        self.pad = nn.ConstantPad2d(2, 3.5)
        self.gelu = nn.GELU()
        self.param_dict = nn.ParameterDict({
            'param1': nn.Parameter(torch.randn(10, 10)),
            'param2': nn.Parameter(torch.randn(10, 10))
        })
        self.bce_loss = nn.BCEWithLogitsLoss()
        self.ctc_loss = nn.CTCLoss()
        self.ce_loss = nn.CrossEntropyLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, depth, height, width)
        x = self.bn1(x)
        x = self.lazy_bn(x)
        x = self.sync_bn(x)
        
        # Reshape to 2D for ConstantPad2d
        batch_size, channels, depth, height, width = x.shape
        x = x.view(batch_size, channels * depth, height, width)
        x = self.pad(x)
        
        # Reshape back to 3D
        x = x.view(batch_size, channels, depth, height + 4, width + 4)
        
        x = self.gelu(x)
        
        # Use ParameterDict
        param1 = self.param_dict['param1']
        param2 = self.param_dict['param2']
        x = torch.matmul(x.view(batch_size, -1), param1)
        x = torch.matmul(x, param2)
        
        # Compute losses (for demonstration purposes, not typically done in forward)
        target_bce = torch.randint(0, 2, (batch_size, 10)).float()
        target_ctc = torch.randint(0, 10, (batch_size, 10))
        target_ce = torch.randint(0, 10, (batch_size,))
        
        bce_loss = self.bce_loss(x, target_bce)
        ctc_loss = self.ctc_loss(x, target_ctc, torch.full((batch_size,), 10), torch.full((batch_size,), 10))
        ce_loss = self.ce_loss(x, target_ce)
        
        return x, bce_loss, ctc_loss, ce_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 5, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

