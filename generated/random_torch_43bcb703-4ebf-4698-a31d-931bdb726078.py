
# This is a random torch model generated by the following modules: ['Conv1d', 'KLDivLoss', 'Conv2d', 'AdaptiveLogSoftmaxWithLoss', 'LSTM', 'AdaptiveAvgPool2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1d = nn.Conv1d(1, 10, kernel_size=5)
        self.conv2d_1 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2d_2 = nn.Conv2d(20, 30, kernel_size=5)
        self.adaptive_avg_pool2d = nn.AdaptiveAvgPool2d((5, 5))
        self.lstm = nn.LSTM(750, 100, batch_first=True)
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(100, 10, [40, 60])
        self.kldivloss = nn.KLDivLoss(reduction='batchmean')

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, sequence_length)
        x = self.conv1d(x)  # Shape: (batch_size, 10, sequence_length - 4)
        
        # Reshape for Conv2d
        x = x.unsqueeze(-1)  # Shape: (batch_size, 10, sequence_length - 4, 1)
        x = self.conv2d_1(x)  # Shape: (batch_size, 20, sequence_length - 8, 1)
        x = self.conv2d_2(x)  # Shape: (batch_size, 30, sequence_length - 12, 1)
        
        # AdaptiveAvgPool2d
        x = self.adaptive_avg_pool2d(x)  # Shape: (batch_size, 30, 5, 5)
        
        # Flatten for LSTM
        x = x.view(x.size(0), -1)  # Shape: (batch_size, 30 * 5 * 5 = 750)
        x = x.unsqueeze(1)  # Shape: (batch_size, 1, 750)
        
        # LSTM
        x, _ = self.lstm(x)  # Shape: (batch_size, 1, 100)
        x = x.squeeze(1)  # Shape: (batch_size, 100)
        
        # AdaptiveLogSoftmaxWithLoss
        output = self.adaptive_log_softmax.log_prob(x)  # Shape: (batch_size, 10)
        
        # KLDivLoss (assuming target is provided externally)
        # target = torch.randn_like(output).softmax(dim=1)
        # loss = self.kldivloss(output.log(), target)
        
        return output


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 100).cuda()  # Example input shape: (batch_size, channels, sequence_length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
