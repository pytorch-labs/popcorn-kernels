
# This is a random torch model generated by the following modules: ['Unflatten', 'ReplicationPad3d', 'CTCLoss', 'AdaptiveMaxPool2d', 'Bilinear']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.unflatten = nn.Unflatten(1, (1, 64))
        self.replication_pad3d = nn.ReplicationPad3d(1)
        self.adaptive_max_pool2d = nn.AdaptiveMaxPool2d((32, 32))
        self.bilinear = nn.Bilinear(32 * 32, 32 * 32, 128)
        self.ctc_loss = nn.CTCLoss()

    def forward(self, x):
        # Unflatten the input to add a channel dimension
        x = self.unflatten(x)
        
        # Pad the input using ReplicationPad3d
        x = self.replication_pad3d(x)
        
        # Reshape to 2D for AdaptiveMaxPool2d
        x = x.view(x.size(0), x.size(1), -1, x.size(-1))
        x = self.adaptive_max_pool2d(x)
        
        # Flatten the output for Bilinear layer
        x = x.view(x.size(0), -1)
        x = self.bilinear(x, x)
        
        # Compute CTC loss (assuming target and input_length are provided)
        # For demonstration, we'll just return the output without computing the loss
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

