
# This is a random torch model generated by the following modules: ['GaussianNLLLoss', 'ReLU6', 'LogSigmoid', 'ParameterDict']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        # Using ParameterDict to store parameters
        self.params = nn.ParameterDict({
            'param1': nn.Parameter(torch.randn(10)),
            'param2': nn.Parameter(torch.randn(10)),
        })
        # Using ReLU6 and LogSigmoid activation functions
        self.relu6 = nn.ReLU6()
        self.log_sigmoid = nn.LogSigmoid()
        # Using GaussianNLLLoss as part of the forward pass
        self.gaussian_nll_loss = nn.GaussianNLLLoss()

    def forward(self, x):
        # Apply ReLU6 activation
        x = self.relu6(x)
        
        # Apply LogSigmoid activation
        x = self.log_sigmoid(x)
        
        # Reshape the input to match the parameter shapes
        x = x.view(-1, 10)
        
        # Use the parameters from ParameterDict
        x = x * self.params['param1'] + self.params['param2']
        
        # Compute the mean and variance for GaussianNLLLoss
        mean = x.mean(dim=1, keepdim=True)
        var = x.var(dim=1, keepdim=True)
        
        # Generate a target tensor for GaussianNLLLoss
        target = torch.randn_like(mean)
        
        # Compute the GaussianNLLLoss
        loss = self.gaussian_nll_loss(mean, target, var)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

