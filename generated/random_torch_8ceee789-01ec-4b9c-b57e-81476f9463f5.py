
# This is a random torch model generated by the following modules: ['GaussianNLLLoss', 'CTCLoss', 'LocalResponseNorm', 'Conv2d', 'ReLU6', 'Linear', 'GRU']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.lrn = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=1.0)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.relu6 = nn.ReLU6()
        self.fc1 = nn.Linear(32 * 8 * 8, 128)
        self.gru = nn.GRU(input_size=128, hidden_size=64, num_layers=2, batch_first=True)
        self.fc2 = nn.Linear(64, 10)
        self.gaussian_nll_loss = nn.GaussianNLLLoss()
        self.ctc_loss = nn.CTCLoss()

    def forward(self, x):
        # Assume input x is of shape (batch_size, channels, height, width)
        x = self.conv1(x)
        x = self.lrn(x)
        x = self.conv2(x)
        x = self.relu6(x)
        x = F.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = self.fc1(x)
        x = x.unsqueeze(1)  # Add a sequence dimension for GRU
        x, _ = self.gru(x)
        x = x[:, -1, :]  # Take the last output of the GRU
        x = self.fc2(x)
        return x

    def compute_loss(self, output, target, var=None):
        if var is not None:
            return self.gaussian_nll_loss(output, target, var)
        else:
            return self.ctc_loss(output, target)


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
