
# This is a random torch model generated by the following modules: ['Embedding', 'ReplicationPad3d', 'Softmax', 'CELU', 'EmbeddingBag', 'MSELoss', 'LazyConv2d', 'GLU', 'AvgPool1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.embedding_bag = nn.EmbeddingBag(1000, 128, mode='mean')  # EmbeddingBag layer
        self.replication_pad3d = nn.ReplicationPad3d(1)  # ReplicationPad3d layer
        self.lazy_conv2d = nn.LazyConv2d(64, kernel_size=3)  # LazyConv2d layer
        self.glu = nn.GLU(dim=1)  # GLU layer
        self.avg_pool1d = nn.AvgPool1d(kernel_size=2)  # AvgPool1d layer
        self.celu = nn.CELU()  # CELU activation
        self.softmax = nn.Softmax(dim=1)  # Softmax layer
        self.mse_loss = nn.MSELoss()  # MSELoss layer

    def forward(self, x):
        # Assuming x is a tensor of arbitrary shape
        if x.dtype == torch.long:
            x = self.embedding(x)  # Apply Embedding
        else:
            x = x.view(-1, 1).long()  # Reshape and convert to long for Embedding
            x = self.embedding(x)
        
        x = x.unsqueeze(1).unsqueeze(1)  # Add dimensions for ReplicationPad3d
        x = self.replication_pad3d(x)  # Apply ReplicationPad3d
        
        x = x.squeeze(1).squeeze(1)  # Remove extra dimensions
        x = x.view(-1, 128, 1, 1)  # Reshape for LazyConv2d
        x = self.lazy_conv2d(x)  # Apply LazyConv2d
        
        x = x.view(x.size(0), -1)  # Flatten for GLU
        x = self.glu(x)  # Apply GLU
        
        x = x.unsqueeze(1)  # Add dimension for AvgPool1d
        x = self.avg_pool1d(x)  # Apply AvgPool1d
        
        x = x.squeeze(1)  # Remove extra dimension
        x = self.celu(x)  # Apply CELU activation
        
        x = self.softmax(x)  # Apply Softmax
        
        # Assuming we have a target tensor for MSELoss
        target = torch.rand_like(x)
        loss = self.mse_loss(x, target)  # Apply MSELoss
        
        return x, loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10,)).cuda()  # Example input for Embedding
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

