
# This is a random torch model generated by the following modules: ['BCEWithLogitsLoss', 'ELU', 'UpsamplingBilinear2d', 'Unflatten', 'CELU', 'AdaptiveLogSoftmaxWithLoss', 'CircularPad3d', 'AdaptiveAvgPool2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.elu1 = nn.ELU()
        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.unflatten1 = nn.Unflatten(1, (16, 8, 8))
        self.celu1 = nn.CELU()
        self.circular_pad1 = nn.CircularPad3d(1)
        self.adaptive_avg_pool1 = nn.AdaptiveAvgPool2d((16, 16))
        self.elu2 = nn.ELU()
        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=2)
        self.celu2 = nn.CELU()
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(16, 10)
        self.bce_with_logits_loss = nn.BCEWithLogitsLoss()

    def forward(self, x):
        x = self.elu1(x)
        x = self.upsample1(x)
        x = x.view(x.size(0), -1)  # Flatten for unflatten
        x = self.unflatten1(x)
        x = self.celu1(x)
        x = x.unsqueeze(1)  # Add channel dimension for CircularPad3d
        x = self.circular_pad1(x)
        x = x.squeeze(1)  # Remove channel dimension after padding
        x = self.adaptive_avg_pool1(x)
        x = self.elu2(x)
        x = self.upsample2(x)
        x = self.celu2(x)
        x = x.view(x.size(0), -1)  # Flatten for AdaptiveLogSoftmaxWithLoss
        x = self.adaptive_log_softmax(x, torch.zeros(x.size(0), dtype=torch.long).to(x.device))  # Dummy target
        x = self.bce_with_logits_loss(x.output, torch.zeros_like(x.output))  # Dummy target
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

