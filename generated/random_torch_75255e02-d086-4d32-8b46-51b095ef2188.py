
# This is a random torch model generated by the following modules: ['AdaptiveMaxPool1d', 'ReLU6', 'Dropout1d', 'UpsamplingNearest2d', 'NLLLoss2d', 'ELU']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.adaptive_max_pool1d = nn.AdaptiveMaxPool1d(output_size=10)
        self.relu6 = nn.ReLU6()
        self.dropout1d = nn.Dropout1d(p=0.5)
        self.upsampling_nearest2d = nn.UpsamplingNearest2d(scale_factor=2)
        self.elu = nn.ELU()
        self.nll_loss2d = nn.NLLLoss2d()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, height, width)
        # First, reshape to 1D for AdaptiveMaxPool1d
        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, height * width)
        x = self.adaptive_max_pool1d(x)
        x = self.relu6(x)
        x = self.dropout1d(x)
        
        # Reshape back to 2D for UpsamplingNearest2d
        x = x.view(x.size(0), x.size(1), int(x.size(2) ** 0.5), int(x.size(2) ** 0.5))  # Reshape to (batch_size, channels, sqrt(height * width), sqrt(height * width))
        x = self.upsampling_nearest2d(x)
        x = self.elu(x)
        
        # Apply NLLLoss2d (assuming we have a target tensor for loss calculation)
        # Note: NLLLoss2d is typically used for loss calculation, not as a layer in the forward pass.
        # For the sake of using the module, we will generate a dummy target tensor.
        target = torch.randint(0, x.size(1), (x.size(0), x.size(2), x.size(3)), device=x.device)
        loss = self.nll_loss2d(F.log_softmax(x, dim=1), target)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape (batch_size, channels, height, width)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

