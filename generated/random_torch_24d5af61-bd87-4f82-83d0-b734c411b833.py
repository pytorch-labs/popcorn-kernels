
# This is a random torch model generated by the following modules: ['LazyLinear', 'AvgPool3d', 'CELU', 'AdaptiveAvgPool3d', 'Conv2d', 'ReflectionPad3d', 'MultiLabelSoftMarginLoss', 'MaxPool2d', 'EmbeddingBag', 'GRUCell']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.avgpool3d = nn.AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))
        self.celu = nn.CELU()
        self.adaptiveavgpool3d = nn.AdaptiveAvgPool3d((4, 4, 4))
        self.reflectionpad3d = nn.ReflectionPad3d(1)
        self.grucell = nn.GRUCell(input_size=256, hidden_size=128)
        self.embeddingbag = nn.EmbeddingBag(num_embeddings=1000, embedding_dim=128, mode='mean')
        self.lazylinear = nn.LazyLinear(out_features=10)
        self.loss = nn.MultiLabelSoftMarginLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, height, width)
        x = self.conv1(x)  # Shape: (batch_size, 16, height, width)
        x = self.maxpool1(x)  # Shape: (batch_size, 16, height/2, width/2)
        
        # Reshape for 3D operations
        x = x.unsqueeze(2)  # Shape: (batch_size, 16, 1, height/2, width/2)
        x = self.avgpool3d(x)  # Shape: (batch_size, 16, 1, height/4, width/4)
        x = self.celu(x)
        x = self.adaptiveavgpool3d(x)  # Shape: (batch_size, 16, 4, 4, 4)
        x = self.reflectionpad3d(x)  # Shape: (batch_size, 16, 6, 6, 6)
        
        # Flatten for GRUCell
        x = x.view(x.size(0), -1)  # Shape: (batch_size, 16*6*6*6)
        x = self.grucell(x)  # Shape: (batch_size, 128)
        
        # EmbeddingBag requires a different input type (indices and offsets)
        indices = torch.randint(0, 1000, (x.size(0), 10)).to(x.device)  # Random indices
        offsets = torch.arange(0, x.size(0) * 10, 10).to(x.device)  # Offsets
        x = self.embeddingbag(indices, offsets)  # Shape: (batch_size, 128)
        
        x = self.lazylinear(x)  # Shape: (batch_size, 10)
        
        # Assuming target is a binary matrix of shape (batch_size, 10)
        target = torch.randint(0, 2, (x.size(0), 10)).float().to(x.device)
        loss = self.loss(x, target)
        
        return x, loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
