
# This is a random torch model generated by the following modules: ['ModuleList', 'AdaptiveMaxPool2d', 'AdaptiveMaxPool3d', 'Bilinear', 'SiLU', 'Flatten', 'TransformerDecoder']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.module_list = nn.ModuleList([
            nn.AdaptiveMaxPool2d((16, 16)),
            nn.AdaptiveMaxPool3d((8, 8, 8)),
            nn.Bilinear(16 * 16, 8 * 8 * 8, 256),
            nn.SiLU(),
            nn.Flatten(),
            nn.TransformerDecoder(
                nn.TransformerDecoderLayer(d_model=256, nhead=8),
                num_layers=3
            )
        ])
        self.fc = nn.Linear(256, 10)

    def forward(self, x):
        # Apply AdaptiveMaxPool2d
        x = self.module_list[0](x)
        
        # Reshape for AdaptiveMaxPool3d
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.module_list[1](x)
        
        # Flatten the output for Bilinear
        x_2d = x.view(x.size(0), -1)
        x_3d = x.view(x.size(0), -1)
        x = self.module_list[2](x_2d, x_3d)
        
        # Apply SiLU activation
        x = self.module_list[3](x)
        
        # Flatten the output
        x = self.module_list[4](x)
        
        # Apply TransformerDecoder
        x = x.unsqueeze(0)  # Add sequence dimension
        x = self.module_list[5](x, x)
        x = x.squeeze(0)  # Remove sequence dimension
        
        # Final fully connected layer
        x = self.fc(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

