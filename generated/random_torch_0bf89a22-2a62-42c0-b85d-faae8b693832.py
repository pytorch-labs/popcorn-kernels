
# This is a random torch model generated by the following modules: ['GRU', 'Linear', 'ConvTranspose2d', 'CosineEmbeddingLoss', 'RNNCellBase', 'Dropout1d', 'RReLU']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.gru = nn.GRU(input_size=128, hidden_size=256, num_layers=2, batch_first=True)
        self.linear1 = nn.Linear(256, 128)
        self.conv_transpose = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)
        self.rnn_cell = nn.RNNCell(input_size=128, hidden_size=64)
        self.dropout1d = nn.Dropout1d(p=0.5)
        self.rrelu = nn.RReLU()
        self.linear2 = nn.Linear(32 * 8 * 8, 10)
        self.cosine_loss = nn.CosineEmbeddingLoss()

    def forward(self, x):
        # Assuming x is of shape (batch_size, sequence_length, input_size)
        batch_size = x.size(0)
        
        # GRU layer
        x, _ = self.gru(x)  # Output shape: (batch_size, sequence_length, hidden_size)
        
        # Linear layer
        x = self.linear1(x)  # Output shape: (batch_size, sequence_length, 128)
        
        # Reshape for ConvTranspose2d
        x = x.view(batch_size, 64, 4, 4)  # Reshape to (batch_size, 64, 4, 4)
        
        # ConvTranspose2d layer
        x = self.conv_transpose(x)  # Output shape: (batch_size, 32, 8, 8)
        
        # RNNCell layer
        x = x.view(batch_size, -1)  # Flatten to (batch_size, 32*8*8)
        hx = torch.zeros(batch_size, 64).to(x.device)  # Initialize hidden state
        x = self.rnn_cell(x, hx)  # Output shape: (batch_size, 64)
        
        # Dropout1d layer
        x = self.dropout1d(x.unsqueeze(2)).squeeze(2)  # Output shape: (batch_size, 64)
        
        # RReLU activation
        x = self.rrelu(x)  # Output shape: (batch_size, 64)
        
        # Linear layer
        x = self.linear2(x.view(batch_size, -1))  # Output shape: (batch_size, 10)
        
        # CosineEmbeddingLoss (requires two inputs and a target)
        # For demonstration, we'll create a dummy target and another input
        dummy_input2 = torch.randn_like(x)
        target = torch.ones(batch_size).to(x.device)
        loss = self.cosine_loss(x, dummy_input2, target)
        
        return x, loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 128).cuda()  # (batch_size, sequence_length, input_size)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
