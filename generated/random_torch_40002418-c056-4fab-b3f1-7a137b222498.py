
# This is a random torch model generated by the following modules: ['Sigmoid', 'TransformerDecoder', 'MaxUnpool3d', 'Sequential']
import torch
import torch.nn as nn


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.sigmoid = nn.Sigmoid()
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=64, nhead=8), num_layers=3
        )
        self.max_unpool3d = nn.MaxUnpool3d(kernel_size=2, stride=2)
        self.sequential = nn.Sequential(
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
        )

    def forward(self, x):
        # Apply Sigmoid
        x = self.sigmoid(x)
        
        # Reshape for TransformerDecoder (assuming input is 3D: [batch_size, seq_len, features])
        batch_size, *dims = x.shape
        x = x.view(batch_size, -1, 64)  # Reshape to [batch_size, seq_len, 64]
        
        # Apply TransformerDecoder
        memory = torch.randn_like(x)  # Random memory for TransformerDecoder
        x = self.transformer_decoder(x, memory)
        
        # Reshape for MaxUnpool3d (assuming input is 5D: [batch_size, channels, depth, height, width])
        x = x.view(batch_size, 1, 8, 8, 8)  # Reshape to [batch_size, 1, 8, 8, 8]
        indices = torch.randint(0, 8, x.shape).to(x.device)  # Random indices for MaxUnpool3d
        x = self.max_unpool3d(x, indices)
        
        # Reshape for Sequential (assuming input is 2D: [batch_size, features])
        x = x.view(batch_size, -1)  # Reshape to [batch_size, features]
        
        # Apply Sequential
        x = self.sequential(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 64, 64, 64).cuda()  # Arbitrary input shape
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
