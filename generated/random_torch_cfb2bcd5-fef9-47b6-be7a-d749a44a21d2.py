
# This is a random torch model generated by the following modules: ['AvgPool1d', 'MultiheadAttention', 'FractionalMaxPool3d', 'MultiLabelMarginLoss', 'AdaptiveMaxPool3d', 'L1Loss']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.avg_pool1d = nn.AvgPool1d(kernel_size=2)
        self.multihead_attention = nn.MultiheadAttention(embed_dim=64, num_heads=8)
        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))
        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d(output_size=(4, 4, 4))
        self.l1_loss = nn.L1Loss()
        self.multi_label_margin_loss = nn.MultiLabelMarginLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, length)
        x = self.avg_pool1d(x)  # Shape: (batch_size, channels, length // 2)
        
        # Reshape for MultiheadAttention
        x = x.permute(2, 0, 1)  # Shape: (length // 2, batch_size, channels)
        x, _ = self.multihead_attention(x, x, x)  # Shape: (length // 2, batch_size, channels)
        x = x.permute(1, 2, 0)  # Shape: (batch_size, channels, length // 2)
        
        # Reshape for FractionalMaxPool3d
        x = x.unsqueeze(1)  # Shape: (batch_size, 1, channels, length // 2)
        x = x.unsqueeze(-1)  # Shape: (batch_size, 1, channels, length // 2, 1)
        x = x.expand(-1, -1, -1, -1, 8)  # Shape: (batch_size, 1, channels, length // 2, 8)
        x = self.fractional_max_pool3d(x)  # Shape: (batch_size, 1, 8, 8, 8)
        
        # AdaptiveMaxPool3d
        x = self.adaptive_max_pool3d(x)  # Shape: (batch_size, 1, 4, 4, 4)
        
        # Flatten for loss computation
        x = x.view(x.size(0), -1)  # Shape: (batch_size, 1 * 4 * 4 * 4)
        
        # Dummy target for loss computation
        target = torch.zeros_like(x)
        l1_loss = self.l1_loss(x, target)
        
        # Dummy target for MultiLabelMarginLoss
        target_labels = torch.zeros((x.size(0), 10), dtype=torch.long)
        multi_label_loss = self.multi_label_margin_loss(x, target_labels)
        
        # Return both losses for demonstration purposes
        return l1_loss, multi_label_loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 64, 128).cuda()  # Shape: (batch_size, channels, length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
