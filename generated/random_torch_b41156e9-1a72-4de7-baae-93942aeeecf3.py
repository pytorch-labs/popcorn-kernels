
# This is a random torch model generated by the following modules: ['AdaptiveMaxPool3d', 'ConstantPad2d', 'LazyConvTranspose3d', 'UpsamplingBilinear2d', 'Bilinear', 'MultiLabelSoftMarginLoss', 'Conv3d', 'Sequential']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.pad = nn.ConstantPad2d(2, 3.5)
        self.conv3d_1 = nn.Conv3d(1, 10, kernel_size=3)
        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((5, 5, 5))
        self.lazy_conv_transpose3d = nn.LazyConvTranspose3d(20, kernel_size=3)
        self.upsampling_bilinear2d = nn.UpsamplingBilinear2d(scale_factor=2)
        self.bilinear = nn.Bilinear(10, 10, 50)
        self.conv3d_2 = nn.Conv3d(20, 10, kernel_size=3)
        self.sequential = nn.Sequential(
            nn.Conv3d(10, 5, kernel_size=3),
            nn.AdaptiveMaxPool3d((3, 3, 3)),
            nn.LazyConvTranspose3d(10, kernel_size=3)
        )
        self.loss = nn.MultiLabelSoftMarginLoss()

    def forward(self, x):
        # Assuming input is 3D, we add a channel dimension if necessary
        if x.dim() == 3:
            x = x.unsqueeze(1)  # Add channel dimension
        
        # Apply ConstantPad2d (assuming we reshape to 2D temporarily)
        original_shape = x.shape
        x = x.view(original_shape[0], original_shape[1], -1)  # Flatten spatial dimensions
        x = self.pad(x)
        x = x.view(original_shape[0], original_shape[1], *original_shape[2:])  # Reshape back
        
        # Apply Conv3d
        x = self.conv3d_1(x)
        
        # Apply AdaptiveMaxPool3d
        x = self.adaptive_max_pool3d(x)
        
        # Apply LazyConvTranspose3d
        x = self.lazy_conv_transpose3d(x)
        
        # Apply UpsamplingBilinear2d (assuming we reshape to 2D temporarily)
        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions
        x = self.upsampling_bilinear2d(x)
        x = x.view(x.size(0), x.size(1), *x.shape[2:])  # Reshape back
        
        # Apply Bilinear (assuming we reshape to 2D temporarily)
        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions
        x = self.bilinear(x, x)  # Using the same tensor for both inputs
        
        # Apply Conv3d again
        x = self.conv3d_2(x)
        
        # Apply Sequential
        x = self.sequential(x)
        
        # Compute loss (assuming we have some target)
        target = torch.randint(0, 2, (x.size(0), x.size(1))).float()  # Random target for demonstration
        loss = self.loss(x, target)
        
        return x, loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 32, 32, 32).cuda()  # Arbitrary 3D input
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

