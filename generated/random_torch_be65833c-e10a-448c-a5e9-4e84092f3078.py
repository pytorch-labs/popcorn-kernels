
# This is a random torch model generated by the following modules: ['AdaptiveMaxPool2d', 'MaxPool2d', 'LayerNorm', 'AlphaDropout', 'ReplicationPad2d', 'Softmax2d', 'Module', 'SyncBatchNorm', 'Tanh', 'RNN']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.adaptive_max_pool = nn.AdaptiveMaxPool2d((16, 16))
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.layer_norm = nn.LayerNorm([16, 16])
        self.alpha_dropout = nn.AlphaDropout(p=0.5)
        self.replication_pad = nn.ReplicationPad2d(2)
        self.softmax = nn.Softmax2d()
        self.sync_batch_norm = nn.SyncBatchNorm(16)
        self.tanh = nn.Tanh()
        self.rnn = nn.RNN(input_size=16, hidden_size=32, num_layers=2, batch_first=True)

    def forward(self, x):
        # Apply AdaptiveMaxPool2d
        x = self.adaptive_max_pool(x)
        
        # Apply MaxPool2d
        x = self.max_pool(x)
        
        # Apply LayerNorm
        x = self.layer_norm(x)
        
        # Apply AlphaDropout
        x = self.alpha_dropout(x)
        
        # Apply ReplicationPad2d
        x = self.replication_pad(x)
        
        # Apply Softmax2d
        x = self.softmax(x)
        
        # Apply SyncBatchNorm
        x = self.sync_batch_norm(x)
        
        # Apply Tanh
        x = self.tanh(x)
        
        # Reshape for RNN
        batch_size, channels, height, width = x.size()
        x = x.view(batch_size, channels * height, width)
        
        # Apply RNN
        x, _ = self.rnn(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()  # Example input with 3 channels and 64x64 spatial dimensions
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

