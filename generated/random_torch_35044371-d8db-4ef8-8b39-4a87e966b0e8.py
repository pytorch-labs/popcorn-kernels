
# This is a random torch model generated by the following modules: ['ELU', 'LayerNorm', 'LazyBatchNorm2d', 'LogSigmoid', 'FeatureAlphaDropout', 'RMSNorm', 'ReflectionPad2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.elu1 = nn.ELU()
        self.layer_norm1 = nn.LayerNorm(64)
        self.lazy_batch_norm2d1 = nn.LazyBatchNorm2d()
        self.log_sigmoid1 = nn.LogSigmoid()
        self.feature_alpha_dropout1 = nn.FeatureAlphaDropout()
        self.rms_norm1 = nn.RMSNorm(64)
        self.reflection_pad2d1 = nn.ReflectionPad2d(2)
        
        self.elu2 = nn.ELU()
        self.layer_norm2 = nn.LayerNorm(128)
        self.lazy_batch_norm2d2 = nn.LazyBatchNorm2d()
        self.log_sigmoid2 = nn.LogSigmoid()
        self.feature_alpha_dropout2 = nn.FeatureAlphaDropout()
        self.rms_norm2 = nn.RMSNorm(128)
        self.reflection_pad2d2 = nn.ReflectionPad2d(2)
        
        self.elu3 = nn.ELU()
        self.layer_norm3 = nn.LayerNorm(256)
        self.lazy_batch_norm2d3 = nn.LazyBatchNorm2d()
        self.log_sigmoid3 = nn.LogSigmoid()
        self.feature_alpha_dropout3 = nn.FeatureAlphaDropout()
        self.rms_norm3 = nn.RMSNorm(256)
        self.reflection_pad2d3 = nn.ReflectionPad2d(2)

    def forward(self, x):
        # First block
        x = self.reflection_pad2d1(x)
        x = self.lazy_batch_norm2d1(x)
        x = self.elu1(x)
        x = x.permute(0, 2, 3, 1)  # Change shape for LayerNorm
        x = self.layer_norm1(x)
        x = x.permute(0, 3, 1, 2)  # Change shape back
        x = self.log_sigmoid1(x)
        x = self.feature_alpha_dropout1(x)
        x = x.permute(0, 2, 3, 1)  # Change shape for RMSNorm
        x = self.rms_norm1(x)
        x = x.permute(0, 3, 1, 2)  # Change shape back
        
        # Second block
        x = self.reflection_pad2d2(x)
        x = self.lazy_batch_norm2d2(x)
        x = self.elu2(x)
        x = x.permute(0, 2, 3, 1)  # Change shape for LayerNorm
        x = self.layer_norm2(x)
        x = x.permute(0, 3, 1, 2)  # Change shape back
        x = self.log_sigmoid2(x)
        x = self.feature_alpha_dropout2(x)
        x = x.permute(0, 2, 3, 1)  # Change shape for RMSNorm
        x = self.rms_norm2(x)
        x = x.permute(0, 3, 1, 2)  # Change shape back
        
        # Third block
        x = self.reflection_pad2d3(x)
        x = self.lazy_batch_norm2d3(x)
        x = self.elu3(x)
        x = x.permute(0, 2, 3, 1)  # Change shape for LayerNorm
        x = self.layer_norm3(x)
        x = x.permute(0, 3, 1, 2)  # Change shape back
        x = self.log_sigmoid3(x)
        x = self.feature_alpha_dropout3(x)
        x = x.permute(0, 2, 3, 1)  # Change shape for RMSNorm
        x = self.rms_norm3(x)
        x = x.permute(0, 3, 1, 2)  # Change shape back
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
