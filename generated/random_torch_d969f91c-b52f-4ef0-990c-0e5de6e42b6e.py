
# This is a random torch model generated by the following modules: ['BatchNorm3d', 'ConstantPad3d', 'LazyConv3d', 'InstanceNorm3d', 'TransformerEncoderLayer']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.bn1 = nn.BatchNorm3d(1)
        self.pad1 = nn.ConstantPad3d(1, 0.5)
        self.conv1 = nn.LazyConv3d(10, kernel_size=3)
        self.in1 = nn.InstanceNorm3d(10)
        self.transformer_encoder1 = nn.TransformerEncoderLayer(d_model=10, nhead=2)
        self.bn2 = nn.BatchNorm3d(10)
        self.pad2 = nn.ConstantPad3d(1, 0.5)
        self.conv2 = nn.LazyConv3d(20, kernel_size=3)
        self.in2 = nn.InstanceNorm3d(20)
        self.transformer_encoder2 = nn.TransformerEncoderLayer(d_model=20, nhead=2)
        self.bn3 = nn.BatchNorm3d(20)
        self.pad3 = nn.ConstantPad3d(1, 0.5)
        self.conv3 = nn.LazyConv3d(30, kernel_size=3)
        self.in3 = nn.InstanceNorm3d(30)
        self.transformer_encoder3 = nn.TransformerEncoderLayer(d_model=30, nhead=2)
        self.bn4 = nn.BatchNorm3d(30)
        self.pad4 = nn.ConstantPad3d(1, 0.5)
        self.conv4 = nn.LazyConv3d(40, kernel_size=3)
        self.in4 = nn.InstanceNorm3d(40)
        self.transformer_encoder4 = nn.TransformerEncoderLayer(d_model=40, nhead=2)
        self.bn5 = nn.BatchNorm3d(40)
        self.pad5 = nn.ConstantPad3d(1, 0.5)
        self.conv5 = nn.LazyConv3d(50, kernel_size=3)
        self.in5 = nn.InstanceNorm3d(50)
        self.transformer_encoder5 = nn.TransformerEncoderLayer(d_model=50, nhead=2)

    def forward(self, x):
        x = self.bn1(x)
        x = self.pad1(x)
        x = self.conv1(x)
        x = self.in1(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape for TransformerEncoderLayer
        x = self.transformer_encoder1(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape back
        x = self.bn2(x)
        x = self.pad2(x)
        x = self.conv2(x)
        x = self.in2(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape for TransformerEncoderLayer
        x = self.transformer_encoder2(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape back
        x = self.bn3(x)
        x = self.pad3(x)
        x = self.conv3(x)
        x = self.in3(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape for TransformerEncoderLayer
        x = self.transformer_encoder3(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape back
        x = self.bn4(x)
        x = self.pad4(x)
        x = self.conv4(x)
        x = self.in4(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape for TransformerEncoderLayer
        x = self.transformer_encoder4(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape back
        x = self.bn5(x)
        x = self.pad5(x)
        x = self.conv5(x)
        x = self.in5(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape for TransformerEncoderLayer
        x = self.transformer_encoder5(x)
        x = x.permute(0, 2, 1, 3, 4)  # Reshape back
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 32, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
