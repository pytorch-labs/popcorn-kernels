
# This is a random torch model generated by the following modules: ['L1Loss', 'ParameterList', 'LazyLinear', 'AdaptiveAvgPool2d', 'RNNCellBase', 'MaxPool3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((5, 5))
        self.max_pool3d = nn.MaxPool3d(kernel_size=2, stride=2)
        self.rnn_cell = nn.RNNCellBase(input_size=25, hidden_size=50)
        self.lazy_linear = nn.LazyLinear(out_features=100)
        self.parameter_list = nn.ParameterList([nn.Parameter(torch.randn(100)) for _ in range(5)])
        self.l1_loss = nn.L1Loss()

    def forward(self, x):
        # Assume input x is of shape (batch_size, channels, height, width, depth)
        # First, apply AdaptiveAvgPool2d to reduce spatial dimensions
        x = self.adaptive_avg_pool(x)
        
        # Reshape to fit MaxPool3d input
        x = x.unsqueeze(2)  # Add a depth dimension
        x = self.max_pool3d(x)
        
        # Flatten the tensor for RNNCellBase
        x = x.view(x.size(0), -1)
        
        # Apply RNNCellBase
        hx = torch.zeros(x.size(0), 50).to(x.device)  # Initialize hidden state
        x = self.rnn_cell(x, hx)
        
        # Apply LazyLinear
        x = self.lazy_linear(x)
        
        # Apply ParameterList (sum all parameters and add to x)
        for param in self.parameter_list:
            x = x + param
        
        # Apply L1Loss (dummy target for demonstration)
        target = torch.zeros_like(x)
        loss = self.l1_loss(x, target)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
