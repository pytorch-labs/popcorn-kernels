
# This is a random torch model generated by the following modules: ['Flatten', 'PairwiseDistance', 'Embedding', 'Container', 'GRUCell', 'NLLLoss', 'LazyConv2d', 'LSTMCell', 'Hardsigmoid']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.lazy_conv2d = nn.LazyConv2d(out_channels=64, kernel_size=3)  # LazyConv2d layer
        self.gru_cell1 = nn.GRUCell(input_size=64, hidden_size=128)  # GRUCell layer
        self.gru_cell2 = nn.GRUCell(input_size=128, hidden_size=64)  # GRUCell layer
        self.lstm_cell = nn.LSTMCell(input_size=64, hidden_size=32)  # LSTMCell layer
        self.hardsigmoid = nn.Hardsigmoid()  # Hardsigmoid layer
        self.flatten = nn.Flatten()  # Flatten layer
        self.pairwise_distance = nn.PairwiseDistance()  # PairwiseDistance layer
        self.nll_loss = nn.NLLLoss()  # NLLLoss layer

    def forward(self, x):
        # Assume input x is of shape (batch_size, sequence_length)
        x = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)
        
        # Reshape for LazyConv2d
        x = x.unsqueeze(1)  # Shape: (batch_size, 1, sequence_length, embedding_dim)
        x = self.lazy_conv2d(x)  # Shape: (batch_size, 64, sequence_length - kernel_size + 1, embedding_dim - kernel_size + 1)
        
        # Flatten the spatial dimensions
        x = self.flatten(x)  # Shape: (batch_size, 64 * (sequence_length - kernel_size + 1) * (embedding_dim - kernel_size + 1))
        
        # Pass through GRUCell layers
        hx1 = torch.zeros(x.size(0), 128).to(x.device)  # Initial hidden state for GRUCell1
        hx1 = self.gru_cell1(x, hx1)  # Shape: (batch_size, 128)
        
        hx2 = torch.zeros(x.size(0), 64).to(x.device)  # Initial hidden state for GRUCell2
        hx2 = self.gru_cell2(hx1, hx2)  # Shape: (batch_size, 64)
        
        # Pass through LSTMCell layer
        hx_lstm = torch.zeros(x.size(0), 32).to(x.device)  # Initial hidden state for LSTMCell
        cx_lstm = torch.zeros(x.size(0), 32).to(x.device)  # Initial cell state for LSTMCell
        hx_lstm, cx_lstm = self.lstm_cell(hx2, (hx_lstm, cx_lstm))  # Shape: (batch_size, 32)
        
        # Apply Hardsigmoid
        x = self.hardsigmoid(hx_lstm)  # Shape: (batch_size, 32)
        
        # Compute PairwiseDistance
        x = self.pairwise_distance(x, torch.zeros_like(x))  # Shape: (batch_size,)
        
        # Compute NLLLoss (assuming target is provided externally)
        # For demonstration, we create a dummy target
        target = torch.zeros(x.size(0), dtype=torch.long).to(x.device)
        loss = self.nll_loss(x.unsqueeze(1), target)  # Shape: scalar
        
        return loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # Example input: (batch_size=10, sequence_length=20)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
