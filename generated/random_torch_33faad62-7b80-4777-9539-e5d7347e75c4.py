
# This is a random torch model generated by the following modules: ['LSTM', 'ModuleDict', 'Bilinear', 'CosineSimilarity', 'LazyBatchNorm3d', 'MultiMarginLoss', 'MSELoss', 'Unflatten', 'TransformerEncoder']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=2, batch_first=True)
        self.module_dict = nn.ModuleDict({
            'bilinear': nn.Bilinear(64, 64, 32),
            'cosine_sim': nn.CosineSimilarity(dim=1),
            'lazy_bn': nn.LazyBatchNorm3d(),
            'transformer_encoder': nn.TransformerEncoder(
                nn.TransformerEncoderLayer(d_model=64, nhead=8),
                num_layers=3
            )
        })
        self.unflatten = nn.Unflatten(1, (8, 8))
        self.multi_margin_loss = nn.MultiMarginLoss()
        self.mse_loss = nn.MSELoss()

    def forward(self, x):
        # Assume x is of shape (batch_size, sequence_length, input_size)
        batch_size, seq_len, input_size = x.shape
        
        # LSTM
        x, _ = self.lstm(x)  # Output shape: (batch_size, seq_len, hidden_size)
        
        # Take the last hidden state
        x = x[:, -1, :]  # Output shape: (batch_size, hidden_size)
        
        # Bilinear
        x = self.module_dict['bilinear'](x, x)  # Output shape: (batch_size, 32)
        
        # LazyBatchNorm3d (reshape to 5D tensor)
        x = x.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)  # Output shape: (batch_size, 32, 1, 1, 1)
        x = self.module_dict['lazy_bn'](x)  # Output shape: (batch_size, 32, 1, 1, 1)
        x = x.squeeze(-1).squeeze(-1).squeeze(-1)  # Output shape: (batch_size, 32)
        
        # Unflatten
        x = self.unflatten(x)  # Output shape: (batch_size, 8, 8)
        
        # TransformerEncoder (reshape to 3D tensor)
        x = x.permute(1, 0, 2)  # Output shape: (8, batch_size, 8)
        x = self.module_dict['transformer_encoder'](x)  # Output shape: (8, batch_size, 8)
        x = x.permute(1, 0, 2)  # Output shape: (batch_size, 8, 8)
        
        # CosineSimilarity
        x = self.module_dict['cosine_sim'](x[:, 0, :], x[:, 1, :])  # Output shape: (batch_size,)
        
        # MultiMarginLoss and MSELoss (dummy losses for demonstration)
        target = torch.randint(0, 10, (batch_size,)).to(x.device)
        loss1 = self.multi_margin_loss(x.unsqueeze(0), target.unsqueeze(0))
        loss2 = self.mse_loss(x, torch.zeros_like(x))
        
        return x, loss1, loss2


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(2, 10, 128).cuda()  # (batch_size, sequence_length, input_size)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

