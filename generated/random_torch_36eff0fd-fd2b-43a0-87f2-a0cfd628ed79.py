
# This is a random torch model generated by the following modules: ['CTCLoss', 'AdaptiveLogSoftmaxWithLoss', 'LazyInstanceNorm3d', 'UpsamplingNearest2d', 'ReLU', 'FeatureAlphaDropout', 'FractionalMaxPool3d', 'LocalResponseNorm', 'ModuleDict', 'ChannelShuffle']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.lazy_instance_norm3d = nn.LazyInstanceNorm3d()
        self.upsampling_nearest2d = nn.UpsamplingNearest2d(scale_factor=2)
        self.relu = nn.ReLU()
        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)
        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))
        self.local_response_norm = nn.LocalResponseNorm(size=5)
        self.module_dict = nn.ModuleDict({
            'conv1': nn.Conv2d(1, 10, kernel_size=5),
            'conv2': nn.Conv2d(10, 20, kernel_size=5)
        })
        self.channel_shuffle = nn.ChannelShuffle(groups=2)
        self.ctc_loss = nn.CTCLoss()
        self.adaptive_log_softmax_with_loss = nn.AdaptiveLogSoftmaxWithLoss(in_features=20, n_classes=10, cutoffs=[2, 4])

    def forward(self, x):
        # Assuming input is 3D, we add a dummy batch and channel dimension
        x = x.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, D, H, W]
        
        # Apply LazyInstanceNorm3d
        x = self.lazy_instance_norm3d(x)
        
        # Reshape to 4D for UpsamplingNearest2d
        x = x.squeeze(0)  # Shape: [1, D, H, W]
        x = x.permute(1, 0, 2, 3)  # Shape: [D, 1, H, W]
        x = self.upsampling_nearest2d(x)
        
        # Apply ReLU
        x = self.relu(x)
        
        # Apply FeatureAlphaDropout
        x = self.feature_alpha_dropout(x)
        
        # Reshape back to 5D for FractionalMaxPool3d
        x = x.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: [1, 1, D, H, W]
        x = self.fractional_max_pool3d(x)
        
        # Apply LocalResponseNorm
        x = self.local_response_norm(x)
        
        # Reshape to 4D for ModuleDict convolutions
        x = x.squeeze(0)  # Shape: [1, D, H, W]
        x = x.permute(1, 0, 2, 3)  # Shape: [D, 1, H, W]
        
        # Apply ModuleDict convolutions
        x = self.module_dict['conv1'](x)
        x = self.module_dict['conv2'](x)
        
        # Apply ChannelShuffle
        x = self.channel_shuffle(x)
        
        # Reshape for AdaptiveLogSoftmaxWithLoss
        x = x.permute(0, 2, 3, 1)  # Shape: [D, H, W, C]
        x = x.reshape(-1, x.size(-1))  # Shape: [D*H*W, C]
        
        # Apply AdaptiveLogSoftmaxWithLoss
        x = self.adaptive_log_softmax_with_loss(x, torch.zeros(x.size(0), dtype=torch.long))
        
        # Return the output (log probabilities)
        return x.log_prob


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(32, 32, 32)  # Arbitrary 3D input
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
