
# This is a random torch model generated by the following modules: ['CosineEmbeddingLoss', 'PixelShuffle', 'Sequential', 'LazyInstanceNorm3d', 'LPPool1d', 'GELU', 'Linear', 'MarginRankingLoss', 'Softplus']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.sequential = nn.Sequential(
            nn.LazyInstanceNorm3d(),
            nn.LPPool1d(norm_type=2, kernel_size=3, stride=2),
            nn.GELU(),
            nn.Linear(128, 256),
            nn.Softplus()
        )
        self.pixel_shuffle = nn.PixelShuffle(2)
        self.linear = nn.Linear(256, 128)
        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()
        self.margin_ranking_loss = nn.MarginRankingLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, height, width)
        x = self.pixel_shuffle(x)  # Shape: (batch_size, channels / 4, height * 2, width * 2)
        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions
        x = self.sequential(x)  # Shape: (batch_size, 256)
        x = self.linear(x)  # Shape: (batch_size, 128)
        
        # Dummy targets for loss functions
        target1 = torch.randint(0, 2, (x.size(0),)).float().to(x.device)
        target2 = torch.randint(0, 2, (x.size(0),)).float().to(x.device)
        
        # Compute losses (not used in actual output, just to utilize the modules)
        cosine_loss = self.cosine_embedding_loss(x, x, target1)
        margin_loss = self.margin_ranking_loss(x, x, target2)
        
        return x, cosine_loss, margin_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 16, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
