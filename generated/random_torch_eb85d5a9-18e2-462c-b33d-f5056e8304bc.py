
# This is a random torch model generated by the following modules: ['TransformerDecoder', 'RMSNorm', 'TripletMarginWithDistanceLoss', 'ConstantPad1d', 'AvgPool1d', 'AdaptiveLogSoftmaxWithLoss', 'GELU', 'MultiheadAttention']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.constant_pad = nn.ConstantPad1d(2, 0)
        self.avg_pool = nn.AvgPool1d(kernel_size=2)
        self.rms_norm = RMSNorm(64)  # Assuming RMSNorm is a custom layer with input size 64
        self.multihead_attention = nn.MultiheadAttention(embed_dim=64, num_heads=8)
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=64, nhead=8), num_layers=3
        )
        self.gelu = nn.GELU()
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(
            in_features=64, n_classes=10, cutoffs=[2, 4, 6]
        )
        self.triplet_loss = nn.TripletMarginWithDistanceLoss(
            distance_function=lambda x, y: F.pairwise_distance(x, y, p=2)
        )

    def forward(self, x):
        # Assume x is of shape (batch_size, seq_len, features)
        x = self.constant_pad(x)  # Pad the input
        x = self.avg_pool(x)  # Apply average pooling
        x = self.rms_norm(x)  # Apply RMSNorm
        x = x.permute(1, 0, 2)  # Reshape for MultiheadAttention (seq_len, batch_size, features)
        x, _ = self.multihead_attention(x, x, x)  # Apply MultiheadAttention
        x = self.transformer_decoder(x, x)  # Apply TransformerDecoder
        x = self.gelu(x)  # Apply GELU activation
        x = x.permute(1, 0, 2)  # Reshape back to (batch_size, seq_len, features)
        x = x.mean(dim=1)  # Reduce sequence dimension
        output = self.adaptive_log_softmax(x)  # Apply AdaptiveLogSoftmaxWithLoss
        return output

    def compute_loss(self, anchor, positive, negative):
        return self.triplet_loss(anchor, positive, negative)


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(10, 20, 64).cuda()  # (batch_size, seq_len, features)
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []


# Assuming RMSNorm is a custom layer
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-8):
        super().__init__()
        self.scale = dim**-0.5
        self.eps = eps
        self.g = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale
        return x / norm.clamp(min=self.eps) * self.g
