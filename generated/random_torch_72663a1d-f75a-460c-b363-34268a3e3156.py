
# This is a random torch model generated by the following modules: ['ConvTranspose3d', 'LogSoftmax', 'LazyConv3d', 'ReLU6', 'FractionalMaxPool2d', 'ZeroPad1d', 'GaussianNLLLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(1, 10, kernel_size=3, stride=2, padding=1)
        self.lazy_conv3d = nn.LazyConv3d(20, kernel_size=3, stride=1, padding=1)
        self.relu6 = nn.ReLU6()
        self.fractional_max_pool2d = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))
        self.zero_pad1d = nn.ZeroPad1d(2)
        self.log_softmax = nn.LogSoftmax(dim=1)
        self.gaussian_nll_loss = nn.GaussianNLLLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, depth, height, width)
        x = self.conv_transpose3d(x)  # Shape: (batch_size, 10, depth_out, height_out, width_out)
        x = self.lazy_conv3d(x)  # Shape: (batch_size, 20, depth_out, height_out, width_out)
        x = self.relu6(x)
        
        # Reshape to 2D for FractionalMaxPool2d
        batch_size, channels, depth, height, width = x.shape
        x = x.view(batch_size * depth, channels, height, width)  # Shape: (batch_size * depth, 20, height, width)
        x = self.fractional_max_pool2d(x)  # Shape: (batch_size * depth, 20, 14, 14)
        
        # Reshape back to 3D
        x = x.view(batch_size, depth, channels, 14, 14)  # Shape: (batch_size, depth, 20, 14, 14)
        x = x.permute(0, 2, 1, 3, 4)  # Shape: (batch_size, 20, depth, 14, 14)
        
        # Flatten for ZeroPad1d
        x = x.view(batch_size, 20 * depth * 14 * 14)  # Shape: (batch_size, 20 * depth * 14 * 14)
        x = x.unsqueeze(1)  # Shape: (batch_size, 1, 20 * depth * 14 * 14)
        x = self.zero_pad1d(x)  # Shape: (batch_size, 1, 20 * depth * 14 * 14 + 4)
        
        # Apply LogSoftmax
        x = x.squeeze(1)  # Shape: (batch_size, 20 * depth * 14 * 14 + 4)
        x = self.log_softmax(x)
        
        # GaussianNLLLoss requires target and variance, so we return x for further processing
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 16, 32, 32).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

