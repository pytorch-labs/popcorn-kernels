
# This is a random torch model generated by the following modules: ['TransformerDecoder', 'KLDivLoss', 'MaxUnpool1d', 'CTCLoss', 'LeakyReLU', 'FractionalMaxPool3d']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=512, nhead=8), num_layers=3
        )
        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)
        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))
        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')
        self.ctc_loss = nn.CTCLoss()

    def forward(self, x):
        # Assume x is of shape (batch_size, sequence_length, d_model)
        batch_size, sequence_length, d_model = x.shape
        
        # Reshape for TransformerDecoder
        x = x.permute(1, 0, 2)  # (sequence_length, batch_size, d_model)
        memory = torch.zeros_like(x)
        x = self.transformer_decoder(x, memory)
        x = x.permute(1, 0, 2)  # (batch_size, sequence_length, d_model)
        
        # Reshape for MaxUnpool1d
        x = x.permute(0, 2, 1)  # (batch_size, d_model, sequence_length)
        indices = torch.randint(0, sequence_length, (batch_size, d_model, sequence_length // 2)).to(x.device)
        x = self.max_unpool1d(x, indices)
        x = x.permute(0, 2, 1)  # (batch_size, sequence_length, d_model)
        
        # Apply LeakyReLU
        x = self.leaky_relu(x)
        
        # Reshape for FractionalMaxPool3d
        x = x.view(batch_size, 1, sequence_length, d_model, 1)  # (batch_size, 1, sequence_length, d_model, 1)
        x = self.fractional_max_pool3d(x)
        x = x.view(batch_size, -1)  # Flatten for loss computation
        
        # Compute KLDivLoss (dummy target)
        target_kl = torch.rand_like(x).softmax(dim=-1)
        kl_loss = self.kl_div_loss(x.log_softmax(dim=-1), target_kl)
        
        # Compute CTCLoss (dummy target)
        target_lengths = torch.randint(1, sequence_length + 1, (batch_size,)).to(x.device)
        input_lengths = torch.full((batch_size,), sequence_length).to(x.device)
        ctc_loss = self.ctc_loss(x.log_softmax(dim=-1), torch.randint(0, d_model, (batch_size, sequence_length)).to(x.device), input_lengths, target_lengths)
        
        return x, kl_loss, ctc_loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 512).cuda()  # (batch_size, sequence_length, d_model)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
