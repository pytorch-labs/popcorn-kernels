
# This is a random torch model generated by the following modules: ['Conv1d', 'ConvTranspose2d', 'Softmax', 'AvgPool3d', 'ConstantPad2d', 'Flatten', 'TransformerEncoder', 'LogSoftmax']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1d = nn.Conv1d(1, 10, kernel_size=5)
        self.conv_transpose2d = nn.ConvTranspose2d(10, 20, kernel_size=5)
        self.avg_pool3d = nn.AvgPool3d(kernel_size=2)
        self.constant_pad2d = nn.ConstantPad2d(2, 3.0)
        self.flatten = nn.Flatten()
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=20, nhead=5), num_layers=2
        )
        self.softmax = nn.Softmax(dim=1)
        self.log_softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        # Assuming input is of shape (batch_size, channels, length)
        x = self.conv1d(x)  # Shape: (batch_size, 10, length - 4)
        
        # Reshape for ConvTranspose2d
        x = x.unsqueeze(-1)  # Shape: (batch_size, 10, length - 4, 1)
        x = self.conv_transpose2d(x)  # Shape: (batch_size, 20, length - 4 + 4, 1 + 4)
        
        # Reshape for AvgPool3d
        x = x.unsqueeze(-1)  # Shape: (batch_size, 20, length, 5, 1)
        x = self.avg_pool3d(x)  # Shape: (batch_size, 20, length // 2, 5 // 2, 1 // 2)
        
        # Reshape for ConstantPad2d
        x = x.squeeze(-1)  # Shape: (batch_size, 20, length // 2, 5 // 2)
        x = self.constant_pad2d(x)  # Shape: (batch_size, 20, length // 2 + 4, 5 // 2 + 4)
        
        # Flatten for TransformerEncoder
        x = self.flatten(x)  # Shape: (batch_size, 20 * (length // 2 + 4) * (5 // 2 + 4))
        
        # Reshape for TransformerEncoder
        x = x.view(x.size(0), -1, 20)  # Shape: (batch_size, seq_len, 20)
        x = self.transformer_encoder(x)  # Shape: (batch_size, seq_len, 20)
        
        # Apply Softmax and LogSoftmax
        x = self.softmax(x)
        x = self.log_softmax(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 64).cuda()  # Example input shape: (batch_size, channels, length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
