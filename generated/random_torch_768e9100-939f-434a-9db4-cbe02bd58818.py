
# This is a random torch model generated by the following modules: ['LogSoftmax', 'Bilinear', 'FractionalMaxPool3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.bilinear1 = nn.Bilinear(10, 20, 30)
        self.bilinear2 = nn.Bilinear(30, 40, 50)
        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(5, 5, 5))
        self.log_softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, depth, height, width)
        # First, apply FractionalMaxPool3d to reduce spatial dimensions
        x = self.fractional_max_pool3d(x)
        
        # Reshape x to fit the input requirements of the Bilinear layer
        # Flatten the spatial dimensions and channels
        batch_size, channels, depth, height, width = x.shape
        x = x.view(batch_size, channels * depth * height * width)
        
        # Split the flattened tensor into two parts for the Bilinear layer
        x1 = x[:, :10]  # First part of the input
        x2 = x[:, 10:30]  # Second part of the input
        
        # Apply the first Bilinear layer
        x = self.bilinear1(x1, x2)
        
        # Split the output of the first Bilinear layer for the second Bilinear layer
        x1 = x[:, :30]  # First part of the input
        x2 = x[:, 30:70]  # Second part of the input
        
        # Apply the second Bilinear layer
        x = self.bilinear2(x1, x2)
        
        # Apply LogSoftmax to the final output
        x = self.log_softmax(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 10, 10, 10).cuda()  # Example input shape: (batch_size, channels, depth, height, width)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

