
# This is a random torch model generated by the following modules: ['AlphaDropout', 'SiLU', 'PixelUnshuffle', 'Fold', 'SELU', 'RMSNorm']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.alpha_dropout1 = nn.AlphaDropout(p=0.5)
        self.silu1 = nn.SiLU()
        self.pixel_unshuffle1 = nn.PixelUnshuffle(downscale_factor=2)
        self.fold1 = nn.Fold(output_size=(16, 16), kernel_size=(2, 2), stride=(2, 2))
        self.selu1 = nn.SELU()
        self.rms_norm1 = RMSNorm(64)  # Assuming RMSNorm is a custom layer with input size 64
        self.alpha_dropout2 = nn.AlphaDropout(p=0.5)
        self.silu2 = nn.SiLU()
        self.pixel_unshuffle2 = nn.PixelUnshuffle(downscale_factor=2)
        self.fold2 = nn.Fold(output_size=(8, 8), kernel_size=(2, 2), stride=(2, 2))
        self.selu2 = nn.SELU()
        self.rms_norm2 = RMSNorm(32)  # Assuming RMSNorm is a custom layer with input size 32

    def forward(self, x):
        x = self.alpha_dropout1(x)
        x = self.silu1(x)
        x = self.pixel_unshuffle1(x)
        x = self.fold1(x)
        x = self.selu1(x)
        x = self.rms_norm1(x)
        x = self.alpha_dropout2(x)
        x = self.silu2(x)
        x = self.pixel_unshuffle2(x)
        x = self.fold2(x)
        x = self.selu2(x)
        x = self.rms_norm2(x)
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()  # Assuming input is a 3-channel image of size 64x64
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []


# Assuming RMSNorm is a custom layer, here is a simple implementation
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-8):
        super().__init__()
        self.scale = dim ** -0.5
        self.eps = eps
        self.g = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        norm = torch.norm(x, p=2, dim=-1, keepdim=True) * self.scale
        return x / norm.clamp(min=self.eps) * self.g
