
# This is a random torch model generated by the following modules: ['ReLU6', 'RNN', 'FractionalMaxPool2d', 'ModuleDict', 'GaussianNLLLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.relu6 = nn.ReLU6()
        self.rnn = nn.RNN(input_size=64, hidden_size=128, num_layers=2, batch_first=True)
        self.fractional_max_pool = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))
        self.module_dict = nn.ModuleDict({
            'fc1': nn.Linear(128, 64),
            'fc2': nn.Linear(64, 32)
        })
        self.gaussian_nll_loss = nn.GaussianNLLLoss()

    def forward(self, x):
        # Apply ReLU6 activation
        x = self.relu6(x)
        
        # Reshape for RNN input
        batch_size, channels, height, width = x.shape
        x = x.view(batch_size, channels * height, width)
        
        # Apply RNN
        x, _ = self.rnn(x)
        
        # Reshape back to 4D tensor for FractionalMaxPool2d
        x = x.view(batch_size, channels, height, width)
        
        # Apply FractionalMaxPool2d
        x = self.fractional_max_pool(x)
        
        # Flatten for fully connected layers
        x = x.view(batch_size, -1)
        
        # Apply ModuleDict layers
        x = self.module_dict['fc1'](x)
        x = self.module_dict['fc2'](x)
        
        # GaussianNLLLoss requires a target, so we return the output and a dummy target
        # For the purpose of this model, we assume the target is the same as the output
        target = x.detach().clone()
        return x, target

    def compute_loss(self, output, target):
        # Compute GaussianNLLLoss
        return self.gaussian_nll_loss(output, target, torch.ones_like(output))


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 28, 28).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
