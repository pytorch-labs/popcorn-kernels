
# This is a random torch model generated by the following modules: ['LazyConvTranspose1d', 'UpsamplingBilinear2d', 'ZeroPad3d', 'FeatureAlphaDropout', 'RNN', 'Conv2d', 'Hardswish']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv_transpose1d = nn.LazyConvTranspose1d(out_channels=32, kernel_size=3, stride=2)
        self.upsampling_bilinear2d = nn.UpsamplingBilinear2d(scale_factor=2)
        self.zero_pad3d = nn.ZeroPad3d(padding=1)
        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)
        self.rnn = nn.RNN(input_size=32, hidden_size=64, num_layers=2, batch_first=True)
        self.conv2d = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.hardswish = nn.Hardswish()

    def forward(self, x):
        # Assuming input is of shape (batch_size, channels, height, width)
        x = x.unsqueeze(2)  # Add a dimension to make it 5D for ZeroPad3d
        x = self.zero_pad3d(x)
        x = x.squeeze(2)  # Remove the added dimension
        
        # Reshape for ConvTranspose1d
        x = x.view(x.size(0), x.size(1), -1)  # Flatten height and width
        x = self.conv_transpose1d(x)
        
        # Reshape back to 4D for Conv2d
        x = x.view(x.size(0), x.size(1), int(x.size(2)**0.5), int(x.size(2)**0.5))
        x = self.conv2d(x)
        x = self.hardswish(x)
        
        # Upsample
        x = self.upsampling_bilinear2d(x)
        
        # Apply FeatureAlphaDropout
        x = self.feature_alpha_dropout(x)
        
        # Reshape for RNN
        x = x.view(x.size(0), x.size(1), -1).transpose(1, 2)  # (batch_size, seq_len, features)
        x, _ = self.rnn(x)
        
        # Reshape back to 4D
        x = x.transpose(1, 2).view(x.size(0), x.size(2), int(x.size(1)**0.5), int(x.size(1)**0.5))
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape (batch_size, channels, height, width)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

