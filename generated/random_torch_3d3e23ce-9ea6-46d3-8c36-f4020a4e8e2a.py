
# This is a random torch model generated by the following modules: ['Embedding', 'LazyConvTranspose1d', 'ReplicationPad3d', 'Tanh', 'Unflatten', 'CELU']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Assuming vocab size of 1000
        self.conv_transpose1 = nn.LazyConvTranspose1d(out_channels=64, kernel_size=3, stride=2)
        self.conv_transpose2 = nn.LazyConvTranspose1d(out_channels=32, kernel_size=3, stride=2)
        self.replication_pad = nn.ReplicationPad3d(padding=(1, 1, 1, 1, 1, 1))
        self.tanh = nn.Tanh()
        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 8, 8))
        self.celu = nn.CELU()

    def forward(self, x):
        # Assuming x is a tensor of indices for embedding
        x = self.embedding(x)
        
        # Reshape for ConvTranspose1d
        x = x.permute(0, 2, 1)  # (batch, channels, sequence_length)
        
        # Apply ConvTranspose1d layers
        x = self.conv_transpose1(x)
        x = self.conv_transpose2(x)
        
        # Reshape for ReplicationPad3d
        x = x.unsqueeze(-1).unsqueeze(-1)  # Add two extra dimensions
        x = self.replication_pad(x)
        
        # Apply Tanh activation
        x = self.tanh(x)
        
        # Unflatten the tensor
        x = self.unflatten(x)
        
        # Apply CELU activation
        x = self.celu(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (1, 10)).cuda()  # (batch_size, sequence_length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

