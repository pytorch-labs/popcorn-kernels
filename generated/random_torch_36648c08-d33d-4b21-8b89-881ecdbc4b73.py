
# This is a random torch model generated by the following modules: ['UpsamplingNearest2d', 'ELU', 'LogSigmoid', 'CTCLoss', 'Embedding', 'Dropout1d', 'AdaptiveAvgPool1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.dropout1d = nn.Dropout1d(p=0.5)  # Dropout1d layer
        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(64)  # AdaptiveAvgPool1d layer
        self.upsampling_nearest2d = nn.UpsamplingNearest2d(scale_factor=2)  # UpsamplingNearest2d layer
        self.elu = nn.ELU()  # ELU activation
        self.log_sigmoid = nn.LogSigmoid()  # LogSigmoid activation
        self.ctc_loss = nn.CTCLoss()  # CTCLoss layer

    def forward(self, x):
        # Assuming x is a 1D tensor of indices for the embedding layer
        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)
        x = x.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_len)
        x = self.dropout1d(x)  # Apply Dropout1d
        x = self.adaptive_avg_pool1d(x)  # (batch_size, embedding_dim, 64)
        
        # Reshape for UpsamplingNearest2d
        x = x.unsqueeze(2)  # (batch_size, embedding_dim, 1, 64)
        x = self.upsampling_nearest2d(x)  # (batch_size, embedding_dim, 2, 128)
        
        # Flatten for ELU and LogSigmoid
        x = x.view(x.size(0), -1)  # (batch_size, embedding_dim * 2 * 128)
        x = self.elu(x)  # Apply ELU
        x = self.log_sigmoid(x)  # Apply LogSigmoid
        
        # For CTCLoss, we need log probabilities and target sequences
        # Assuming x is the log probabilities and we have a target sequence
        log_probs = x.unsqueeze(0)  # (1, batch_size, num_classes)
        targets = torch.randint(1, 1000, (x.size(0), 10), dtype=torch.long)  # Random target sequence
        input_lengths = torch.full((log_probs.size(1),), log_probs.size(0), dtype=torch.long)
        target_lengths = torch.randint(1, 10, (log_probs.size(1),), dtype=torch.long)
        
        loss = self.ctc_loss(log_probs, targets, input_lengths, target_lengths)
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (32, 10)).cuda()  # (batch_size, seq_len)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

