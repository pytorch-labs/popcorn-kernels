
# This is a random torch model generated by the following modules: ['PixelUnshuffle', 'LazyBatchNorm2d', 'Tanhshrink', 'Transformer', 'AvgPool3d', 'Conv3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.pixel_unshuffle = nn.PixelUnshuffle(downscale_factor=2)
        self.bn1 = nn.LazyBatchNorm2d()
        self.tanhshrink = nn.Tanhshrink()
        self.transformer = nn.Transformer(d_model=64, nhead=8, num_encoder_layers=3, num_decoder_layers=3)
        self.avg_pool3d = nn.AvgPool3d(kernel_size=2)
        self.conv3d = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.LazyBatchNorm2d()
        self.conv3d_2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.LazyBatchNorm2d()

    def forward(self, x):
        # Assuming input is 4D (batch, channels, height, width)
        x = self.pixel_unshuffle(x)  # Output shape: (batch, channels * 4, height / 2, width / 2)
        x = self.bn1(x)  # Output shape: (batch, channels * 4, height / 2, width / 2)
        x = self.tanhshrink(x)  # Output shape: (batch, channels * 4, height / 2, width / 2)
        
        # Reshape for 3D operations
        x = x.unsqueeze(2)  # Add a depth dimension: (batch, channels * 4, 1, height / 2, width / 2)
        x = self.conv3d(x)  # Output shape: (batch, 64, 1, height / 2, width / 2)
        x = self.avg_pool3d(x)  # Output shape: (batch, 64, 1, height / 4, width / 4)
        x = self.conv3d_2(x)  # Output shape: (batch, 128, 1, height / 4, width / 4)
        
        # Remove depth dimension and reshape for Transformer
        x = x.squeeze(2)  # Output shape: (batch, 128, height / 4, width / 4)
        x = x.view(x.size(0), x.size(1), -1)  # Output shape: (batch, 128, (height / 4) * (width / 4))
        x = x.permute(2, 0, 1)  # Transformer expects (sequence_length, batch, feature_dim)
        
        # Apply Transformer
        x = self.transformer(x, x)  # Output shape: (sequence_length, batch, feature_dim)
        
        # Reshape back to 4D
        x = x.permute(1, 2, 0)  # Output shape: (batch, feature_dim, sequence_length)
        x = x.view(x.size(0), x.size(1), int(x.size(2) ** 0.5), int(x.size(2) ** 0.5))  # Output shape: (batch, 128, height / 4, width / 4)
        
        # Final batch norm
        x = self.bn3(x)  # Output shape: (batch, 128, height / 4, width / 4)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()  # Example input: batch of 1, 3 channels, 64x64 image
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
