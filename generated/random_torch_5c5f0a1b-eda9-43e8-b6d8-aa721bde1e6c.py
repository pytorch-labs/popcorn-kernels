
# This is a random torch model generated by the following modules: ['RNN', 'EmbeddingBag', 'TransformerDecoderLayer', 'PixelShuffle', 'MultiMarginLoss', 'AdaptiveAvgPool3d', 'AdaptiveLogSoftmaxWithLoss', 'Softmin', 'PoissonNLLLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding_bag = nn.EmbeddingBag(1000, 64, mode='mean')
        self.rnn = nn.RNN(64, 128, num_layers=2, batch_first=True)
        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=128, nhead=8)
        self.pixel_shuffle = nn.PixelShuffle(2)
        self.adaptive_avg_pool3d = nn.AdaptiveAvgPool3d((8, 8, 8))
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(128, 1000, [100, 300])
        self.softmin = nn.Softmin(dim=1)
        self.multi_margin_loss = nn.MultiMarginLoss()
        self.poisson_nll_loss = nn.PoissonNLLLoss()

    def forward(self, x):
        # Assume x is a tensor of arbitrary shape
        # Reshape x to fit the EmbeddingBag input
        x = x.long()  # Convert to long for embedding
        x = x.view(-1, x.size(-1))  # Flatten to (batch_size, seq_len)
        x = self.embedding_bag(x)  # (batch_size, 64)
        
        # Reshape for RNN
        x = x.unsqueeze(1)  # (batch_size, 1, 64)
        x, _ = self.rnn(x)  # (batch_size, 1, 128)
        
        # Reshape for TransformerDecoderLayer
        x = x.transpose(0, 1)  # (1, batch_size, 128)
        x = self.transformer_decoder_layer(x, x)  # (1, batch_size, 128)
        x = x.transpose(0, 1)  # (batch_size, 1, 128)
        
        # Reshape for PixelShuffle
        x = x.view(-1, 32, 4, 4)  # (batch_size, 32, 4, 4)
        x = self.pixel_shuffle(x)  # (batch_size, 8, 8, 8)
        
        # Reshape for AdaptiveAvgPool3d
        x = x.unsqueeze(1)  # (batch_size, 1, 8, 8, 8)
        x = self.adaptive_avg_pool3d(x)  # (batch_size, 1, 8, 8, 8)
        
        # Reshape for AdaptiveLogSoftmaxWithLoss
        x = x.view(-1, 128)  # (batch_size * 8 * 8 * 8, 128)
        x = self.adaptive_log_softmax.log_prob(x)  # (batch_size * 8 * 8 * 8, 1000)
        
        # Apply Softmin
        x = self.softmin(x)  # (batch_size * 8 * 8 * 8, 1000)
        
        # Reshape for MultiMarginLoss
        x = x.view(-1, 1000)  # (batch_size, 8 * 8 * 8 * 1000)
        target = torch.randint(0, 1000, (x.size(0),)).to(x.device)
        loss = self.multi_margin_loss(x, target)
        
        # Reshape for PoissonNLLLoss
        x = x.view(-1)  # (batch_size * 8 * 8 * 8 * 1000)
        target = torch.rand_like(x) * 10  # Random target for PoissonNLLLoss
        loss += self.poisson_nll_loss(x, target)
        
        return loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # Example input for EmbeddingBag
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

