
# This is a random torch model generated by the following modules: ['GRUCell', 'LazyConvTranspose3d', 'AvgPool3d', 'LeakyReLU', 'Tanh', 'LogSigmoid', 'LSTMCell', 'LPPool2d', 'LayerNorm', 'RNNCellBase']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.gru_cell = nn.GRUCell(input_size=128, hidden_size=256)
        self.lazy_conv_transpose3d = nn.LazyConvTranspose3d(out_channels=64, kernel_size=3, stride=2)
        self.avg_pool3d = nn.AvgPool3d(kernel_size=2, stride=2)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.1)
        self.tanh = nn.Tanh()
        self.log_sigmoid = nn.LogSigmoid()
        self.lstm_cell = nn.LSTMCell(input_size=256, hidden_size=128)
        self.lp_pool2d = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)
        self.layer_norm = nn.LayerNorm(128)
        self.rnn_cell_base = nn.RNNCellBase(input_size=128, hidden_size=64)

    def forward(self, x):
        # Assuming x is of shape (batch_size, channels, depth, height, width)
        x = self.lazy_conv_transpose3d(x)  # Apply LazyConvTranspose3d
        x = self.avg_pool3d(x)  # Apply AvgPool3d
        x = self.leaky_relu(x)  # Apply LeakyReLU
        x = self.tanh(x)  # Apply Tanh
        x = self.log_sigmoid(x)  # Apply LogSigmoid
        
        # Reshape for RNN cells
        batch_size, channels, depth, height, width = x.shape
        x = x.view(batch_size, -1)  # Flatten to (batch_size, features)
        
        # Apply GRUCell
        hx_gru = torch.zeros(batch_size, 256).to(x.device)
        hx_gru = self.gru_cell(x, hx_gru)
        
        # Apply LSTMCell
        hx_lstm = torch.zeros(batch_size, 128).to(x.device)
        cx_lstm = torch.zeros(batch_size, 128).to(x.device)
        hx_lstm, cx_lstm = self.lstm_cell(hx_gru, (hx_lstm, cx_lstm))
        
        # Apply RNNCellBase
        hx_rnn = torch.zeros(batch_size, 64).to(x.device)
        hx_rnn = self.rnn_cell_base(hx_lstm, hx_rnn)
        
        # Apply LayerNorm
        x = self.layer_norm(hx_rnn)
        
        # Reshape back to 3D for LPPool2d
        x = x.view(batch_size, 1, 8, 8)  # Arbitrary reshape for LPPool2d
        x = self.lp_pool2d(x)  # Apply LPPool2d
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
