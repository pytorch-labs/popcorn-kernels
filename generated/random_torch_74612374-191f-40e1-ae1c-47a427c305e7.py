
# This is a random torch model generated by the following modules: ['MaxUnpool1d', 'Hardsigmoid', 'KLDivLoss', 'GRUCell', 'CircularPad3d', 'LeakyReLU', 'MaxPool3d', 'Softmax2d', 'ReLU6']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)
        self.hardsigmoid = nn.Hardsigmoid()
        self.gru_cell = nn.GRUCell(input_size=64, hidden_size=128)
        self.circular_pad3d = nn.CircularPad3d(padding=1)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)
        self.max_pool3d = nn.MaxPool3d(kernel_size=2, stride=2)
        self.softmax2d = nn.Softmax2d()
        self.relu6 = nn.ReLU6()
        self.kldivloss = nn.KLDivLoss(reduction='batchmean')

    def forward(self, x):
        # Assuming x is of shape (batch_size, channels, length)
        x = x.unsqueeze(1)  # Add a dimension to make it 3D for MaxUnpool1d
        x, indices = F.max_pool1d(x, kernel_size=2, stride=2, return_indices=True)
        x = self.max_unpool1d(x, indices)
        x = x.squeeze(1)  # Remove the added dimension

        # Apply Hardsigmoid
        x = self.hardsigmoid(x)

        # Reshape for GRUCell
        x = x.view(x.size(0), -1)  # Flatten the input for GRUCell
        hx = torch.zeros(x.size(0), 128).to(x.device)  # Initialize hidden state
        x = self.gru_cell(x, hx)

        # Reshape for 3D operations
        x = x.view(x.size(0), 1, 8, 8, 8)  # Reshape to 3D

        # Apply CircularPad3d
        x = self.circular_pad3d(x)

        # Apply LeakyReLU
        x = self.leaky_relu(x)

        # Apply MaxPool3d
        x = self.max_pool3d(x)

        # Reshape for Softmax2d
        x = x.view(x.size(0), 1, x.size(2), x.size(3))  # Reshape to 2D
        x = self.softmax2d(x)

        # Apply ReLU6
        x = self.relu6(x)

        # Compute KLDivLoss (assuming we have a target distribution)
        target = torch.softmax(torch.randn_like(x), dim=1)
        loss = self.kldivloss(F.log_softmax(x, dim=1), target)

        return x, loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 64, 64).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
