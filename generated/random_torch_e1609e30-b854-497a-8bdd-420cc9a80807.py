
# This is a random torch model generated by the following modules: ['LayerNorm', 'ZeroPad3d', 'RReLU', 'PReLU', 'Softmin', 'MultiLabelMarginLoss', 'ReLU', 'CELU', 'RMSNorm', 'MaxUnpool1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.layer_norm = nn.LayerNorm(128)
        self.zero_pad3d = nn.ZeroPad3d(1)
        self.rrelu = nn.RReLU()
        self.prelu = nn.PReLU()
        self.softmin = nn.Softmin(dim=1)
        self.multi_label_margin_loss = nn.MultiLabelMarginLoss()
        self.relu = nn.ReLU()
        self.celu = nn.CELU()
        self.rms_norm = nn.LayerNorm(128)  # Using LayerNorm as a placeholder for RMSNorm
        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, depth, height, width)
        x = self.zero_pad3d(x)  # ZeroPad3d
        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions
        x = self.layer_norm(x)  # LayerNorm
        x = self.rrelu(x)  # RReLU
        x = self.prelu(x)  # PReLU
        x = self.softmin(x)  # Softmin
        x = self.relu(x)  # ReLU
        x = self.celu(x)  # CELU
        x = self.rms_norm(x)  # RMSNorm (using LayerNorm as placeholder)
        
        # Reshape for MaxUnpool1d
        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, length)
        x, indices = F.max_pool1d(x, kernel_size=2, stride=2, return_indices=True)
        x = self.max_unpool1d(x, indices)  # MaxUnpool1d
        
        # MultiLabelMarginLoss is not used in forward pass as it is a loss function
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

