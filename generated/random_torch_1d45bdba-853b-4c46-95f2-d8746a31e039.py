
# This is a random torch model generated by the following modules: ['ConvTranspose2d', 'GRUCell', 'MaxUnpool3d', 'InstanceNorm1d', 'CircularPad1d', 'CosineEmbeddingLoss', 'MSELoss', 'Dropout1d', 'Conv2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.conv_transpose1 = nn.ConvTranspose2d(16, 32, kernel_size=2, stride=2)
        self.dropout1d = nn.Dropout1d(p=0.5)
        self.instance_norm1d = nn.InstanceNorm1d(32)
        self.gru_cell = nn.GRUCell(32, 64)
        self.circular_pad1d = nn.ConstantPad1d(2, 0)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.max_unpool3d = nn.MaxUnpool3d(kernel_size=2, stride=2)
        self.mse_loss = nn.MSELoss()
        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()

    def forward(self, x):
        # Initial convolution
        x = F.relu(self.conv1(x))
        
        # Transpose convolution
        x = self.conv_transpose1(x)
        
        # Reshape for 1D operations
        batch_size, channels, height, width = x.shape
        x = x.view(batch_size, channels, -1)  # Reshape to (batch_size, channels, height*width)
        
        # Dropout1d
        x = self.dropout1d(x)
        
        # InstanceNorm1d
        x = self.instance_norm1d(x)
        
        # Reshape for GRUCell
        x = x.permute(0, 2, 1)  # Reshape to (batch_size, height*width, channels)
        x = x.reshape(-1, channels)  # Reshape to (batch_size * height * width, channels)
        
        # GRUCell
        hx = torch.zeros(x.size(0), 64).to(x.device)
        x = self.gru_cell(x, hx)
        
        # Reshape back to 2D
        x = x.view(batch_size, height, width, -1)
        x = x.permute(0, 3, 1, 2)  # Reshape to (batch_size, 64, height, width)
        
        # CircularPad1d (applied to 1D slices)
        x = x.permute(0, 2, 3, 1)  # Reshape to (batch_size, height, width, 64)
        x = x.reshape(batch_size * height * width, -1)
        x = self.circular_pad1d(x)
        x = x.view(batch_size, height, width, -1)
        x = x.permute(0, 3, 1, 2)  # Reshape back to (batch_size, 64, height, width)
        
        # Second convolution
        x = F.relu(self.conv2(x))
        
        # MaxUnpool3d (requires indices from a previous MaxPool3d)
        # Since we don't have a MaxPool3d, we'll just pass through
        x = x.unsqueeze(2)  # Add a dummy dimension for 3D
        x = self.max_unpool3d(x, torch.zeros_like(x).long())  # Dummy indices
        
        # Remove the dummy dimension
        x = x.squeeze(2)
        
        # Compute MSE loss (dummy target)
        target = torch.zeros_like(x)
        mse_loss = self.mse_loss(x, target)
        
        # Compute CosineEmbeddingLoss (dummy inputs)
        input1 = torch.randn(10, 128).to(x.device)
        input2 = torch.randn(10, 128).to(x.device)
        target_cosine = torch.ones(10).to(x.device)
        cosine_loss = self.cosine_embedding_loss(input1, input2, target_cosine)
        
        # Return the final output and losses
        return x, mse_loss, cosine_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

