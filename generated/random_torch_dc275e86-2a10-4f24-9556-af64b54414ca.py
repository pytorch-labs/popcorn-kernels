
# This is a random torch model generated by the following modules: ['RNNCell', 'UpsamplingBilinear2d', 'CTCLoss', 'Hardsigmoid', 'GRU', 'ReplicationPad2d', 'LazyInstanceNorm1d', 'PairwiseDistance']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.rnn_cell = nn.RNNCell(input_size=128, hidden_size=256)
        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)
        self.hardsigmoid = nn.Hardsigmoid()
        self.gru = nn.GRU(input_size=256, hidden_size=128, num_layers=2, batch_first=True)
        self.replication_pad = nn.ReplicationPad2d(padding=2)
        self.instance_norm = nn.LazyInstanceNorm1d()
        self.pairwise_distance = nn.PairwiseDistance(p=2)
        self.ctc_loss = nn.CTCLoss()

    def forward(self, x):
        # Assuming x is a 4D tensor (batch, channels, height, width)
        batch_size, channels, height, width = x.shape
        
        # Reshape x to fit RNNCell input
        x = x.view(batch_size, -1, 128)  # Reshape to (batch_size, seq_len, input_size)
        hx = torch.zeros(batch_size, 256).to(x.device)  # Initialize hidden state for RNNCell
        
        # Apply RNNCell
        for i in range(x.size(1)):
            hx = self.rnn_cell(x[:, i, :], hx)
        
        # Reshape hx to fit UpsamplingBilinear2d input
        hx = hx.view(batch_size, 16, 16)  # Reshape to (batch_size, 16, 16)
        hx = hx.unsqueeze(1)  # Add channel dimension
        
        # Apply UpsamplingBilinear2d
        hx = self.upsample(hx)
        
        # Apply ReplicationPad2d
        hx = self.replication_pad(hx)
        
        # Reshape hx to fit GRU input
        hx = hx.view(batch_size, -1, 256)  # Reshape to (batch_size, seq_len, input_size)
        
        # Apply GRU
        hx, _ = self.gru(hx)
        
        # Apply LazyInstanceNorm1d
        hx = hx.permute(0, 2, 1)  # Permute to (batch_size, features, seq_len)
        hx = self.instance_norm(hx)
        hx = hx.permute(0, 2, 1)  # Permute back to (batch_size, seq_len, features)
        
        # Apply Hardsigmoid
        hx = self.hardsigmoid(hx)
        
        # Reshape hx to fit PairwiseDistance input
        hx = hx.view(batch_size, -1)  # Reshape to (batch_size, features)
        
        # Apply PairwiseDistance
        hx = self.pairwise_distance(hx, hx)
        
        # Reshape hx to fit CTCLoss input
        hx = hx.unsqueeze(0)  # Add batch dimension
        
        # Apply CTCLoss (assuming dummy target for demonstration)
        target = torch.randint(1, 10, (batch_size, 20), dtype=torch.long).to(x.device)
        input_lengths = torch.full((batch_size,), hx.size(1), dtype=torch.long).to(x.device)
        target_lengths = torch.randint(10, 20, (batch_size,), dtype=torch.long).to(x.device)
        loss = self.ctc_loss(hx.log_softmax(2), target, input_lengths, target_lengths)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

