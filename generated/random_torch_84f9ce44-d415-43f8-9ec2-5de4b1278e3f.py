
# This is a random torch model generated by the following modules: ['LazyConv1d', 'TransformerDecoder', 'ModuleList', 'GRUCell']
import torch
import torch.nn as nn


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1 = nn.LazyConv1d(out_channels=32, kernel_size=3)
        self.conv2 = nn.LazyConv1d(out_channels=64, kernel_size=3)
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=64, nhead=8), num_layers=3
        )
        self.gru_cells = nn.ModuleList([nn.GRUCell(input_size=64, hidden_size=64) for _ in range(2)])
        self.fc = nn.LazyLinear(out_features=10)

    def forward(self, x):
        # Assume input x is of shape (batch_size, sequence_length, features)
        x = x.permute(0, 2, 1)  # Reshape to (batch_size, features, sequence_length) for Conv1d
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.permute(2, 0, 1)  # Reshape to (sequence_length, batch_size, features) for TransformerDecoder

        # Create a dummy memory for the TransformerDecoder
        memory = torch.zeros_like(x)
        x = self.transformer_decoder(x, memory)

        # Process through GRUCells
        hx = torch.zeros(x.size(1), 64).to(x.device)  # Initialize hidden state
        for gru_cell in self.gru_cells:
            hx = gru_cell(x[-1], hx)  # Use the last output of the transformer as input to GRUCell

        # Final fully connected layer
        x = self.fc(hx)
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 100, 32).cuda()  # (batch_size, sequence_length, features)
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
