
# This is a random torch model generated by the following modules: ['GroupNorm', 'Hardshrink', 'PReLU', 'ELU', 'LazyBatchNorm3d', 'Dropout1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.group_norm1 = nn.GroupNorm(2, 4)  # Assuming 4 channels for GroupNorm
        self.hardshrink = nn.Hardshrink()
        self.prelu = nn.PReLU()
        self.elu = nn.ELU()
        self.lazy_batch_norm3d = nn.LazyBatchNorm3d()
        self.dropout1d = nn.Dropout1d(p=0.5)
        self.group_norm2 = nn.GroupNorm(2, 4)  # Repeating GroupNorm
        self.prelu2 = nn.PReLU()  # Repeating PReLU

    def forward(self, x):
        # Assuming input is of shape (batch_size, channels, *dims)
        x = self.group_norm1(x)
        x = self.hardshrink(x)
        x = self.prelu(x)
        x = self.elu(x)
        
        # Reshape to 5D for LazyBatchNorm3d
        x = x.unsqueeze(-1).unsqueeze(-1)  # Adding two extra dimensions
        x = self.lazy_batch_norm3d(x)
        
        # Reshape back to 3D for Dropout1d
        x = x.squeeze(-1).squeeze(-1)
        x = self.dropout1d(x)
        
        x = self.group_norm2(x)
        x = self.prelu2(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 4, 32, 32).cuda()  # Assuming 4 channels and 32x32 spatial dimensions
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
