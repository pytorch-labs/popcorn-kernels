
# This is a random torch model generated by the following modules: ['ParameterList', 'BatchNorm2d', 'ReLU', 'PoissonNLLLoss', 'BCEWithLogitsLoss', 'KLDivLoss', 'LazyInstanceNorm3d', 'SyncBatchNorm']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(10)) for _ in range(3)])
        self.bn1 = nn.BatchNorm2d(10)
        self.bn2 = nn.BatchNorm2d(20)
        self.relu = nn.ReLU()
        self.lazy_instance_norm = nn.LazyInstanceNorm3d()
        self.sync_bn = nn.SyncBatchNorm(30)
        self.poisson_loss = nn.PoissonNLLLoss()
        self.bce_loss = nn.BCEWithLogitsLoss()
        self.kl_loss = nn.KLDivLoss()

    def forward(self, x):
        # Apply LazyInstanceNorm3d
        x = self.lazy_instance_norm(x)
        
        # Reshape for BatchNorm2d
        x = x.view(-1, 10, 8, 8)
        
        # Apply BatchNorm2d and ReLU
        x = self.relu(self.bn1(x))
        
        # Reshape for SyncBatchNorm
        x = x.view(-1, 20, 4, 4)
        
        # Apply SyncBatchNorm
        x = self.sync_bn(x)
        
        # Reshape for final output
        x = x.view(-1, 30)
        
        # Compute losses (just for demonstration, not typically done in forward)
        target = torch.randn_like(x)
        poisson_loss = self.poisson_loss(x, target)
        bce_loss = self.bce_loss(x, target)
        kl_loss = self.kl_loss(F.log_softmax(x, dim=1), F.softmax(target, dim=1))
        
        # Return the losses and the output
        return x, poisson_loss, bce_loss, kl_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 8, 8).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
