
# This is a random torch model generated by the following modules: ['GaussianNLLLoss', 'MultiLabelMarginLoss', 'LSTM']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self, input_size=128, hidden_size=64, num_layers=2, output_size=10) -> None:
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.gaussian_nll_loss = nn.GaussianNLLLoss()
        self.multi_label_margin_loss = nn.MultiLabelMarginLoss()

    def forward(self, x, target=None):
        # Assuming x is of shape (batch_size, seq_len, input_size)
        batch_size, seq_len, _ = x.size()
        
        # Pass through LSTM
        lstm_out, _ = self.lstm(x)
        
        # Take the output of the last time step
        lstm_out = lstm_out[:, -1, :]
        
        # Pass through a fully connected layer
        output = self.fc(lstm_out)
        
        # If target is provided, compute losses
        if target is not None:
            # Assuming target is of shape (batch_size, output_size) for MultiLabelMarginLoss
            # and (batch_size, output_size) for GaussianNLLLoss
            # For GaussianNLLLoss, we need to provide a variance tensor
            variance = torch.ones_like(output)
            gaussian_loss = self.gaussian_nll_loss(output, target, variance)
            multi_label_loss = self.multi_label_margin_loss(output, target)
            return output, gaussian_loss, multi_label_loss
        
        return output


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 128).cuda()  # (batch_size, seq_len, input_size)
    target = torch.randn(1, 10).cuda()  # (batch_size, output_size)
    return [x, target]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

