
# This is a random torch model generated by the following modules: ['RMSNorm', 'PixelShuffle', 'LazyBatchNorm3d', 'InstanceNorm3d', 'Flatten', 'Linear', 'MultiMarginLoss', 'LazyConv3d', 'Conv1d', 'Softmin']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.rms_norm = nn.LayerNorm(128)  # RMSNorm is not directly available in PyTorch, using LayerNorm as a substitute
        self.pixel_shuffle = nn.PixelShuffle(2)
        self.lazy_batch_norm3d = nn.LazyBatchNorm3d()
        self.instance_norm3d = nn.InstanceNorm3d(64)
        self.flatten = nn.Flatten()
        self.linear1 = nn.Linear(128, 64)
        self.linear2 = nn.Linear(64, 10)
        self.multi_margin_loss = nn.MultiMarginLoss()
        self.lazy_conv3d = nn.LazyConv3d(64, kernel_size=3)
        self.conv1d = nn.Conv1d(64, 128, kernel_size=3)
        self.softmin = nn.Softmin(dim=1)

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, depth, height, width)
        x = self.lazy_conv3d(x)  # Shape: (batch_size, 64, depth, height, width)
        x = self.lazy_batch_norm3d(x)  # Shape: (batch_size, 64, depth, height, width)
        x = self.instance_norm3d(x)  # Shape: (batch_size, 64, depth, height, width)
        
        # Reshape for Conv1d
        x = x.view(x.size(0), x.size(1), -1)  # Shape: (batch_size, 64, depth * height * width)
        x = self.conv1d(x)  # Shape: (batch_size, 128, depth * height * width - 2)
        
        # Reshape for RMSNorm
        x = x.view(x.size(0), -1)  # Shape: (batch_size, 128 * (depth * height * width - 2))
        x = self.rms_norm(x)  # Shape: (batch_size, 128 * (depth * height * width - 2))
        
        # Reshape for PixelShuffle
        x = x.view(x.size(0), 128, -1)  # Shape: (batch_size, 128, depth * height * width - 2)
        x = self.pixel_shuffle(x)  # Shape: (batch_size, 32, depth * height * width - 2 * 2)
        
        # Flatten for Linear layers
        x = self.flatten(x)  # Shape: (batch_size, 32 * (depth * height * width - 2 * 2))
        x = self.linear1(x)  # Shape: (batch_size, 64)
        x = self.linear2(x)  # Shape: (batch_size, 10)
        
        # Apply Softmin
        x = self.softmin(x)  # Shape: (batch_size, 10)
        
        # MultiMarginLoss requires target labels, so it's not applied in the forward pass
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32, 32).cuda()  # Example input shape: (batch_size, channels, depth, height, width)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
