
# This is a random torch model generated by the following modules: ['MSELoss', 'FeatureAlphaDropout', 'Dropout', 'CrossEntropyLoss', 'GLU', 'Embedding', 'Softsign', 'UpsamplingNearest2d', 'ModuleList', 'LSTMCell']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.lstm_cell1 = nn.LSTMCell(128, 64)    # First LSTMCell
        self.lstm_cell2 = nn.LSTMCell(64, 32)     # Second LSTMCell
        self.glu = nn.GLU(dim=1)                  # GLU layer
        self.upsample = nn.UpsamplingNearest2d(scale_factor=2)  # Upsampling layer
        self.dropout = nn.Dropout(0.5)            # Dropout layer
        self.feature_alpha_dropout = nn.FeatureAlphaDropout()  # FeatureAlphaDropout layer
        self.softsign = nn.Softsign()             # Softsign activation
        self.module_list = nn.ModuleList([nn.Linear(32, 16), nn.Linear(16, 8)])  # ModuleList with Linear layers
        self.mse_loss = nn.MSELoss()              # MSELoss
        self.cross_entropy_loss = nn.CrossEntropyLoss()  # CrossEntropyLoss

    def forward(self, x):
        # Assume input x is a tensor of shape (batch_size, sequence_length)
        x = self.embedding(x)  # (batch_size, sequence_length, 128)
        x = x.mean(dim=1)      # (batch_size, 128)
        
        # Initialize hidden states for LSTMCell
        hx1 = torch.zeros(x.size(0), 64).to(x.device)
        cx1 = torch.zeros(x.size(0), 64).to(x.device)
        hx2 = torch.zeros(x.size(0), 32).to(x.device)
        cx2 = torch.zeros(x.size(0), 32).to(x.device)
        
        # Pass through LSTMCells
        hx1, cx1 = self.lstm_cell1(x, (hx1, cx1))
        hx2, cx2 = self.lstm_cell2(hx1, (hx2, cx2))
        
        # Reshape for GLU
        x = hx2.unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, 32)
        x = self.glu(x)                    # (batch_size, 1, 1, 16)
        
        # Upsample
        x = self.upsample(x)               # (batch_size, 1, 2, 16)
        
        # Apply dropout and feature alpha dropout
        x = self.dropout(x)
        x = self.feature_alpha_dropout(x)
        
        # Apply Softsign activation
        x = self.softsign(x)               # (batch_size, 1, 2, 16)
        
        # Flatten and pass through ModuleList
        x = x.view(x.size(0), -1)          # (batch_size, 32)
        for layer in self.module_list:
            x = layer(x)
        
        # Compute losses (dummy targets for illustration)
        target_mse = torch.randn_like(x)
        target_ce = torch.randint(0, 8, (x.size(0),)).to(x.device)
        
        mse_loss = self.mse_loss(x, target_mse)
        ce_loss = self.cross_entropy_loss(x, target_ce)
        
        return x, mse_loss, ce_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # (batch_size, sequence_length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
