
# This is a random torch model generated by the following modules: ['MaxPool2d', 'ConvTranspose2d', 'CosineEmbeddingLoss', 'CircularPad2d', 'GroupNorm', 'CTCLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv_transpose1 = nn.ConvTranspose2d(1, 10, kernel_size=5, stride=1)
        self.circular_pad1 = nn.CircularPad2d(2)
        self.group_norm1 = nn.GroupNorm(2, 10)
        self.conv_transpose2 = nn.ConvTranspose2d(10, 20, kernel_size=5, stride=1)
        self.group_norm2 = nn.GroupNorm(4, 20)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()
        self.ctc_loss = nn.CTCLoss()

    def forward(self, x):
        # Assume input x is of shape (batch_size, channels, height, width)
        x = self.maxpool1(x)
        x = self.conv_transpose1(x)
        x = self.circular_pad1(x)
        x = self.group_norm1(x)
        x = self.conv_transpose2(x)
        x = self.group_norm2(x)
        x = self.maxpool2(x)
        
        # For demonstration, let's assume we have some target tensors for the loss functions
        # These would typically be provided externally in a real scenario
        target1 = torch.randint(0, 10, (x.size(0),), dtype=torch.long)
        target2 = torch.randint(0, 10, (x.size(0),), dtype=torch.long)
        
        # Compute CosineEmbeddingLoss (dummy example)
        cosine_loss = self.cosine_embedding_loss(x.view(x.size(0), -1), target1.float().unsqueeze(1), torch.ones(x.size(0)))
        
        # Compute CTC Loss (dummy example)
        log_probs = F.log_softmax(x.view(x.size(0), -1), dim=1)
        input_lengths = torch.full((x.size(0),), log_probs.size(1), dtype=torch.long)
        target_lengths = torch.randint(1, 10, (x.size(0),), dtype=torch.long)
        ctc_loss = self.ctc_loss(log_probs, target2, input_lengths, target_lengths)
        
        # Return the sum of the losses for demonstration purposes
        return cosine_loss + ctc_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
