
# This is a random torch model generated by the following modules: ['EmbeddingBag', 'CELU', 'Softmin', 'Sequential', 'Flatten', 'RNNBase', 'Upsample', 'LazyLinear']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding_bag = nn.EmbeddingBag(1000, 64, mode='mean')
        self.celu = nn.CELU()
        self.softmin = nn.Softmin(dim=1)
        self.sequential = nn.Sequential(
            nn.LazyLinear(128),
            nn.ReLU(),
            nn.LazyLinear(64),
            nn.ReLU()
        )
        self.flatten = nn.Flatten()
        self.rnn = nn.RNNBase(64, 32, num_layers=2, batch_first=True)
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.lazy_linear = nn.LazyLinear(10)

    def forward(self, x):
        # Assuming x is a 1D tensor of indices for EmbeddingBag
        x = self.embedding_bag(x)
        x = self.celu(x)
        x = self.softmin(x)
        x = self.sequential(x)
        x = self.flatten(x)
        x = x.unsqueeze(1)  # Add a sequence dimension for RNN
        x, _ = self.rnn(x)
        x = x.squeeze(1)  # Remove the sequence dimension
        x = x.unsqueeze(2).unsqueeze(3)  # Add spatial dimensions for Upsample
        x = self.upsample(x)
        x = x.squeeze(3).squeeze(2)  # Remove spatial dimensions
        x = self.lazy_linear(x)
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (32,)).cuda()  # Example input for EmbeddingBag
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
