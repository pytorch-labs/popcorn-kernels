
# This is a random torch model generated by the following modules: ['HuberLoss', 'InstanceNorm1d', 'Softmax', 'NLLLoss2d', 'AdaptiveMaxPool3d', 'MaxPool3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.instance_norm = nn.InstanceNorm1d(64)
        self.adaptive_max_pool = nn.AdaptiveMaxPool3d((8, 8, 8))
        self.max_pool = nn.MaxPool3d(kernel_size=2, stride=2)
        self.softmax = nn.Softmax(dim=1)
        self.nll_loss_2d = nn.NLLLoss2d()
        self.huber_loss = nn.HuberLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, depth, height, width)
        x = self.adaptive_max_pool(x)  # Shape: (batch_size, channels, 8, 8, 8)
        x = self.max_pool(x)  # Shape: (batch_size, channels, 4, 4, 4)
        
        # Reshape to apply InstanceNorm1d
        batch_size, channels, depth, height, width = x.shape
        x = x.view(batch_size, channels, -1)  # Shape: (batch_size, channels, depth*height*width)
        x = self.instance_norm(x)  # Shape: (batch_size, channels, depth*height*width)
        
        # Reshape back to 3D
        x = x.view(batch_size, channels, depth, height, width)  # Shape: (batch_size, channels, depth, height, width)
        
        # Apply Softmax along the channel dimension
        x = self.softmax(x)  # Shape: (batch_size, channels, depth, height, width)
        
        # Dummy target for NLLLoss2d (assuming classification task)
        target = torch.randint(0, channels, (batch_size, depth, height, width)).to(x.device)
        
        # Apply NLLLoss2d
        loss = self.nll_loss_2d(x, target)
        
        # Dummy target for HuberLoss (assuming regression task)
        target_huber = torch.randn_like(x)
        
        # Apply HuberLoss
        loss_huber = self.huber_loss(x, target_huber)
        
        # Return both losses for demonstration purposes
        return loss, loss_huber


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 16, 16, 16).cuda()  # Example input shape: (batch_size, channels, depth, height, width)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

