
# This is a random torch model generated by the following modules: ['MaxPool1d', 'TransformerEncoderLayer', 'MultiLabelSoftMarginLoss', 'CELU', 'RNN', 'RMSNorm', 'Linear', 'PixelUnshuffle', 'Softshrink']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.maxpool1d = nn.MaxPool1d(kernel_size=2, stride=2)
        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=8)
        self.celu = nn.CELU()
        self.rnn = nn.RNN(input_size=64, hidden_size=128, num_layers=2, batch_first=True)
        self.rmsnorm = nn.LayerNorm(128)
        self.linear1 = nn.Linear(128, 64)
        self.linear2 = nn.Linear(64, 32)
        self.pixel_unshuffle = nn.PixelUnshuffle(downscale_factor=2)
        self.softshrink = nn.Softshrink(lambd=0.5)
        self.loss = nn.MultiLabelSoftMarginLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, height, width)
        x = self.pixel_unshuffle(x)  # Unshuffle to increase channels
        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, height*width)
        x = self.maxpool1d(x)  # Apply MaxPool1d
        x = x.permute(0, 2, 1)  # Permute for TransformerEncoderLayer
        x = self.transformer_encoder_layer(x)  # Apply TransformerEncoderLayer
        x = x.permute(0, 2, 1)  # Permute back
        x, _ = self.rnn(x)  # Apply RNN
        x = self.rmsnorm(x)  # Apply RMSNorm
        x = self.celu(x)  # Apply CELU
        x = x.mean(dim=1)  # Global average pooling
        x = self.linear1(x)  # Apply Linear
        x = self.softshrink(x)  # Apply Softshrink
        x = self.linear2(x)  # Apply Linear
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
