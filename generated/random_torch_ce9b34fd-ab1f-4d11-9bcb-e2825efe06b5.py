
# This is a random torch model generated by the following modules: ['LSTM', 'ZeroPad3d', 'Fold', 'Linear', 'Softmin', 'Tanh', 'GroupNorm', 'GLU', 'LeakyReLU']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=2, batch_first=True)
        self.zero_pad = nn.ZeroPad3d(padding=(1, 1, 1, 1, 1, 1))
        self.fold = nn.Fold(output_size=(8, 8), kernel_size=(2, 2))
        self.linear1 = nn.Linear(64, 128)
        self.linear2 = nn.Linear(128, 64)
        self.softmin = nn.Softmin(dim=1)
        self.tanh = nn.Tanh()
        self.group_norm = nn.GroupNorm(num_groups=4, num_channels=64)
        self.glu = nn.GLU(dim=1)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.1)

    def forward(self, x):
        # Assuming input x is of shape (batch_size, sequence_length, input_size)
        batch_size, seq_len, input_size = x.size()
        
        # LSTM
        x, _ = self.lstm(x)  # Output shape: (batch_size, seq_len, hidden_size)
        
        # Reshape for ZeroPad3d
        x = x.view(batch_size, 1, seq_len, 64, 64)  # Reshape to (batch_size, 1, seq_len, 64, 64)
        x = self.zero_pad(x)  # Output shape: (batch_size, 1, seq_len + 2, 64 + 2, 64 + 2)
        
        # Reshape for Fold
        x = x.view(batch_size, -1, 64 * 64)  # Reshape to (batch_size, channels, height * width)
        x = self.fold(x)  # Output shape: (batch_size, channels, 8, 8)
        
        # Reshape for Linear
        x = x.view(batch_size, -1)  # Flatten to (batch_size, channels * 8 * 8)
        x = self.linear1(x)  # Output shape: (batch_size, 128)
        x = self.leaky_relu(x)
        
        # GroupNorm
        x = x.view(batch_size, 64, 2)  # Reshape to (batch_size, 64, 2)
        x = self.group_norm(x)  # Output shape: (batch_size, 64, 2)
        
        # GLU
        x = self.glu(x)  # Output shape: (batch_size, 32, 2)
        
        # Reshape for Linear
        x = x.view(batch_size, -1)  # Flatten to (batch_size, 32 * 2)
        x = self.linear2(x)  # Output shape: (batch_size, 64)
        
        # Tanh
        x = self.tanh(x)
        
        # Softmin
        x = self.softmin(x)  # Output shape: (batch_size, 64)
        
        return x

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 128).cuda()  # Example input: (batch_size=1, sequence_length=10, input_size=128)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
