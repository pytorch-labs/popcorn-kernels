
# This is a random torch model generated by the following modules: ['MaxPool1d', 'TransformerEncoderLayer', 'GELU']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.maxpool1 = nn.MaxPool1d(kernel_size=2)
        self.transformer_encoder1 = nn.TransformerEncoderLayer(d_model=64, nhead=8)
        self.gelu1 = nn.GELU()
        self.transformer_encoder2 = nn.TransformerEncoderLayer(d_model=64, nhead=8)
        self.maxpool2 = nn.MaxPool1d(kernel_size=2)
        self.gelu2 = nn.GELU()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, sequence_length, feature_dim)
        x = x.permute(0, 2, 1)  # Reshape to (batch_size, feature_dim, sequence_length) for MaxPool1d
        x = self.maxpool1(x)
        x = x.permute(0, 2, 1)  # Reshape back to (batch_size, sequence_length, feature_dim) for TransformerEncoderLayer
        x = self.transformer_encoder1(x)
        x = self.gelu1(x)
        x = self.transformer_encoder2(x)
        x = x.permute(0, 2, 1)  # Reshape to (batch_size, feature_dim, sequence_length) for MaxPool1d
        x = self.maxpool2(x)
        x = x.permute(0, 2, 1)  # Reshape back to (batch_size, sequence_length, feature_dim)
        x = self.gelu2(x)
        return x

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 32, 64).cuda()  # Example input shape (batch_size=1, sequence_length=32, feature_dim=64)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
