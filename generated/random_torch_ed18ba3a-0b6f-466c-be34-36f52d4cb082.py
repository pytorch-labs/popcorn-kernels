
# This is a random torch model generated by the following modules: ['KLDivLoss', 'Embedding', 'Conv1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer with 1000 vocab size and 128 embedding dim
        self.conv1 = nn.Conv1d(128, 64, kernel_size=3, padding=1)  # Conv1d layer with 64 output channels
        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)  # Conv1d layer with 32 output channels
        self.kldivloss = nn.KLDivLoss(reduction='batchmean')  # KLDivLoss layer

    def forward(self, x):
        # Assuming x is a tensor of indices for the embedding layer
        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)
        x = x.permute(0, 2, 1)  # Shape: (batch_size, embedding_dim, seq_len) for Conv1d
        x = F.relu(self.conv1(x))  # Shape: (batch_size, 64, seq_len)
        x = F.relu(self.conv2(x))  # Shape: (batch_size, 32, seq_len)
        x = x.permute(0, 2, 1)  # Shape: (batch_size, seq_len, 32) for KLDivLoss
        x = F.log_softmax(x, dim=-1)  # Apply log_softmax for KLDivLoss
        target = torch.ones_like(x) / x.size(-1)  # Create a uniform target distribution
        loss = self.kldivloss(x, target)  # Compute KLDivLoss
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # Random indices for embedding layer
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

