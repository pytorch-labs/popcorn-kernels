
# This is a random torch model generated by the following modules: ['ConstantPad2d', 'FractionalMaxPool3d', 'PReLU', 'PoissonNLLLoss', 'AdaptiveAvgPool3d', 'RMSNorm', 'Conv1d', 'AvgPool2d', 'HuberLoss', 'Embedding']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Assuming vocab size of 1000
        self.constant_pad = nn.ConstantPad2d(2, 0)
        self.conv1d = nn.Conv1d(128, 64, kernel_size=3)
        self.prelu = nn.PReLU()
        self.avgpool2d = nn.AvgPool2d(kernel_size=2)
        self.fractional_maxpool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(10, 10, 10))
        self.adaptive_avgpool3d = nn.AdaptiveAvgPool3d((5, 5, 5))
        self.rmsnorm = RMSNorm(64)  # Assuming RMSNorm is a custom layer
        self.poisson_nll_loss = nn.PoissonNLLLoss()
        self.huber_loss = nn.HuberLoss()

    def forward(self, x):
        # Assuming input x is a tensor of shape (batch_size, sequence_length)
        x = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)
        x = x.permute(0, 2, 1)  # Shape: (batch_size, embedding_dim, sequence_length)
        x = self.conv1d(x)  # Shape: (batch_size, 64, sequence_length - kernel_size + 1)
        x = self.prelu(x)
        
        # Reshape for 2D operations
        x = x.unsqueeze(2)  # Shape: (batch_size, 64, 1, sequence_length - kernel_size + 1)
        x = self.constant_pad(x)  # Shape: (batch_size, 64, 5, sequence_length - kernel_size + 5)
        x = self.avgpool2d(x)  # Shape: (batch_size, 64, 2, (sequence_length - kernel_size + 5) // 2)
        
        # Reshape for 3D operations
        x = x.unsqueeze(2)  # Shape: (batch_size, 64, 1, 2, (sequence_length - kernel_size + 5) // 2)
        x = self.fractional_maxpool3d(x)  # Shape: (batch_size, 64, 10, 10, 10)
        x = self.adaptive_avgpool3d(x)  # Shape: (batch_size, 64, 5, 5, 5)
        
        # Flatten for RMSNorm
        x = x.view(x.size(0), x.size(1), -1)  # Shape: (batch_size, 64, 125)
        x = self.rmsnorm(x)
        
        # Dummy loss calculation (assuming target is provided)
        target = torch.randint(0, 10, (x.size(0),)).to(x.device)
        loss1 = self.poisson_nll_loss(x.mean(dim=(1, 2)), target.float())
        loss2 = self.huber_loss(x.mean(dim=(1, 2)), target.float())
        
        return x, loss1 + loss2

# Assuming RMSNorm is a custom layer
class RMSNorm(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.scale = dim ** 0.5
        self.gamma = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        return F.normalize(x, dim=-1) * self.scale * self.gamma

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # Example input: (batch_size, sequence_length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
