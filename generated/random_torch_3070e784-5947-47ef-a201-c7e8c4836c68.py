
# This is a random torch model generated by the following modules: ['ModuleDict', 'LazyConv1d', 'LazyConvTranspose3d', 'Upsample', 'Tanhshrink', 'CircularPad3d', 'ELU', 'GLU', 'RNNCellBase']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.module_dict = nn.ModuleDict({
            'conv1': nn.LazyConv1d(out_channels=32, kernel_size=3),
            'conv_transpose3d': nn.LazyConvTranspose3d(out_channels=64, kernel_size=3),
            'upsample': nn.Upsample(scale_factor=2),
            'tanhshrink': nn.Tanhshrink(),
            'circular_pad3d': nn.CircularPad3d(padding=1),
            'elu': nn.ELU(alpha=1.0),
            'glu': nn.GLU(dim=1),
            'rnn_cell': nn.RNNCellBase(input_size=64, hidden_size=128)
        })
        
        # Repeat some modules up to 5 times
        self.conv1_repeated = nn.LazyConv1d(out_channels=32, kernel_size=3)
        self.conv_transpose3d_repeated = nn.LazyConvTranspose3d(out_channels=64, kernel_size=3)
        self.upsample_repeated = nn.Upsample(scale_factor=2)
        self.tanhshrink_repeated = nn.Tanhshrink()
        self.circular_pad3d_repeated = nn.CircularPad3d(padding=1)

    def forward(self, x):
        # Apply initial modules
        x = self.module_dict['conv1'](x)
        x = self.module_dict['tanhshrink'](x)
        x = self.module_dict['elu'](x)
        
        # Reshape for 3D operations
        x = x.unsqueeze(-1).unsqueeze(-1)  # Add two extra dimensions for 3D operations
        x = self.module_dict['circular_pad3d'](x)
        x = self.module_dict['conv_transpose3d'](x)
        x = self.module_dict['upsample'](x)
        
        # Apply repeated modules
        x = self.conv1_repeated(x.squeeze(-1).squeeze(-1))  # Remove extra dimensions for 1D operations
        x = self.tanhshrink_repeated(x)
        x = self.elu_repeated(x)
        
        # Reshape for RNNCellBase
        x = x.view(x.size(0), -1)  # Flatten for RNNCellBase
        hx = torch.zeros(x.size(0), 128).to(x.device)  # Initialize hidden state
        x = self.module_dict['rnn_cell'](x, hx)
        
        # Apply GLU
        x = x.unsqueeze(1)  # Add channel dimension for GLU
        x = self.module_dict['glu'](x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64).cuda()  # Example input shape: (batch_size, channels, sequence_length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

