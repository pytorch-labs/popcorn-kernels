
# This is a random torch model generated by the following modules: ['LayerNorm', 'Module', 'LazyConvTranspose2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.layernorm1 = nn.LayerNorm(64)  # Assuming input size of 64 for LayerNorm
        self.layernorm2 = nn.LayerNorm(128)  # Assuming input size of 128 for LayerNorm
        self.lazy_conv_transpose1 = nn.LazyConvTranspose2d(out_channels=32, kernel_size=4, stride=2, padding=1)
        self.lazy_conv_transpose2 = nn.LazyConvTranspose2d(out_channels=16, kernel_size=4, stride=2, padding=1)
        self.lazy_conv_transpose3 = nn.LazyConvTranspose2d(out_channels=8, kernel_size=4, stride=2, padding=1)

    def forward(self, x):
        # Apply LayerNorm to the input
        x = self.layernorm1(x)
        
        # Reshape the input to fit the ConvTranspose2d layers
        x = x.view(-1, 64, 8, 8)  # Reshape to (batch_size, channels, height, width)
        
        # Apply ConvTranspose2d layers
        x = self.lazy_conv_transpose1(x)
        x = self.lazy_conv_transpose2(x)
        x = self.lazy_conv_transpose3(x)
        
        # Apply another LayerNorm
        x = x.view(x.size(0), -1)  # Flatten the tensor for LayerNorm
        x = self.layernorm2(x)
        
        # Reshape back to a 4D tensor
        x = x.view(x.size(0), 8, 32, 32)  # Reshape to (batch_size, channels, height, width)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 64).cuda()  # Input shape is arbitrary, but we assume 64 features for LayerNorm
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
