
# This is a random torch model generated by the following modules: ['MaxPool1d', 'FeatureAlphaDropout', 'RMSNorm', 'BatchNorm2d', 'UpsamplingNearest2d', 'MultiheadAttention', 'Bilinear', 'RReLU']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.maxpool1d = nn.MaxPool1d(kernel_size=2)
        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)
        self.rms_norm = nn.RMSNorm(64)  # Assuming 64 features for RMSNorm
        self.batch_norm2d = nn.BatchNorm2d(32)  # Assuming 32 channels for BatchNorm2d
        self.upsampling_nearest2d = nn.UpsamplingNearest2d(scale_factor=2)
        self.multihead_attention = nn.MultiheadAttention(embed_dim=64, num_heads=8)
        self.bilinear = nn.Bilinear(64, 64, 128)  # Assuming input and output features for Bilinear
        self.rrelu = nn.RReLU()

    def forward(self, x):
        # Reshape input to fit MaxPool1d
        x = x.view(x.size(0), x.size(1), -1)  # Reshape to (batch_size, channels, sequence_length)
        x = self.maxpool1d(x)
        
        # Apply FeatureAlphaDropout
        x = self.feature_alpha_dropout(x)
        
        # Reshape for RMSNorm
        x = x.view(x.size(0), -1)  # Flatten for RMSNorm
        x = self.rms_norm(x)
        
        # Reshape for BatchNorm2d
        x = x.view(x.size(0), 32, -1, -1)  # Reshape to (batch_size, 32, height, width)
        x = self.batch_norm2d(x)
        
        # Apply UpsamplingNearest2d
        x = self.upsampling_nearest2d(x)
        
        # Reshape for MultiheadAttention
        x = x.view(x.size(0), x.size(1), -1).permute(2, 0, 1)  # Reshape to (sequence_length, batch_size, embed_dim)
        x, _ = self.multihead_attention(x, x, x)
        
        # Reshape for Bilinear
        x = x.permute(1, 2, 0).contiguous().view(x.size(1), -1)  # Reshape to (batch_size, embed_dim * sequence_length)
        x = self.bilinear(x, x)  # Using the same input for both inputs to Bilinear
        
        # Apply RReLU
        x = self.rrelu(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 64, 32, 32).cuda()  # Arbitrary input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

