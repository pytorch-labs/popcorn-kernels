
# This is a random torch model generated by the following modules: ['AdaptiveAvgPool2d', 'GLU', 'Bilinear', 'LazyConv2d', 'LSTM', 'CrossEntropyLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.lazy_conv1 = nn.LazyConv2d(out_channels=32, kernel_size=3)
        self.lazy_conv2 = nn.LazyConv2d(out_channels=64, kernel_size=3)
        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((5, 5))
        self.glu = nn.GLU(dim=1)
        self.bilinear = nn.Bilinear(64 * 5 * 5, 64 * 5 * 5, 128)
        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=2, batch_first=True)
        self.fc = nn.Linear(64, 10)
        self.cross_entropy_loss = nn.CrossEntropyLoss()

    def forward(self, x):
        # Apply LazyConv2d layers
        x = self.lazy_conv1(x)
        x = self.lazy_conv2(x)
        
        # Apply AdaptiveAvgPool2d
        x = self.adaptive_avg_pool(x)
        
        # Reshape for GLU
        x = x.view(x.size(0), -1, x.size(2) * x.size(3))
        x = self.glu(x)
        
        # Reshape for Bilinear
        x = x.view(x.size(0), -1)
        x = self.bilinear(x, x)
        
        # Reshape for LSTM
        x = x.unsqueeze(1)
        x, _ = self.lstm(x)
        x = x[:, -1, :]
        
        # Final fully connected layer
        x = self.fc(x)
        
        # Apply CrossEntropyLoss (assuming target is provided externally)
        # Note: CrossEntropyLoss is typically used in the loss computation, not in the forward pass.
        # Here, we return the logits for the loss computation.
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

