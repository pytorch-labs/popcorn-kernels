
# This is a random torch model generated by the following modules: ['Softshrink', 'UpsamplingNearest2d', 'NLLLoss', 'BCEWithLogitsLoss', 'InstanceNorm1d', 'PixelShuffle', 'AdaptiveAvgPool1d', 'EmbeddingBag', 'GRUCell', 'GRU']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding_bag = nn.EmbeddingBag(1000, 64, mode='mean')
        self.gru_cell = nn.GRUCell(64, 128)
        self.gru = nn.GRU(128, 256, batch_first=True)
        self.instance_norm1d = nn.InstanceNorm1d(256)
        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(128)
        self.upsampling_nearest2d = nn.UpsamplingNearest2d(scale_factor=2)
        self.pixel_shuffle = nn.PixelShuffle(2)
        self.softshrink = nn.Softshrink(0.5)
        self.nll_loss = nn.NLLLoss()
        self.bce_with_logits_loss = nn.BCEWithLogitsLoss()

    def forward(self, x):
        # Assuming x is a 1D tensor of indices for EmbeddingBag
        x = self.embedding_bag(x)
        
        # Reshape for GRUCell
        x = x.unsqueeze(0)  # Add sequence dimension
        x = self.gru_cell(x.squeeze(0), torch.zeros(x.size(1), 128).to(x.device))
        
        # Reshape for GRU
        x = x.unsqueeze(0)  # Add sequence dimension
        x, _ = self.gru(x)
        
        # Apply InstanceNorm1d
        x = x.permute(0, 2, 1)  # Swap sequence and feature dimensions
        x = self.instance_norm1d(x)
        x = x.permute(0, 2, 1)  # Swap back
        
        # Apply AdaptiveAvgPool1d
        x = self.adaptive_avg_pool1d(x)
        
        # Reshape for UpsamplingNearest2d
        x = x.unsqueeze(1)  # Add channel dimension
        x = x.unsqueeze(-1)  # Add height dimension
        x = self.upsampling_nearest2d(x)
        
        # Apply PixelShuffle
        x = self.pixel_shuffle(x)
        
        # Apply Softshrink
        x = self.softshrink(x)
        
        # Compute NLLLoss (dummy target)
        target = torch.randint(0, 256, (x.size(0),)).to(x.device)
        nll_loss = self.nll_loss(F.log_softmax(x.mean(dim=(2, 3)), dim=1), target)
        
        # Compute BCEWithLogitsLoss (dummy target)
        bce_target = torch.randint(0, 2, (x.size(0), 1)).float().to(x.device)
        bce_loss = self.bce_with_logits_loss(x.mean(dim=(2, 3)), bce_target)
        
        # Return both losses for demonstration purposes
        return nll_loss, bce_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10,)).cuda()  # Example input for EmbeddingBag
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

