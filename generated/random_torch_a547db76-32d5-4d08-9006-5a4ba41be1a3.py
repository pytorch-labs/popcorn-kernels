
# This is a random torch model generated by the following modules: ['SiLU', 'ZeroPad2d', 'Dropout', 'MaxUnpool2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.silu1 = nn.SiLU()
        self.zero_pad1 = nn.ZeroPad2d(2)
        self.dropout1 = nn.Dropout(0.5)
        self.max_unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)
        self.silu2 = nn.SiLU()
        self.zero_pad2 = nn.ZeroPad2d(1)
        self.dropout2 = nn.Dropout(0.3)
        self.max_unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)
        self.silu3 = nn.SiLU()

    def forward(self, x):
        # Apply SiLU activation
        x = self.silu1(x)
        
        # Zero padding
        x = self.zero_pad1(x)
        
        # Dropout
        x = self.dropout1(x)
        
        # MaxUnpool2d requires indices from a previous MaxPool2d operation
        # For simplicity, we assume the input has been through a MaxPool2d layer before
        # Here, we simulate the indices by performing a MaxPool2d operation
        pool_size = 2
        x, indices = F.max_pool2d(x, pool_size, return_indices=True)
        
        # MaxUnpool2d
        x = self.max_unpool1(x, indices)
        
        # Apply SiLU activation again
        x = self.silu2(x)
        
        # Zero padding
        x = self.zero_pad2(x)
        
        # Dropout
        x = self.dropout2(x)
        
        # Another MaxUnpool2d operation
        x, indices = F.max_pool2d(x, pool_size, return_indices=True)
        x = self.max_unpool2(x, indices)
        
        # Final SiLU activation
        x = self.silu3(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
