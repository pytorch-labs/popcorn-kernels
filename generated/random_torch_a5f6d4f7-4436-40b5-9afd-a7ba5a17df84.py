
# This is a random torch model generated by the following modules: ['AdaptiveMaxPool3d', 'Unflatten', 'LazyInstanceNorm1d', 'Softmax2d', 'TransformerDecoder', 'ConstantPad2d', 'GaussianNLLLoss', 'NLLLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.pad = nn.ConstantPad2d(2, 3.0)
        self.unflatten = nn.Unflatten(1, (1, 1))
        self.norm = nn.LazyInstanceNorm1d()
        self.pool = nn.AdaptiveMaxPool3d((5, 5, 5))
        self.softmax = nn.Softmax2d()
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=64, nhead=8), num_layers=3
        )
        self.gaussian_loss = nn.GaussianNLLLoss()
        self.nll_loss = nn.NLLLoss()

    def forward(self, x):
        # Apply padding
        x = self.pad(x)
        
        # Unflatten the input
        x = self.unflatten(x)
        
        # Apply instance normalization
        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions
        x = self.norm(x)
        x = x.view(x.size(0), x.size(1), *x.shape[2:])  # Reshape back
        
        # Apply adaptive max pooling
        x = x.unsqueeze(1)  # Add a dummy dimension for 3D pooling
        x = self.pool(x)
        x = x.squeeze(1)  # Remove the dummy dimension
        
        # Apply softmax
        x = self.softmax(x)
        
        # Prepare input for TransformerDecoder
        x = x.view(x.size(0), -1, x.size(-1))  # Reshape for TransformerDecoder
        memory = torch.randn(x.size(0), 10, x.size(-1)).to(x.device)  # Random memory for TransformerDecoder
        x = self.transformer_decoder(x, memory)
        
        # Compute GaussianNLLLoss (for demonstration purposes, using random targets)
        target = torch.randn_like(x)
        var = torch.ones_like(x)
        gaussian_loss = self.gaussian_loss(x, target, var)
        
        # Compute NLLLoss (for demonstration purposes, using random targets)
        log_probs = F.log_softmax(x, dim=1)
        nll_target = torch.randint(0, x.size(1), (x.size(0),)).to(x.device)
        nll_loss = self.nll_loss(log_probs, nll_target)
        
        # Return both losses for demonstration
        return gaussian_loss, nll_loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
