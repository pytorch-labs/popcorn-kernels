
# This is a random torch model generated by the following modules: ['LSTM', 'Softplus', 'AdaptiveMaxPool3d', 'TransformerEncoder']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.lstm1 = nn.LSTM(input_size=128, hidden_size=256, num_layers=2, batch_first=True)
        self.softplus = nn.Softplus()
        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((8, 8, 8))
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=256, nhead=8), num_layers=3
        )
        self.fc = nn.Linear(256 * 8 * 8 * 8, 10)

    def forward(self, x):
        # Assuming x is of shape (batch_size, seq_len, input_size)
        batch_size, seq_len, input_size = x.shape
        
        # Pass through LSTM
        x, _ = self.lstm1(x)  # Output shape: (batch_size, seq_len, hidden_size)
        
        # Apply Softplus
        x = self.softplus(x)  # Output shape: (batch_size, seq_len, hidden_size)
        
        # Reshape for AdaptiveMaxPool3d
        x = x.view(batch_size, 1, seq_len, -1)  # Reshape to (batch_size, 1, seq_len, hidden_size)
        x = x.permute(0, 3, 1, 2)  # Reshape to (batch_size, hidden_size, 1, seq_len)
        x = x.unsqueeze(2)  # Reshape to (batch_size, hidden_size, 1, 1, seq_len)
        
        # Apply AdaptiveMaxPool3d
        x = self.adaptive_max_pool3d(x)  # Output shape: (batch_size, hidden_size, 8, 8, 8)
        
        # Reshape for TransformerEncoder
        x = x.view(batch_size, -1, 256)  # Reshape to (batch_size, 8*8*8, 256)
        
        # Pass through TransformerEncoder
        x = self.transformer_encoder(x)  # Output shape: (batch_size, 8*8*8, 256)
        
        # Flatten and pass through final fully connected layer
        x = x.view(batch_size, -1)  # Reshape to (batch_size, 256 * 8 * 8 * 8)
        x = self.fc(x)  # Output shape: (batch_size, 10)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 128).cuda()  # Example input shape: (batch_size=1, seq_len=10, input_size=128)
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

