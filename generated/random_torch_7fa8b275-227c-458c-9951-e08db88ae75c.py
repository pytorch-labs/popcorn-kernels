
# This is a random torch model generated by the following modules: ['AdaptiveLogSoftmaxWithLoss', 'GELU', 'Tanh', 'Sigmoid', 'Flatten', 'Embedding', 'AdaptiveMaxPool3d', 'Fold']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)
        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((16, 16, 16))
        self.fold = nn.Fold(output_size=(32, 32), kernel_size=(3, 3))
        self.flatten = nn.Flatten()
        self.gelu = nn.GELU()
        self.tanh = nn.Tanh()
        self.sigmoid = nn.Sigmoid()
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(128, 1000, [100, 500])

    def forward(self, x):
        # Assuming input is a 1D tensor of indices for embedding
        x = self.embedding(x)
        
        # Reshape to 5D tensor for AdaptiveMaxPool3d
        x = x.view(-1, 1, 32, 32, 32)
        x = self.adaptive_max_pool3d(x)
        
        # Reshape to 4D tensor for Fold
        x = x.view(-1, 1, 16, 16, 16)
        x = x.permute(0, 1, 4, 2, 3).contiguous()
        x = x.view(-1, 16 * 16, 16)
        x = self.fold(x)
        
        # Flatten the output
        x = self.flatten(x)
        
        # Apply activation functions
        x = self.gelu(x)
        x = self.tanh(x)
        x = self.sigmoid(x)
        
        # Reshape for AdaptiveLogSoftmaxWithLoss
        x = x.view(-1, 128)
        x = self.adaptive_log_softmax.log_prob(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10,)).cuda()  # Example input for embedding
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
