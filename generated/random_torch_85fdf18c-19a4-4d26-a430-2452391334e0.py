
# This is a random torch model generated by the following modules: ['RNN', 'Softmax2d', 'SELU', 'ConstantPad3d', 'ModuleList', 'CosineEmbeddingLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
        self.softmax2d = nn.Softmax2d()
        self.selu = nn.SELU()
        self.pad = nn.ConstantPad3d(padding=1, value=0)
        self.module_list = nn.ModuleList([nn.Linear(20, 10) for _ in range(3)])
        self.loss = nn.CosineEmbeddingLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, sequence_length, input_size)
        batch_size, seq_len, input_size = x.size()
        
        # Pass through RNN
        x, _ = self.rnn(x)  # Output shape: (batch_size, seq_len, hidden_size)
        
        # Reshape for Softmax2d
        x = x.view(batch_size, -1, seq_len, 20)  # Reshape to (batch_size, channels, height, width)
        
        # Apply Softmax2d
        x = self.softmax2d(x)
        
        # Reshape back to (batch_size, seq_len, hidden_size)
        x = x.view(batch_size, seq_len, 20)
        
        # Apply SELU
        x = self.selu(x)
        
        # Pad the tensor
        x = x.unsqueeze(1)  # Add a dimension for padding
        x = self.pad(x)  # Output shape: (batch_size, 1, seq_len + 2, 20 + 2)
        
        # Flatten for Linear layers
        x = x.view(batch_size, -1)  # Flatten to (batch_size, (seq_len + 2) * (20 + 2))
        
        # Pass through ModuleList of Linear layers
        for layer in self.module_list:
            x = layer(x)
        
        # Compute CosineEmbeddingLoss (dummy target for demonstration)
        target = torch.ones(batch_size, 10)  # Dummy target
        loss = self.loss(x, target, torch.ones(batch_size))
        
        return x, loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 5, 10).cuda()  # Example input: (batch_size=1, seq_len=5, input_size=10)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

