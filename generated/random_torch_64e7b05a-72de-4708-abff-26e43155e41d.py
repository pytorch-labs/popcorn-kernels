
# This is a random torch model generated by the following modules: ['AdaptiveAvgPool2d', 'SmoothL1Loss', 'BCEWithLogitsLoss', 'AlphaDropout', 'ReplicationPad2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((5, 5))
        self.replication_pad = nn.ReplicationPad2d(2)
        self.alpha_dropout = nn.AlphaDropout(p=0.5)
        self.smooth_l1_loss = nn.SmoothL1Loss()
        self.bce_with_logits_loss = nn.BCEWithLogitsLoss()

    def forward(self, x):
        # Apply ReplicationPad2d
        x = self.replication_pad(x)
        
        # Apply AdaptiveAvgPool2d
        x = self.adaptive_avg_pool(x)
        
        # Apply AlphaDropout
        x = self.alpha_dropout(x)
        
        # Flatten the tensor for loss computation
        x = x.view(x.size(0), -1)
        
        # Generate a random target tensor for loss computation
        target_smooth_l1 = torch.randn_like(x)
        target_bce = torch.randint(0, 2, (x.size(0), x.size(1)), dtype=torch.float32).to(x.device)
        
        # Compute SmoothL1Loss
        loss_smooth_l1 = self.smooth_l1_loss(x, target_smooth_l1)
        
        # Compute BCEWithLogitsLoss
        loss_bce = self.bce_with_logits_loss(x, target_bce)
        
        # Return both losses
        return loss_smooth_l1, loss_bce


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
