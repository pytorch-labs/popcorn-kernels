
# This is a random torch model generated by the following modules: ['AdaptiveLogSoftmaxWithLoss', 'ConvTranspose1d', 'ConvTranspose2d', 'Embedding']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Assuming vocabulary size of 1000
        self.conv_transpose1d = nn.ConvTranspose1d(128, 64, kernel_size=5, stride=2)
        self.conv_transpose2d = nn.ConvTranspose2d(64, 32, kernel_size=5, stride=2)
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(32, 1000, [100, 300])

    def forward(self, x):
        # Assuming x is a tensor of token indices (batch_size, sequence_length)
        x = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)
        x = x.permute(0, 2, 1)  # (batch_size, embedding_dim, sequence_length)
        x = F.relu(self.conv_transpose1d(x))  # (batch_size, 64, new_sequence_length)
        x = x.unsqueeze(-1)  # (batch_size, 64, new_sequence_length, 1)
        x = F.relu(self.conv_transpose2d(x))  # (batch_size, 32, new_height, new_width)
        x = x.mean(dim=[2, 3])  # (batch_size, 32)
        output = self.adaptive_log_softmax(x, x)  # For demonstration, using x as target
        return output


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # (batch_size, sequence_length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

