
# This is a random torch model generated by the following modules: ['RMSNorm', 'Embedding', 'ConvTranspose2d', 'NLLLoss', 'LPPool1d', 'ConstantPad1d', 'BatchNorm2d', 'Container']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.rms_norm = RMSNorm(128)  # RMSNorm layer
        self.conv_transpose1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # ConvTranspose2d layer
        self.batch_norm1 = nn.BatchNorm2d(64)  # BatchNorm2d layer
        self.conv_transpose2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # ConvTranspose2d layer
        self.batch_norm2 = nn.BatchNorm2d(32)  # BatchNorm2d layer
        self.lp_pool = nn.LPPool1d(norm_type=2, kernel_size=3, stride=2)  # LPPool1d layer
        self.constant_pad = nn.ConstantPad1d(padding=1, value=0)  # ConstantPad1d layer
        self.container = nn.Sequential(  # Container layer
            nn.Linear(32 * 8 * 8, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )
        self.nll_loss = nn.NLLLoss()  # NLLLoss layer

    def forward(self, x):
        # Assume input x is a tensor of shape (batch_size, sequence_length)
        x = self.embedding(x)  # Shape: (batch_size, sequence_length, 128)
        x = self.rms_norm(x)  # Shape: (batch_size, sequence_length, 128)
        
        # Reshape for ConvTranspose2d
        x = x.unsqueeze(1)  # Shape: (batch_size, 1, sequence_length, 128)
        x = x.permute(0, 3, 2, 1)  # Shape: (batch_size, 128, sequence_length, 1)
        
        x = self.conv_transpose1(x)  # Shape: (batch_size, 64, sequence_length * 2, 2)
        x = self.batch_norm1(x)  # Shape: (batch_size, 64, sequence_length * 2, 2)
        x = F.relu(x)
        
        x = self.conv_transpose2(x)  # Shape: (batch_size, 32, sequence_length * 4, 4)
        x = self.batch_norm2(x)  # Shape: (batch_size, 32, sequence_length * 4, 4)
        x = F.relu(x)
        
        # Reshape for LPPool1d
        x = x.permute(0, 2, 1, 3)  # Shape: (batch_size, sequence_length * 4, 32, 4)
        x = x.reshape(x.size(0), x.size(1), -1)  # Shape: (batch_size, sequence_length * 4, 32 * 4)
        x = x.permute(0, 2, 1)  # Shape: (batch_size, 32 * 4, sequence_length * 4)
        
        x = self.lp_pool(x)  # Shape: (batch_size, 32 * 4, (sequence_length * 4 - 3) // 2 + 1)
        x = self.constant_pad(x)  # Shape: (batch_size, 32 * 4, (sequence_length * 4 - 3) // 2 + 1 + 2)
        
        # Reshape for Linear layer
        x = x.reshape(x.size(0), -1)  # Shape: (batch_size, 32 * 4 * ((sequence_length * 4 - 3) // 2 + 1 + 2))
        x = self.container(x)  # Shape: (batch_size, 10)
        
        # Apply log_softmax for NLLLoss
        x = F.log_softmax(x, dim=1)  # Shape: (batch_size, 10)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (1, 32)).cuda()  # Shape: (batch_size, sequence_length)
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []


# Custom RMSNorm implementation
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.scale = dim ** -0.5
        self.eps = eps
        self.g = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        norm = torch.norm(x, p=2, dim=-1, keepdim=True) * self.scale
        return x / norm.clamp(min=self.eps) * self.g
