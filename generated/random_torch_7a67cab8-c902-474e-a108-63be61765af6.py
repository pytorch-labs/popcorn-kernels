
# This is a random torch model generated by the following modules: ['Embedding', 'MultiMarginLoss', 'Hardtanh', 'Hardsigmoid', 'LazyBatchNorm3d', 'Hardswish', 'LazyConvTranspose1d', 'TransformerDecoderLayer', 'ReflectionPad3d', 'PairwiseDistance']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)
        self.hardtanh = nn.Hardtanh(min_val=-1.0, max_val=1.0)
        self.hardsigmoid = nn.Hardsigmoid()
        self.lazy_bn3d = nn.LazyBatchNorm3d()
        self.hardswish = nn.Hardswish()
        self.lazy_conv_transpose1d = nn.LazyConvTranspose1d(out_channels=64, kernel_size=3, stride=2)
        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=128, nhead=8)
        self.reflection_pad3d = nn.ReflectionPad3d(1)
        self.pairwise_distance = nn.PairwiseDistance(p=2)
        self.multi_margin_loss = nn.MultiMarginLoss()

    def forward(self, x):
        # Assume x is a tensor of arbitrary shape
        x = x.long()  # Convert to long for embedding
        x = self.embedding(x)
        x = x.unsqueeze(1).unsqueeze(1)  # Reshape to 5D for LazyBatchNorm3d
        x = self.lazy_bn3d(x)
        x = x.squeeze(1).squeeze(1)  # Reshape back to 3D
        x = self.hardtanh(x)
        x = self.hardsigmoid(x)
        x = x.permute(0, 2, 1)  # Reshape for ConvTranspose1d
        x = self.lazy_conv_transpose1d(x)
        x = x.permute(0, 2, 1)  # Reshape back
        x = self.hardswish(x)
        x = x.unsqueeze(1).unsqueeze(1)  # Reshape to 5D for ReflectionPad3d
        x = self.reflection_pad3d(x)
        x = x.squeeze(1).squeeze(1)  # Reshape back to 3D
        x = x.unsqueeze(0)  # Add batch dimension for TransformerDecoderLayer
        x = self.transformer_decoder_layer(x, x)  # Self-attention
        x = x.squeeze(0)  # Remove batch dimension
        x = self.pairwise_distance(x, x)  # Pairwise distance
        loss = self.multi_margin_loss(x, torch.zeros_like(x).long())  # Dummy loss
        return x, loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 32)).cuda()  # Arbitrary shape for embedding
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

