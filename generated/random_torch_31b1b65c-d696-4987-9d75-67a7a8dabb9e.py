
# This is a random torch model generated by the following modules: ['ConvTranspose2d', 'LazyBatchNorm3d', 'NLLLoss2d', 'BCELoss', 'NLLLoss', 'EmbeddingBag']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv_transpose1 = nn.ConvTranspose2d(1, 10, kernel_size=5)
        self.conv_transpose2 = nn.ConvTranspose2d(10, 20, kernel_size=5)
        self.batch_norm = nn.LazyBatchNorm3d()
        self.embedding_bag = nn.EmbeddingBag(1000, 10, mode='mean')
        self.nll_loss = nn.NLLLoss()
        self.bce_loss = nn.BCELoss()
        self.nll_loss2d = nn.NLLLoss2d()

    def forward(self, x):
        # Apply ConvTranspose2d layers
        x = self.conv_transpose1(x)
        x = self.conv_transpose2(x)
        
        # Reshape for LazyBatchNorm3d
        x = x.unsqueeze(0)  # Add a batch dimension
        x = x.unsqueeze(0)  # Add a channel dimension
        x = self.batch_norm(x)
        
        # Reshape for EmbeddingBag
        x = x.view(-1, 20)  # Flatten to (batch_size, 20)
        x = self.embedding_bag(x.long())
        
        # Apply NLLLoss2d (requires a target, so we generate one)
        target = torch.randint(0, 10, (x.size(0),)).long()
        x = self.nll_loss2d(x.unsqueeze(0).unsqueeze(0), target.unsqueeze(0).unsqueeze(0))
        
        # Apply BCELoss (requires a target, so we generate one)
        target_bce = torch.randn_like(x).sigmoid()
        x = self.bce_loss(x, target_bce)
        
        # Apply NLLLoss (requires a target, so we generate one)
        target_nll = torch.randint(0, 10, (x.size(0),)).long()
        x = self.nll_loss(x, target_nll)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

