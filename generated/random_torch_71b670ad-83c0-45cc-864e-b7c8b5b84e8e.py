
# This is a random torch model generated by the following modules: ['PoissonNLLLoss', 'LazyConv2d', 'EmbeddingBag', 'BCELoss', 'TransformerEncoderLayer', 'GRUCell', 'Conv3d', 'L1Loss', 'LayerNorm', 'SiLU']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.lazy_conv2d = nn.LazyConv2d(out_channels=32, kernel_size=3)
        self.conv3d = nn.Conv3d(32, 64, kernel_size=3)
        self.gru_cell = nn.GRUCell(input_size=64, hidden_size=128)
        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)
        self.embedding_bag = nn.EmbeddingBag(num_embeddings=1000, embedding_dim=128)
        self.layer_norm = nn.LayerNorm(128)
        self.silu = nn.SiLU()
        self.poisson_nll_loss = nn.PoissonNLLLoss()
        self.bce_loss = nn.BCELoss()
        self.l1_loss = nn.L1Loss()

    def forward(self, x):
        # Assuming x is a 4D tensor (batch_size, channels, height, width)
        x = self.lazy_conv2d(x)  # Output shape: (batch_size, 32, height-2, width-2)
        x = x.unsqueeze(2)  # Add a dimension to make it 5D for Conv3d
        x = self.conv3d(x)  # Output shape: (batch_size, 64, depth-2, height-4, width-4)
        x = x.mean(dim=[2, 3, 4])  # Global average pooling to reduce to (batch_size, 64)
        
        # GRUCell expects input of shape (batch_size, input_size)
        hx = torch.zeros(x.size(0), 128).to(x.device)  # Initialize hidden state
        x = self.gru_cell(x, hx)  # Output shape: (batch_size, 128)
        
        # TransformerEncoderLayer expects input of shape (seq_len, batch_size, d_model)
        x = x.unsqueeze(0)  # Add sequence dimension
        x = self.transformer_encoder_layer(x)  # Output shape: (seq_len, batch_size, 128)
        x = x.squeeze(0)  # Remove sequence dimension
        
        # EmbeddingBag expects input of shape (batch_size,)
        indices = torch.randint(0, 1000, (x.size(0),)).to(x.device)
        x = self.embedding_bag(indices)  # Output shape: (batch_size, 128)
        
        x = self.layer_norm(x)  # Output shape: (batch_size, 128)
        x = self.silu(x)  # Output shape: (batch_size, 128)
        
        # Loss functions are typically used during training, but we can apply them here for demonstration
        target = torch.randn_like(x)
        poisson_loss = self.poisson_nll_loss(x, target)
        bce_loss = self.bce_loss(torch.sigmoid(x), torch.sigmoid(target))
        l1_loss = self.l1_loss(x, target)
        
        # Return the losses as outputs for demonstration purposes
        return poisson_loss, bce_loss, l1_loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
