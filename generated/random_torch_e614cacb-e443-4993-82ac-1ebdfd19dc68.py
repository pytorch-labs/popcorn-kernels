
# This is a random torch model generated by the following modules: ['ConstantPad1d', 'LazyConv1d', 'MarginRankingLoss', 'ConstantPad3d', 'LazyConvTranspose1d', 'LazyInstanceNorm1d', 'AdaptiveLogSoftmaxWithLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.pad1d = nn.ConstantPad1d(2, 1.0)
        self.conv1d = nn.LazyConv1d(out_channels=32, kernel_size=3)
        self.pad3d = nn.ConstantPad3d(1, 2.0)
        self.conv_transpose1d = nn.LazyConvTranspose1d(out_channels=64, kernel_size=4)
        self.instance_norm1d = nn.LazyInstanceNorm1d()
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(in_features=64, n_classes=10, cutoffs=[2, 4])
        self.margin_ranking_loss = nn.MarginRankingLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, length)
        x = self.pad1d(x)  # Apply ConstantPad1d
        x = self.conv1d(x)  # Apply LazyConv1d
        x = x.unsqueeze(2).unsqueeze(3)  # Reshape to 4D for ConstantPad3d
        x = self.pad3d(x)  # Apply ConstantPad3d
        x = x.squeeze(3).squeeze(2)  # Reshape back to 2D for LazyConvTranspose1d
        x = self.conv_transpose1d(x)  # Apply LazyConvTranspose1d
        x = self.instance_norm1d(x)  # Apply LazyInstanceNorm1d
        x = x.transpose(1, 2)  # Transpose for AdaptiveLogSoftmaxWithLoss
        x = self.adaptive_log_softmax.log_prob(x)  # Apply AdaptiveLogSoftmaxWithLoss
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64).cuda()  # Example input shape (batch_size=1, channels=3, length=64)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
