
# This is a random torch model generated by the following modules: ['Tanhshrink', 'AdaptiveAvgPool1d', 'RMSNorm', 'SyncBatchNorm', 'CELU', 'PixelShuffle', 'ReplicationPad1d', 'MaxPool3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.tanhshrink = nn.Tanhshrink()
        self.adaptive_avg_pool1d = nn.AdaptiveAvgPool1d(output_size=64)
        self.rms_norm = RMSNorm(dim=64)  # Assuming RMSNorm is a custom layer
        self.sync_batch_norm = nn.SyncBatchNorm(num_features=64)
        self.celu = nn.CELU()
        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=2)
        self.replication_pad1d = nn.ReplicationPad1d(padding=1)
        self.max_pool3d = nn.MaxPool3d(kernel_size=2, stride=2)

    def forward(self, x):
        # Apply Tanhshrink
        x = self.tanhshrink(x)
        
        # Reshape for AdaptiveAvgPool1d
        x = x.view(x.size(0), x.size(1), -1)  # Flatten last dimensions
        x = self.adaptive_avg_pool1d(x)
        
        # Apply RMSNorm
        x = self.rms_norm(x)
        
        # Apply SyncBatchNorm
        x = self.sync_batch_norm(x)
        
        # Apply CELU
        x = self.celu(x)
        
        # Reshape for PixelShuffle
        x = x.view(x.size(0), x.size(1), 1, 1)  # Reshape to 4D
        x = self.pixel_shuffle(x)
        
        # Reshape for ReplicationPad1d
        x = x.view(x.size(0), x.size(1), -1)  # Flatten last dimensions
        x = self.replication_pad1d(x)
        
        # Reshape for MaxPool3d
        x = x.view(x.size(0), x.size(1), 1, x.size(2), 1)  # Reshape to 5D
        x = self.max_pool3d(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 64, 128).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []


# Assuming RMSNorm is a custom layer, here is a simple implementation
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.scale = dim ** -0.5
        self.eps = eps
        self.g = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        norm = torch.norm(x, p=2, dim=-1, keepdim=True) * self.scale
        return x / norm.clamp(min=self.eps) * self.g
