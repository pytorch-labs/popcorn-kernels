
# This is a random torch model generated by the following modules: ['ModuleDict', 'AdaptiveLogSoftmaxWithLoss', 'LazyConvTranspose1d', 'LocalResponseNorm', 'NLLLoss2d', 'Embedding', 'ZeroPad3d', 'LazyConv1d', 'ReflectionPad2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.module_dict = nn.ModuleDict({
            'conv1': nn.LazyConv1d(out_channels=32, kernel_size=3),
            'conv_transpose1': nn.LazyConvTranspose1d(out_channels=64, kernel_size=3),
            'conv2': nn.LazyConv1d(out_channels=128, kernel_size=3),
            'embedding': nn.Embedding(num_embeddings=100, embedding_dim=64),
            'zero_pad': nn.ZeroPad3d(padding=(1, 1, 1, 1, 1, 1)),
            'reflection_pad': nn.ReflectionPad2d(padding=(1, 1, 1, 1)),
            'local_response_norm': nn.LocalResponseNorm(size=5),
            'adaptive_log_softmax': nn.AdaptiveLogSoftmaxWithLoss(in_features=128, n_classes=10, cutoffs=[2, 4]),
            'nll_loss': nn.NLLLoss2d()
        })

    def forward(self, x):
        # Assume x is of shape (batch_size, sequence_length, embedding_dim)
        x = self.module_dict['embedding'](x.long())  # Embedding layer
        x = x.permute(0, 2, 1)  # Reshape for Conv1d: (batch_size, embedding_dim, sequence_length)
        
        x = self.module_dict['conv1'](x)  # LazyConv1d
        x = self.module_dict['local_response_norm'](x)  # LocalResponseNorm
        
        x = self.module_dict['conv_transpose1'](x)  # LazyConvTranspose1d
        x = self.module_dict['zero_pad'](x.unsqueeze(2)).squeeze(2)  # ZeroPad3d (applied to 1D data)
        
        x = self.module_dict['conv2'](x)  # LazyConv1d
        x = self.module_dict['reflection_pad'](x.unsqueeze(2)).squeeze(2)  # ReflectionPad2d (applied to 1D data)
        
        x = x.permute(0, 2, 1)  # Reshape for AdaptiveLogSoftmaxWithLoss: (batch_size, sequence_length, features)
        x = x.mean(dim=1)  # Reduce sequence dimension
        
        output = self.module_dict['adaptive_log_softmax'](x, torch.zeros(x.size(0), dtype=torch.long))  # AdaptiveLogSoftmaxWithLoss
        return output


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 100, (10, 20)).cuda()  # (batch_size, sequence_length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

