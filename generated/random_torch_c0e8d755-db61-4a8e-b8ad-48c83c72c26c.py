
# This is a random torch model generated by the following modules: ['ModuleList', 'CircularPad2d', 'FractionalMaxPool2d', 'LSTMCell', 'ReflectionPad1d', 'GroupNorm', 'AvgPool3d', 'PReLU', 'Hardtanh', 'SoftMarginLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.module_list = nn.ModuleList([
            nn.CircularPad2d(2),
            nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14)),
            nn.LSTMCell(input_size=14*14, hidden_size=128),
            nn.ReflectionPad1d(2),
            nn.GroupNorm(num_groups=8, num_channels=128),
            nn.AvgPool3d(kernel_size=2),
            nn.PReLU(),
            nn.Hardtanh(min_val=-1.0, max_val=1.0),
            nn.SoftMarginLoss()
        ])
        self.fc = nn.Linear(128, 10)

    def forward(self, x):
        # Apply CircularPad2d
        x = self.module_list[0](x)
        
        # Apply FractionalMaxPool2d
        x = self.module_list[1](x)
        
        # Reshape for LSTMCell
        batch_size = x.size(0)
        x = x.view(batch_size, -1)
        
        # Initialize hidden and cell states for LSTMCell
        hx = torch.zeros(batch_size, 128).to(x.device)
        cx = torch.zeros(batch_size, 128).to(x.device)
        
        # Apply LSTMCell
        hx, cx = self.module_list[2](x, (hx, cx))
        
        # Reshape for ReflectionPad1d
        x = hx.unsqueeze(1)
        
        # Apply ReflectionPad1d
        x = self.module_list[3](x)
        
        # Apply GroupNorm
        x = self.module_list[4](x)
        
        # Reshape for AvgPool3d
        x = x.unsqueeze(1).unsqueeze(1)
        
        # Apply AvgPool3d
        x = self.module_list[5](x)
        
        # Apply PReLU
        x = self.module_list[6](x)
        
        # Apply Hardtanh
        x = self.module_list[7](x)
        
        # Flatten for FC layer
        x = x.view(batch_size, -1)
        
        # Apply FC layer
        x = self.fc(x)
        
        # Apply SoftMarginLoss (assuming binary classification)
        target = torch.randint(0, 2, (batch_size,)).float().to(x.device)
        loss = self.module_list[8](x.squeeze(), target)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 28, 28).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
