
# This is a random torch model generated by the following modules: ['ReplicationPad2d', 'CELU', 'LogSoftmax', 'Hardshrink', 'PReLU', 'Embedding', 'MaxPool3d', 'ZeroPad2d', 'SELU', 'ModuleDict']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.replication_pad = nn.ReplicationPad2d(2)  # ReplicationPad2d layer
        self.zero_pad = nn.ZeroPad2d(1)  # ZeroPad2d layer
        self.max_pool3d = nn.MaxPool3d(kernel_size=(2, 2, 2))  # MaxPool3d layer
        self.prelu = nn.PReLU()  # PReLU layer
        self.celu = nn.CELU()  # CELU layer
        self.selu = nn.SELU()  # SELU layer
        self.hardshrink = nn.Hardshrink()  # Hardshrink layer
        self.log_softmax = nn.LogSoftmax(dim=1)  # LogSoftmax layer
        self.module_dict = nn.ModuleDict({
            'linear1': nn.Linear(128, 64),
            'linear2': nn.Linear(64, 32)
        })  # ModuleDict layer

    def forward(self, x):
        # Assume input is a tensor of arbitrary shape
        # If input is not a 1D tensor (e.g., for embedding), reshape it
        if x.dim() > 1:
            x = x.view(-1)
        
        # Apply embedding
        x = self.embedding(x)
        
        # Reshape for 2D operations
        x = x.view(-1, 128, 8, 8)
        
        # Apply padding layers
        x = self.replication_pad(x)
        x = self.zero_pad(x)
        
        # Reshape for 3D operations
        x = x.unsqueeze(1)  # Add a dummy dimension for 3D
        x = self.max_pool3d(x)
        
        # Remove the dummy dimension
        x = x.squeeze(1)
        
        # Apply activation functions
        x = self.prelu(x)
        x = self.celu(x)
        x = self.selu(x)
        x = self.hardshrink(x)
        
        # Flatten for linear layers
        x = x.view(x.size(0), -1)
        
        # Apply ModuleDict layers
        x = self.module_dict['linear1'](x)
        x = self.module_dict['linear2'](x)
        
        # Apply LogSoftmax
        x = self.log_softmax(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10,)).cuda()  # Random input for embedding
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
