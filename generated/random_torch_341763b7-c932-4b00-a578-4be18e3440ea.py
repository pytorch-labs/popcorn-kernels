
# This is a random torch model generated by the following modules: ['AvgPool1d', 'LazyConvTranspose3d', 'Mish', 'Embedding', 'Dropout', 'BatchNorm3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Assuming vocab size of 1000
        self.dropout1 = nn.Dropout(0.5)
        self.bn1 = nn.BatchNorm3d(128)
        self.conv_transpose1 = nn.LazyConvTranspose3d(64)
        self.mish1 = nn.Mish()
        self.avg_pool1 = nn.AvgPool1d(2)
        self.conv_transpose2 = nn.LazyConvTranspose3d(32)
        self.mish2 = nn.Mish()
        self.dropout2 = nn.Dropout(0.3)
        self.bn2 = nn.BatchNorm3d(32)
        self.conv_transpose3 = nn.LazyConvTranspose3d(16)
        self.mish3 = nn.Mish()
        self.avg_pool2 = nn.AvgPool1d(2)
        self.conv_transpose4 = nn.LazyConvTranspose3d(8)
        self.mish4 = nn.Mish()
        self.dropout3 = nn.Dropout(0.2)
        self.bn3 = nn.BatchNorm3d(8)
        self.conv_transpose5 = nn.LazyConvTranspose3d(4)
        self.mish5 = nn.Mish()

    def forward(self, x):
        # Assuming x is a tensor of shape (batch_size, sequence_length)
        x = self.embedding(x)  # (batch_size, sequence_length, 128)
        x = x.unsqueeze(1)  # (batch_size, 1, sequence_length, 128)
        x = x.unsqueeze(-1)  # (batch_size, 1, sequence_length, 128, 1)
        x = self.dropout1(x)
        x = self.bn1(x)
        x = self.conv_transpose1(x)  # (batch_size, 64, sequence_length, 128, 1)
        x = self.mish1(x)
        x = x.squeeze(-1)  # (batch_size, 64, sequence_length, 128)
        x = x.permute(0, 2, 1, 3)  # (batch_size, sequence_length, 64, 128)
        x = x.reshape(x.size(0), x.size(1), -1)  # (batch_size, sequence_length, 64*128)
        x = self.avg_pool1(x)  # (batch_size, sequence_length, 64*128/2)
        x = x.unsqueeze(-1)  # (batch_size, sequence_length, 64*128/2, 1)
        x = x.unsqueeze(1)  # (batch_size, 1, sequence_length, 64*128/2, 1)
        x = self.conv_transpose2(x)  # (batch_size, 32, sequence_length, 64*128/2, 1)
        x = self.mish2(x)
        x = self.dropout2(x)
        x = self.bn2(x)
        x = x.squeeze(-1)  # (batch_size, 32, sequence_length, 64*128/2)
        x = x.permute(0, 2, 1, 3)  # (batch_size, sequence_length, 32, 64*128/2)
        x = x.reshape(x.size(0), x.size(1), -1)  # (batch_size, sequence_length, 32*64*128/2)
        x = self.conv_transpose3(x.unsqueeze(-1).unsqueeze(1))  # (batch_size, 16, sequence_length, 32*64*128/2, 1)
        x = self.mish3(x)
        x = self.avg_pool2(x.squeeze(-1).permute(0, 2, 1, 3).reshape(x.size(0), x.size(2), -1))  # (batch_size, sequence_length, 16*32*64*128/4)
        x = x.unsqueeze(-1).unsqueeze(1)  # (batch_size, 1, sequence_length, 16*32*64*128/4, 1)
        x = self.conv_transpose4(x)  # (batch_size, 8, sequence_length, 16*32*64*128/4, 1)
        x = self.mish4(x)
        x = self.dropout3(x)
        x = self.bn3(x)
        x = x.squeeze(-1)  # (batch_size, 8, sequence_length, 16*32*64*128/4)
        x = x.permute(0, 2, 1, 3)  # (batch_size, sequence_length, 8, 16*32*64*128/4)
        x = x.reshape(x.size(0), x.size(1), -1)  # (batch_size, sequence_length, 8*16*32*64*128/4)
        x = self.conv_transpose5(x.unsqueeze(-1).unsqueeze(1))  # (batch_size, 4, sequence_length, 8*16*32*64*128/4, 1)
        x = self.mish5(x)
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (1, 128)).cuda()  # (batch_size, sequence_length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
