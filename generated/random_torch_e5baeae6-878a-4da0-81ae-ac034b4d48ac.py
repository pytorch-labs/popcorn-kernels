
# This is a random torch model generated by the following modules: ['RMSNorm', 'AdaptiveLogSoftmaxWithLoss', 'ConstantPad3d', 'Unfold']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.rms_norm1 = RMSNorm(64)  # Assuming input channels are 64
        self.rms_norm2 = RMSNorm(128)  # Assuming input channels are 128
        self.pad = nn.ConstantPad3d(1, 0.5)  # Padding with 0.5
        self.unfold = nn.Unfold(kernel_size=(3, 3), stride=(1, 1))
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(128, 10, [40, 80])  # Assuming 10 classes

    def forward(self, x):
        # Apply RMSNorm
        x = self.rms_norm1(x)
        
        # Apply ConstantPad3d
        x = self.pad(x)
        
        # Apply Unfold
        x = self.unfold(x)
        
        # Reshape to fit RMSNorm input
        x = x.view(-1, 128, x.size(2), x.size(3))
        
        # Apply RMSNorm again
        x = self.rms_norm2(x)
        
        # Reshape for AdaptiveLogSoftmaxWithLoss
        x = x.view(x.size(0), -1)
        
        # Apply AdaptiveLogSoftmaxWithLoss
        output = self.adaptive_log_softmax(x, torch.randint(0, 10, (x.size(0),)).to(x.device))
        
        return output.log_prob


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 64, 32, 32).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []


# Assuming RMSNorm is a custom module, here's a simple implementation
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.scale = dim ** -0.5
        self.eps = eps
        self.g = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        norm = torch.norm(x, p=2, dim=1, keepdim=True) * self.scale
        return x / norm.clamp(min=self.eps) * self.g
