
# This is a random torch model generated by the following modules: ['RNNCellBase', 'CELU', 'CTCLoss', 'PixelUnshuffle', 'AdaptiveMaxPool3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.rnn_cell = nn.RNNCellBase(input_size=64, hidden_size=128)
        self.celu = nn.CELU()
        self.pixel_unshuffle = nn.PixelUnshuffle(downscale_factor=2)
        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d(output_size=(8, 8, 8))
        self.ctc_loss = nn.CTCLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, sequence_length, input_size)
        batch_size, sequence_length, input_size = x.size()
        
        # Reshape for RNNCellBase
        x = x.view(-1, input_size)  # Flatten to (batch_size * sequence_length, input_size)
        hx = torch.zeros(batch_size * sequence_length, 128).to(x.device)  # Initialize hidden state
        
        # Apply RNNCellBase
        hx = self.rnn_cell(x, hx)
        
        # Reshape back to (batch_size, sequence_length, hidden_size)
        hx = hx.view(batch_size, sequence_length, -1)
        
        # Apply CELU
        hx = self.celu(hx)
        
        # Reshape for PixelUnshuffle
        # Assuming hx is of shape (batch_size, sequence_length, hidden_size)
        # We need to reshape it to (batch_size, channels, height, width)
        # Let's assume hidden_size is divisible by 4 for simplicity
        channels = hx.size(-1) // 4
        hx = hx.view(batch_size, sequence_length, channels, 2, 2)
        hx = hx.permute(0, 2, 1, 3, 4)  # Reshape to (batch_size, channels, sequence_length, 2, 2)
        
        # Apply PixelUnshuffle
        hx = self.pixel_unshuffle(hx)
        
        # Reshape for AdaptiveMaxPool3d
        # Assuming hx is of shape (batch_size, channels, sequence_length, height, width)
        hx = hx.permute(0, 2, 1, 3, 4)  # Reshape to (batch_size, sequence_length, channels, height, width)
        
        # Apply AdaptiveMaxPool3d
        hx = self.adaptive_max_pool3d(hx)
        
        # Reshape for CTCLoss
        # Assuming hx is of shape (batch_size, sequence_length, channels, 8, 8, 8)
        # We need to reshape it to (sequence_length, batch_size, num_classes)
        # Let's assume channels * 8 * 8 * 8 is the number of classes
        num_classes = hx.size(2) * hx.size(3) * hx.size(4) * hx.size(5)
        hx = hx.view(batch_size, sequence_length, -1)
        hx = hx.permute(1, 0, 2)  # Reshape to (sequence_length, batch_size, num_classes)
        
        # Dummy target for CTCLoss (assuming target sequence length is 10)
        target = torch.randint(1, num_classes, (batch_size, 10), dtype=torch.long).to(x.device)
        target_lengths = torch.full((batch_size,), 10, dtype=torch.long).to(x.device)
        input_lengths = torch.full((batch_size,), sequence_length, dtype=torch.long).to(x.device)
        
        # Apply CTCLoss
        loss = self.ctc_loss(hx, target, input_lengths, target_lengths)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 64).cuda()  # (batch_size, sequence_length, input_size)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

