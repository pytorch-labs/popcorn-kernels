
# This is a random torch model generated by the following modules: ['TransformerDecoder', 'Container', 'BatchNorm3d', 'CircularPad2d', 'Dropout', 'TransformerEncoderLayer', 'AdaptiveLogSoftmaxWithLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=512, nhead=8), num_layers=3
        )
        self.container = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
        )
        self.batch_norm3d = nn.BatchNorm3d(128)
        self.circular_pad2d = nn.CircularPad2d(2)
        self.dropout = nn.Dropout(0.5)
        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(128, 10, [50, 100])

    def forward(self, x):
        # Assuming input x is of shape (batch_size, seq_len, d_model)
        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, d_model)
        x = self.transformer_decoder(x, x)
        x = x.permute(1, 0, 2)  # Back to (batch_size, seq_len, d_model)
        
        # Pass through container
        x = self.container(x)
        
        # Reshape for BatchNorm3d
        x = x.unsqueeze(2).unsqueeze(3)  # Add dummy dimensions for 3D
        x = self.batch_norm3d(x)
        x = x.squeeze(3).squeeze(2)  # Remove dummy dimensions
        
        # CircularPad2d requires 4D input (batch_size, channels, height, width)
        x = x.unsqueeze(2).unsqueeze(3)  # Add dummy height and width dimensions
        x = self.circular_pad2d(x)
        x = x.squeeze(3).squeeze(2)  # Remove dummy dimensions
        
        # Dropout
        x = self.dropout(x)
        
        # TransformerEncoderLayer expects (seq_len, batch_size, d_model)
        x = x.permute(1, 0, 2)
        x = self.transformer_encoder_layer(x)
        x = x.permute(1, 0, 2)
        
        # AdaptiveLogSoftmaxWithLoss expects (batch_size, seq_len, d_model)
        x = x.mean(dim=1)  # Average over sequence length
        x = self.adaptive_log_softmax(x, torch.zeros(x.size(0), dtype=torch.long).to(x.device))
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(10, 20, 512).cuda()  # (batch_size, seq_len, d_model)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

