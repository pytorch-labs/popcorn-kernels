
# This is a random torch model generated by the following modules: ['Softmax', 'Conv3d', 'FractionalMaxPool3d', 'AdaptiveLogSoftmaxWithLoss', 'SmoothL1Loss', 'MaxUnpool2d', 'ReLU6', 'BatchNorm2d', 'PixelUnshuffle']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv3d_1 = nn.Conv3d(1, 10, kernel_size=3)
        self.batchnorm2d_1 = nn.BatchNorm2d(10)
        self.relu6_1 = nn.ReLU6()
        self.pixel_unshuffle_1 = nn.PixelUnshuffle(2)
        self.conv3d_2 = nn.Conv3d(10, 20, kernel_size=3)
        self.fractional_max_pool3d_1 = nn.FractionalMaxPool3d(kernel_size=2, output_size=(10, 10, 10))
        self.max_unpool2d_1 = nn.MaxUnpool2d(kernel_size=2, stride=2)
        self.smooth_l1_loss = nn.SmoothL1Loss()
        self.adaptive_log_softmax_with_loss = nn.AdaptiveLogSoftmaxWithLoss(20, 10)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        # Initial 3D convolution
        x = self.conv3d_1(x)
        
        # Reshape to 2D for BatchNorm2d
        x = x.view(x.size(0), x.size(1), -1, x.size(4))
        x = self.batchnorm2d_1(x)
        
        # Reshape back to 3D
        x = x.view(x.size(0), x.size(1), x.size(2) // x.size(3), x.size(3), -1)
        
        # Apply ReLU6
        x = self.relu6_1(x)
        
        # Pixel Unshuffle
        x = x.view(x.size(0), x.size(1), -1, x.size(4))
        x = self.pixel_unshuffle_1(x)
        
        # Reshape back to 3D
        x = x.view(x.size(0), x.size(1), x.size(2) // x.size(3), x.size(3), -1)
        
        # Second 3D convolution
        x = self.conv3d_2(x)
        
        # Fractional Max Pooling
        x = self.fractional_max_pool3d_1(x)
        
        # Reshape to 2D for MaxUnpool2d
        x = x.view(x.size(0), x.size(1), -1, x.size(4))
        x, indices = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)
        x = self.max_unpool2d_1(x, indices)
        
        # Reshape back to 3D
        x = x.view(x.size(0), x.size(1), x.size(2) // x.size(3), x.size(3), -1)
        
        # Smooth L1 Loss (assuming target is same as input for simplicity)
        target = torch.zeros_like(x)
        loss = self.smooth_l1_loss(x, target)
        
        # Adaptive Log Softmax with Loss
        x = x.view(x.size(0), -1)
        x = self.adaptive_log_softmax_with_loss(x, torch.zeros(x.size(0), dtype=torch.long))
        
        # Softmax
        x = self.softmax(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 32, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
