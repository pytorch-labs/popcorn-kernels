
# This is a random torch model generated by the following modules: ['BatchNorm2d', 'GRUCell', 'MultiheadAttention', 'Unfold', 'Conv2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.unfold = nn.Unfold(kernel_size=2, stride=2)
        self.gru_cell = nn.GRUCell(32 * 2 * 2, 64)
        self.multihead_attn = nn.MultiheadAttention(embed_dim=64, num_heads=8)
        self.fc = nn.Linear(64, 10)

    def forward(self, x):
        # Apply Conv2d and BatchNorm2d
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        
        # Unfold the feature maps
        x = self.unfold(x)
        x = x.permute(0, 2, 1)  # Reshape for GRUCell
        
        # Process each patch with GRUCell
        batch_size, num_patches, _ = x.shape
        hx = torch.zeros(batch_size, 64).to(x.device)
        for i in range(num_patches):
            hx = self.gru_cell(x[:, i, :], hx)
        
        # Apply MultiheadAttention
        hx = hx.unsqueeze(0)  # Add sequence dimension
        attn_output, _ = self.multihead_attn(hx, hx, hx)
        attn_output = attn_output.squeeze(0)
        
        # Final classification layer
        x = self.fc(attn_output)
        return F.log_softmax(x, dim=1)


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
