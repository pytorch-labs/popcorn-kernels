
# This is a random torch model generated by the following modules: ['Conv2d', 'AvgPool2d', 'Transformer', 'TripletMarginLoss', 'CircularPad3d', 'AdaptiveMaxPool2d', 'LazyConv3d', 'EmbeddingBag']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.transformer = nn.Transformer(d_model=64, nhead=8, num_encoder_layers=3, num_decoder_layers=3)
        self.circular_pad = nn.CircularPad3d(padding=(1, 1, 1, 1, 1, 1))
        self.adaptive_maxpool = nn.AdaptiveMaxPool2d((5, 5))
        self.lazy_conv3d = nn.LazyConv3d(out_channels=32, kernel_size=3, stride=1, padding=1)
        self.embedding_bag = nn.EmbeddingBag(num_embeddings=1000, embedding_dim=64, mode='mean')
        self.triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)

    def forward(self, x):
        # Assuming x is a 4D tensor (batch_size, channels, height, width)
        x = self.conv1(x)
        x = self.avgpool1(x)
        
        # Reshape for Transformer
        batch_size, channels, height, width = x.shape
        x = x.view(batch_size, channels, -1).permute(2, 0, 1)  # (seq_len, batch_size, channels)
        x = self.transformer(x, x)
        
        # Reshape back to 4D
        x = x.permute(1, 2, 0).view(batch_size, channels, height, width)
        
        # Apply CircularPad3d
        x = x.unsqueeze(2)  # Add a dummy dimension for 3D padding
        x = self.circular_pad(x)
        x = x.squeeze(2)  # Remove the dummy dimension
        
        # Apply AdaptiveMaxPool2d
        x = self.adaptive_maxpool(x)
        
        # Reshape for LazyConv3d
        x = x.unsqueeze(2)  # Add a dummy dimension for 3D convolution
        x = self.lazy_conv3d(x)
        x = x.squeeze(2)  # Remove the dummy dimension
        
        # Reshape for EmbeddingBag
        x = x.view(batch_size, -1)  # Flatten for embedding
        x = self.embedding_bag(x.long())  # Convert to long for embedding
        
        # TripletMarginLoss requires three inputs: anchor, positive, negative
        anchor = x
        positive = torch.roll(x, shifts=1, dims=0)
        negative = torch.roll(x, shifts=2, dims=0)
        loss = self.triplet_loss(anchor, positive, negative)
        
        return loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
