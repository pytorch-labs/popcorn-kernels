
# This is a random torch model generated by the following modules: ['AvgPool1d', 'Unflatten', 'Conv3d', 'BatchNorm1d', 'Hardsigmoid', 'GRUCell', 'ZeroPad2d', 'Embedding']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.zero_pad2d = nn.ZeroPad2d(2)  # ZeroPad2d layer
        self.conv3d_1 = nn.Conv3d(1, 16, kernel_size=(3, 3, 3))  # Conv3d layer
        self.conv3d_2 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3))  # Conv3d layer
        self.avg_pool1d = nn.AvgPool1d(kernel_size=2)  # AvgPool1d layer
        self.batch_norm1d = nn.BatchNorm1d(32)  # BatchNorm1d layer
        self.gru_cell = nn.GRUCell(32, 64)  # GRUCell layer
        self.hardsigmoid = nn.Hardsigmoid()  # Hardsigmoid layer
        self.unflatten = nn.Unflatten(1, (8, 8))  # Unflatten layer

    def forward(self, x):
        # Assume x is a 1D tensor of indices for embedding
        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)
        x = x.unsqueeze(1)  # Add a channel dimension for Conv3d: (batch_size, 1, seq_len, embedding_dim, 1)
        x = self.zero_pad2d(x)  # Pad the spatial dimensions: (batch_size, 1, seq_len + 4, embedding_dim + 4, 1)
        x = self.conv3d_1(x)  # Apply Conv3d: (batch_size, 16, seq_len + 2, embedding_dim + 2, 1)
        x = self.conv3d_2(x)  # Apply Conv3d: (batch_size, 32, seq_len, embedding_dim, 1)
        x = x.squeeze(-1)  # Remove the last dimension: (batch_size, 32, seq_len, embedding_dim)
        x = x.mean(dim=-1)  # Average over the embedding dimension: (batch_size, 32, seq_len)
        x = self.avg_pool1d(x)  # Apply AvgPool1d: (batch_size, 32, seq_len // 2)
        x = self.batch_norm1d(x)  # Apply BatchNorm1d: (batch_size, 32, seq_len // 2)
        x = x.transpose(1, 2)  # Transpose for GRUCell: (batch_size, seq_len // 2, 32)
        hx = torch.zeros(x.size(0), 64).to(x.device)  # Initialize hidden state for GRUCell
        for t in range(x.size(1)):
            hx = self.gru_cell(x[:, t, :], hx)  # Apply GRUCell
        x = self.hardsigmoid(hx)  # Apply Hardsigmoid
        x = self.unflatten(x)  # Unflatten: (batch_size, 8, 8)
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # (batch_size, seq_len)
    return [x]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

