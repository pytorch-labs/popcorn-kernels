
# This is a random torch model generated by the following modules: ['TransformerDecoder', 'FractionalMaxPool3d', 'FeatureAlphaDropout']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        # Define the TransformerDecoder with 4 layers
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=512, nhead=8), num_layers=4
        )
        # Define the FractionalMaxPool3d layer
        self.fractional_max_pool3d = nn.FractionalMaxPool3d(kernel_size=2, output_size=(8, 8, 8))
        # Define the FeatureAlphaDropout layer
        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)

    def forward(self, x):
        # Assuming input x is of shape (batch_size, seq_len, d_model)
        # Reshape x to fit the TransformerDecoder input shape
        x = x.view(-1, 10, 512)  # Reshape to (batch_size, seq_len=10, d_model=512)
        
        # Create a dummy memory tensor for the TransformerDecoder
        memory = torch.randn(x.size(0), 10, 512).to(x.device)
        
        # Pass through the TransformerDecoder
        x = self.transformer_decoder(x, memory)
        
        # Reshape x to fit the FractionalMaxPool3d input shape
        x = x.view(-1, 512, 10, 10, 10)  # Reshape to (batch_size, channels=512, depth=10, height=10, width=10)
        
        # Pass through the FractionalMaxPool3d
        x = self.fractional_max_pool3d(x)
        
        # Reshape x to fit the FeatureAlphaDropout input shape
        x = x.view(-1, 512, 8, 8, 8)  # Reshape to (batch_size, channels=512, depth=8, height=8, width=8)
        
        # Pass through the FeatureAlphaDropout
        x = self.feature_alpha_dropout(x)
        
        # Flatten the output for final processing
        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 512*8*8*8)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 512).cuda()  # Input shape (batch_size=1, seq_len=10, d_model=512)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

