
# This is a random torch model generated by the following modules: ['BCELoss', 'GRU', 'LazyBatchNorm2d', 'Dropout', 'SmoothL1Loss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.gru1 = nn.GRU(input_size=128, hidden_size=256, num_layers=2, batch_first=True)
        self.gru2 = nn.GRU(input_size=256, hidden_size=128, num_layers=1, batch_first=True)
        self.bn1 = nn.LazyBatchNorm2d()
        self.bn2 = nn.LazyBatchNorm2d()
        self.dropout1 = nn.Dropout(0.5)
        self.dropout2 = nn.Dropout(0.3)
        self.bce_loss = nn.BCELoss()
        self.smooth_l1_loss = nn.SmoothL1Loss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, sequence_length, feature_dim)
        batch_size, seq_len, feature_dim = x.size()
        
        # Reshape for GRU
        x, _ = self.gru1(x)  # Output shape: (batch_size, seq_len, 256)
        x = self.dropout1(x)
        
        # Reshape for BatchNorm
        x = x.view(batch_size, seq_len, 256, 1)  # Reshape to (batch_size, seq_len, 256, 1)
        x = self.bn1(x)
        x = x.view(batch_size, seq_len, 256)  # Reshape back
        
        x, _ = self.gru2(x)  # Output shape: (batch_size, seq_len, 128)
        x = self.dropout2(x)
        
        # Reshape for BatchNorm
        x = x.view(batch_size, seq_len, 128, 1)  # Reshape to (batch_size, seq_len, 128, 1)
        x = self.bn2(x)
        x = x.view(batch_size, seq_len, 128)  # Reshape back
        
        # Compute BCE Loss (dummy target for demonstration)
        target_bce = torch.randint(0, 2, (batch_size, seq_len, 128)).float()
        bce_loss = self.bce_loss(torch.sigmoid(x), target_bce)
        
        # Compute Smooth L1 Loss (dummy target for demonstration)
        target_smooth_l1 = torch.randn_like(x)
        smooth_l1_loss = self.smooth_l1_loss(x, target_smooth_l1)
        
        # Return the sum of losses for demonstration purposes
        return bce_loss + smooth_l1_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 128).cuda()  # Example input: (batch_size=1, sequence_length=10, feature_dim=128)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
