
# This is a random torch model generated by the following modules: ['ConstantPad1d', 'ReplicationPad2d', 'LazyConv3d', 'Dropout2d', 'RNNCell', 'GaussianNLLLoss', 'Upsample', 'MaxPool2d', 'NLLLoss', 'Module']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.pad1d = nn.ConstantPad1d(2, 3.0)
        self.pad2d = nn.ReplicationPad2d(1)
        self.conv3d = nn.LazyConv3d(out_channels=16, kernel_size=3)
        self.dropout2d = nn.Dropout2d(p=0.5)
        self.rnn_cell = nn.RNNCell(input_size=16, hidden_size=32)
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.nll_loss = nn.NLLLoss()
        self.gaussian_nll_loss = nn.GaussianNLLLoss()

    def forward(self, x):
        # Apply ConstantPad1d
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.pad1d(x)
        x = x.squeeze(1)  # Remove the channel dimension

        # Apply ReplicationPad2d
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.pad2d(x)
        x = x.squeeze(1)  # Remove the channel dimension

        # Reshape for LazyConv3d
        x = x.unsqueeze(1)  # Add a channel dimension
        x = x.unsqueeze(2)  # Add a depth dimension
        x = self.conv3d(x)
        x = x.squeeze(2)  # Remove the depth dimension

        # Apply Dropout2d
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.dropout2d(x)
        x = x.squeeze(1)  # Remove the channel dimension

        # Reshape for RNNCell
        batch_size, *rest = x.shape
        x = x.view(batch_size, -1)  # Flatten the input for RNNCell
        hx = torch.zeros(batch_size, 32).to(x.device)  # Initialize hidden state
        x = self.rnn_cell(x, hx)

        # Reshape for Upsample
        x = x.unsqueeze(1).unsqueeze(1)  # Add height and width dimensions
        x = self.upsample(x)
        x = x.squeeze(1).squeeze(1)  # Remove height and width dimensions

        # Reshape for MaxPool2d
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.maxpool2d(x)
        x = x.squeeze(1)  # Remove the channel dimension

        # Compute NLLLoss (requires log probabilities and target)
        log_probs = F.log_softmax(x, dim=1)
        target = torch.randint(0, x.size(1), (batch_size,)).to(x.device)
        nll_loss = self.nll_loss(log_probs, target)

        # Compute GaussianNLLLoss (requires input, target, and var)
        var = torch.ones_like(x)
        gaussian_nll_loss = self.gaussian_nll_loss(x, x, var)

        return nll_loss, gaussian_nll_loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
