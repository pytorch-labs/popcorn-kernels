
# This is a random torch model generated by the following modules: ['CELU', 'NLLLoss', 'Softplus', 'Hardshrink']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.celu1 = nn.CELU()
        self.celu2 = nn.CELU()
        self.softplus1 = nn.Softplus()
        self.softplus2 = nn.Softplus()
        self.hardshrink1 = nn.Hardshrink()
        self.hardshrink2 = nn.Hardshrink()
        self.nllloss = nn.NLLLoss()

    def forward(self, x):
        # Apply CELU activation
        x = self.celu1(x)
        x = self.celu2(x)
        
        # Apply Softplus activation
        x = self.softplus1(x)
        x = self.softplus2(x)
        
        # Apply Hardshrink activation
        x = self.hardshrink1(x)
        x = self.hardshrink2(x)
        
        # Flatten the tensor for NLLLoss
        x = x.view(x.size(0), -1)
        
        # Apply log_softmax for NLLLoss compatibility
        x = F.log_softmax(x, dim=1)
        
        # Assuming a target tensor for NLLLoss (this is just a placeholder)
        target = torch.zeros(x.size(0), dtype=torch.long).to(x.device)
        
        # Compute NLLLoss
        loss = self.nllloss(x, target)
        
        return loss

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
