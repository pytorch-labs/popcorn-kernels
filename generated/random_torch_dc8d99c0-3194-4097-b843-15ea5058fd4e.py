
# This is a random torch model generated by the following modules: ['Embedding', 'SiLU', 'MaxUnpool2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.silu1 = nn.SiLU()  # SiLU activation
        self.silu2 = nn.SiLU()  # SiLU activation
        self.max_unpool2d = nn.MaxUnpool2d(kernel_size=2, stride=2)  # MaxUnpool2d layer

    def forward(self, x):
        # Assume input x is a 1D tensor of indices for embedding
        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)
        
        # Reshape to 4D tensor for MaxUnpool2d
        x = x.unsqueeze(1)  # Shape: (batch_size, 1, seq_len, embedding_dim)
        
        # Apply SiLU activation
        x = self.silu1(x)
        
        # Dummy pooling indices for MaxUnpool2d
        pool_output, indices = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)
        
        # Apply MaxUnpool2d
        x = self.max_unpool2d(pool_output, indices)
        
        # Apply another SiLU activation
        x = self.silu2(x)
        
        # Reshape back to 2D tensor
        x = x.squeeze(1)  # Shape: (batch_size, seq_len, embedding_dim)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # Example input: (batch_size, seq_len)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

