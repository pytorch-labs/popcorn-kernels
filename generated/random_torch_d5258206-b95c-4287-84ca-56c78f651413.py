
# This is a random torch model generated by the following modules: ['EmbeddingBag', 'Dropout', 'SELU', 'ConvTranspose1d', 'LazyConv1d', 'Unflatten', 'ELU', 'CELU', 'ModuleList', 'BCELoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding_bag = nn.EmbeddingBag(num_embeddings=1000, embedding_dim=64, mode='mean')
        self.dropout = nn.Dropout(p=0.5)
        self.selu = nn.SELU()
        self.conv_transpose1d = nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=3, stride=2)
        self.lazy_conv1d = nn.LazyConv1d(out_channels=16, kernel_size=3)
        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(16, 8))
        self.elu = nn.ELU(alpha=1.0)
        self.celu = nn.CELU(alpha=1.0)
        self.module_list = nn.ModuleList([nn.Linear(128, 64), nn.Linear(64, 32)])
        self.bce_loss = nn.BCELoss()

    def forward(self, x):
        # Assume x is a tensor of arbitrary shape
        x = x.view(-1)  # Flatten the input to 1D for EmbeddingBag
        x = self.embedding_bag(x.view(-1, 1).long())  # Convert to long for embedding
        x = self.dropout(x)
        x = self.selu(x)
        
        # Reshape for ConvTranspose1d
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.conv_transpose1d(x)
        
        # Apply LazyConv1d
        x = self.lazy_conv1d(x)
        
        # Unflatten the tensor
        x = self.unflatten(x)
        
        # Apply ELU and CELU
        x = self.elu(x)
        x = self.celu(x)
        
        # Flatten the tensor for the ModuleList
        x = x.view(x.size(0), -1)
        
        # Apply layers from ModuleList
        for layer in self.module_list:
            x = layer(x)
        
        # Apply BCE loss (assuming x is logits and we have a target)
        # For demonstration, we'll create a dummy target
        target = torch.sigmoid(torch.randn_like(x))  # Random target for BCE loss
        loss = self.bce_loss(torch.sigmoid(x), target)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(10, 100).cuda()  # Arbitrary input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
