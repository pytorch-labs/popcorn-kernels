
# This is a random torch model generated by the following modules: ['GaussianNLLLoss', 'Tanh', 'ELU', 'MaxPool3d', 'LeakyReLU', 'Container', 'UpsamplingBilinear2d', 'GRU', 'GRUCell']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.maxpool3d = nn.MaxPool3d(kernel_size=2)
        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)
        self.gru = nn.GRU(input_size=64, hidden_size=128, num_layers=2, batch_first=True)
        self.gru_cell = nn.GRUCell(input_size=128, hidden_size=64)
        self.tanh = nn.Tanh()
        self.elu = nn.ELU()
        self.leaky_relu = nn.LeakyReLU()
        self.container = nn.Sequential(
            nn.Linear(64, 32),
            nn.LeakyReLU(),
            nn.Linear(32, 16)
        )
        self.gaussian_nll_loss = nn.GaussianNLLLoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, depth, height, width)
        x = self.maxpool3d(x)  # Apply MaxPool3d
        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions
        x = x.permute(0, 2, 1)  # Reshape for GRU (batch_size, seq_len, input_size)
        
        # Apply GRU
        x, _ = self.gru(x)
        
        # Apply GRUCell to each time step
        hx = torch.zeros(x.size(0), 64).to(x.device)  # Initialize hidden state
        outputs = []
        for i in range(x.size(1)):
            hx = self.gru_cell(x[:, i, :], hx)
            outputs.append(hx)
        x = torch.stack(outputs, dim=1)
        
        # Apply Tanh, ELU, and LeakyReLU
        x = self.tanh(x)
        x = self.elu(x)
        x = self.leaky_relu(x)
        
        # Reshape for UpsamplingBilinear2d
        x = x.view(x.size(0), x.size(1), 8, 8)  # Reshape to (batch_size, channels, height, width)
        x = self.upsample(x)
        
        # Apply Container (Sequential)
        x = x.view(x.size(0), -1)  # Flatten for Linear layers
        x = self.container(x)
        
        # GaussianNLLLoss is typically used for loss computation, not in forward pass
        # So we return the output and let the user compute the loss if needed
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 16, 32, 32).cuda()  # Example input shape (batch_size, channels, depth, height, width)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
