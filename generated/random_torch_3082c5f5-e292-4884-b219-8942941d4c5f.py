
# This is a random torch model generated by the following modules: ['ReplicationPad3d', 'MaxUnpool1d', 'GRUCell', 'ModuleDict', 'Mish', 'MaxPool1d', 'Unflatten', 'LazyBatchNorm3d']
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.replication_pad = nn.ReplicationPad3d(1)
        self.max_pool = nn.MaxPool1d(kernel_size=2, return_indices=True)
        self.max_unpool = nn.MaxUnpool1d(kernel_size=2)
        self.gru_cell = nn.GRUCell(input_size=10, hidden_size=20)
        self.module_dict = nn.ModuleDict({
            'mish': nn.Mish(),
            'lazy_batch_norm': nn.LazyBatchNorm3d()
        })
        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(10, 2))

    def forward(self, x):
        # Apply ReplicationPad3d
        x = self.replication_pad(x)
        
        # Reshape for MaxPool1d
        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions
        
        # Apply MaxPool1d and save indices for MaxUnpool1d
        x, indices = self.max_pool(x)
        
        # Apply MaxUnpool1d
        x = self.max_unpool(x, indices)
        
        # Reshape for GRUCell
        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch
        x = self.gru_cell(x, torch.zeros(x.size(0), 20).to(x.device))
        
        # Apply ModuleDict layers
        x = self.module_dict['mish'](x)
        x = x.unsqueeze(2).unsqueeze(3).unsqueeze(4)  # Reshape for LazyBatchNorm3d
        x = self.module_dict['lazy_batch_norm'](x)
        
        # Apply Unflatten
        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch
        x = self.unflatten(x)
        
        return x

def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 10, 10, 10).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
