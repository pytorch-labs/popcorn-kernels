
# This is a random torch model generated by the following modules: ['Embedding', 'Tanh', 'LazyLinear', 'Mish', 'BCELoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self, vocab_size: int = 1000, embedding_dim: int = 128) -> None:
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lazy_linear1 = nn.LazyLinear(256)
        self.lazy_linear2 = nn.LazyLinear(128)
        self.lazy_linear3 = nn.LazyLinear(64)
        self.lazy_linear4 = nn.LazyLinear(32)
        self.lazy_linear5 = nn.LazyLinear(1)
        self.tanh = nn.Tanh()
        self.mish = nn.Mish()
        self.bce_loss = nn.BCELoss()

    def forward(self, x):
        # Assume input x is a tensor of integers (indices for embedding)
        x = self.embedding(x)
        
        # Flatten the tensor to 2D (batch_size, seq_len * embedding_dim)
        x = x.view(x.size(0), -1)
        
        # Apply LazyLinear layers with activation functions
        x = self.mish(self.lazy_linear1(x))
        x = self.tanh(self.lazy_linear2(x))
        x = self.mish(self.lazy_linear3(x))
        x = self.tanh(self.lazy_linear4(x))
        x = torch.sigmoid(self.lazy_linear5(x))  # Sigmoid to get output in [0, 1] for BCELoss
        
        return x

    def compute_loss(self, output, target):
        return self.bce_loss(output, target.float())


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # (batch_size, seq_len)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

