
# This is a random torch model generated by the following modules: ['NLLLoss2d', 'LogSigmoid', 'AdaptiveMaxPool1d', 'UpsamplingBilinear2d', 'Conv1d', 'TransformerDecoderLayer', 'Softshrink', 'Embedding']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.conv1d = nn.Conv1d(128, 64, kernel_size=3, padding=1)  # Conv1d layer
        self.adaptive_max_pool1d = nn.AdaptiveMaxPool1d(10)  # AdaptiveMaxPool1d layer
        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=64, nhead=8)  # TransformerDecoderLayer
        self.upsampling_bilinear2d = nn.UpsamplingBilinear2d(scale_factor=2)  # UpsamplingBilinear2d layer
        self.softshrink = nn.Softshrink(lambd=0.5)  # Softshrink layer
        self.log_sigmoid = nn.LogSigmoid()  # LogSigmoid layer
        self.nll_loss2d = nn.NLLLoss2d()  # NLLLoss2d layer (used for loss calculation)

    def forward(self, x):
        # Assume input x is a tensor of arbitrary shape
        # First, reshape the input to be compatible with the embedding layer
        x = x.long()  # Convert to long for embedding
        x = self.embedding(x)  # Apply embedding
        x = x.permute(0, 2, 1)  # Permute for Conv1d
        x = self.conv1d(x)  # Apply Conv1d
        x = self.adaptive_max_pool1d(x)  # Apply AdaptiveMaxPool1d
        x = x.permute(2, 0, 1)  # Permute for TransformerDecoderLayer
        x = self.transformer_decoder_layer(x, x)  # Apply TransformerDecoderLayer
        x = x.permute(1, 2, 0)  # Permute back
        x = x.unsqueeze(2)  # Add a dimension for UpsamplingBilinear2d
        x = self.upsampling_bilinear2d(x)  # Apply UpsamplingBilinear2d
        x = x.squeeze(2)  # Remove the added dimension
        x = self.softshrink(x)  # Apply Softshrink
        x = self.log_sigmoid(x)  # Apply LogSigmoid
        # For NLLLoss2d, we need a target tensor, so we'll just return x for now
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (1, 32)).cuda()  # Random input tensor for embedding
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

