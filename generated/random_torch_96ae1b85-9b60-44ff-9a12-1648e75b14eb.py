
# This is a random torch model generated by the following modules: ['NLLLoss', 'ELU', 'RNN', 'Sequential', 'ConvTranspose2d', 'Bilinear', 'Fold', 'L1Loss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.elu = nn.ELU()
        self.rnn = nn.RNN(input_size=64, hidden_size=128, num_layers=2, batch_first=True)
        self.sequential = nn.Sequential(
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ELU(),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.ELU()
        )
        self.bilinear = nn.Bilinear(32, 32, 16)
        self.fold = nn.Fold(output_size=(32, 32), kernel_size=(4, 4), stride=(2, 2))
        self.l1_loss = nn.L1Loss()
        self.nll_loss = nn.NLLLoss()

    def forward(self, x):
        # Apply ELU activation
        x = self.elu(x)
        
        # Reshape for RNN
        batch_size, channels, height, width = x.size()
        x = x.view(batch_size, -1, 64)  # Reshape to (batch_size, seq_len, input_size)
        
        # Pass through RNN
        x, _ = self.rnn(x)
        
        # Reshape back to 4D tensor for ConvTranspose2d
        x = x.view(batch_size, 128, height // 2, width // 2)
        
        # Pass through Sequential with ConvTranspose2d and ELU
        x = self.sequential(x)
        
        # Reshape for Bilinear
        x = x.view(batch_size, 32, -1)
        x = self.bilinear(x, x)
        
        # Reshape for Fold
        x = x.view(batch_size, -1, 32 * 32)
        x = self.fold(x)
        
        # Compute L1 Loss with a dummy target
        dummy_target = torch.zeros_like(x)
        l1_loss_value = self.l1_loss(x, dummy_target)
        
        # Compute NLL Loss with a dummy target
        log_probs = F.log_softmax(x.view(batch_size, -1), dim=1)
        dummy_target_class = torch.zeros(batch_size, dtype=torch.long).to(x.device)
        nll_loss_value = self.nll_loss(log_probs, dummy_target_class)
        
        # Return both losses
        return l1_loss_value, nll_loss_value


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
