
# This is a random torch model generated by the following modules: ['ReplicationPad2d', 'ZeroPad3d', 'TransformerDecoderLayer', 'Upsample', 'Bilinear', 'SELU', 'Softplus', 'LazyConv1d', 'CTCLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.replication_pad = nn.ReplicationPad2d(2)
        self.zero_pad = nn.ZeroPad3d(1)
        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=64, nhead=8)
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.bilinear = nn.Bilinear(64, 64, 128)
        self.selu = nn.SELU()
        self.softplus = nn.Softplus()
        self.lazy_conv1d = nn.LazyConv1d(out_channels=32, kernel_size=3)
        self.ctc_loss = nn.CTCLoss()

    def forward(self, x):
        # Apply ReplicationPad2d
        x = self.replication_pad(x)
        
        # Reshape for ZeroPad3d
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.zero_pad(x)
        
        # Reshape for TransformerDecoderLayer
        x = x.view(x.size(0), -1, 64)  # Reshape to (batch_size, seq_len, d_model)
        memory = torch.randn_like(x)  # Random memory for TransformerDecoderLayer
        x = self.transformer_decoder_layer(x, memory)
        
        # Reshape for Upsample
        x = x.view(x.size(0), 64, 8, 8)  # Reshape to (batch_size, channels, height, width)
        x = self.upsample(x)
        
        # Reshape for Bilinear
        x = x.view(x.size(0), 64, -1)  # Reshape to (batch_size, 64, features)
        x2 = torch.randn_like(x)  # Random second input for Bilinear
        x = self.bilinear(x, x2)
        
        # Apply SELU
        x = self.selu(x)
        
        # Apply Softplus
        x = self.softplus(x)
        
        # Reshape for LazyConv1d
        x = x.view(x.size(0), -1, 128)  # Reshape to (batch_size, channels, seq_len)
        x = self.lazy_conv1d(x)
        
        # Reshape for CTC Loss (dummy target and input lengths)
        log_probs = x.log_softmax(2)  # Apply log_softmax for CTC Loss
        targets = torch.randint(0, 32, (x.size(1),), dtype=torch.long)  # Random targets
        input_lengths = torch.full((x.size(0),), x.size(1), dtype=torch.long)
        target_lengths = torch.randint(1, x.size(1), (x.size(0),), dtype=torch.long)
        loss = self.ctc_loss(log_probs, targets, input_lengths, target_lengths)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()  # Arbitrary input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
