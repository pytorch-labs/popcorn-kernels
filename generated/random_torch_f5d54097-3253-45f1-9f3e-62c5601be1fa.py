
# This is a random torch model generated by the following modules: ['GELU', 'CircularPad3d', 'ModuleList', 'SyncBatchNorm', 'Embedding']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer
        self.circular_pad = nn.ModuleList([nn.CircularPad3d(1) for _ in range(2)])  # CircularPad3d repeated twice
        self.sync_batch_norm = nn.SyncBatchNorm(128)  # SyncBatchNorm layer
        self.gelu = nn.GELU()  # GELU activation

    def forward(self, x):
        # Assume input is of shape (batch_size, sequence_length)
        x = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)
        x = x.unsqueeze(1)  # Add a channel dimension: (batch_size, 1, sequence_length, embedding_dim)
        
        # Apply CircularPad3d twice
        for pad in self.circular_pad:
            x = pad(x)  # Shape remains the same after padding
        
        # Reshape for SyncBatchNorm
        x = x.permute(0, 2, 3, 1)  # Shape: (batch_size, sequence_length, embedding_dim, 1)
        x = self.sync_batch_norm(x)  # Apply SyncBatchNorm
        x = x.permute(0, 3, 1, 2)  # Shape: (batch_size, 1, sequence_length, embedding_dim)
        
        # Apply GELU activation
        x = self.gelu(x)
        
        # Flatten the output
        x = x.view(x.size(0), -1)  # Shape: (batch_size, sequence_length * embedding_dim)
        
        return x


def get_inputs():
    # Randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (10, 20)).cuda()  # Example input: (batch_size=10, sequence_length=20)
    return [x]


def get_init_inputs():
    # Randomly generate tensors required for initialization based on the model architecture
    return []

