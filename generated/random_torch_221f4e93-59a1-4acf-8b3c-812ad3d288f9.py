
# This is a random torch model generated by the following modules: ['Embedding', 'AdaptiveMaxPool3d', 'ReplicationPad1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Embedding layer with vocab size 1000 and embedding dim 128
        self.pad1 = nn.ReplicationPad1d(2)  # ReplicationPad1d with padding size 2
        self.pad2 = nn.ReplicationPad1d(1)  # ReplicationPad1d with padding size 1
        self.pool1 = nn.AdaptiveMaxPool3d((10, 10, 10))  # AdaptiveMaxPool3d to output size (10, 10, 10)
        self.pool2 = nn.AdaptiveMaxPool3d((5, 5, 5))  # AdaptiveMaxPool3d to output size (5, 5, 5)

    def forward(self, x):
        # Assume input x is a tensor of arbitrary shape
        # First, we need to convert the input to a suitable shape for the embedding layer
        # For simplicity, let's assume x is a 1D tensor of indices
        x = x.long()  # Convert to long tensor for embedding
        x = self.embedding(x)  # Apply embedding layer
        
        # Reshape the output to a 3D tensor for padding and pooling
        x = x.unsqueeze(1)  # Add a channel dimension
        x = x.unsqueeze(-1)  # Add a depth dimension
        
        # Apply padding layers
        x = self.pad1(x)
        x = self.pad2(x)
        
        # Apply pooling layers
        x = self.pool1(x)
        x = self.pool2(x)
        
        # Flatten the output for final processing
        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (1, 32)).cuda()  # Random indices for embedding layer
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

