
# This is a random torch model generated by the following modules: ['Softsign', 'GroupNorm', 'LeakyReLU', 'AdaptiveLogSoftmaxWithLoss', 'ZeroPad2d', 'Dropout3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.softsign = nn.Softsign()
        self.group_norm1 = nn.GroupNorm(2, 10)  # Assuming 10 channels for GroupNorm
        self.leaky_relu = nn.LeakyReLU(0.1)
        self.zero_pad2d = nn.ZeroPad2d(2)
        self.dropout3d = nn.Dropout3d(0.5)
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(10, 5, [2, 3])  # Assuming 10 classes and 5 in_features

    def forward(self, x):
        # Apply ZeroPad2d to the input
        x = self.zero_pad2d(x)
        
        # Apply Dropout3d (assuming input is 5D: batch, channels, depth, height, width)
        if x.dim() == 4:
            x = x.unsqueeze(2)  # Add a depth dimension
        x = self.dropout3d(x)
        
        # Apply GroupNorm (assuming input is 4D: batch, channels, height, width)
        if x.dim() == 5:
            x = x.squeeze(2)  # Remove the depth dimension
        x = self.group_norm1(x)
        
        # Apply LeakyReLU
        x = self.leaky_relu(x)
        
        # Apply Softsign
        x = self.softsign(x)
        
        # Reshape for AdaptiveLogSoftmaxWithLoss
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = self.adaptive_log_softmax.log_prob(x)  # Apply AdaptiveLogSoftmax
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 32, 32).cuda()  # Assuming 10 channels and 32x32 spatial dimensions
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
