
# This is a random torch model generated by the following modules: ['UpsamplingNearest2d', 'InstanceNorm2d', 'CosineEmbeddingLoss', 'Hardtanh', 'GaussianNLLLoss', 'AdaptiveMaxPool2d', 'ParameterList', 'NLLLoss2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.upsample = nn.UpsamplingNearest2d(scale_factor=2)
        self.instance_norm = nn.InstanceNorm2d(3)
        self.hardtanh = nn.Hardtanh(min_val=-1.0, max_val=1.0)
        self.adaptive_max_pool = nn.AdaptiveMaxPool2d((16, 16))
        self.parameter_list = nn.ParameterList([nn.Parameter(torch.randn(3, 3)) for _ in range(3)])
        
        # Loss functions are not typically used in the forward pass of a model,
        # but we will include them for demonstration purposes.
        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()
        self.gaussian_nll_loss = nn.GaussianNLLLoss()
        self.nll_loss_2d = nn.NLLLoss2d()

    def forward(self, x):
        # Upsample the input
        x = self.upsample(x)
        
        # Apply instance normalization
        x = self.instance_norm(x)
        
        # Apply Hardtanh activation
        x = self.hardtanh(x)
        
        # Apply adaptive max pooling
        x = self.adaptive_max_pool(x)
        
        # Apply parameters from ParameterList
        for param in self.parameter_list:
            x = x + param.unsqueeze(0).unsqueeze(0).expand_as(x)
        
        # Compute cosine embedding loss (dummy computation)
        target = torch.ones(x.size(0), dtype=torch.float32, device=x.device)
        cosine_loss = self.cosine_embedding_loss(x.view(x.size(0), -1), x.view(x.size(0), -1), target)
        
        # Compute Gaussian NLL loss (dummy computation)
        mean = torch.zeros_like(x)
        var = torch.ones_like(x)
        gaussian_loss = self.gaussian_nll_loss(x, mean, var)
        
        # Compute NLL loss 2D (dummy computation)
        log_probs = F.log_softmax(x, dim=1)
        nll_loss = self.nll_loss_2d(log_probs, torch.zeros(x.size(0), dtype=torch.long, device=x.device))
        
        # Return the final output (for demonstration purposes, we return the pooled output)
        return x, cosine_loss, gaussian_loss, nll_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 3, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

