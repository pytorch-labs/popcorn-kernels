
# This is a random torch model generated by the following modules: ['SmoothL1Loss', 'CrossMapLRN2d', 'RReLU', 'BatchNorm3d', 'ConvTranspose1d', 'SoftMarginLoss', 'ModuleDict', 'InstanceNorm3d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv_transpose1d = nn.ConvTranspose1d(1, 10, kernel_size=5)
        self.batch_norm3d = nn.BatchNorm3d(10)
        self.instance_norm3d = nn.InstanceNorm3d(10)
        self.cross_map_lrn2d = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)
        self.rrelu = nn.RReLU()
        self.module_dict = nn.ModuleDict({
            'conv_transpose1d_2': nn.ConvTranspose1d(10, 20, kernel_size=5),
            'batch_norm3d_2': nn.BatchNorm3d(20),
            'instance_norm3d_2': nn.InstanceNorm3d(20)
        })
        self.smooth_l1_loss = nn.SmoothL1Loss()
        self.soft_margin_loss = nn.SoftMarginLoss()

    def forward(self, x):
        # Assuming input is 3D, reshape to 1D for ConvTranspose1d
        x = x.view(x.size(0), 1, -1)
        x = self.conv_transpose1d(x)
        
        # Reshape to 5D for BatchNorm3d and InstanceNorm3d
        x = x.view(x.size(0), 10, 1, 1, -1)
        x = self.batch_norm3d(x)
        x = self.instance_norm3d(x)
        
        # Reshape to 4D for CrossMapLRN2d
        x = x.view(x.size(0), 10, 1, -1)
        x = self.cross_map_lrn2d(x)
        
        # Apply RReLU
        x = self.rrelu(x)
        
        # Use ModuleDict for additional layers
        x = self.module_dict['conv_transpose1d_2'](x.view(x.size(0), 10, -1))
        x = x.view(x.size(0), 20, 1, 1, -1)
        x = self.module_dict['batch_norm3d_2'](x)
        x = self.module_dict['instance_norm3d_2'](x)
        
        # Reshape to 2D for loss computation
        x = x.view(x.size(0), -1)
        
        # Compute losses (dummy targets for illustration)
        target_smooth_l1 = torch.randn_like(x)
        target_soft_margin = torch.randn_like(x).sign()
        
        smooth_l1_loss = self.smooth_l1_loss(x, target_smooth_l1)
        soft_margin_loss = self.soft_margin_loss(x, target_soft_margin)
        
        # Return the sum of losses (for illustration purposes)
        return smooth_l1_loss + soft_margin_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 64, 64).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
