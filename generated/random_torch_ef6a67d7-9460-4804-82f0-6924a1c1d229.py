
# This is a random torch model generated by the following modules: ['RNNCell', 'Softmax', 'ModuleDict', 'Tanh', 'MultiLabelSoftMarginLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self, input_size: int = 128, hidden_size: int = 64, num_classes: int = 10) -> None:
        super().__init__()
        self.hidden_size = hidden_size
        
        # Define the RNNCell
        self.rnn_cell = nn.RNNCell(input_size, hidden_size)
        
        # Define a ModuleDict to hold multiple layers
        self.module_dict = nn.ModuleDict({
            'tanh1': nn.Tanh(),
            'tanh2': nn.Tanh(),
            'tanh3': nn.Tanh(),
        })
        
        # Define a linear layer for classification
        self.fc = nn.Linear(hidden_size, num_classes)
        
        # Define the loss function
        self.loss_fn = nn.MultiLabelSoftMarginLoss()

    def forward(self, x):
        # Assuming x is of shape (batch_size, sequence_length, input_size)
        batch_size, sequence_length, input_size = x.size()
        
        # Initialize hidden state
        hx = torch.zeros(batch_size, self.hidden_size).to(x.device)
        
        # Process the sequence through the RNNCell
        for t in range(sequence_length):
            hx = self.rnn_cell(x[:, t, :], hx)
        
        # Apply the Tanh activation from the ModuleDict
        hx = self.module_dict['tanh1'](hx)
        hx = self.module_dict['tanh2'](hx)
        hx = self.module_dict['tanh3'](hx)
        
        # Pass through the final linear layer
        output = self.fc(hx)
        
        # Apply Softmax to the output
        output = F.softmax(output, dim=1)
        
        return output

    def compute_loss(self, output, target):
        # Compute the loss using MultiLabelSoftMarginLoss
        return self.loss_fn(output, target)


def get_inputs():
    # Randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 128).cuda()  # (batch_size, sequence_length, input_size)
    target = torch.randint(0, 2, (1, 10)).cuda()  # (batch_size, num_classes)
    return [x, target]

def get_init_inputs():
    # Randomly generate tensors required for initialization based on the model architecture
    return []
