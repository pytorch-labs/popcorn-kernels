
# This is a random torch model generated by the following modules: ['BCEWithLogitsLoss', 'RNNCell', 'Dropout', 'Tanh', 'KLDivLoss', 'MaxUnpool1d', 'NLLLoss', 'Softplus', 'FractionalMaxPool2d', 'RMSNorm']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.rnn_cell1 = nn.RNNCell(input_size=128, hidden_size=256)
        self.rnn_cell2 = nn.RNNCell(input_size=256, hidden_size=128)
        self.dropout = nn.Dropout(p=0.5)
        self.tanh = nn.Tanh()
        self.max_unpool1d = nn.MaxUnpool1d(kernel_size=2, stride=2)
        self.softplus = nn.Softplus()
        self.fractional_max_pool2d = nn.FractionalMaxPool2d(kernel_size=2, output_size=(14, 14))
        self.rms_norm = RMSNorm(128)
        self.bce_loss = nn.BCEWithLogitsLoss()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
        self.nll_loss = nn.NLLLoss()

    def forward(self, x):
        # Assume input x is of shape (batch_size, sequence_length, input_size)
        batch_size, seq_len, input_size = x.shape
        hx1 = torch.zeros(batch_size, 256).to(x.device)
        hx2 = torch.zeros(batch_size, 128).to(x.device)
        
        # Process through RNN cells
        for t in range(seq_len):
            hx1 = self.rnn_cell1(x[:, t, :], hx1)
            hx1 = self.dropout(hx1)
            hx1 = self.tanh(hx1)
            hx2 = self.rnn_cell2(hx1, hx2)
            hx2 = self.softplus(hx2)
        
        # Reshape for MaxUnpool1d
        hx2 = hx2.unsqueeze(1)  # Add channel dimension
        hx2 = self.max_unpool1d(hx2, indices=torch.arange(hx2.size(2)).unsqueeze(0).repeat(hx2.size(0), 1).to(x.device))
        
        # Reshape for FractionalMaxPool2d
        hx2 = hx2.view(batch_size, 1, 16, 16)  # Reshape to 2D
        hx2 = self.fractional_max_pool2d(hx2)
        
        # Apply RMSNorm
        hx2 = hx2.view(batch_size, -1)  # Flatten for RMSNorm
        hx2 = self.rms_norm(hx2)
        
        # Dummy loss calculations (assuming some target tensors)
        target_bce = torch.randint(0, 2, (batch_size, 128)).float().to(x.device)
        target_kl = torch.randn(batch_size, 128).softmax(dim=1).to(x.device)
        target_nll = torch.randint(0, 10, (batch_size,)).to(x.device)
        
        bce_loss = self.bce_loss(hx2, target_bce)
        kl_loss = self.kl_loss(F.log_softmax(hx2, dim=1), target_kl)
        nll_loss = self.nll_loss(F.log_softmax(hx2, dim=1), target_nll)
        
        return bce_loss, kl_loss, nll_loss


class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.scale = dim ** -0.5
        self.eps = eps
        self.g = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        norm = torch.norm(x, dim=1, keepdim=True) * self.scale
        return x / norm.clamp(min=self.eps) * self.g


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 128).cuda()  # (batch_size, sequence_length, input_size)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

