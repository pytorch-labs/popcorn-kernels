
# This is a random torch model generated by the following modules: ['CrossMapLRN2d', 'RMSNorm', 'BatchNorm1d', 'TransformerEncoderLayer', 'AdaptiveMaxPool3d', 'Embedding', 'FeatureAlphaDropout', 'Softsign', 'ChannelShuffle']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.embedding = nn.Embedding(1000, 128)  # Assuming vocab size of 1000
        self.channel_shuffle = nn.ChannelShuffle(4)
        self.cross_map_lrn = nn.CrossMapLRN2d(size=5, alpha=1e-4, beta=0.75, k=1.0)
        self.rms_norm = nn.RMSNorm(128)
        self.batch_norm1d = nn.BatchNorm1d(128)
        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)
        self.adaptive_max_pool3d = nn.AdaptiveMaxPool3d((1, 1, 1))
        self.feature_alpha_dropout = nn.FeatureAlphaDropout(p=0.5)
        self.softsign = nn.Softsign()

    def forward(self, x):
        # Assume input is a 1D tensor of indices for embedding
        x = self.embedding(x)
        
        # Reshape for 2D operations
        x = x.unsqueeze(1)  # Add a channel dimension
        x = self.channel_shuffle(x)
        x = self.cross_map_lrn(x)
        
        # Reshape for 1D operations
        x = x.squeeze(1)
        x = self.rms_norm(x)
        x = self.batch_norm1d(x)
        
        # Reshape for Transformer
        x = x.unsqueeze(0)  # Add a sequence dimension
        x = self.transformer_encoder_layer(x)
        
        # Reshape for 3D operations
        x = x.unsqueeze(1).unsqueeze(1)  # Add height and width dimensions
        x = self.adaptive_max_pool3d(x)
        
        # Reshape for dropout
        x = x.squeeze(1).squeeze(1).squeeze(1)
        x = self.feature_alpha_dropout(x)
        
        # Final activation
        x = self.softsign(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randint(0, 1000, (32,)).cuda()  # Batch size of 32, sequence length of 10
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

