
# This is a random torch model generated by the following modules: ['ReLU', 'InstanceNorm2d', 'PReLU', 'Dropout2d', 'LazyConv3d', 'GRUCell', 'RNNBase', 'TransformerDecoderLayer', 'ReplicationPad1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv3d = nn.LazyConv3d(out_channels=16, kernel_size=3)
        self.instance_norm = nn.InstanceNorm2d(num_features=16)
        self.prelu = nn.PReLU()
        self.dropout2d = nn.Dropout2d(p=0.5)
        self.gru_cell = nn.GRUCell(input_size=16, hidden_size=32)
        self.rnn = nn.RNNBase(mode='LSTM', input_size=32, hidden_size=64, num_layers=2)
        self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=64, nhead=8)
        self.replication_pad1d = nn.ReplicationPad1d(padding=2)
        self.relu = nn.ReLU()

    def forward(self, x):
        # Apply LazyConv3d
        x = self.conv3d(x)
        
        # Reshape for InstanceNorm2d
        x = x.view(x.size(0), x.size(1), x.size(2), -1)
        x = self.instance_norm(x)
        
        # Reshape back to 3D
        x = x.view(x.size(0), x.size(1), x.size(2), x.size(3), -1)
        
        # Apply PReLU
        x = self.prelu(x)
        
        # Apply Dropout2d
        x = x.view(x.size(0), x.size(1), x.size(2), -1)
        x = self.dropout2d(x)
        
        # Reshape back to 3D
        x = x.view(x.size(0), x.size(1), x.size(2), x.size(3), -1)
        
        # Flatten for GRUCell
        x = x.view(x.size(0), -1, x.size(-1))
        x = self.gru_cell(x[:, 0, :], torch.zeros(x.size(0), 32, device=x.device))
        
        # Reshape for RNNBase
        x = x.unsqueeze(0)
        x, _ = self.rnn(x)
        
        # Reshape for TransformerDecoderLayer
        x = x.permute(1, 0, 2)
        x = self.transformer_decoder_layer(x, x)
        
        # Reshape for ReplicationPad1d
        x = x.permute(0, 2, 1)
        x = self.replication_pad1d(x)
        
        # Apply ReLU
        x = self.relu(x)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 32, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
