
# This is a random torch model generated by the following modules: ['ReplicationPad3d', 'BCEWithLogitsLoss', 'RNNCellBase', 'AdaptiveLogSoftmaxWithLoss']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.pad = nn.ReplicationPad3d(1)
        self.rnn_cell1 = nn.RNNCellBase(input_size=10, hidden_size=20)
        self.rnn_cell2 = nn.RNNCellBase(input_size=20, hidden_size=30)
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(in_features=30, n_classes=10, cutoffs=[2, 4])
        self.bce_loss = nn.BCEWithLogitsLoss()

    def forward(self, x):
        # Apply padding
        x = self.pad(x)
        
        # Reshape for RNNCellBase
        batch_size = x.size(0)
        x = x.view(batch_size, -1, 10)  # Reshape to (batch_size, seq_len, input_size)
        
        # Initialize hidden states
        h1 = torch.zeros(batch_size, 20).to(x.device)
        h2 = torch.zeros(batch_size, 30).to(x.device)
        
        # Process through RNN cells
        for t in range(x.size(1)):
            h1 = self.rnn_cell1(x[:, t, :], h1)
            h2 = self.rnn_cell2(h1, h2)
        
        # Apply adaptive log softmax
        output = self.adaptive_log_softmax(h2, torch.zeros(batch_size, dtype=torch.long).to(x.device))
        
        # Apply BCEWithLogitsLoss (for demonstration, we use a dummy target)
        dummy_target = torch.zeros(batch_size, 10).to(x.device)
        loss = self.bce_loss(output.output, dummy_target)
        
        return loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 1, 10, 10, 10).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

