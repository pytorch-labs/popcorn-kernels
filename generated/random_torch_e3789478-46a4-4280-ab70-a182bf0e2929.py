
# This is a random torch model generated by the following modules: ['LSTM', 'ReflectionPad3d', 'RReLU', 'ConvTranspose2d', 'Dropout1d', 'ReLU6', 'LSTMCell', 'LazyLinear', 'ReflectionPad1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.lstm1 = nn.LSTM(input_size=128, hidden_size=64, num_layers=2, batch_first=True)
        self.reflection_pad3d = nn.ReflectionPad3d(1)
        self.rrelu = nn.RReLU()
        self.conv_transpose2d = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2)
        self.dropout1d = nn.Dropout1d(p=0.5)
        self.relu6 = nn.ReLU6()
        self.lstm_cell = nn.LSTMCell(input_size=32, hidden_size=16)
        self.lazy_linear = nn.LazyLinear(out_features=10)
        self.reflection_pad1d = nn.ReflectionPad1d(2)

    def forward(self, x):
        # Assuming input x is of shape (batch_size, sequence_length, input_size)
        batch_size, sequence_length, input_size = x.shape
        
        # LSTM
        x, _ = self.lstm1(x)  # Output shape: (batch_size, sequence_length, hidden_size=64)
        
        # Reshape for ReflectionPad3d
        x = x.view(batch_size, sequence_length, 8, 8, -1)  # Reshape to (batch_size, sequence_length, 8, 8, 8)
        x = self.reflection_pad3d(x)  # Output shape: (batch_size, sequence_length, 10, 10, 10)
        
        # RReLU
        x = self.rrelu(x)
        
        # Reshape for ConvTranspose2d
        x = x.view(batch_size * sequence_length, 64, 10, 10)  # Reshape to (batch_size * sequence_length, 64, 10, 10)
        x = self.conv_transpose2d(x)  # Output shape: (batch_size * sequence_length, 32, 21, 21)
        
        # Dropout1d
        x = x.view(batch_size * sequence_length, 32, -1)  # Reshape to (batch_size * sequence_length, 32, 441)
        x = self.dropout1d(x)  # Output shape: (batch_size * sequence_length, 32, 441)
        
        # ReLU6
        x = self.relu6(x)
        
        # Reshape for LSTMCell
        x = x.view(batch_size * sequence_length, 32 * 441)  # Reshape to (batch_size * sequence_length, 32 * 441)
        hx = torch.zeros(batch_size * sequence_length, 16).to(x.device)
        cx = torch.zeros(batch_size * sequence_length, 16).to(x.device)
        x = self.lstm_cell(x, (hx, cx))[0]  # Output shape: (batch_size * sequence_length, 16)
        
        # LazyLinear
        x = self.lazy_linear(x)  # Output shape: (batch_size * sequence_length, 10)
        
        # Reshape for ReflectionPad1d
        x = x.view(batch_size, sequence_length, -1)  # Reshape to (batch_size, sequence_length, 10)
        x = x.permute(0, 2, 1)  # Reshape to (batch_size, 10, sequence_length)
        x = self.reflection_pad1d(x)  # Output shape: (batch_size, 10, sequence_length + 4)
        
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 128).cuda()  # Example input shape: (batch_size=1, sequence_length=10, input_size=128)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
