
# This is a random torch model generated by the following modules: ['ModuleDict', 'BatchNorm1d', 'LazyConv1d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.module_dict = nn.ModuleDict({
            'conv1': nn.LazyConv1d(out_channels=32, kernel_size=3),
            'conv2': nn.LazyConv1d(out_channels=64, kernel_size=3),
            'bn1': nn.BatchNorm1d(32),
            'bn2': nn.BatchNorm1d(64),
        })
        self.fc = nn.LazyLinear(10)

    def forward(self, x):
        # Ensure the input is 3D (batch_size, channels, sequence_length)
        if x.dim() == 2:
            x = x.unsqueeze(1)  # Add a channel dimension if missing
        elif x.dim() > 3:
            x = x.view(x.size(0), -1)  # Flatten extra dimensions
            x = x.unsqueeze(1)

        x = self.module_dict['conv1'](x)
        x = self.module_dict['bn1'](x)
        x = F.relu(x)

        x = self.module_dict['conv2'](x)
        x = self.module_dict['bn2'](x)
        x = F.relu(x)

        # Global average pooling
        x = x.mean(dim=2)

        x = self.fc(x)
        return F.log_softmax(x, dim=1)


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 128).cuda()  # Arbitrary input shape (batch_size, sequence_length)
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
