
# This is a random torch model generated by the following modules: ['Upsample', 'BCELoss', 'CosineEmbeddingLoss', 'LazyConvTranspose3d', 'Linear', 'ConvTranspose2d', 'LPPool2d', 'AdaptiveLogSoftmaxWithLoss', 'TripletMarginWithDistanceLoss', 'Module']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.lazy_conv_transpose3d = nn.LazyConvTranspose3d(out_channels=16, kernel_size=3)
        self.conv_transpose2d = nn.ConvTranspose2d(in_channels=16, out_channels=32, kernel_size=3)
        self.upsample = nn.Upsample(scale_factor=2)
        self.lp_pool2d = nn.LPPool2d(norm_type=2, kernel_size=2, stride=2)
        self.linear1 = nn.Linear(32 * 16 * 16, 128)
        self.linear2 = nn.Linear(128, 64)
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(in_features=64, n_classes=10, cutoffs=[4, 8])
        self.bce_loss = nn.BCELoss()
        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()
        self.triplet_margin_loss = nn.TripletMarginWithDistanceLoss()

    def forward(self, x):
        # Assuming input is 3D, reshape to 5D for LazyConvTranspose3d
        x = x.unsqueeze(1).unsqueeze(1)  # Add two dimensions to make it 5D
        x = self.lazy_conv_transpose3d(x)
        x = x.squeeze(1).squeeze(1)  # Remove the extra dimensions to make it 3D again
        
        # Reshape to 4D for ConvTranspose2d
        x = x.view(-1, 16, 8, 8)
        x = self.conv_transpose2d(x)
        
        # Upsample
        x = self.upsample(x)
        
        # LPPool2d
        x = self.lp_pool2d(x)
        
        # Flatten for Linear layers
        x = x.view(x.size(0), -1)
        x = F.relu(self.linear1(x))
        x = self.linear2(x)
        
        # AdaptiveLogSoftmaxWithLoss
        x = self.adaptive_log_softmax.log_prob(x)
        
        # Compute losses (dummy targets for demonstration)
        target = torch.randint(0, 10, (x.size(0),)).to(x.device)
        bce_loss = self.bce_loss(torch.sigmoid(x), torch.rand_like(x))
        cosine_loss = self.cosine_embedding_loss(x, torch.rand_like(x), torch.ones(x.size(0)).to(x.device))
        triplet_loss = self.triplet_margin_loss(x, torch.rand_like(x), torch.rand_like(x))
        
        return x, bce_loss, cosine_loss, triplet_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 32, 32).cuda()
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
