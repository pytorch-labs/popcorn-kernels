
# This is a random torch model generated by the following modules: ['AdaptiveLogSoftmaxWithLoss', 'ModuleDict', 'ConstantPad1d', 'Bilinear']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.pad = nn.ConstantPad1d(2, 3.0)
        self.bilinear1 = nn.Bilinear(10, 10, 20)
        self.bilinear2 = nn.Bilinear(20, 20, 30)
        self.module_dict = nn.ModuleDict({
            'linear1': nn.Linear(30, 40),
            'linear2': nn.Linear(40, 50)
        })
        self.adaptive_log_softmax = nn.AdaptiveLogSoftmaxWithLoss(50, 10, [20, 30, 40])

    def forward(self, x):
        # Assuming input x is of shape (batch_size, sequence_length, feature_dim)
        x = self.pad(x)  # Apply ConstantPad1d
        x = x.view(-1, 10, 10)  # Reshape for Bilinear layer
        x = self.bilinear1(x[:, 0], x[:, 1])  # Apply first Bilinear layer
        x = self.bilinear2(x, x)  # Apply second Bilinear layer
        x = self.module_dict['linear1'](x)  # Apply first Linear layer from ModuleDict
        x = self.module_dict['linear2'](x)  # Apply second Linear layer from ModuleDict
        x = self.adaptive_log_softmax.log_prob(x)  # Apply AdaptiveLogSoftmaxWithLoss
        return x


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 10, 10).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []

