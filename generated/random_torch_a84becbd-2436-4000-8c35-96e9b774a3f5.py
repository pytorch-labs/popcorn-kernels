
# This is a random torch model generated by the following modules: ['GLU', 'Linear', 'MarginRankingLoss', 'ZeroPad2d', 'CrossEntropyLoss', 'MSELoss', 'ReplicationPad3d', 'LazyConv1d', 'LazyInstanceNorm2d']
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.lazy_conv1d = nn.LazyConv1d(out_channels=32, kernel_size=3)
        self.glu = nn.GLU(dim=1)
        self.zero_pad2d = nn.ZeroPad2d(2)
        self.replication_pad3d = nn.ReplicationPad3d(1)
        self.lazy_instance_norm2d = nn.LazyInstanceNorm2d()
        self.linear1 = nn.Linear(32, 64)
        self.linear2 = nn.Linear(64, 10)
        self.margin_ranking_loss = nn.MarginRankingLoss()
        self.cross_entropy_loss = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()

    def forward(self, x):
        # Assuming input x is of shape (batch_size, channels, length)
        x = self.lazy_conv1d(x)  # Shape: (batch_size, 32, length-2)
        x = self.glu(x)  # Shape: (batch_size, 16, length-2)
        
        # Reshape for ZeroPad2d
        x = x.unsqueeze(2)  # Shape: (batch_size, 16, 1, length-2)
        x = self.zero_pad2d(x)  # Shape: (batch_size, 16, 5, length+2)
        
        # Reshape for ReplicationPad3d
        x = x.unsqueeze(2)  # Shape: (batch_size, 16, 1, 5, length+2)
        x = self.replication_pad3d(x)  # Shape: (batch_size, 16, 3, 7, length+4)
        
        # Reshape for LazyInstanceNorm2d
        x = x.squeeze(2)  # Shape: (batch_size, 16, 7, length+4)
        x = self.lazy_instance_norm2d(x)  # Shape: (batch_size, 16, 7, length+4)
        
        # Global average pooling
        x = x.mean(dim=[2, 3])  # Shape: (batch_size, 16)
        
        # Fully connected layers
        x = F.relu(self.linear1(x))  # Shape: (batch_size, 64)
        x = self.linear2(x)  # Shape: (batch_size, 10)
        
        # Loss functions (assuming we have target tensors for demonstration)
        target_class = torch.randint(0, 10, (x.size(0),)).to(x.device)
        target_margin = torch.randn(x.size(0)).to(x.device)
        target_mse = torch.randn_like(x)
        
        # Compute losses (not used in forward pass, just for demonstration)
        margin_loss = self.margin_ranking_loss(x[:, 0], x[:, 1], target_margin)
        ce_loss = self.cross_entropy_loss(x, target_class)
        mse_loss = self.mse_loss(x, target_mse)
        
        return x, margin_loss, ce_loss, mse_loss


def get_inputs():
    # randomly generate input tensors based on the model architecture
    x = torch.randn(1, 16, 64).cuda()  # Example input shape
    return [x]

def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
